
import json
import math
import torch
import igraph as ig
import leidenalg
import numpy as np
from pyvis.network import Network
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from cluster.gpt import *

# =========================
# é…ç½®å‚æ•°ï¼ˆä½ ä¸»è¦è°ƒè¿™é‡Œï¼‰
# =========================

import os

# =========================
# æ‰¹é‡å¤„ç†è·¯å¾„é…ç½®
# =========================


OUTPUT_SUFFIX = "_leiden_mix"


# â€”â€” è¯­ä¹‰è¾¹å‚æ•° â€”â€”
KNN_K = 15                  # æ¯ä¸ªå…³é”®è¯è¿å‡ ä¸ªè¯­ä¹‰é‚»å±…
SEM_WEIGHT = 0.8            # è¯­ä¹‰è¾¹æƒé‡ç³»æ•°

# â€”â€” å…±ç°è¾¹å‚æ•° â€”â€”
SIM_THRESHOLD = 0.15
WEIGHT_POWER = 2.0
CO_WEIGHT = 0.6            # å…±ç°è¾¹æƒé‡ç³»æ•°

# â€”â€” Leiden å‚æ•° â€”â€”
RESOLUTION = 0.6
# =========================
# 1. æ„å›¾ï¼šembedding + å…±ç°
# =========================

def build_graph_with_semantic_edges(json_path, ckpt_path):
    # ---------- è¯»å– JSON ----------
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # ---------- è¯»å– embedding ----------
    ckpt = torch.load(ckpt_path, map_location="cpu")
    embeddings = ckpt["embeddings"].cpu().numpy()   # [N, D]

    # ğŸ”‘ è¯­ä¹‰èšç±»å¿…åš
    embeddings = normalize(embeddings, norm="l2")

    # ---------- å»ºç«‹èŠ‚ç‚¹ ----------
    g = ig.Graph()
    id_map = {}

    for idx, node in enumerate(data["nodes"]):
        id_map[node["id"]] = idx
        g.add_vertex(
            name=str(node["id"]),
            label=node.get("label", "")
        )

    edge_dict = {}  # (i, j) -> weight

    # =========================
    # 1ï¸âƒ£ å…±ç°è¾¹ï¼ˆJSON åŸæœ‰ï¼‰
    # =========================
    for link in data["links"]:
        sim = link.get("value", 0.0)
        cnt = link.get("count", 1)

        if sim < SIM_THRESHOLD:
            continue

        i = id_map[link["source"]]
        j = id_map[link["target"]]

        w = (sim ** WEIGHT_POWER) * (1 + math.log1p(cnt))
        edge_dict[(i, j)] = edge_dict.get((i, j), 0.0) + CO_WEIGHT * w

    # =========================
    # 2ï¸âƒ£ è¯­ä¹‰ KNN è¾¹ï¼ˆembeddingï¼‰
    # =========================
    knn = NearestNeighbors(
        n_neighbors=KNN_K + 1,
        metric="cosine"
    ).fit(embeddings)

    distances, indices = knn.kneighbors(embeddings)

    for i in range(len(embeddings)):
        for j_idx, dist in zip(indices[i][1:], distances[i][1:]):
            j = j_idx
            sim = 1 - dist   # cosine similarity

            w = SEM_WEIGHT * sim
            key = (min(i, j), max(i, j))
            edge_dict[key] = edge_dict.get(key, 0.0) + w

    # =========================
    # 3ï¸âƒ£ å†™å…¥ igraph
    # =========================
    edges = []
    weights = []

    for (i, j), w in edge_dict.items():
        edges.append((i, j))
        weights.append(w)

    g.add_edges(edges)
    g.es["weight"] = weights

    return g, id_map,edge_dict


# =========================
# 2. Leiden èšç±»
# =========================

def leiden_clustering(g, resolution=1.0):
    partition = leidenalg.find_partition(
        g,
        leidenalg.RBConfigurationVertexPartition,
        weights="weight",
        resolution_parameter=resolution
    )
    return partition.membership


# =========================
# 3. èšç±»ç»“æœå†™å› JSON
# =========================
import json
from collections import defaultdict, Counter
def write_clusters_to_json(input_path, output_path, id_map, membership, label_len,top_k=10):
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # group -> Counter(label)
    cluster_keyword_counter = defaultdict(Counter)

    # =======================
    # 1ï¸âƒ£ å†™å› group + ç»Ÿè®¡å…³é”®è¯
    # =======================
    for node in data["nodes"]:
        node_id = node["id"]
        group = int(membership[id_map[node_id]])
        node["group"] = group

        label = node.get("label")
        if label:
            weight = node.get("size", 1)  # å…œåº•ï¼Œé˜²æ­¢æ²¡æœ‰ size
            cluster_keyword_counter[group][label] += weight

    # =======================
    # 2ï¸âƒ£ ç»„ç»‡ cluster_statsï¼ˆå…¨é‡ç»Ÿè®¡ï¼‰
    # =======================
    cluster_stats = {}
    for group, counter in cluster_keyword_counter.items():
        cluster_stats[str(group)] = {
            "total": sum(counter.values()),
            "keywords": dict(counter)
        }

    # =======================
    # 3ï¸âƒ£ ç»„ç»‡ Top-K listï¼ˆä½ æ–°è¦çš„ï¼‰åˆ©ç”¨å¤§æ¨¡å‹ç»™å‡ºå…³é”®è¯
    # =======================
    cluster_top_keywords = {}
    for group, counter in cluster_keyword_counter.items():
        topk_list = [kw for kw, _ in counter.most_common(min(len(counter),top_k))]
        cluster_top_keywords[str(group)] = topk_list
    print(cluster_top_keywords)
    lables=extract_cluster_kw(str(cluster_top_keywords),label_len)
   
    # =======================
    # 4ï¸âƒ£ å†™å› JSONï¼ˆä¸¤ä»¶äº‹éƒ½ä¿ç•™ï¼‰
    # =======================
    data["cluster_stats"] = cluster_stats
    data["cluster_labels"]=lables

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# =========================
# 4. å¯è§†åŒ–
# =========================

def main_college(name):
    JSON_DIR = fr"E:\å¤§äº”ä¸Šå­¦æœŸé€‰ä¿®è¯¾\keshi\final_work\frontend\src\json\{name}"
    EMB_DIR  = fr"E:\å¤§äº”ä¸Šå­¦æœŸé€‰ä¿®è¯¾\keshi\final_work\backend\index\{name}"

    print(f"ğŸ”¹ æ‰¹é‡å¤„ç† {name} JSON + Embedding")

    json_files = [
        f for f in os.listdir(JSON_DIR)
        if f.endswith(".json")
    ]

    print(f"ğŸ“ å‘ç° {len(json_files)} ä¸ª JSON æ–‡ä»¶")

    for json_file in json_files:
        json_path = os.path.join(JSON_DIR, json_file)

        # ---- æ¨æ–­å¹´ä»½æˆ–åç¼€ ----
        base_name = os.path.splitext(json_file)[0]
        # ä¾‹ï¼šé‡‘èå­¦é™¢_2020

        ckpt_name = f"embeddings_{base_name}.ckpt"
        ckpt_path = os.path.join(EMB_DIR, ckpt_name)

        if not os.path.exists(ckpt_path):
            print(f"âš ï¸ ç¼ºå°‘ embeddingï¼š{ckpt_name}ï¼Œè·³è¿‡")
            continue
        
        OUTPUT_SUBDIR = "leiden_output"
        output_dir = os.path.join(JSON_DIR, OUTPUT_SUBDIR)
        os.makedirs(output_dir, exist_ok=True)

        output_json = os.path.join(
            output_dir,
            f"{base_name}{OUTPUT_SUFFIX}.json"
        )

        print(f"\nâ–¶ å¤„ç†ï¼š{json_file}")
        print(f"   â†³ Embedding: {ckpt_name}")

        # =========================
        # 1. æ„å›¾
        # =========================
        g, id_map, edge_dict = build_graph_with_semantic_edges(
            json_path,
            ckpt_path
        )

        print(f"   èŠ‚ç‚¹æ•°: {g.vcount()}, è¾¹æ•°: {g.ecount()}")

        # =========================
        # 2. Leiden
        # =========================
        membership = leiden_clustering(g, RESOLUTION)
        n_cluster = len(set(membership))
        print(f"   ç¤¾åŒºæ•°: {n_cluster}")

        # =========================
        # 3. å†™å› JSON
        # =========================
        write_clusters_to_json(
            json_path,
            output_json,
            id_map,
            membership,
            label_len=n_cluster
        )

        print(f"   âœ… è¾“å‡º: {output_json}")

    print("\nğŸ‰ å…¨éƒ¨æ–‡ä»¶å¤„ç†å®Œæˆ")


def visualize_graph_with_clusters(json_path, edge_dict, id_map, output_html):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    net = Network(
        height="900px",
        width="100%",
        bgcolor="#ffffff",
        font_color="black"
    )

    # ====== 1ï¸âƒ£ ç‰©ç†å¼•æ“ï¼ˆå†³å®šâ€œæ”¾åˆ°ä¸€ç‰‡å»â€ï¼‰======
    net.force_atlas_2based(
        gravity=-50,
        central_gravity=0.01,
        spring_length=120,
        spring_strength=0.08,
        damping=0.4,
        overlap=0
    )

    # ====== 2ï¸âƒ£ èŠ‚ç‚¹ï¼ˆé¢œè‰² = ç°‡ï¼‰======
    for node in data["nodes"]:
        net.add_node(
            node["id"],
            label=node["label"],
            group=node["group"],   # ğŸ”‘ åŒç°‡åŒè‰²
            size=14 + node.get("size", 1),
            title=f"Cluster {node['group']}"
        )

    # ====== 3ï¸âƒ£ è¾¹ï¼ˆå…±ç° + è¯­ä¹‰ï¼‰======
    for (i, j), w in edge_dict.items():
        source = int(list(id_map.keys())[list(id_map.values()).index(i)])
        target = int(list(id_map.keys())[list(id_map.values()).index(j)])

        net.add_edge(
            source,
            target,
            value=w,
            color="rgba(180,180,180,0.4)"
        )

    net.write_html(output_html)

# =========================
# 5. ä¸»æµç¨‹
# =========================

def main():
    print("ğŸ”¹ æ„å»ºèåˆå›¾ï¼ˆè¯­ä¹‰ + å…±ç°ï¼‰...")
    g, id_map, edge_dict = build_graph_with_semantic_edges(INPUT_JSON, CKPT_PATH)

    print(f"ğŸ”¹ èŠ‚ç‚¹æ•°: {g.vcount()}, è¾¹æ•°: {g.ecount()}")

    print("ğŸ”¹ æ‰§è¡Œ Leiden èšç±»...")
    membership = leiden_clustering(g, RESOLUTION)

    print(f"âœ… ç¤¾åŒºæ•°: {len(set(membership))}")

    print("ğŸ”¹ å†™å› JSON...")
    write_clusters_to_json(INPUT_JSON, OUTPUT_JSON, id_map, membership,len(set(membership)))

    print("ğŸ”¹ ç”Ÿæˆå¯è§†åŒ–...")
    #visualize_graph_with_clusters(OUTPUT_JSON, edge_dict, id_map, OUTPUT_HTML)

    print("ğŸ‰ å®Œæˆ")
    #print(f"ğŸ‘‰ {OUTPUT_HTML}")


if __name__ == "__main__":
    main_college("åŒ–å­¦å­¦é™¢")
