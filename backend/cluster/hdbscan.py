def process_cluster_hdbscan(
    min_cluster_size=15,
    min_samples=3,
    use_umap=True
):
    """
    UMAP -> HDBSCANï¼ˆç”¨äºèšç±»ï¼‰
    """
    import torch
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import normalize
    from sklearn.decomposition import PCA

    try:
        import umap
    except ImportError:
        raise ImportError("è¯·å…ˆå®‰è£… umap-learn")

    try:
        import hdbscan
    except ImportError:
        raise ImportError("è¯·å…ˆå®‰è£… hdbscan")

    # =====================
    # 1. è¯»å– embedding
    # =====================
    ckpt_path = "embeddings_æ·±åœ³é‡‘èå·¥ç¨‹å­¦é™¢.ckpt"
    ckpt = torch.load(ckpt_path, map_location="cpu")

    embeddings = ckpt["embeddings"].cpu().numpy()  # [N, D]
    keywords = ckpt.get("keywords", None)

    # ğŸ”‘ è¯­ä¹‰èšç±»å¿…åšï¼šL2 normalize
    embeddings = normalize(embeddings, norm="l2")

    # =====================
    # 2. UMAPï¼ˆç”¨äºèšç±»ï¼‰
    # =====================
    reducer = umap.UMAP(
        n_neighbors=15,      # å°ä¸€ç‚¹ï¼Œæ›´å®¹æ˜“å½¢æˆç°‡
        min_dist=0.0,        # æ‹‰ç´§ç°‡
        n_components=5,      # ç»™ HDBSCAN ç”¨çš„ä½ç»´ç©ºé—´
        metric="cosine",
        random_state=42
    )
    X_umap = reducer.fit_transform(embeddings)

    # =====================
    # 3. HDBSCANï¼ˆåœ¨ UMAP ç©ºé—´ï¼‰
    # =====================
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric="euclidean",
        cluster_selection_method="leaf"
    )

    labels = clusterer.fit_predict(X_umap)

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = np.sum(labels == -1)

    print(f"å‘ç°ç°‡æ•°: {n_clusters}")
    print(f"å™ªå£°ç‚¹æ•°: {n_noise}")

    # =====================
    # 4. å†é™åˆ° 2D ä»…ç”¨äºå¯è§†åŒ–
    # =====================
    X_vis = PCA(n_components=2).fit_transform(X_umap)

    # =====================
    # 5. å¯è§†åŒ–
    # =====================
    plt.figure(figsize=(8, 8))
    unique_labels = set(labels)

    for label in unique_labels:
        idx = labels == label
        if label == -1:
            plt.scatter(
                X_vis[idx, 0],
                X_vis[idx, 1],
                c="lightgray",
                s=15,
                label="Noise"
            )
        else:
            plt.scatter(
                X_vis[idx, 0],
                X_vis[idx, 1],
                s=30,
                label=f"Cluster {label}"
            )

    plt.title("UMAP â†’ HDBSCAN (clustering)")
    plt.axis("off")
    plt.legend(markerscale=1.1)
    plt.show()

    return labels

  

def write_cluster_to_graph(
    graph_path,
    labels,
    output_path=None
):
    """
    å°† HDBSCAN çš„ labels å†™å› graph.json çš„ nodes[].group
    """
    if output_path is None:
        output_path = graph_path

    with open(graph_path, "r", encoding="utf-8") as f:
        graph = json.load(f)

    nodes = graph["nodes"]

    assert len(nodes) == len(labels), \
        f"èŠ‚ç‚¹æ•° {len(nodes)} ä¸ labels æ•° {len(labels)} ä¸ä¸€è‡´"

    for i, node in enumerate(nodes):
        node["group"] = int(labels[i])   # ç›´æ¥è¦†ç›– group

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(graph, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²å†™å› cluster ç»“æœåˆ° {output_path}")

