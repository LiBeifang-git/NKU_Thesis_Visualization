Year,Title,Link,Abstract
2024,Accelerating Flow-Based Sampling for Large-𝑁 Gauge Theories,https://hdl.handle.net/1721.1/153909,"Due to its consistency with numerous experimental observations, the Standard Model of particle physics is widely accepted as the best known formulation of elementary particles and their interactions. However, making experimental predictions using the Standard Model involves mathematical and computational challenges due to its complexity. Quantum chromodynamics (QCD), which can be described as an SU(3) gauge theory due to the 3 quark colors and 8 gluon types, is one sector of the Standard Model for which computing solutions is especially challenging. A natural theoretical generalization of QCD is the class of all SU(𝑁) gauge theories; these theories also provide a method for some QCD computations in the 𝑁 → ∞ limit. To study these theories numerically, approximations are calculated from configuration samples due to the mathematical complexity and lack of analytical solutions.

In this thesis, we explore asymptotically efficient flow-based sampling algorithms for the twisted Eguchi-Kawai (TEK) model, a method for analyzing large-𝑁 QCD numerically. We introduce an original architecture based on SU(2) matrix multiplication that allows for efficient Jacobian computation. In addition, we explore the possibility of transfer learning with respect to the number of colors 𝑁 and demonstrate that a model trained quickly on the SU(𝑁) setting also provides useful information in SU(𝑁′), 𝑁′ > 𝑁 cases."
2024,Relative Robot Localization and Frame Alignment for Multi-Robot Collaboration,https://hdl.handle.net/1721.1/157823,"The growing field of collaborative robotics has the potential to enable and improve the execution of many challenging robot applications. For instance, with teamwork between multiple agents, dynamic object tracking can more completely cover an environment and trajectory planning becomes safer. However, for robots to share the quickly changing spatial information involved in these tasks, robots need to be able to express information originally sensed or planned in their own frame into the frame of neighboring agents. This can be challenging in cases where robots have no global pose information resulting in steady accumulation of error, or drift, in their local pose estimates. To mitigate the effects of drift, neighboring agents must make up-to-date estimates of the alignment between their frames, which can be difficult due to ambiguous alignments and the presence of outlier measurements. To address these issues, the first contribution of this thesis is a method for performing fast incremental frame alignment between pairs of robots, enabling collaborative multiple object tracking (MOT), the task of monitoring the locations of dynamic objects in an environment. To perform frame alignment, robots build up maps of recently seen static objects and use these maps and the detections of tracked dynamic objects to correct for frame drift. Using frame alignment estimates, agents share object detection information and account for additional uncertainty associated with the alignment estimate. The second contribution of this thesis presents a method to perform frame alignment with no initial guess. Many potential frame alignments are computed and we develop a filter that uses temporal consistency to reject outlier alignments and only accept a series of alignments that are consistent over time. We demonstrate in hardware experiments our ability to perform frame alignment in difficult scenarios and improve the quality of collaborative object tracking onboard real robots."
2024,Driving Emerging Technologies From Concept to Reality: A Case Study of Carbon Nanotubes,https://hdl.handle.net/1721.1/153326,"The evolution of electronics has transformed nearly every aspect of society and has been fueled by decades of relentless device scaling. However, electronics is facing a paradigm shift as serious obstacles challenge future progress. Continued scaling is growing increasingly difficult and already yields diminishing energy efficiency benefits. At the same time, other obstacles such as data bandwidth bottlenecks, interconnect density limitations, and reliability are limiting computing performance. Therefore, traditional routes to progress are insufficient, and new approaches must be investigated if we are to continue the technological advancement society has come to expect.

One major thrust toward overcoming these obstacles is the search for alternative, beyond-silicon technologies. Yet despite the promise of these emerging nanotechnologies, their nascency has made their integration into practical and useful electronic systems challenging. In my thesis, I aim to tackle this challenge and present a roadmap for how such new and immature nanotechnologies can be leveraged to not only set the foundation for futuristic next-generation hardware, but also realize practical systems that can have an impact today. As a case study, I use carbon nanotubes (CNTs) to demonstrate a realistic roadmap for commercially realizing these next-generation technologies. First, I show, for the first time, that every type of today’s conventional circuitry (digital, analog, and mixed-signal circuits) can be fabricated with CNT field-effect transistors (CNFETs). This provides a pathway for adopting these futuristic technologies today. Second, to show how CNFETs can play a role in the next generation of computing systems, I leverage the unique low-temperature fabrication of CNFETs alongside emerging memory technologies to achieve the finest 3D integration of emerging technologies to date, which I further use to enable a new circuit design technique. Third, to show how CNFETs can enable futuristic electronic systems that can impact application spaces beyond conventional computing, I leverage VLSI-compatible foundry fabrication of CNFETs to realize BioSensor chips capable of detecting and identifying infectious pathogens in liquid. These experimental demonstrations of CNFETs in today’s (conventional circuitry), tomorrow’s (dense fine-grained 3D systems), and futuristic (healthcare diagnostics) applications explicitly demonstrate a practical roadmap for how emerging nanotechnologies can be developed for near-term adoption while providing longer-term motivation for enabling next-generation electronic systems."
2024,Perturbation-invariant Speech Representation Learning by Online Clustering,https://hdl.handle.net/1721.1/153784,"Despite success across various tasks, self-supervised speech models face significant challenges in enhancing content-related performance with unlabeled data, requiring substantial computational resources. Meanwhile, learning from clustered discrete units has been shown to facilitate accurate phonetic representations. Thus, this thesis investigates speaker and noise-invariant speech representations. First, Speaker-invariant Clustering (Spin) is proposed to extract content representations through online clustering and speaker-invariant cross-view prediction. Second, Robust Spin (R-Spin) is devised to extend Spin to handle more distorted speech signals by leveraging acoustic pieces. Furthermore, this thesis includes a diverse set of evaluation and visualization techniques to quantitatively and qualitatively analyze the perturbation invariability of the proposed methods. This thesis offers approaches to producing perturbation-invariant speech representations and deeply investigates the characteristics of the learned representations, providing insights into these models and cultivating future extension possibilities."
2024,Toward In-Context Teaching,https://hdl.handle.net/1721.1/156326,"When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill. Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students’ changing state of knowledge. There is increasing interest in using computational models, particularly large language models, as pedagogical tools. As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples. But how effectively can these models adapt as teachers to students of different types? To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods. We additionally introduce (3) AToM, a new probabilistic method for adaptive teaching that jointly infers students’ past beliefs and optimizes for the correctness of future beliefs. In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models. In human experiments, both AToM and LLMs outperform non-adaptive random example selection. Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it."
2024,Standardization of Electronic Component Datasheets to Improve Systematic Data Extraction,https://hdl.handle.net/1721.1/156745,"This thesis addresses the challenge of standardizing electronic component datasheets to improve systematic data extraction. The absence of uniformity in datasheet design complicates the process of systematically extracting critical information, leading to significant manual effort and potential errors. This research explores the current state of datasheet standardization and examines existing systematic data extraction efforts from semi-structured documents. It highlights the limitations of current methods and emphasizes the need for further standardization to facilitate accurate and efficient data extraction. The thesis proposes a detailed methodology for transitioning electronic component datasheets from semistructured to structured formats through standardization. By defining common standards and specific structures for different types of datasheets, this approach aims to enhance both human readability and machine processing. The thesis concludes by discussing the broader implications of these standards and their potential applications in other fields. Through this work, the goal is to streamline the datasheet creation process, reduce manual intervention, and ultimately improve the accuracy and efficiency of systematic data extraction in the electronic components industry."
2024,Graphs of Convex Sets with Applications to Optimal Control and Motion Planning,https://hdl.handle.net/1721.1/156598,"This thesis introduces a new class of problems at the interface of combinatorial and convex optimization. We consider graphs where each vertex is paired with a convex program, and each edge couples two programs through additional convex costs and constraints. We call such a graph a Graph of Convex Sets (GCS). Over a GCS we can formulate any optimization problem that we can formulate over an ordinary weighted graph, with scalar costs on the vertices and edges. In fact, for any fixed choice of the variables in the convex programs, a GCS reduces to a weighted graph where we can seek, e.g., a path, a matching, a tour, or a spanning tree of minimum cost. The challenge in a GCS problem lies in solving the discrete and the continuous components of the problem jointly. By combining the modelling power of graphs and convex optimization, GCSs are a flexible framework to formulate and solve many real-world problems. The graph and the combinatorial goal (e.g., finding a path or a tour) model the high-level discrete skeleton of a problem. The convex costs and constraints fill in the low-level continuous details. The primary contribution of this thesis is an efficient and unified method for solving any GCS problem. Starting from an integer-linear-programming formulation of an optimization problem over a weighted graph, this method formulates the corresponding GCS problem as an efficient Mixed-Integer Convex Program (MICP). This MICP can then be solved to global optimality using common branch-and-bound solvers, or approximately by rounding the solution of its convex relaxation. Importantly, both the formulation of the MICP and its solution are fully automatic, and a user of our framework does not need any expertise in mixed-integer optimization. We first describe the GCS framework and the formulation of our MICP in general terms, without presupposing the specific combinatorial problem to be solved over the GCS. We illustrate our techniques through multiple examples spanning logistics, transportation, scheduling, navigation, and computational geometry. Then we focus on the Shortest-Path Problem (SPP) in GCS. This problem is particularly interesting since it generalizes a wide variety of multi-stage decision-making problems and, using our techniques, it can be solved very effectively. We consider two main applications of the SPP in GCS: optimal control of dynamical systems and collision-free motion iplanning. In these two areas, our techniques either generalize or significantly improve upon algorithms and optimization methods that have been developed for decades and are widely used in academia and industry. Lastly, the techniques introduced in this thesis are implemented in the software packages Drake and gcspy. The former is a large and mature software for robotics. It is open-source and widely used by the community. The second is a very simple and lightweight Python package which is also open source. In this thesis, we will illustrate the usage of gcspy through multiple basic examples."
2024,Improving Generative Models for 3D Molecular Structures,https://hdl.handle.net/1721.1/156161,"Generative models have recently emerged as a promising avenue for navigating the high-dimensional space of molecular structures. Such models must be designed carefully to respect the rotation and translation symmetries of molecules. In this thesis, we first provide an overview of existing methods and techniques in this rapidly developing field. Next, we present Symphony, an𝐸(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments, improving upon existing autoregressive models for molecule generation and approaching the performance of diffusion models. The material in this thesis is primarily sourced from the publication “Symphony: SymmetryEquivariant Point-Centered Spherical Harmonics for 3D Molecule Generation"" [13] authored by Ameya Daigavane, Song Kim, Mario Geiger and Tess Smidt, and published at the International Conference on Learning Representations (ICLR), 2024."
2024,"Design of an Affordable, Precise Irrigation Controller that Lowers the Barrier to Water- and Energy-Sustainable Agriculture",https://hdl.handle.net/1721.1/154164,"With climate change and population growth exacerbating global food insecurity, it has become urgent to establish more water- and energy-efficient means to raise agricultural production. Available techniques to bolster crop productivity, such as solar-powered drip irrigation (SPDI) and precision irrigation, are currently cost-prohibitive for farmers in low-and middle-income countries (LMICs), where food insecurity will be most severe. This thesis demonstrates one method to reduce the barrier to these systems, by pairing them with a Predictive Optimal Water and Energy Irrigation (POWEIr) controller that optimizes irrigation schedules to make efficient use of solar and water resources for maximum crop yield. In doing so, POWEIr also decreases SPDI system costs.

First, this work confirms the hypothesis that scheduling irrigation activity to match the availability of variable solar power enables SPDI cost savings. For a fixed irrigation system, a SPDI full-season operation simulation study was conducted and the impact of adjusting the pumping load dynamically to match solar power availability was assessed. When evaluated against conventional operation, this process of profile matching enabled a power system lifetime cost decrease of >18% while delivering 100% of the required irrigation for a simulated two-hectare Kenyan tomato farm with over 50 m well depth.

To exploit these cost and reliability benefits, this work proposes the POWEIr controller. The POWEIr controller leverages machine learning and utilizes a small set of inexpensive sensors to optimize irrigation schedules based on solar energy and crop water demand pre-dictions. The performance of the POWEIr controller was evaluated with an experimental SPDI prototype and compared to simulated typical farming practices. For the same irrigation delivered, a six-fold decrease in the required battery capacity was observed. With no batteries, the POWEIr controller still satisfied a greater fraction of the irrigation demand. Overall, compared to typical practice, the controller provided more reliable irrigation using solar power, with minimal battery usage.

High reliability at low cost necessitates that the POWEIr controller’s irrigation schedules are robust to errors in agronomy inputs and weather data. Sensitivity to these errors was assessed by evaluating the impact on simulated irrigation amounts and crop yield. It was found possible to rely on weather data from an economical station, costing $190, 83% less than a better-equipped research-quality alternative, with negligible consequences to crop yields. This conclusion held steadfast across diverse crop and soil types. The crop coefficient was the most significant factor affecting irrigation performance, thereby pointing to the need for calibration of this factor alone. This underscores the POWEIr controller’s capability to accurately optimize irrigation schedules for only essential water use while relying on affordable sensors and minimal calibration.

Finally, the POWEIr controller was piloted on farms in Jordan and Morocco and performance was benchmarked against measured local, conventional drip irrigation practices on similar farms. It provided up to 44% and 43% savings in water use and pumping energy consumption, respectively, for similar crop yields. This result demonstrates theory to practice of accessible precision agriculture technology and offers tangible evidence of the POWEIr controller’s potential to raise agricultural sustainability."
2024,"Automated Engineering Design for Reusable Concrete
Building Structures",https://hdl.handle.net/1721.1/157357,"Concrete contributes to 8% of global CO2 emission through reinforced concrete (RC) structural system. Unlike steel and timber structures, RC components are rarely reused due to the inseparable phase between concrete and steel. This results in down cycling of the components into aggregates or landfill material. The Pixelframe structural system [1] was proposed to facilitate the reusability of concrete components by implementing the existing external post-tensioning system in bridge structures and fiber reinforced system to design building beams and columns. This work presents an automated engineering design workflow for Pixelframe, including a engineering mechanics of the system that conforms to ACI 318- 19 [2] and fib Model Code 2010 [3], half-scale tests to verify the preliminary behavior of the system, and a scalable design algorithm for minimum embodied carbon designs. The workflow also uncovers new insights on choosing ranges of concrete strengths based on the element lengths and potential carbon reduction from refining the number of different concrete strengths in a building. This work demonstrates the utilization of existing building systems in the context of reusability and the potential of automated computational structures in aiding the design decisions to facilitate the circular economy of concrete structures."
2024,Robot Graph Grammars: Towards Custom Robots for Every Task,https://hdl.handle.net/1721.1/153850,"As robots find broader applications outside factory floors, they face an increasing number of challenges. For example, they must accommodate rugged terrain, limited battery capacity, and complex dynamics. Existing robots are largely designed by hand to meet a given set of specifications. While highly capable, these manually-designed robots tend to leave performance on the table. These difficulties have motivated research into automatic robot design tools. Early tools were often limited in the range of robot topologies they can explore, however. Current graph-based robot representations can expand the space of possible designs, but it is not always clear how the resulting designs can be fabricated.

To enable efficient design exploration and ensure fabricability, we propose graph grammars as a universal robot design representation. Graph grammars use rewriting rules to incrementally add complexity or select among distinct design alternatives. Because only fabricable components and connections are expressed in the grammar, the generated robot topologies are valid by construction. Through recursion and branching, graph grammars can also generate a large variety of possible designs. To tackle this expansive search space, we propose a specialized learning-based search algorithm called Graph Heuristic Search (GHS). GHS focuses limited simulation resources on the most promising designs. We compare GHS to random search and Monte-Carlo tree search baselines, showing that GHS finds higher-performing designs in less wall-clock time. We combine graph grammars and GHS with other techniques such as differentiable simulation to efficiently optimize multiple types of mobile robots. In doing so, we show that graph grammars are a principled yet general design representation for robot co-design. Their efficiency and versatility brings us one step closer to the dream of generating custom robots for every task."
2024,Methods for Extracting and Analyzing Political Content on TikTok,https://hdl.handle.net/1721.1/156993,"In this thesis, I investigate the dynamics of political discourse on TikTok, with a focus on crafting a comprehensive methodology for extracting and analyzing political content related to the 2024 U.S. Presidential Election. This research utilizes a blend of advanced computational tools and crowd-sourced evaluations to delve into the mechanisms through which political influence is both exerted and perceived on the platform. For data collection, the study employed TikAPI, a tool designed for systematic scraping of TikTok videos, which targeted specific political hashtags to amass a substantial dataset. This dataset was analyzed using a variety of innovative methods, including snowball sampling to ensure a representative range of political engagement, and integration with Python to automate the data collection process. Additionally, I utilized Large Language Models (LLMs) to evaluate the relevance and persuasive impact of the content, and these machine-generated insights were then benchmarked against human judgments. Overall, the findings indicate a slight preference for Republican discourse on TikTok. Moreover, I demonstrate that OpenAI’s GPT can effectively classify videos by topic, although human input remains essential for more nuanced tasks such as stance detection and evaluation of persuasive effect. This exploration into the political landscape of TikTok represents one of the first of its kind, with the primary aim of this thesis being to develop a methodology that will support future research in this field."
2024,Exploiting irregular parallelism to accelerate FPGA routing,https://hdl.handle.net/1721.1/157224,"In the era of hardware specialization, field-programmable gate arrays (FPGAs) provide a promising platform for computer architects, combining the programmability of software with the speed and performance of hardware. Despite this, compiling hardware programs onto FPGAs can be incredibly time-consuming, making it hard to develop and iterate on complex FPGA programs. Of particular relevance is the routing phase, which takes a circuit’s technology-mapped netlist and routes its signals using the switches and wires present on a given FPGA architecture, often with a target of minimizing critical path delay. This optimization problem is known to be NP-hard, and existing algorithms for approximating it exhibit very little regular parallelism.
This thesis accelerates the routing phase of VTR 8.0, a commonly used, open-source research tool for FPGA CAD flow. We show that despite the lack of regular parallelism, routing still exhibits significant irregular parallelism. This parallelism can be exploited on parallel architectures that provide hardware support for ordered tasks and fine-grained speculation, such as the Swarm architecture. Using Swarm, we exploit the parallelism present at the core of VTR’s algorithm, achieving a 35.9x speedup on a single routing iteration of a large benchmark (cholesky_mc) on 256 cores."
2024,Equitable Bus Route Electrification Based on a Mixed Integer Linear Programming Approach,https://hdl.handle.net/1721.1/156770,"While public transportation has seen improvement over time with advancements in vehicle technology and urban planning, low-income populations do not see the full benefit of these advancements. The common approach to transportation planning is to distribute benefits in the most cost-efficient manner, meaning neighborhoods with the best existing infrastructure are likely to receive more timely benefits than low-income areas that require more costly updates. This disparity can be thought of as a lack of equity in transportation planning, where equity means that the population that needs a public service the most should benefit the most from improvement of that service.

This work focuses on improving equity within the proposed electrification of the Massachusetts Bay Transportation Authority (MBTA) bus network in Boston. We are interested in which routes should receive updates first to maximize equity, while understanding that focusing on equity poses an inherent cost trade-off. To solve this problem, an optimal subset of routes must be selected for electrification using an objective function that prioritizes routes with the lowest income riders and the highest levels of pollution from diesel buses.

Assuming an optimal cost structure for the full transition to battery-electric buses, and also assuming that not all depots and routes will be electrified on the same time scale, we use Mixed Integer Linear Programming (MILP) methods and a quantification of transportation equity in various objective functions to decide which bus routes originating from the Cabot depot should be prioritized for electrification benefits from an equity standpoint. We then analyze the sensitivity of our results to changes in the cost constraint and conclude the degree to which equity factors correspond to higher energy transition costs. The results show that high-pollution routes are less attractive from a cost standpoint than low-income ridership routes. It is also shown that a given percentage of total electrification costs can electrify a subset of routes with even larger percentages of total pollution and low-income ridership, meaning that the benefits of including equity factors are high for given cost levels in our problem scope."
2024,ZeroWD: Supporting Zero-Waste Garment Design with Linked Edits,https://hdl.handle.net/1721.1/153878,"In traditional garment manufacturing, the way fabrics are cut produces significant waste due to inefficiencies in the design and layout of the garment panels on fabric. Recently fashion designers have begun to explore different ways to design and layout garment panels in more efficient ways. An extreme example of this efficient fashion design process is zero waste fashion design, which aims to use all available fabric in the resulting garment. Currently many zero waste fashion designers manually cut out the 2D patterns and experiment with their 3D shape. With zero-waste design being inherently strictly constrained by the dimensions of the fabric, designers need to perform meticulous calculations for tasks as resizing and restyling. In our work, we propose ZeroWD, a novel interactive design tool that assists zero-waste fashion design by bringing pattern layout and cutting earlier in the design process. With our tool, designers can design zero waste garment panels and simulate the garment’s 3D shape with realtime feedback. By embedding zero-waste design constraints into the system, we enable designers to focus on the creative design rather than tedious constraint solving. Our user study demonstrates that ZeroWD can help fashion designers create garments with minimal waste."
2024,"Illuminating the Nature of Dark Matter through Observation, Simulation and Machine Learning",https://hdl.handle.net/1721.1/156592,"Dark matter constitutes 85% of the matter content in the universe, yet its microscopic nature remains elusive. Discovering the nature of dark matter will not only greatly further our understanding of the universe but will almost certainly shed light on what lies beyond the Standard Model of particle physics. In this thesis, I discuss the progress I have made in the hunt for dark matter by proposing direct observation strategies in the present-day universe, building simulations for dark matter energy injection imprints in the early universe, and using Machine Learning to address the unique challenges in both of these tasks. Specifically, I explore using the echoes of astrophysical radio sources to probe axion dark matter, self-consistently simulating dark matter energy injection in the era of reionization, employing simple Neural Networks to improve these early universe simulations, and utilizing Machine Learning-powered inference techniques to tackle the problem of the Galactic Center 1-ray Excess."
2024,"Advancements in Management Science: Applications to Online Retail, Healthcare, and Non-Profit Fundraising",https://hdl.handle.net/1721.1/157734,"Management science is an evolving-field that requires novel models and algorithms, combining methods from statistics, optimization, and machine learning. This thesis presents advancements in management science across three domains: revenue management, healthcare, and non-profit funding platforms. The chapters in this thesis develop rigorous algorithms and techniques which are relevant in practice, and present data-driven insights into each of the application areas. 

Chapter 2 studies a personalized dynamic pricing problem commonly faced by online retailers. Customers arrive sequentially to the selling platform, and for each arrival the seller must make an immediate pricing decision for that customer. The seller aims to learn the demand as a function of price and customer covariates through price experimentation, while simultaneously earning as much total revenue as possible. Previous work on this topic have adopted a classical online learning setup, where the retailer begins the selling horizon with no information about the problem and gains all knowledge about the demand function from the online selling phase. However, this assumption is often not true in practice. Many retailers already possess some information about their product's demand from market research or previous sales data, and not utilizing this information is clearly suboptimal. The chapter develops a novel framework that allows the seller to incorporate historical data on pricing decisions and realized demand, and moreover enables one to study the effect that certain characteristics of this historical dataset have on online selling performance. Using this framework, a dynamic pricing algorithm is proposed which effectively uses both historical and real time data, and achieves provably optimal performance. Furthermore, a new distance measure is developed to quantify how close the historical pricing decisions are to being optimal. Using this distance measure, the chapter shows a surprising inverse relationship between this measure and the achievable online performance.  

Chapter 3 focuses on applying causal inference techniques to study the treatment efficacy of different antibiotics on patients with urinary tract infection. Up to 50% of women will experience a urinary tract infection (UTI) in their lifetime, making it the third most common indication for antibiotic treatment in the United States. Though national treatment guidelines encourage using one of three antibiotics as the first-line treatment, other second-line and alternative antibiotics are still commonly prescribed in practice. Studies on the efficacy of first-line versus second-line and alternative antibiotics for UTI are limited and dated. The chapter presents a retrospective cohort study using the claims database from Independence Blue Cross to determine the relative efficacy and adverse event rates between different categories of antibiotics. By combining causal inference techniques with automated feature extraction using the Observational Medical Outcomes Partnership (OMOP) common data model, evidence is found which supports the use of guideline-recommended first-line treatments for uncomplicated UTI. Specifically, the rate of treatment efficacy is higher for first-line antibiotics relative to alternatives. Surprisingly, the analysis also finds evidence which supports increased efficacy of first line agents relative to second-line antibiotics, which are of broader spectrum, albeit the effect difference is smaller compared to the comparison between first-line antibiotics and alternatives. This large-scale cohort study which includes a comprehensive collection of covariates provides much-needed evidence to support the continued recommendation of first-line drugs for the treatment of UTI. The chapter also suggests the feasibility for performing complex causal inference analyses using automated feature engineering packages for OMOP-formatted datasets.

Chapter 4 studies an online matching problem where sequentially arriving donors must be matched to projects needing funding on peer-to-peer philanthropic crowdfunding platforms such as DonorsChoose.org. Empirical studies have shown that (i) donors have heterogeneous preferences over the projects, and (ii) many return to make more than one donation. Facing such donors, the platform’s aim is to match each donor to one of their preferred projects so as to maximize the total donation without over-funding any projects and without knowing the arrival pattern. Previous work in the literature have not studied the effect of returning donors on algorithm performance. The chapter shows an upper bound on the best achievable worst-case performance of any online algorithm which reveals the relationship between donor return rate and algorithm performance. Furthermore, numerical analysis shows that a simple known algorithm achieves a performance that improves with the number of returning donors without differentiating between the original and return donors. The algorithm is intuitive and straightforward to implement, and the results shed light on the practical value that returning traffic can bring for fundraising platforms."
2024,Dynamics of Gradient Flow with Contrastive Learning,https://hdl.handle.net/1721.1/157033,"Contrastive learning (CL), in di erent forms, has been shown to learn discriminatory representations for downstream tasks without the need of human labeling. In the representation space learnt via CL, each class collapses to a distinct vertex of a simplex on a hypersphere during training. This property, also seen in other types of learning tasks, might explain why CL works as well as it does. Having class collapse on the test distribution, which determines how well the model generalizes to new samples and new classes, is tied to class collapse on the training distribution under certain conditions as studied by Galanti et al. (2022). In the case of CL, minimizing the contrastive loss has been shown to lead to collapse during training by Graf et al. (2021). In a recent study, Xue et al. (2023) show that the minimizing the contrastive loss is not enough to observe class collapse in the representation space for a single layer linear model and that we need minimum norm minimizers for the collapse to happen. However, their results don't explain how class collapse can occur without adding an explicit bias. The implicit bias of the gradient descent is a likely candidate to explain this phenomena. Here, we investigate the gradient ow of the spectral contrastive loss and give a theoretical description of the learning dynamics."
2024,Classical Commitments to Quantum States,https://hdl.handle.net/1721.1/156278,"We define the notion of a classical commitment to quantum state scheme, which allows a quantum prover to compute a classical commitment to a quantum state and later open each qubit of the state in either the standard or Hadamard basis, while limiting communication with the verifier to a classical channel. Our scheme strengthens the notion of a measurement protocol from [Mah18], which is binding only in the standard basis. We construct our commitment scheme from the post-quantum Learning With Errors (LWE) assumption, and rely directly on any noisy trapdoor claw-free function family that satisfies the adaptive hardcore bit property first introduced in [Bra+18]."
2024,Machine Learning for Sepsis Prognosis: Prediction Models and Dissecting Electronic Health Records,https://hdl.handle.net/1721.1/156618,"Sepsis is the body's extreme response to an infection. It is a life-threatening medical emergency. Given the heavy burden sepsis has posed on the health care system, extensive research in the area has been performed to facilitate sepsis diagnosis. Sepsis prognosis can support the assessment of the likely progression of the disease and thus inform treatment decisions, but it is much less explored. Here I present two approaches to build sepsis prognosis models. First, I introduced the idea of assessing neutrophil function from simple-to-obtain phase microscopy images. I developed an experimental pipeline using measurement of reactive oxygen species genera=on as a label of neutrophil function. I generated a large neutrophil imaging dataset and explored different deep learning approaches to predict neutrophil activation state. Second, I developed machine learning models to predict sepsis patient future clinical score using electronic health records. As part of the effort, I developed a multidatabase extraction pipeline to facilitate electronic health records extraction process. My work demonstrates the potential of using deep learning models to evaluate functional aspects of the immune system and to predict sepsis patient future state, which could provide significant insight into sepsis prognostic monitoring and is easy to adapt in clinical settings. It is of great significance to understand the input data in developing reliable and generalizable machine learning for healthcare models. It is also increasingly apparent that machine learning for healthcare models can predict patient sensitive information from data that does not explicitly encode it. However, we lack a clear understanding of the extent of the problem: what types of sensitive information can be predicted and how it generalizes to different models or different datasets. We lack approaches to develop models that can make clinical inferences but not infer sensitive information. Critically, we also lack approaches to explain such data encoding. Using electronic health records, I thoroughly investigated the ability of machine learning models to encode a wide range of patient sensitive information. I developed a strategy to ensure that clinical prediction is minimally based on patient-sensitive information. I presented an approach that can explain feature importance in patient sensitive information encoding. This set of studies not only allows us to gain deep understanding of the sepsis patient clinical score prediction model but also are applicable to a variety of machine learning models utilizing time-series data."
2024,Harvesting Innovation: Exploring the Potential of an AI-Enabled Platform to Revolutionize Agricultural Labor Markets,https://hdl.handle.net/1721.1/156032,"Labor shortages in agriculture are a global problem that cause revenue losses and resource waste. In the US, immigration is the main source of such laborers, but labor immigration has decreased by 75% in recent years and costs due to unharvested crops due to labor shortage in  agriculture were estimated at USD 3.1 Billion per year in 2014.

This thesis investigates the persistent labor shortages in the agricultural sectors of the southern United States and Mexico, exploring the feasibility of alleviating these shortages through a labor matching platform enhanced by artificial intelligence (AI). With a focus on the economic implications and structural deficiencies in agricultural labor markets, the study examines how a digital platform can bridge the gap between supply and demand for agricultural labor. 

The research employs a multi-dimensional approach that includes extensive literature review, in-depth interviews with stakeholders, system dynamics modeling, and action research involving the launch of a company and release of a Minimum Viable Product (MVP). The MVP, a foundational component of the proposed digital platform, has been tested in the market to gather quantitative data and insights using web advertising. 

The findings highlight the platform’s potential to streamline labor matching processes, improve transparency, and increase efficiency in the agricultural labor market. Additionally, the integration of AI provides intelligent matchmaking capabilities, predicting and aligning labor needs with available workers more effectively. 

Not only does this thesis provide a potential business model to tackle a critical economic problem, but it also contributes to the broader discourse on the role of technology in transforming traditional industries in advanced and emerging economies."
2024,Machine-Learning based Ship Traffic Prediction in the Suez Canal,https://hdl.handle.net/1721.1/153995,"This study implements and evaluates two approaches for predicting the average annual daily ship traffic (AADT) of ships within the Suez Canal with a focus on evaluating how deep-learning techniques can be leveraged for both approaches. The first approach is a novel method that utilizes both satellite imagery and AIS technology to predict the AADT. In order to do so, a 2-stage model is implemented that combines an image detection model followed by a correction factor model. The image detection model employs Mask R-CNN, a deep-learning neural network, and the correction factor model utilizes Long Short-Term Memory (LSTM), a recurrent neural network, to train on historical AIS data. Results of the 2-stage model using LSTM demonstrate positive indication of technical feasibility for the approach due to ground-truth AADT values falling within the interquartile range of predictions for all validation sets. Furthermore, although the interquartile ranges have considerable variation, the 2-stage model with LSTM had a mean absolute percentage error (MAPE) of 13.2% based on its median AADT predictions; this is a successful outcome especially when considering the high variance of vessel traffic and the noisiness that comes with satellite imagery’s small sampling rate as just snapshot moments in time. In addition to the 2-stage model, this study also implements a second approach involving a discrete-event simulation (DES) to estimate AADT, and we evaluate how the DES can benefit from using deep-learning techniques like LSTM. Results from the DES model with LSTM indicate an improved 90.8% reduction on the interquartile range of AADT predictions in comparison to that of the 2-stage model. Additionally, the DES model with LSTM had an MAPE of 3.8% for its median AADT predictions, demonstrating strong predictive accuracy. Overall, patterns within the AIS data indicate that despite the effects of Covid-19 in 2020, there is an increase in traffic in subsequent years especially in 2022 due to a rebound effect."
2024,Force Feedback and Tactile Sensing for Robotic Teleoperation of Contact Rich Manipulation Tasks,https://hdl.handle.net/1721.1/156561,"Imitation learning has shown promising results in teaching robots new skills. We propose augmenting the ALOHA bimanual teleoperation system with haptic feedback to obtain higher quality expert demonstrations. We add two types of haptic feedback: force feedback and cutaneous feedback in both a real and simulation teleoperation system. Additionally, we propose to add tactile sensors to observe the impact of tactile data to imitation learning models in solving fine manipulation tasks."
2024,Simulating Dynamical Systems from Data,https://hdl.handle.net/1721.1/153861,"The ever-increasing availability of data from dynamical systems offers an opportunity for automated data-driven decision-making in various domains. However, a significant barrier to realizing this potential is the issues inherent to these datasets: high-dimensionality, noise, sparsity, and confounding. In this thesis, we propose methods to exploit the richness in the structure of such datasets to overcome the above-mentioned problems while undertaking various inference tasks.

Central to these methods is a key factorization characterizing the function governing the dynamics. Specifically, we harness trajectories from different, yet related, dynamical systems. We posit that the function governing the dynamics of each individual system can be factorized into a linear combination of latent separable functions of the state and action. Crucially, these latent functions are shared across the different dynamical systems. This principled factorization structure provides guidance on how to devise theoretically sound methods that perform well empirically across a variety of tasks. These tasks include time series imputation and forecasting, change point detection, reinforcement learning, and trace-driven simulation in networked systems.

Exploiting the principled factorization structure has paved the way for the contributions we make in different tasks. First, we propose and analyze algorithms for mean and variance estimation and forecasting of time series with varying noise models, data missingness patterns, and assumptions on the factorization structure. These algorithms employ variants of the classical multivariate singular spectrum analysis (mSSA) algorithm and establish a link between time series analysis and Matrix/Tensor Completion. Second, we develop and analyze an algorithm for change point detection inspired by the factorization structure and based on the cumulative sum (CUSUM) statistic. This work extends the analysis of CUSUM statistics traditionally done for the setting of independent observations. Finally, we explore the potential gains of considering the factorization structure in simulating Markov Decision Processes (MDPs). We then build upon this approach to accommodate MDPs with time varying parameters with the specific application of trace-driven simulation in networked systems."
2024,Modeling Control Signals for Reconstruction-based Time Series Anomaly Detection,https://hdl.handle.net/1721.1/156789,"Automated time series anomaly detection methods can provide insights while reducing the load placed on human experts in a variety of settings. Machine-generated signals, such as those produced by sensors, often contains control signals in addition to the target observation signal. These signals may provide additional insight about the normal vs. abnormal properties of the observation signal. Despite this fact, even recent anomaly detection methods using deep learning give limited consideration to the relationship between observation and control signals, often failing to handle the control signal at all. This work proposes pre-processing, modeling, and evaluation methods for multivariate, heterogeneous time series to examine how using information from the control signal can improve anomaly detection. We develop a deep learning reconstruction-based pipeline and test its performance on the NASA Soil Moisture Active Passive (SMAP) satellite and the Mars Science Laboratory (MSL) Rover, which contains heterogeneous sensing data from exploratory missions. The pipeline follows the Sintel machine learning framework and is accessible through the Meissa library, which builds on the capabilities of the open-source library Orion for end-to-end unsupervised time series anomaly detection pipelines."
2024,Design and Fabrication of High Frequency Electromagnetic Coil for Magnetic Particle Imaging,https://hdl.handle.net/1721.1/156311,"Magnetic Particle Imaging (MPI) is a promising modality which uses Magnetic Nanoparticles (MNPs) for tracer-based imaging in biomedical applications. Aside from their use in imaging, MNPs are increasingly being utilized for therapeutics, controlled targeted drug delivery, and diagnostics. These techniques depend on the behavior of MNPs when exposed to alternating magnetic field of a certain frequency and amplitude. However, the frequency typically used for imaging is 25kHz, while the transduction behaviors desired for these biomedical applications are seen at low radio-frequencies and higher amplitude fields than ones used for imaging. This work presents a high frequency electromagnetic coil which fulfills operational, safety, and geometric parameters necessary for incorporation in a custom MPI system and will allow us to simultaneously image and stimulate at specific locations within the body of a mouse. Optimization of the instrument is done through experimentation and electromagnetic theory, with focuses on parasitic elements and metallurgical phenomena. A resonant tank and direct cooling with a water pump allows for increased field strength while maintaining thermal and radio-frequency energy absorption standards for in vivo experiments."
2024,"Fast Multistage Compilation of Machine Learning
Computation Graphs",https://hdl.handle.net/1721.1/156774,"Machine learning applications are increasingly requiring fast and more computational power. Many applications like language models have become so large that they are run on distributed systems in parallel. However, getting into the details of optimally scheduling or even just running machine learning models on distributed systems can be a distraction for researchers ideating models. Hence there has been development of abstractions to facilitate running machine learning models in parallel on distributed systems. We present a compiler for the StreamIt language- a language made for abstract signal processing and multicore programming. We use that abstraction as a way to distribute the computation of machine learning models programmed in PyTorch."
2024,Understanding the Mechanisms of Shock-Induced Deformation in Polymeric Systems,https://hdl.handle.net/1721.1/153688,"Shock waves, commonly resulting from ballistic or explosive impact in military scenarios, are highly destructive and pose significant health risks due to their rapid, intense energy transfer. To mitigate these effects, it is crucial to design armor capable of absorbing and dissipating shock wave energy, thus safeguarding the wearer. Traditional materials like woods and metals often fall short in meeting these requirements due to their low toughness, high weight, limited flexibility, or manufacturing challenges. Furthermore, even when suitable materials are identified, they often have unclear physical deformation mechanisms contributing to their dissipative properties; thus, it is difficult to generalize the benefits of one material in order to find alternatives.

This thesis focuses on the study of polymeric materials, which often outperform traditional materials in terms of shock energy absorption. The vast design space of polymers affords them a diverse set of physical properties and adaptability to different applications. Researchers can tailor these properties by altering the polymers' type, composition, or structure. In order to obtain detailed mechanistic understanding of the shock response of some polymeric materials, atomic and molecular scale simulations are leveraged and the configurational and vibrational responses of the systems analyzed. The thesis work consists of three major projects: (1) a theoretical study of the statistics of Gaussian polymer chains under harmonic applied fields, (2) a computational study of semicrystalline and crystalline models of polyethylene (PE) undergoing shock deformation, and (3) a computational study of liquid ethanol systems undergoing shock deformation.

The theoretical study of Gaussian chains generalizes previous literature models and is shown to well approximate a variety of applications involving polymer confinement. A wave equation is derived from the force-compression relationship of the Gaussian chains--with the addition of a viscous damping fluid--and is connected to the well-studied Korteweg de Vries equation. The effects of material properties (dispersion, dissipation, and nonlinear elasticity) on the propagation of a shock wave are demonstrated explicitly.

The computational study of semicrystalline PE considers systems of three different crystallinity fractions; their configurational changes under shock are analyzed, distinguishing the responses of the crystalline and noncrystalline regions of the systems. Physical mechanisms underlying the deformation response of these materials, including crystallographic slip and loss of nematic order, are identified as a function of shock pressure. The prominence of these mechanisms is shown to be a non-monotonic function of lamellar thickness and crystallinity fraction, and they each contribute to shock energy storage.

The computational study of ethanol reveals the configurational and vibrational changes undergone by the liquid in response to elevated shock pressure and temperature. Shifts of vibrational peaks are observed and compared with experimental results from our collaborators. The hydrogen bonding network responds to energy from the shock wave, resulting in structural and dynamical alterations which may have implications for thermal conductivity and reduction of hot spot formation from shocks.

Overall, this thesis presents a multipronged and detailed approach to understanding shock deformation in polymeric and organic systems, offering mechanistic insights and methodological frameworks applicable to a broader range of materials."
2024,Toward Real-time Earth Observation with Satellite Constellation Crosslinks and Propulsion,https://hdl.handle.net/1721.1/154033,"The development of remote sensing small satellite constellations has created the potential for high-resolution Earth-observation data to reach end users faster. This work investigates how propulsion and intersatellite links enable constellations to continuously collect and deliver data faster than constellations without these capabilities.

This work has four contributions. The first contribution is a constellation simulation framework that is based on open-source libraries. This simulation framework can propagate satellites and execute propulsive maneuvers. The second contribution is a planning and scheduling algorithm for propulsive maneuvers, target observation times, and optimal data routing paths. The third contribution is the development of high performance constellation designs with respect to constellation cost and the following metrics: age of information, system response time, and total pass time. The cost model is developed from two separate models: the Small Satellite Cost Model (SSCM) and a launch cost model developed in this work. The fourth contribution is a set of cost-estimating relationships (CERs) that models the trade-off between cost and system performance in terms of the aforementioned metrics.

The new simulation framework of contribution 1 is verified against the industry-standard software Systems Tool Kit (STK). The simulation framework is used to run 21 different constellation designs, 3 different satellite models, and 432 distinct ground targets. These scenarios are run during each of the four seasons to eliminate geometric biases for a total of 108,864 individual scenario simulations. A single satellite executing the reconfiguration algorithm produces up to a 125% increase in pass time over seven days when compared to an identical satellite without propulsive capabilities. For an access cone with a nadir half-angle of 20 ,thereconfigurationalgorithm produces a 67% increase in pass time. Comparing the cost of inter-satellite link (ISL) and reconfiguration-capable satellites versus (i) only ISL-capable satellites and (ii) a baseline satellite without ISL or reconfigurable capabilities, a Pareto optimal analysis revealed 29% of designs had both propulsion and intersatellite link capabilities when optimizing for age of information, 7% of designs had both propulsion and intersatellite link capabilities when optimizing for system response time, and 33% of designs had both propulsion and intersatellite link capabilities when optimizing for total pass time. The CERs show that for constellations costing between $150M and $1B (FY24), age of information can be reduced by 32 seconds for every million dollars spent, system response time can be reduced by 35 seconds for every million dollars spent, and total pass time over 3 days can be increased by 2 seconds for every million dollars spent."
2024,"Generative Design Tools: Implications on Design Process, Designer Behavior, and Design Outcomes",https://hdl.handle.net/1721.1/153664,"Generative design tools, empowered by recent advancements in computational algorithms, offer the opportunity for human designers and design tools to collaborate in new, more advanced modes throughout various stages of the product design process to facilitate the creation of higher performing and more complex products. Much of the research focuses on the technical development and application of these tools, while less attention has been paid to how generative design tools are used from the designer’s perspective. Three main contributions of this dissertation include a development of a generative design process, observations of the implications of the use of generative design tools, and an understanding of how designers balance multiple objectives throughout a generative design process.  A grounded theory approach based on the experiences of designers was first used to develop a generative design process. Six in-depth interviews were conducted with experienced designers from different disciplines who use commercial generative design tools in their work, detailing the design processes they followed. A qualitative-based coding and analysis of the interviews was used to generate 161 coded themes describing the design process. Through these themes, a provisional process diagram for generative design and its uses in the early-stage design process is proposed to outline explicit and implicit stages of the design process.  Several implications of the use of generative design tools on the design process and designer behavior were developed through additional analysis of the interviews. The early 5  stages of defining tool inputs bring about a constraint-driven process in which designers focus on the abstraction of the design problem. Designers will iterate through the inputs to improve both quantitative and qualitative metrics, such as engineering performance and product styling. This learning-through-iteration allows designers to gain a thorough understanding of the design problem and solution space. This can bring about creative applications of generative design tools in early-stage design to provide guidance for traditionally designed products.  It was observed that generative design tools primarily allow for quantitative inputs to the tool while qualitative metrics, in particular aesthetics, are considered indirectly by designers. To explore this further, controlled lab experiments were conducted to understand how designers balance quantitative and qualitative objectives while using generative design tools. Thirty-four participants completed two design tasks (with and without generative design tools) with the same qualitative and quantitative objectives. Counterintuitively, designs created in the task without generative design tools had a statistically higher quantitative performance than those created with generative design tools. On the other hand, the designs produced with generative design tools displayed a greater aesthetic diversity and expanded a larger portion of the objective space. Participants also expressed the ability to focus on the qualitative objectives by delegating the quantitative objective to the generative design tool. This showcases the potential for generative design tools to assist in the design process and leveraging the expertise of both the human designer and the generative design tool to allow for greater consideration of various objectives throughout the design process."
2024,Essays in Industrial Organization and Labor Economics,https://hdl.handle.net/1721.1/153343,"This thesis consists of three chapters, two in Industrial Organization and one in Labor Economics. The first and second chapters study industrial technologies: the first explores how machine learning changes decision-making of heavy duty truck technicians, while the second studies technological switching in the shale industry. The third chapter studies wage garnishment in the United States.

The first chapter (joint with Adam Harris) uses observational data to explore how a predictive algorithm changes human decision-making. Using a novel, rich decision-level data set from the maintenance of heavy-duty trucks, we document how skilled technicians' decision-making is changed by the introduction of an algorithm designed to predict the risk of truck breakdowns. We develop and estimate a model of technician decision-making that accounts for variation in monetary and non-monetary costs. Using an embedded neural network, we flexibly estimate technicians' beliefs about the probability of truck breakdowns both before and after the introduction of the algorithm. Comparing these estimated beliefs with an objective breakdown probability, we find that the algorithm significantly improves technicians' ability to predict breakdowns: the algorithm narrows the gap between actual and optimal costs by 79%. All of this gain comes from decreased repair costs, suggesting that the algorithm primarily helps technicians avoid low value repairs.

The second chapter studies technology in a different setting: the U.S. shale industry.  In late 2014, global oil prices dropped precipitously, driving U.S. shale producers out of the market. As the number of new wells completed dwindled, productivity began to rise sharply, beginning a steepened upward descent that continued through 2019. This chapter draws on detailed well-level data from the Bakken shale play in North Dakota to tease apart several classic explanations for these trends, including Schumpeterian creative destruction and technological improvement. I document firm-level jumps from gel-based completions to slickwater after the price shock, with earlier jumps for mid cap and private firms. However, I find that improved geological targeting (or ""high-grading"") and slickwater adoption fail to account for over 70% of the productivity increase.

The third chapter (joint with Anthony DeFusco and Brandon Enriquez) uses administrative data to investigate wage garnishment in the United States. Wage garnishment allows creditors to deduct money directly from workers' paychecks to repay defaulted debts. We document new facts about wage garnishment between 2014 and 2019 using data from a large payroll processor who distributes paychecks to approximately 20% of U.S. private-sector workers. As of 2019, over one in every 100 workers was being garnished for delinquent debt. The average garnished worker experiences garnishment for five months, during which approximately 11% of gross earnings is remitted to their creditor(s). The beginning of a new garnishment is associated with an increase in job turnover rates but no intensive margin change in hours worked."
2024,Optical Property Prediction and Molecular Discovery through Multi-Fidelity Deep Learning and Computational Chemistry,https://hdl.handle.net/1721.1/155385,"Optical properties are crucial for the design of molecules for numerous applications, including for display technologies and biological imaging. The accurate prediction of these properties has been the subject of decades of work in both physics-based approaches and statistical modeling. Recently, large datasets of both computed and experimental optical properties have become available, along with the advent of powerful deep learning approaches cable of learning meaningful representations from these large datasets. This thesis presents new approaches for predicting optical properties by fusing the experimental and computational data in multi-fidelity models that achieve greater accuracy and generalizability than previous methods. Additionally, it conducts a thorough benchmark of various strategies for handling multi-fidelity data to inform the modeling choices of future practitioners working with optical properties and beyond. Despite the greater availability of optical property data recently, the near-infrared (NIR) region of the spectrum remains more data-sparse despite its promise in many applications. This thesis demonstrates the shortcomings of existing methods for predicting optical properties in this region of chemical space and recommends best practices for future research in this area. Finally, this thesis highlights successful usage of data-driven optical property prediction for the discovery of novel molecules for specific applications."
2024,Broadband single and multimode quantum light generation using optical nonlinearities,https://hdl.handle.net/1721.1/156309,"There is a growing effort in many fields of physics to bridge the classical and quantum realms. To our best understanding, our world is governed by the laws of quantum mechanics, but some of its most interesting features - such as the ability to morph uncertainty and noise - are washed out when system sizes become too large. Light is the ideal playground to investigate the interplay between the classical and quantum domains, with its well-known particle-wave duality and diverse behaviors at both the classical wave and single photon levels. To this end, there is significant interest in generating quantum states of light that can be harnessed for applications in the classical world we are most familiar with. However, maintaining ""quantumness'' as the number of photons grows large has proved challenging due to the detrimental effects of loss. In this thesis, I describe two theoretical proposals to make macroscopic quantum light a reality. I focus on bright intensity squeezed states of light that have intensity noise far below the standard quantum limit. If realized, these states would bring the quantum mechanical phenomenon of squeezing to macroscopic intensities, which in turn could pave the way towards widespread quantum light sources that offer enhanced signal to noise ratios. I describe two distinct methods that use tools from nonlinear optics and dissipation engineering to realize broadband squeezing in both single and multiple frequency modes. I show that the squeezing can be tunable across a wide range of the electromagnetic spectrum that spans frequencies where quantum light has never been generated."
2024,The Impact of Thermostat Automation and Retail Rate Designs on Cooling and Heating Flexibility: Balancing Consumer Preferences and an Efficient Grid,https://hdl.handle.net/1721.1/155612,"Flexibility in household energy consumption is crucial for improving grid efficiency and reducing peak electricity demand. The ongoing impact of climate change and the move towards electrification worsen these challenges, emphasizing the need for effective peak demand reduction strategies. Current approaches often involve peak pricing retail tariffs, behavioral responses to grid operator notifications, or expensive technologies such as demand-side batteries. However, these methodologies rely on unpredictable consumer participation or substantial capital investments. On the other hand, the growing use of smart thermostats presents an opportunity for passive, efficient control of household energy consumption. Combining smart thermostats with appropriate price signals creates an opportunity to optimize the balance between energy cost and thermal comfort. This work examines the role of smart thermostat automation and dynamic retail rate designs in maximizing heating and cooling flexibility while ensuring consumer comfort. The research introduces a new approach to demand-side management by using reinforcement learning (RL) to optimize thermostat settings based on individual thermal preferences and price signals. A comprehensive testbed simulation framework was developed to analyze these effects, incorporating bottom-up energy modeling, individualized thermal comfort profiles using smart thermostat data, and advanced thermostat controls to investigate the impacts of various rate designs on residential energy demand. The study evaluates these impacts at a population level, considering the effects on over 80 household archetypes across a localized region. Key findings show that partitioned time-of-use rates with moderate pricing shifts effectively reduce energy usage without creating new peaks, unlike more aggressive pricing strategies that can lead to pre-cooling-induced new peaks. These insights offer valuable guidance for policymakers and utility operators in designing rate frameworks that decrease overall electricity consumption and peak demand without compromising personal comfort."
2024,"Modeling spatial mapping, memory and their underlying mechanisms in the hippocampal complex",https://hdl.handle.net/1721.1/156973,"Humans form mental representations of the space and environment around them. This ability is fundamental to tasks such as navigation, spatial reasoning, and understanding the relationships between objects in the environment. Spatial mapping in humans involves several cognitive processes, including perception, memory, and spatial reasoning. Memory plays a crucial role in spatial mapping. As individuals move through an environment, they encode and store information about the spatial layout, which they can later recall to navigate or perform tasks. Further, spatial memory involves similar brain regions as those implicated in sequential episodic memories. Research on human spatial mapping has greatly advanced our understanding of how humans form these mental representations, but leaves us some ways from a complete understanding. In particular, it has been difficult to understand what makes human spatial representations generalizable enabling few-shot learning of maps of novel spaces, how humans store the vast amount of spatial information (maps) experienced through their lifetimes, and what is the connection between spatial memory and episodic memory in the brain, and why is it significant? In this thesis, I aim to answer these questions. First, I ask whether hierarchical spatial representations form the basis of generalizable spatial representations leading to efficient exploration of novel spaces. I present a Map Induction framework that uses a compositional hierarchy to represent spaces, and present results on its utility for exploring novel spaces. Second, I ask how humans store the vast amount of information (e.g., compositional map primitives required to form hierarchical spatial representations) experienced through their lifetimes. I present a neural model called MESH (motivated by brain’s entorhinal-hippocampal system), that has an exponential capacity and shows a gradual decay in retrieval quality with an increase in the number of stored memories rather than a catastrophic drop. Third, I present Vector-HaSH, a model of the entorhinal-hippocampal circuit that forms an instance of MESH, preserving all its properties. This model unifies-general associative memory, spatial memory and episodic memory providing a computational hypothesis for the unification of spatial and episodic memory roles of the hippocampal complex. Overall this research bridges the computational, algorithmic and implementation levels of analyses for explaining how humans represent and reason about spaces."
2024,Design and Synthesis of Stimuli-Responsive Polymers with Programmable Cleavability,https://hdl.handle.net/1721.1/157070,"Polymers comprise a large portion of modern-day materials, from everyday plastics that we can hold and use, to nanomaterials imperceptible to the naked eye. Applying synthetic chemistry to impart structural changes to established polymers offers a promising path to introduce novel functionalities for applications ranging from biology to sustainability. In particular, this thesis explores the synthesis, characterization and evaluation of polymeric platforms containing rational incorporation of moieties that can undergo chemical cleavage, effecting enhancements in their design and performance. In the first half, we explore advancements to linker design and controlled release of payloads from molecular bottlebrush polymers. The first chapter introduces bottlebrush polymers as nanocarriers for therapeutics, and provides a detailed literature analysis of the synthetic and architectural developments that have been reported for these constructs, as well as outlooks for the future. The second chapter reports the first synthesis of peptide-containing bivalent bottlebrush (co)polymers (BBPs), featuring caspase-3-cleavable peptides linked to fluorogenic probes that provide a “turnon” signal upon enzymatic cleavage. The impacts of different architectural features of these polymers on enzyme access reveal insights into the interactions of enzymes with BBPs, and provide design criteria for future therapeutic systems leveraging this approach. The third chapter investigates a synergistic approach to treating pancreatic ductal adenocarcinoma (PDAC) with drug-loaded BBPs by leveraging multiple facets of structural modularity, including linker and drug identities and concentration ratios. This mechanism-guided approach to combination therapy is validated with the translation of in vitro studies that identify synergy across axes of both drug release timing and mechanism of action to in vivo validation of enhanced therapeutic efficacy of the combination BBP system. The remaining two chapters are a departure from BBPs, instead introducing a novel approach to cleavable comonomers for improving plastic end-of-life sustainability. The fourth chapter thus provides detailed background on the current plastic waste outlook, vinyl polymers and their synthesis, radical ring-opening polymerization, and current approaches to cleavable comonomers and the end-of-life options they offer commodity polymers. The fifth and final chapter reports the first “mixed” cleavable comonomer approach to degradable polymers towards a polyacrylic acid system optimized for biodegradability. A computational model offers parameters for controlling degradation fragment molecular weight and dispersity that are validated experimentally, and the material performance properties of the homopolymer are retained for its cleavable analog. Overall, this thesis leverages structure-activity relationships of cleavable functionalities in stimuli-responsive polymers, and expands the scope under which they can be utilized during their productive lifetime or processed thereafter."
2024,Numerical and Analytical Methods in Low-Dimensional Strongly Correlated Quantum Systems,https://hdl.handle.net/1721.1/157584,"The study of low-dimensional strongly correlated quantum systems lies at the intersection of intricate theoretical models and practical numerical methods, offering deep insights into condensed matter physics. This thesis explores the application of various numerical and analytical methods to these systems. It addresses universal behaviors and phase transitions, exemplified by the phenomenon of multiversality. Specifically, the transition from a 1D Luttinger liquid to a charge density wave insulator, characterized by partly Kosterlitz-Thouless transition and partly Ising transition, is analyzed using both analytical renormalization group calculations and numerical density matrix renormalization group simulations. Additionally, the thesis introduces a statistical smoothing spline method to pinpoint transition points systematically. The work extends to quantum dynamics, presenting a generic theoretical framework for analyzing quantum-classical adiabatic dynamics with learning algorithms. A provably efficient adiabatic learning (PEAL) algorithm with favorable scaling properties is developed. The algorithm is numerically validated on the 1D Holstein model, demonstrating its precision in predicting dynamics. Furthermore, the thesis derives a Hamiltonian lattice formulation for the 2+1D compact Maxwell-Chern-Simons theory, providing an analytical solution that aligns with continuum theories and facilitating future numerical applications. Through these explorations, the thesis underscores the complementary roles of numerical and analytical methods in advancing the understanding of complex quantum systems."
2024,Process Digitalization: 3D Deep Learning in Manufacturing Applications,https://hdl.handle.net/1721.1/156013,"The surge in artificial intelligence (AI) popularity and investment has significantly impacted various sectors, including automotive, aerospace, and defense. Smaller companies at the base of these supply chains often lack the resources and knowledge for AI implementation compared to larger original equipment manufacturers, creating a unique opportunity for these smaller companies to leverage AI for growth. However, many AI initiatives in these smaller firms stall at the prototyping phase. This research outlines, from planning to execution, steps and considerations for implementing an AI initiative at a small to medium sized manufacturing company. As well, given the importance of 3D data in the industry, the research also conducts a deep dive on working with, analyzing, and integrating 3D data into an AI model using various techniques, from statistical analysis to 3D deep learning. Discussion on the different
data representations including point clouds, voxels, polygon meshes, depth maps, and boundary representations, and their trade-offs help with the determination of which representation is best for different use-cases. Most of the techniques apply to various unstructured data types to enable multi-modal inputs to a descriptive, predictive, or prescriptive AI model. Additionally, beyond the technical requirements, an entire section is dedicated to discussing the human element in this whole process, focusing on a company’s personnel and cultural aspects, which is often where initiatives can succeed or fail."
2024,LLM-Directed Agent Models in Cyberspace,https://hdl.handle.net/1721.1/156291,"Network penetration testing, a proactive method for identifying vulnerabilities in cy- berspace, has long been the domain of human experts. However, rapid advancements in machine learning have opened up new possibilities for automating many of these tasks. This thesis aims to explore the application of Large Language Models (LLMs) for automating penetration tests and Cyber Capture the Flag (CTF) challenges, bridging the gap between static tools and dynamic human intuition in cybersecurity.
This work provides an evaluation framework for assessing the performance of LLMs in autonomously solving CTF challenges, with an emphasis on understanding the capabilities, limitations, and best prompting strategies for LLMs in this domain. Notably, this thesis presents an agent configuration that offers a 102% improvement in challenge completion on a database of PicoCTF challenges compared to the published baseline. By analyzing a variety of agent strategies, response formats, and historical action representations in the context of CTF challenges, this work aims to provide insights into the best practices and limitations in leveraging LLMs for cybersecurity tasks. Additionally, this work proposes a hierarchical architecture to guide an LLM-enabled agent in performing complex, multi-step penetration testing tasks with strategic foresight. This proof of concept approach shows success in entry level challenges. While LLMs exhibit impressive capabilities, they are limited out of the box in their ability to solve complex, multi-step tasks requiring exploration, necessitating approaches such as those described in this work to improve performance in these areas."
2024,Geo-UNet: A Geometrically Constrained Neural Framework for Clinical-Grade Lumen Segmentation in Intravascular Ultrasound,https://hdl.handle.net/1721.1/157219,"Precisely estimating lumen boundaries in intravascular ultrasound (IVUS) is needed for sizing interventional stents to treat deep vein thrombosis (DVT). Unfortunately, current segmentation networks like the UNet lack the precision required for clinical adoption in IVUS workflows. This arises due to the difficulty of automatically learning accurate lumen contour from limited training data while accounting for the radial geometry of IVUS imaging. We propose the Geo-UNet framework to address these issues via a design informed by the geometry of the lumen contour segmentation task, building anatomical constraints directly into the architecture. We first convert the input data and segmentation targets from Cartesian to polar coordinates. Starting from a convUNet feature extractor, we propose a two-task setup, one for conventional pixel-wise labeling and the other for single boundary lumen-contour localization. We directly combine the two predictions by passing the predicted lumen contour through a new activation (named CDFeLU) to filter out spurious pixel-wise predictions. Our unified loss function carefully balances area-based, distance-based, and contour-based penalties to provide near clinical-grade generalization in unseen patient data. We also introduce a lightweight, inference-time technique to enhance segmentation smoothness. The efficacy of our framework on a venous IVUS dataset is shown against state-of-the-art models. We will make the code repository for this project available soon after approval from industry collaborators."
2024,Harnessing Intelligent Audio-Gesture Interfaces For Wearables As A Sleep Aid,https://hdl.handle.net/1721.1/155483,"Insomnia—difficulty in initiating and maintaining sleep—affects a significant portion of the global population. The mainstream adoption of wearable computing presents a unique opportunity to study and aid sleep at an individual level. Here we introduce Zzzonic, a smart sleep-aid application designed for smartwatches that leverages cognitive psychology and human-computerinteraction (HCI) to facilitate sleep onset by engaging users in audio tasks as a formof intrusive thought control. A significant aspect of Zzzonic's functionality is its adaptive control system, which estimates sleep onset latency in realtime by monitoring indicators such as motion anduser response. The system then progressively modifies the characteristics of the audio tasks to minimize sleep onset latency. This thesis evaluates Zzzonic through a series of user trials conducted throughout the development of the app, accessing the capacity to predict and control sleep onset. The results indicate accurately predicting sleep onset latency in realtime as a control signal is possible but there was no evidence indicating the system could minimize slope onset latency. The inclusion of more indicator signals and machine learning techniques is likely to significantly improve realtime sleep onset latency prediction. Future work on computer-modulated intrusive thought control would benefit from the evaluation of task design, intrusive thought indicators and identifying an adequate control framework."
2024,Thermal Interaction of Inert Additives in Energetic Materials,https://hdl.handle.net/1721.1/155906,"Energetic materials are used for a variety of applications, including airbag deployment and solid rocket fuels, that require high energy density and various energy release rates. The energy release rate, determined by how fast the material burns, is often thought to be proportional to the bulk thermal diffusivity of the material. However, the inclusion of insulating inert particles in energetic materials has shown burning rate enhancement in certain cases. Flame front corrugation that increases the reaction front area observed at micron to sub-millimeter scales was proposed previously to explain the phenomenon. However, a recent simulation study observed a significant temperature gradient within the inert particle, implying that the residence time of the inert particle in the flame front could play a role in the thermal interaction between additives and surrounding energetic materials. In this work, we tested these hypotheses by employing a high-speed microscopic imaging system to quantify the burning rate and flame morphology of Al/CuO nanothermites with various SiO2 particle sizes and mass loading. Additionally, we performed flame propagation simulations to quantify the thermal interactions between the energetic materials and the embedded single inert particle. The experimental results show that the burning rate depends on the particle size as well as mass loading. Specifically, as the SiO2 particle size increases from 100 nm to 100 μm, the burning rate is enhanced by 26% at a mass loading of 7.5%. Further computational studies reveal that flame corrugation may not be the sole factor to alter the burning rate. Non-dimensional analyses show that energy absorption and temperature non-uniformity in inert particles have strong correlations with particle diameter. When the characteristic time of heating the inert particle is shorter than the flame residence time, the inert particle acts as a heat sink, leading to a negative impact on burning rates due to the heat removal from the surrounding energetic materials. Experimental studies reveal that additive particle size has an impact on the nanothermite burn rate. Insight into why this may occur is provided by computational studies of a single particle inclusion, as well as images captured of the burn rate experiments, showing the flame front morphology and particle size effects on heat transfer may play a key role in burn rate alteration by inert additives."
2024,Modeling the Future Space Debris Population and Orbital Capacity,https://hdl.handle.net/1721.1/157822,"Increased investments and technological advances in satellite manufacturing and launch services have led to a newly vitalized Low Earth Orbit (LEO) environment. Megaconstellations consisting of hundreds to hundreds of thousands of satellites have been proposed, with SpaceX’s Starlink satellite constellation now reaching more than 5400 operational satellites. This denser LEO environment underscores the urgent need for models to predict and manage the risk of collisions and the sustainable use of space. Many models have been proposed over the years to quantify the risk of collisions between resident space objects, including the seminal paper by Kessler that described the runaway conditions for which LEO could become unusable. In this thesis, the development of the MIT Orbital Capacity Analysis Tool (MOCAT) is described along with conclusions and insights. MOCAT is a novel open-source approach to evaluating the LEO environment and comprises of a Source Sink Evolutionary Model (SSEM) and a Monte Carlo (MC) method. The SSEM simplifies the complex dynamics of space-object interactions into deterministic equations, focusing on the long-term evolution of orbital populations across different altitude shells. The simplified nature of the SSEM allows for computational efficiency, which enables optimization routines such as the exploration of equilibrium solutions for LEO carrying capacity. The improvements to the SSEM in this work through binning in the physical dimension as well as inclusion of Delta-V dynamics from the collision dynamics increases the fidelity of the SSEM. In comparison, MOCAT-MC offers a comprehensive means to simulate the individual interactions between RSOs. The MOCAT-MC tool propagates the orbits of low-earth orbit objects and models their interactions including collisions and explosions, and provides insights into the evolving trends of the LEO population. Of particular note is the computational efficiency of the model, which is essential for managing the complexities inherent in orbital dynamics and the potential large number of objects centuries into the future. Validation results and a range of simulations, including no-future launch scenarios and the launch of proposed megaconstellations totaling more than 80,000 active payloads are explored, resulting in millions of trackable objects. Despite the much fewer megaconstellations planned at the higher altitudes, even a small fraction of failures in post-mission disposal or collision avoidance maneuvers result in an outsized effect on orbital debris accumulation. MOCAT-MC is able to simulate Lethal Non-Trackable (LNT) objects, which comprise the vast majority of the orbital population today. These lethal non-trackable object population will only grow as more payloads and debris are launched into orbit and increase the collision rate. The effect of these objects are modeled and discussed. These two models offer different approaches to modeling the future orbital environment each with its strengths and weaknesses. Validation against existing models in literature shows the utility of MOCAT in informing future space traffic management and constellation design. The MOCAT tool has been created such that researchers can use a common model that is validated, robust, and efficient, allowing for advancement in our ability to forecast and mitigate the risks associated with the increasing density of LEO while advocating for a more sustainable approach to space exploration and utilization."
2024,Practical Considerations For the Deployment of Clinical NLP Systems,https://hdl.handle.net/1721.1/156307,"Although recent advances in scaling large language models (LLMs) have resulted in improvements on many NLP tasks, it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as healthcare. A healthcare system attempting to automate a clinical task must weigh all approaches with respect to safety, efficacy, and efficiency. This thesis investigates the challenges and implications of implementing LLMs in clinical settings, focusing on the three considerations listed above: safety, efficacy, and efficiency. We first explore the potential biases that might be introduced in downstream patient safety by using LLMs in a zero or few-shot setting and find that LLMs can propagate, or even amplify, harmful societal biases in a number of clinical tasks. Then, we examine the privacy considerations of pretraining a language model on protected health information (PHI) bearing clinical text and find that simple probing methods are unable to meaningfully extract sensitive information from an encoder-only language model pretrained on non-deidentified electronic health record (EHR) notes. Finally, we conduct an extensive empirical analysis of 12 language models, ranging from 220M to 175B parameters, measuring their performance on 3 different clinical tasks that test their ability to parse and reason over electronic health records. We show that relatively small specialized clinical models are substantially more effective than larger models trained on general text used through in-context learning. Further, we find that pretraining on clinical tokens allows for smaller, more parameter-efficient models that either match or outperform much larger language models trained on general text. We argue that using a clinical text-specific pretrained language model allows for an efficient, effective, and privacy-conscious approach, enabling a tailored and ethically responsible application of AI in healthcare."
2024,The definition and implementation of a computer programming language based on constraints,http://hdl.handle.net/1721.1/15890,"Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 1980"
2024,Exploring Developmental Change in Ego-Motion Experience Across Infancy,https://hdl.handle.net/1721.1/156992,"Humans flexibly and intuitively use vision to plan and guide navigation through the local environment. How does this ability develop in infancy? One possibility is that the development of visual representations for navigation is driven by passive exposure to the visual statistics of scenes. Another possibility is that active navigation experience using vision to plan and guide locomotion is the driving factor. In order to distinguish between these two hypotheses, it is necessary to understand the nature of infants’ early visual scene experience itself. Surprisingly little prior work has characterized infants’ early experiences with ego-motion through scenes, before and after learning to locomote. We use ecological momentary assessments to quantify infants’ exposure to ego-motion through scenes, and how that changes with locomotor experience. We found that pre-crawling infants who have never independently navigated already experience significant passive visual exposure to forward-facing ego-motion through scenes. Nevertheless, this experience increases substantially with age and locomotor status."
2024,The Power of Perception in Human-AI Interaction: Investigating Psychological Factors and Cognitive Biases that Shape User Belief and Behavior,https://hdl.handle.net/1721.1/157241,"This thesis investigates the psychological factors that influence belief in AI predictions, comparing them to belief in astrology- and personality-based predictions, and examines the ""personal validation effect"" in the context of AI, particularly with Large Language Models (LLMs). Through two interconnected studies involving 238 participants, the first study explores how cognitive style, paranormal beliefs, AI attitudes, and personality traits impact perceptions of the validity, reliability, usefulness, and personalization of predictions from different sources. The study finds a positive correlation between belief in AI predictions and belief in astrology- and personality-based predictions, highlighting a ""rational superstition"" phenomenon where belief is more influenced by mental heuristics and intuition than by critical evaluation. Interestingly, cognitive style did not significantly affect belief in predictions, while paranormal beliefs, positive AI attitudes, and conscientiousness played significant roles. The second study reveals that positive predictions are perceived as significantly more valid, personalized, reliable, and useful than negative ones, emphasizing the strong influence of prediction valence on user perceptions. This underscores the need for AI systems to manage user expectations and foster balanced trust. The thesis concludes with a proposal for future research on how belief in AI predictions influences actual user behavior, exploring it through the lens of self-fulfilling prophecy. Overall, this thesis enhances understanding of human-AI interaction and provides insights for developing AI systems across various applications."
2024,Assessing United States Energy Poverty Policy: Regulatory Design Alternatives and Resource Allocation,https://hdl.handle.net/1721.1/157037,"Guaranteeing sufficient and affordable access to energy services is increasingly critical as climate change continues to worsen, energy costs increase due to the need to meet decarbonization goals, and the trend in general inequality among citizens persists. To ensure the affordability of energy services, in this thesis, I analyze the design of policies and programs addressing energy poverty according to the four strategy decisions that I argue must be made during their ideation: assistance, targeting, funding, and governance. I focus on the strategies designed and implemented in the US and the EU and discuss the benefits and disadvantages of the different approaches followed in both contexts. Based on this comparative analysis, I find there are changes to US federal policy design that should be implemented to better serve households living in energy poverty. Specifically, current allocations under the US Low Income Home Energy Assistance Program (LIHEAP) to states have been nearly static since 1984, while the distribution of energy poverty is dynamic in location and time. To improve the allocation of federal resources, I produce a novel machine learning approach based on sociodemographic and geographical information to estimate energy burden in each US census tract for 2015 and 2020. This analysis reveals an increase in the average household energy burdens, and the range of households experiencing energy poverty broadened. To improve the targeting strategy of LIHEAP, I design an optimized allocation structure that illustrates a shift in funding to the southern US from northern states. To better match household assistance needs, this analysis urges policy makers to revise the distribution of resources to reflect where concentrations of energy poverty exist in the US."
2024,Aquaculture Basket Detection and Tracking for Autonomous Surface Vehicles,https://hdl.handle.net/1721.1/156768,"With the global population on the rise, there is an increased demand for seafood, underscoring the crucial role of aquaculture- the practice of farming aquatic organisms [1]. In the realm of aquaculture, oyster farming is relatively low maintenance, except for the challenge of manually flipping heavy oyster-laden bags. To address this issue, MIT Sea Grant introduced the Oystermaran, an autonomous catamaran specifically designed for this task. This thesis presents contributions to the electronics, controls, and perception systems of the Oystermaran project. In particular, it presents an oyster basket detection and tracking method using the object detector You Only Look Once (YOLO) [2]. In addition, the electronics system has been updated and new manual controllers were created to enable the use of a new f lipping mechanism developed this year. This system is evaluated on data from field testing at Ward Aquafarms, a Cape Cod-based oyster farming business. The results show that oyster baskets can be robustly detected in new environments, despite environmental factors. This marks a significant step towards real-time viability for autonomous oyster farming."
2024,Maintenance and Metalearning of Time Interval Representations,https://hdl.handle.net/1721.1/153772,"When we perform actions in the world, we estimate what is happening around us. That information goes through a series of transformations in the brain in order to execute an action that meets our goals. For example, we might remember the speed of a car in order to decide when to cross the road. These transformations can be simple, for example based on physics models of speed and time, like the car example, or they can be complex and built around evolutionary and experience-based statistical regularities in the world. This thesis uses a sensorimotor time production task to investigate different types of transformation and noise that exist between observation and action. First, I will propose a task which utilizes memory of a time interval in order to probe memory noise, memory storage, and inference over internal noise. To do this, monkeys perform a delayed time reproduction task. I find that the behavior is consistent with the the brain storing the memory as a function of time, and that the inference does not mitigate the internal memory noise. Second, I investigate how estimated prior distributions change when the statistical regularities of the world change. Monkeys perform a blocked time reproduction task, and behavior across policy transitions shows fast adaptation to new policies. I apply this algorithm to a model and fit it to behavioral data.  Third, I display some preliminary neural data gathered during these tasks as well as hypotheses for neural implementation. With these experiments, I utilize a simple task to pick apart transformations that occur between observation and action."
2023,Systemic risk in the interbank lending market,http://hdl.handle.net/1721.1/117814,"Our goal is to understand the functioning of the interbank lending market in times of market stress. Working towards this goal, we conduct theoretical analysis and simulation to study the effects of network structure and shock scenarios on systemic risk in the market. We consider shocks of various sizes at both global and local scales. In terms of risk measures, we study relative systemic loss and the default rate, separating the latter quantity into fundamental default and contagion. Our simulations suggest that all systemic risk measures are similar on the well-studied directed Erdős-Rényi model and the more complex fitness model if we match the mean density and the mean edge weight of these two models. We show through both derivations and simulations that the network size has little effect on systemic risk when the network is sufficiently large. Moreover, as the mean degree grows, the different default rates considered all increase, while relative systemic loss decreases. Furthermore, simulations suggest that local shocks tend to cause more harm than global shocks of the same total size. We also derive upper and lower bounds on a bank's probability of default, only using its neighbors' information. For implementation, we build a method for real-time, automatic, interpretable assessment of financial systemic risk, which only requires temporal snapshots of observable data. Our algorithm takes in partial data, inferring a random graph model, and then generates empirical distributions for risk measures. The first part relies on inferring a fitness model that is compatible with observed information. For the second part, we use simulations to obtain empirical distributions for systemic risk that arises from interbank clearing. We test our method on synthetic data and apply it to the federal funds market using empirical data. Our method is fast enough to be incorporated into algorithms that produce intraday time trajectories of risk prediction. The data requirement is practical for investors as well as regulators, policy-makers, and financial institutions."
2023,Accelerated algorithms for constrained optimization and control,https://hdl.handle.net/1721.1/152459,"Nonlinear optimization with equality and inequality constraints is a ubiquitous problem in several optimization and control problems in large-scale systems. Ensuring feasibility along with reasonable convergence to optimal solution remains an open and pressing problem in this area. 

A class of high-order tuners was recently proposed in adaptive control literature with an effort to lead to accelerated convergence for the case when no constraints are present. In this thesis, we propose a new high-order tuner based algorithm that can
accommodate the presence of equality and inequality constraints. We leverage the linear dependence in solution space to guarantee that equality constraints are always satisfied. We further ensure feasibility with respect to inequality constraints for the specific case of box constraints by introducing time-varying gains in the high-order tuner while retaining the attractive accelerated convergence properties. Theoretical guarantees pertaining to stability are also provided for time-varying regressors. These theoretical propositions are validated by applying them to several categories of optimization problems, in the form of academic examples, power flow optimization and neural network optimization.

We devote special attention to analyze a special case of neural network optimization, namely, linear neural network training problem, to understand the dynamics of nonconvex optimization governed by gradient flow and provide lyapunov stability guarantees for LNNs."
2023,"First-principles control of zeolite synthesis, transformations, and intergrowth",https://hdl.handle.net/1721.1/153084,"Designing new materials enabling of sustainable catalysis and separations is essential to fully decarbonize the industrial sector, but materials discovery is hindered by labor-intensive experimentation. Computational methods such as high-throughput screening or machine learning can filter structures and compositions prior to experiments. Nevertheless, finding synthesis routes to realize computationally proposed materials still relies on trial-and-error. This thesis describes how materials discovery can be streamlined using high-throughput simulations, physics-based representations, and machine learning. In particular, this work analyzes the synthesis, phase transformations, and intergrowth of zeolites, which are industrially relevant catalysts and molecular sieves known for their hard-to-control phase competition and polymorphism.

Part I of this thesis describes the development of methods to simulate and analyze zeolite transformations and template-based synthesis. Diffusionless phase transformations and intergrowths are predicted by using graph-based representations of zeolite frameworks. Moreover, a data-driven analysis indicates how  structural factors are often insufficient to predict outcomes in their synthesis. To address this knowledge gap, computational tools are developed to simulate template-based synthesis conditions for zeolites. The proposed methods accelerate the prediction of binding energies between molecular templates and zeolite hosts, reducing computational costs by up to two orders of magnitude while increasing the reproducibility of the calculations. These tools are then used to simulate hundreds of thousands of template-zeolite pairs in a high-throughput screening pipeline. The simulation results explain thousands of synthesis outcomes from the zeolite literature from the past six decades and recall archetypical templates purely from physical principles.

Part II of this thesis leverages these methods to design new zeolite synthesis routes, thus enabling the control of phase competition, intergrowth, and catalytic properties of the materials. The computational tools guide the experimental synthesis of zeolites with improved catalytic behavior, including pure-phase frameworks with low-cost templates and disordered structures with tunable polymorph selectivity. Finally, the computational approach maps numerous possibilities for further discovery based on over 35 million data points generated using machine learning, which is used to decode this “zeolite genome”. This work provides a roadmap on how synthesis routes for zeolites can be controlled a priori using theory and computation."
2023,"DNA sequence design of non-orthogonal binding networks, and application to DNA data storage",https://hdl.handle.net/1721.1/153069,"DNA has proven itself a powerful tool in a diverse array of nanotechnology-related domains, including molecular computation, nanostructure fabrication, and data storage. Most DNA-based systems focus on using sets of DNA sequences that are orthogonal to each other, such that each DNA sequence has a dedicated binding partner, its complementary sequence. This design approach reduces the number of interactions that must be considered when predicting how a system will behave, at the cost of reducing the information-gathering ability of each molecular unit.

Relatively little research has attempted to solve the problem of designing promiscuous, or non-orthogonal, DNA sequences, which are characterized by their ability to bind to several distinct partners with variable binding affinities. Yet there are many situations in which this type of dense interaction network can be useful. For example, in neural networks, a node will often take inputs from hundreds or thousands of upstream nodes, allowing it to condense large amounts of information into a single output value. While naturally occurring biological networks often make use of promiscuous binding behavior, the field of molecular computing currently lacks a general-purpose and efficient method for non-orthogonal DNA sequence design.

In this thesis, I describe a novel, robust, and broadly applicable method for designing small or large sets of non-orthogonal DNA sequences. This method takes an arbitrary matrix of pairwise binding affinities, and attempts to design DNA sequences such that the differential binding affinity between any two pairs of sequences is proportional to the difference in the corresponding elements of the matrix. The key innovation of this method is the reformulation of the matrix via a binary embedding, which reduces the design specification to a set of binary strings that permit relatively straightforward sequence design.

Not all matrices permit a binary embedding and I consider three cases here: when a binary embedding exists, when it is unknown if it exists, and when it does not exist. When it exists, I show through both simulation and experiment that DNA sequences can be designed with high precision. When it is unknown if a binary embedding exists, I give novel conditions for determining existence via representation of the matrix in a weighted graph. Finally, when an exact binary embedding does not exist, I develop an alternative method using approximate binary embeddings. To demonstrate the power of this method, I apply to the task of similarity searching in a large, simulated DNA databases, where I show that it outperforms the existing state of the art. I hope that this work opens the door to further innovations in designing and applying non-orthogonal DNA sequences to DNA nanotechnology."
2023,Identifying functional groups in microbial communities based on ecological patterns,https://hdl.handle.net/1721.1/152708,"Recent development in sequencing technologies has greatly advanced our understandings of structure and function of microbial communities in various ecosystems. In microbial communities, a metabolic function is often performed by a group of multiple species (i.e., a functional group) at the same time. However, identifying these functional groups remains to be a major challenge for structure-function mapping in microbiome studies. Instead of relying on annotation-based methods that are highly biased for a few model microorganisms, here I tackle this challenge by developing a novel annotation-free approach. In chapter two, I develop the mathematical framework behind the new approach – which we call EQO – and show its power by applying it to a few existing microbiome datasets. I show that, based solely on the patterns of statistical variation in species abundances, EQO identifies functional groups in soil, ocean and animal gut microbiome. The following two chapters discuss an application of this method, which has led to the discovery a potential new form of interaction between bacteria in animal guts, and an unexpected finding in the lab regarding the ecological dynamics of phage-plasmids in marine bacterial populations. In chapter three, I show how applying EQO to an aquaculture dataset leads us to identify potential pathogen-inhibiting groups of bacteria in an animal-associated microbiome. Guided by the computational prediction, I successfully isolate a member of this group that is a novel species with a broad spectrum of interaction against various Vibrio pathogens. By synthesizing and secreting polysaccharides, the novel species causes limited dispersion and reduced virulence of Vibrio. My efforts to understand the ecology of marine bacteria also lead me to study the role of widely distributed phage-plasmids. Combining mathematical models and experimental evidence, I show that loss-of-function mutations and segregational drift recurrently drive productive infections of phage-plasmids within marine bacterial populations. Together, this thesis provides a simple yet powerful approach to abstract functional groups from taxonomic composition in complex microbiome. As a useful hypothesis generating tool, this approach will pave the way for more mechanistic studies of microbiome in the future."
2023,On Semi-supervised Estimation of Distributions,https://hdl.handle.net/1721.1/151386,"We study the problem of estimating the joint probability mass function (pmf) over two random variables. In particular, the estimation is based on the observation of 𝑚 samples containing both variables and 𝑛 samples missing one fixed variable. We adopt the minimax framework with [notation] loss functions, and we show that the composition of uni-variate minimax estimators achieves minimax risk with the optimal first-order constant for 𝑝 ≥ 2, in the regime 𝑚 = 𝑜(𝑛)."
2023,Physics-Inspired Deep Learning for Inverse Problems in MRI,https://hdl.handle.net/1721.1/152787,"We demonstrate the power of combining the forward image acquisition model with deep learning solutions for inverse problems in magnetic resonance imaging (MRI), from individual network layers to the network architecture design and inference procedure.

First, we propose neural network layers that combine image space representations with representations in Fourier space, where MRI data is acquired. These layers can be used as drop-in replacements for standard image space convolutions in a variety of network architectures and yield higher quality reconstructions across a wide range of MR imaging tasks.

Next, we demonstrate a deep learning framework for rigid-body motion correction in MRI, where the forward imaging model informs both the network architecture and the inference procedure. Our method incorporates potentially unknown motion parameters as inputs to the network and then optimizes them for each test example. The optimization is performed via an objective function that forces the reconstructed image and estimated motion parameters to be consistent with the acquired data. This approach reduces the joint image-motion parameter search used by most motion correction strategies to an inference-time search over motion parameters alone, greatly simplifying the complexity of the optimization problem to be solved for a novel image. Our hybrid method achieves the high reconstruction quality metrics that characterize deep learning solutions while retaining the benefits of explicit model-based optimization – in particular, the ability to reject examples where the network produces poor reconstructions. Experiments demonstrate the advantages of this combined approach over purely learning or model-based reconstruction techniques."
2023,The Coordination Imperative: A Comprehensive Approach to Align Customer Demand and Inventory Management for Superior Customer Experience in Retail,https://hdl.handle.net/1721.1/152447,"The rapid growth of customers traversing different channels during their buying journey presents both opportunities and challenges for organizations. Fragmented decision-making and siloed communication between marketing and supply chain teams can lead to inefficiencies and negatively impact customer experience. This thesis proposes a conceptual framework to align customer demand and inventory management. The framework is examined in the empirical context of the fashion industry, focusing on the US market and insights from Brazil and Japan. By introducing a PDCA (Plan Do Check Action) process and cross-functional metrics, such as NPS (Net Promoter Score) and OTIF (On time in Full), this study seeks to encourage cooperation between departments and coalesce decision-making around enhancing customer experience. The research will explore the quantitative and qualitative aspects of the retail industry focusing on fashion and identify opportunities to leverage technology, marketing, and supply chain management for improved performance. Our study validated the existence of siloed operations and the drawbacks caused by silos in today’s business. Through 16 expert interviews, we identify three key factors that contribute to silos between marketing and supply chain. They are technology fragmentation, lack of integrated KPIs, and complexity of multiple channels. Further, the interviews helped uncover how the experts tackled these challenges in daily operations.

The expected deliverable is a framework that combines analyzed customer journeys with cross-functional metrics to support decision-makers in day-to-day operations. The goal is to deliver a world-class customer experience by aligning decisions to coordinate actions. There is potential to incorporate machine learning to suggest experiments and further optimize value delivery for 4  customers by retailers through multiple channels. Our conceptual framework applies to various businesses struggling with coordination between demand generation and fulfillment."
2023,Studying Electronic Textures with Coherent Lensless Imaging,https://hdl.handle.net/1721.1/152958,"X-ray microscopes have opened our collective eyes to the richness of nanoscale texture in systems such as correlated and quantum materials. These microscopes draw their power from the combination of short wavelengths, which provide high resolution, and interaction with atomic resonances, which makes them sensitive to subtle changes in electronic structure. However, x-ray microscopy remains an area where the main limits are technological rather than fundamental. Therefore, major progress is still possible with methodological improvements.

In the past twenty years, research has exploded into the use of coherent x-ray light to improve the quality and resolution of x-ray microscopes. In many cases, using coherent light makes it possible to remove the objective lens in a microscope, replacing it with an algorithmic analysis of the direct scattering data. This can increase the quality and resolution of the resulting quantitative images.

In this thesis, I present the results from a collection of projects aimed at using coherent imaging methods to study the real-space texture of electronic phases of matter with soft x-ray light. I first discuss the methods we developed and implemented to counteract the experimental errors that we found to be ubiquitous in our data, focusing on ptychography, the most commonly used lensless imaging method. Then, I turn to the development of an entirely new single-shot lensless imaging method, randomized probe imaging (RPI).

RPI has proven to be reliable and robust across a broad range of scenarios. The remainder of the thesis is devoted to applications of RPI at a free electron laser and a synchrotron. Also reported are further projects designed to improve the method, as well as attempts to expand our understanding of the mechanisms behind it and its limitations. I sincerely hope that the availability of RPI will help bring x-ray imaging to a broader group of scientists and lead to a better understanding of the nanoscale details of electronic texture."
2023,Systematic Modeling and Design of Sparse Deep Neural Network Accelerators,https://hdl.handle.net/1721.1/151571,"Sparse deep neural networks (DNNs) are an important computation kernel in many data and computation-intensive applications (e.g., image classification, speech recognition, and language processing). The sparsity in such kernels has motivated the development of many sparse DNN accelerators. However, despite the abundant existing proposals, there has not been a systematic way to understand, model, and develop various sparse DNN accelerators. 

To address these limitations, this thesis first presents a taxonomy of sparsity-related acceleration features to allow a systematic understanding of the sparse DNN accelerator design space. Based on the taxonomy, it proposes Sparseloop, the first analytical modeling tool for fast, accurate, and flexible evaluations of sparse DNN accelerators, enabling early-stage exploration of the large and diverse sparse DNN accelerator design space. Across representative accelerator designs and workloads, Sparseloop achieves over 2000× faster modeling speed than cycle-level simulations, maintains relative performance trends, and achieves ≤ 8% average modeling error. 

Employing Sparseloop, this thesis studies the design space and presents HighLight, an efficient and flexible sparse DNN accelerator. Specifically, HighLight accelerates DNNs with a novel sparsity pattern, called hierarchical structured sparsity, with the key insight that we can efficiently accelerate diverse degrees of sparsity (including dense) by having them hierarchically composed of simple sparsity patterns. Compared to existing works, HighLight achieves a geomean of upto 6.4× better energy-delay product (EDP) across workloads with diverse sparsity degrees, and always sits on the EDP-accuracy Pareto frontier for representative DNNs."
2023,SenseMate: An AI-Based Platform to Support Qualitative Coding,https://hdl.handle.net/1721.1/151980,"Unstructured data can be analyzed numerically or qualitatively through methods like sensemaking. One of the key stages of sensemaking is qualitative coding, where the data is divided into units, and each unit is assigned a category or code. Unfortunately, coding is tedious and time-consuming when carried out manually. Finding a balance between manual and fully-automated coding can help increase efficiency while allowing human judgment and preventing systematic machine errors. In this thesis, I propose an accessible semi-automated approach to qualitative coding. First, I apply a novel machine learning method, rationale extraction models, to qualitative coding. These models recommend themes for each unit of analysis in qualitative data and tend to perform better with less ambiguous themes. Through an online experiment, I find that assistance from rationale extraction models increases coding performance and reliability. Next, I execute an iterative, human-centered design process to create SenseMate, an AI-based platform for qualitative coding. After 13 user testing sessions and 3 design iterations, I observe that model overreliance can be minimized through cognitive forcing functions and easy-to-understand model explanations. I also design several ways for users to efficiently provide feedback on machine-generated rationales. To connect my model and design evaluations, I implement a prototype of SenseMate and conduct a summative user evaluation through an online experiment. The evaluation reveals that participants with access to AI assistance have higher coding performances but spend more time on the platform. The effectiveness of various design decisions within SenseMate is also explored. Finally, I discuss a myriad of future work possibilities. Overall, this thesis offers a practical and accessible solution to analyzing unstructured data, which has broad applications for researchers and organizations across various fields."
2023,1/f noise in MOSFETs with ultrathin gate dielectrics,http://hdl.handle.net/1721.1/13192,"Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 1992"
2023,A Robust and Efficient Framework for Slice-to-Volume Reconstruction: Application to Fetal MRI,https://hdl.handle.net/1721.1/151495,"Volumetric reconstruction in presence of motion is a challenging problem in medical imaging. When imaging moving targets, many modalities are limited to fast 2D imaging techniques that provide cross-sectional snapshots (2D images) of the subject with an attempt to ""freeze'' in-plane motion. However, inter-slice movement results in slice misalignment in 3D space, i.e., each image being an independent slice that fails to form a coherent volume for diagnosis and analysis. bTo this end, slice-to-volume reconstruction (SVR) has been proposed to reconstruct a high-quality 3D volume from misaligned 2D observations by performing inter-slice motion correction and super-resolution reconstruction. Existing SVR algorithms, however, have a limited capture range of slice motion and are time-consuming, particularly when producing high-resolution volumes.

This thesis proposes a motion-robust and efficient machine learning framework for SVR, motivated by the application of magnetic resonance imaging (MRI) in assessing fetal brain development. We first introduce a slice-to-volume registration transformer that models input slices as a sequence and performs inter-slice motion correction by simultaneously predicting rigid transformations of all images in 3D space. We then reformulate the reconstruction problem using implicit neural representation, where the underlying volume is represented by a continuous function of 3D coordinates. This resolution-agnostic approach allows efficient reconstruction of high-resolution volumes. Finally, we extend this method to data that suffer from non-rigid motion by introducing an implicit motion field that captures slice-dependent deformation. These advances together enable robust and efficient 3D reconstruction and visualization in fetal MRI, benefiting diagnosis and downstream analysis. Additionally, the proposed framework has the potential for broader clinical implications in various applications that involve similar volumetric reconstruction problems."
2023,A computational framework for emotion understanding,https://hdl.handle.net/1721.1/147271,"The organizing principle of this thesis is that human emotion understanding reflects a model-based solution to a large class of ill-posed inverse problems. To interpret someone's expression, or predict how that person would react in a future situation, observers reason over a logically- and causally-structured intuitive theory of other minds. For this work, I chose a domain that is perceptually and socially rich, yet highly constrained: a real-life high-stakes televised one-shot prisoner's dilemma.

In the first set of studies, I illustrate that forward predictions play a critical role in emotion understanding. Intuitive hypotheses about what someone is likely to feel guide how observers interpret and reason about expressive behavior. By simulating human causal reasoning as abductive inference over latent emotion representations, a parameter-free Bayesian model captured surprising patterns of social cognition.

In the second set of studies, I formalize emotion prediction as a probabilistic generative model. Mental contents inferred via the inversion of an intuitive theory of mind generate the basis for inferring how others will evaluate, or 'appraise', a situation. The Inferred Appraisals model extends inverse planning to simulate how observers infer others' reactions, in the terms of utilities, prediction errors, and counterfactuals on rich social preferences for fairness and reputation. I show that the joint posterior distribution of inferred appraisals provides a powerful method for discovering the latent structure of the human intuitive theory of emotions.

In the third set of studies, I build a stimulus-computable model of emotion understanding. This work emphasizes the importance of testing whether computational models can use emotion-relevant information in service of social cognition. I suggest that building computer systems that approach human-level emotional intelligence requires generative models, where inferred appraisals function as latent causal explanations that link behavior, mental contents, and world states."
2023,"A Closer Look at Classical Measurement, an Algorithm for Deliberation in Rodents, and a Conjecture on Intertemporal Choice",https://hdl.handle.net/1721.1/150703,"In this three-part thesis, Part I is an examination of the measurement process in classical Hamiltonian mechanics. This part is concerned with the tradeoff that exists, when measuring any observable of a system, between the disturbance inflicted upon the system and the information that can be extracted. The main result takes the form of a Heisenberg-like precision-disturbance relation: measuring an observable leaves all compatible observables undisturbed but inevitably disturbs all incompatible observables. The magnitude of the disturbance (the analogue of Ò) is found to be proportional, in a sense that is made precise, to one’s initial uncertainty in the ready-state of the apparatus—a quantity that relates to the temperature of the apparatus.

Part II of this thesis develops a model of the computations taking place in the deliberative decision-making system of rodents, during wakefulness and sleep, with focus on the role of hippocampus (HPC). In this model, medial prefrontal cortex performs high-level planning, and then tasks HPC with fleshing out the details of the plan, as needed. We describe this planning task of HPC as an optimal control problem, which allows us to draw insights from the powerful mathematics of optimal control theory. The model makes novel testable predictions, provides insights into memory consolidation during sleep, and offers a paradigm capable of accommodating a wide range of observed phenomena, such as the theta rhythm, the slow oscillation, spindle oscillations, sharp wave-ripples, θ-sequences, for-ward and reverse SWR-sequences, the formation and strengthening of episodic memories, and a need for two modes of operation—online and offline.

The two parts described above are the main content of this thesis. Part I falls within the purview of classical theoretical physics, while Part II falls in that of computational neuroscience. The two may seem unrelated; however, while each part is self-contained, I see the two as connected. Part III of this thesis is my attempt to provide an outline of a bigger picture, which sees the foregoing as lines of inquiry towards the same far-reaching conjecture—one which has had a strong pull on my imagination during my PhD, and which I hope to be able to address in the future. This conjecture is that the probability calculus of quantum mechanics holds a kind of normative status for a class of decision problems involving intertemporal choice under uncertainty—a class of problems of great importance to artificial intelligence, brain sciences, economics, and, I argue, to physics too."
2023,Visible-Light Integrated Photonics for 3D-Printing and Trapped-Ion Systems,https://hdl.handle.net/1721.1/152677,"Silicon photonics has enabled next-generation optical technologies that have facilitated revolutionary advances for numerous fields spanning science and engineering, including computing, communications, sensing, and quantum engineering. In recent years, the advent of visible-light integrated photonics platforms has opened up the potential for further diverse applications. This thesis builds upon these recent technologies to demonstrate novel applications of visible-light integrated photonics.

First, we combine the fields of silicon photonics and photochemistry to propose the first chip-based 3D printer, consisting of only a single millimeter-scale photonic chip without any moving parts that emits reconfigurable visible-light holograms up into a simple stationary resin well to enable non-mechanical volumetric 3D printing. This work presents a highly-compact, portable, and low-cost solution for the next generation of 3D printers.

Next, we propose integrated-photonics-based system architectures and the design of key integrated-photonics components for both polarization-gradient and electromagnetically-induced-transparency cooling of trapped ions. Further, we experimentally demonstrate a pair of polarization-diverse gratings and design the first integrated polarization rotators and splitters at blue wavelengths, representing a fundamental stepping stone on the path to advanced operations for integrated-photonics-based trapped-ion quantum systems involving multiple polarizations.

Finally, we demonstrate optical trapping and tweezing of microspheres and cancer cells using an integrated optical phased array for the first time, representing a two-orders-of-magnitude increase in the standoff distance of integrated optical tweezers and the first cell experiments using single-beam integrated optical tweezers."
2023,Computational and experimental methods for CRISPR-based saturation mutagenesis screens,https://hdl.handle.net/1721.1/152504,"Genetic variation is a powerful framework for functional characterization of the human genome. The emergence of CRISPR technology has enabled the efficient and diverse installation of genetic variation in situ, leading to its widespread use in functional genomics. The application of high-throughput CRISPR saturation mutagenesis screens for the functional interrogation of the coding and non-coding genome holds great promise in accelerating our understanding of how static DNA sequences encode and influence dynamic processes in human development and disease.

In this thesis, we focus on the development of computational and experimental methods for CRISPR-based saturation mutagenesis screens. First, we developed CRISPR screening uncharacterized region function (CRISPR-SURF), a deconvolution framework for the analysis of CRISPR saturation mutagenesis screens. Drawing inspiration from the field of signal processing, we propose the modeling of CRISPR perturbations across an underlying genomic regulatory signal by means of a convolution operation and apply CRISPR-SURF for the discovery of non-coding regulatory elements involved in gene regulation. Second, we developed PrimeDesign to facilitate the rapid design of prime editing (PE) guide RNAs and demonstrate its utility by using recommended designs to install pathogenic variants in human cells. Complementing PrimeDesign, we developed pegPool as a high-throughput pooled screening strategy for prime editing guide RNA (pegRNA) optimization. We demonstrate the generalizability of pegPool by assessing a total of >18,000 pegRNA designs, with up to 210 designs in a single pool, to identify high efficiency pegRNA constructs targeting genomic sites. Finally, we developed multiplexing of site-specific alterations for in situ characterization (MOSAIC) as a rapid non-viral method for saturation mutagenesis screens at single-nucleotide and codon resolution. Using MOSAIC, we demonstrate in situ saturation mutagenesis of the BCR-ABL1 oncogene to identify drug resistant variants and IRF1 untranslated region (UTR) to map non-coding regulatory elements involved in transcriptional initiation."
2023,Prospects for Quantum Equivariant Neural Networks,https://hdl.handle.net/1721.1/147273,"Convolutional neural networks (CNNs) exploit translational invariance within images. Group equivariant neural networks comprise a natural generalization of convolutional neural networks by exploiting other symmetries arising through different group actions. Informally, a linear map is equivariant if it transfers symmetries from its input space into its output space. Equivariant neural networks guarantee equivariance for arbitrary groups, reducing the system design complexity. Motivated by the theoretical/experimental development of quantum computing, in particular with the quantum advantage derived from other quantum algorithms/subroutines for group theoretic and linear algebraic problems, we explore the potential of quantum computers to realize these structures in machine learning. This work reviews the mathematical machinery necessary from group representation theory, surveys the theory of equivariance, and combines results in non-commutative harmonic analysis and geometric deep learning. Convolutions and cross-correlations are examples of functions which are equivariant to the actions of a group. We present efficient quantum algorithms for performing linear finite-group convolutions and cross-correlations on data stored as quantum states. Potential implementations and quantizations of the infinite group cases also discussed."
2023,Improving Predictability of Wind Power Generation,https://hdl.handle.net/1721.1/150204,"Wind energy plays an important role in decarbonizing the economy and increasingly accounts for a growing share of electricity supply in the United States. However, availability of wind resource is highly dependent on variable factors such as weather and local geographies, making wind power generation forecast a particularly difficult task. This adds to the challenge of grid management, which requires that the supply of electricity equates the demand at all times. Complicating the effort to improve wind power predicitability is a lack of empirical data, since wind power generation data are proprietary and often considered business secrets. To address this lack of empirical study, this thesis uses actual generation data between 2016 to 2021 from seven anonymized wind farms in Midwestern United States that range from 50MW to 235MW in size. The experiments demonstrate how machine learning methods can be used to forecast wind power generation at different time intervals, and how the accuracy of forecasting can be significantly improved when using a combination of newly extracted weather forecast data and weather measurement data. The economic benefits of more accurate forecasting are then studied using a using a simulation with market data from the Midcontinent Independent System Operator and the Southwest Power Pool. The thesis then explores whether predictability of wind power generation can be improved by placing weather stations closer to the wind forecast sites. Implications of these findings can inform investment decisions regarding weather monitoring stations and forecasting models, which can help electricity market participants adapt to a grid with an increasing share of renewable resources."
2023,"Proliferated Low Earth Orbit (pLEO) Satellite
Constellation Handover Cost Analysis",https://hdl.handle.net/1721.1/151410,"In any mobile network, handovers between routing nodes generally cause a reduction in available resources for users. This is very true of proliferated Low Earth orbit (pLEO) satellite constellation networks in which both the satellite and the user are mobile with respect to each other. As satellites travel in their obits, they move into and out of ground users’ views every few minutes [4], and mobile users can move into and out of satellite spot beams frequently as well. When existing communication between a user and its serving satellites (uplink and downlink) terminate, user data must be relayed to the next serving satellite, possibly incurring additional data transmissions and overhead in the form of network management and control actions for acquisition in the network. This issue is becoming more relevant as commercial companies building their own satellite networks must figure out an efficient handover strategy to reduce unnecessary data transmissions and handover overhead. In this thesis, I estimate the satellite handover cost by quantifying the number of transmission hops required to relay existing queued data to/from the next serving satellite. The handover cost of a satellite network will depend on factors such as the network topology and the handover algorithm itself. I will quantify the impact of the aforementioned factors on the satellite network handover cost. A lower handover cost generally implies that the overall monetary cost (capital expenditure and operational expenditure) of a network to the provider (and also the user) is lower as well."
2023,A Data-Driven Approach to Improve Optical Fiber Manufacturing: Focus on Core Deposition,https://hdl.handle.net/1721.1/150220,"This thesis presents an in-depth investigation on characterization of optical fiber preform core manufacturing and the identification of underlying trends in measured production data. While walking through the different operations involved in the process, we explained the challenges associated with insuring refractive index profile precision and glass purity. Starting with unsupervised learning, process by process, we applied linear and non linear dimensionality reduction algorithms (PCA and t-sne) to features matrices created from time series data and have been able to connect data clusters with context information like machines or month of the year. Then considering the core fabrication process as a whole, we studied the propagation of trends in the data sets up to quality measurements using Dice’s statistic to gauge similarities between samples sets. Finally, we developed some data-driven regression models in order to predict the refractive index measured at the end using data from all processes. As a result, Kernel algorithms performed the best and almost as well on raw statistics from all processes as on encoded information about machine sequences and dates. This supervised approach demonstrated some great potential for the development of prediction tools which could help design the optimized production line. An underlying objective is to support Sterlite Technologies Limited in using data-driven approach applied to process control for its plant in Waluj and Shendra starting by implementing good practices for variables measurement, logging and tracking."
2023,Learning Z-Order Indexes with Dynamic Bit Allocation,https://hdl.handle.net/1721.1/150189,"The Z-order curve is a space-filling curve that maps multi-dimensional data to singledimensional values. Z-order has been used in databases to sort multi-dimensional data. Modern data management systems such as Amazon Redshift and Databricks Delta Lake give users the ability to sort on multiple columns using Z-order. However, the Z-order is difficult to tune, with tunable parameters such as which columns to include in the Z-order. Currently, users must specify the columns for Z-order when using the systems and might not necessarily achieve the best performance, as the choice of columns has a significant impact on performance. Another drawback is that the systems give equal weight to the columns, which often does not result in the best performance due to the unequal impact columns have on query performance. Our work aims to automatically determine the best Z-order configuration for a particular dataset and workload. In this thesis, we introduce learning Z-order indexes using an approach we refer to as dynamic bit allocation, which considers not only which columns to include, but also the weight to put on each column. Our learned Z-order indexes outperform existing techniques by up to 11× in query time and up to 30.2× in rows scanned, revealing the potential of tuning Z-order to improve query performance."
2023,The Science and Art of Human and Artificial Intelligence Collaboration,https://hdl.handle.net/1721.1/152001,"While artificial intelligence (AI) appears to be surpassing the performance of human experts on a wide variety of games and real-world tasks, these algorithms are prone to systematic and surprising failures when deployed. In contrast to today’s state-of-the-art algorithms, humans are highly capable of adapting to new contexts. The different strengths and weaknesses of humans and AI motivate a guiding research question for the emerging field of human-AI collaboration: When, where, why, and how does the combination of human problem solving and AI systems lead to a hybrid system that surpasses (or fails to surpass) the performance of either humans or the machine alone? This dissertation addresses various dimensions of this guiding question by conducting large-scale, digital experiments across three distinct tasks and domains: deepfake detection, dermatology diagnosis, and Wordle. First, the experiments in deepfake detection examine the similarities and differences between human and machine vision in identifying visual manipulations of people’s faces in videos and identify important performance trade-offs between hybrid systems and human or AI only systems for deepfake detection. Second, the experiments in dermatology diagnosis reveal that non-visual information is often essential for diagnosing skin disease, diagnostic accuracy disparities across skin color exist in image-only store-and-forward teledermatology, and clinical decision support based on a fair deep learning system can significantly increase physicians’ diagnostic accuracy in this experimental setting. Third, the experiment on Wordle demonstrates that digitally mediated expressions of empathy can counteract the negative effect of anger on human creative problem solving. In addition to these digital experiments, this dissertation presents two algorithmic audits on clinical dermatology images to reveal where systematic errors arise in state-of-the-art algorithms, examines how context influences automated affect recognition, and proposes methods for more effectively incorporating context in applied machine learning. Together, these contributions provide empirical evidence for why human-AI collaborations succeed and fail across a variety of tasks and domains, insights into how to design human-AI collaborations more effectively, and a framework for when and where hybrid systems should rely on human problem solving."
2023,Integral Quadratic Constraints and Safety Certificates for Uncertainty Characterization and Control Safety-Aware Filtering of Proximity Operations Between Satellites,https://hdl.handle.net/1721.1/152485,"Techniques in robust optimization and formal verification methods are used (1) to examine the stability and robust performance of a satellite controller that considers six-dimensional, uncertain state, and often unmodeled dynamics during rendezvous and proximity operations, and (2) to explore the synthesis of control Lyapunov/barrier functions (CLFs/CBFs) using neural networks and stochastic gradient descent to provide safety-aware filtering for the fuel-optimal control policies. A linear quadratic regulator controller for a servicer satellite (Servicer) is analyzed via the dissipativity inequality principle and quadratic constraints. This method allows the capture of unmodeled dynamics to reduce system uncertainty of proximity operations among the Servicer, client satellite (Client), and unsafe regions (e.g., obstacle). The same controller is implemented with a finite time horizon (i.e., model predictive controller) to filter out unsafe control output during an autonomous inspection of a Client. This framework mitigates the collision risk based on integral quadratic constraints (IQCs) worst bounds recommendation, miss distance, Mahalanobis distance, and Probability of Collision (Pc) metrics. Innovative deterministic reachability methods based on integral quadratic constraints and neural Lyapunov functions are compared and connected. The novel contributions of this work focus on formulating mathematical safety guarantees, modeling controller output, and reducing uncertainty on system performance when designing fuel-optimal and safe maneuvers of Servicer around the Client while avoiding unsafe regions in LEO."
2023,Neighborhood Transformation Marginalization forOOD Detection,https://hdl.handle.net/1721.1/151532,"Out-of-distribution (OOD) detection is an important part of enabling the real world deployment of machine learning models. Many recent methods developed to perform OOD detection rely on calculating a score function on a given test point then thresholding the value to classify the point as in-distribution (ID) or OOD. However, calculating a score function on a single example may give biased or inaccurate estimates, especially as examples are sampled further and further OOD. In this paper we propose TraM: Transformation Neighborhood Marginalization, a method to improve the estimation of score functions used for OOD detection by calculating their expectation over a transformation neighborhood. TraM demonstrates improvements on a subset of commonly used OOD score functions in the OpenOOD benchmark, improving a baseline ODIN score function by up to 6 AUROC. However, it is not found to improve other baseline metrics signficantly, indicating the need for further research on this topic."
2023,Top-Down Synthesis for Library Learning,https://hdl.handle.net/1721.1/151374,"This thesis introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus. We present an implementation of the approach in a tool called Stitch and evaluate it against the state-of-the-art deductive library learning algorithm from DreamCoder. Our evaluation shows that Stitch is 3-4 orders of magnitude faster and uses 2 orders of magnitude less memory while maintaining comparable or better library quality (as measured by compressivity). We also demonstrate Stitch’s scalability on corpora containing hundreds of complex programs that are intractable with prior deductive approaches and show empirically that it is robust to terminating the search procedure early—further allowing it to scale to challenging datasets by means of early stopping. We publish the code, the documentation, a tutorial, and a Python library for interfacing with our for our Rust implementation of Stitch.

Tutorial & Documentation (Python Library): https://stitch-bindings.read thedocs.io/en/stable/intro/tutorial.html 

Rust Implementation: https://github.com/mlb2251/stitch 

Artifact (Awarded: Reusable): https://github.com/mlb2251/stitch-artifact"
2023,"Embedding StarLogo Nova into WISE for a Seamless
Student Experience",https://hdl.handle.net/1721.1/151359,"Support for teaching computational thinking has been increasing throughout K-12 schools as the world is being more utilized by computer technology [2]. The Scheller Teacher Education Program (STEP) at MIT uses educational technologies to create innovative learning experiences. An example project is StarLogo Nova, a block-based programming environment that facilitates the creation of agent-based models to study complex systems [17]. Currently StarLogo Nova is a website where students independently login and make projects for their models. However, the overall experience of using StarLogo Nova can be improved as there is no guidance when a student makes a model. In this thesis, we will augment StarLogo such that there will be a concept of activities, a user experience for students to receive instructions and answer questions while still allowing convenient interactions with a StarLogo project, such as editing or viewing a model. To do this, we integrate StarLogo into a platform called WISE (Web-based Inquiry Science Environment)."
2023,Scalable sketching and indexing algorithms for large biological datasets,https://hdl.handle.net/1721.1/147392,"DNA sequencing data continues to progress towards longer sequencing reads with increasingly lower error rates. In order to efficiently process the ever-growing collections of sequencing data, there is a crucial need for more time- and memory-efficient algorithms and data structures. In this thesis, we propose several ways to represent DNA sequences in order to mitigate some of these challenges in practical biological tasks. Firstly, we expand upon an existing k-mer (a substring of length k) -based approach, a universal hitting set (UHS), to sample a subset of locations on a DNA sequence. We show that UHSs can be efficiently constructed using a randomized parallel algorithm, and propose ways in which UHSs can be used in sketching and indexing sequences for downstream analysis. Secondly, we introduce the concept of minimizer-space sequencing data analysis, where a set of minimizers, rather than DNA nucleotides, are the atomic tokens of the alphabet. We propose that minimizer-space representations can be seamlessly applied to the problem of genome assembly, the task of reconstructing a genome from a collection of DNA sequences. By projecting sequences into ordered lists of minimizers, we claim that we can achieve orders-of-magnitude improvement in runtime and memory usage over existing methods without much loss of accuracy. We expect these approaches to be essential for downstream bioinformatics applications, such as read mapping, metagenomics, and pangenomics, as well as to provide ways to better store, search, and compress large collections of sequencing data."
2023,Privacy Law in Practice: Exploring Challenges to Modern Privacy Compliance,https://hdl.handle.net/1721.1/151849,"Modern privacy legislation covers a broad data scope and introduces technically challenging data management requirements. Computer science research has emerged to resolve technical challenges, but proposed system designs could benefit from deeper understandings of user workflows. Existing qualitative work to understand privacy compliance on the ground gives both reason for optimism and alarm. There is a growing community of knowledgeable privacy professionals, but their effectiveness is hindered by organizational dynamics. We conduct 10 semi-structured interviews of privacy experts to further understand challenges faced by privacy practitioners. We find key challenges arising primarily from misaligned organizational incentives and difficulty in policy interpretation. We urge organizations to invest in and empower privacy engineers, researchers to explore different design directions, and policymakers to enable greater user recourse against corporations. We hope our work can help enable privacy respecting institutions and systems."
2023,Data Attribution: From Classifiers to Generative Models,https://hdl.handle.net/1721.1/152676,"The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets. Moreover, existing methods are often tailored to the supervised learning setting, and are not well-defined for generative models.

In this thesis, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We first demonstrate the utility of TRAK across various modalities and scales in the supervised setting: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). Then, we extend TRAK to the generative setting, and show that it can be used to attribute different classes of diffusion models (DDPMs and LDMs)."
2023,Deep-learning Enabled Accurate Bruch’s Membrane Segmentation in Ultrahigh-Resolution Spectral Domain and Ultrahigh-Speed Swept Source Optical Coherence Tomography,https://hdl.handle.net/1721.1/147445,"Aged-related macular degeneration (AMD) and diabetic retinopathy (DR), the leading cause of significant vision loss worldwide, alter the retinal structure and capillary blood flow in eyes. Optical coherence tomography (OCT) and angiography (OCTA), the gold standard imaging modalities in ophthalmic clinics, enable the micrometer-scale visualization of retinal structure and vasculature and provide the ability for early detection and progression monitoring of retinal disease. Ultrahigh resolution, spectral domain OCT prototype (UHR SD-OCT) and ultrahigh speed, swept source OCT prototype (UHS SS-OCT) developed by our group provide the ability to visualize the fine structural changes in the outer retina and vascular changes in the retina respectively, which occur with the disease progression. A few of the most important clinical findings with AMD and DR, such as drusen and choriocapillaris (CC) blood flow deficit, are located adjacent to the Bruch’s membrane (BrM). BrM is a very thin (2–6 µm) extracellular matrix, which is generally not resolved in commercial OCT instrument and therefore challenging to perform segmentation and analysis. It is even more challenging when pathologic changes in retina distort its appearance and contrast. To qualitatively and quantitatively assess the pathologic changes adjacent to BrM, an accurate segmentation is required for robust analysis. This thesis presents an advanced automatic, deep learning-based segmentation framework. The study aims to generate an accurate BrM segmentation for quantitative analysis. The performance of the segmentation is evaluated on both healthy eyes and eyes with retinal diseases, and reproducibility / repeatability is assessed through consecutive repeated imaging sessions on patients as well as longitudinal imaging of patients. This study will facilitate the investigation of in vivo early structural / vascular biomarker for AMD and DR progression."
2023,Towards Precision Oncology: A Predictive and Causal Lens,https://hdl.handle.net/1721.1/152693,"Precision oncology promises personalized care for each patient based on a holistic view of their data. However, several methodological and translational advances are required for successful implementation of this vision in the clinic. These include building temporal models to predict a patient’s survival outcomes in response to therapy, validating these methods with experimental data from Randomized Controlled Trials (RCTs), quantifying the uncertainty in the predictions, and finally, exploring how these elements can be woven together into a clinical decision support tool. In this thesis, I explore each of these aspects in turn: i) first, I build different models of clinical time-series data, with a focus on prediction of survival outcomes and forecasting of core biomarkers, ii) next, I design methods to give additional “context” for these models, including uncertainty quantification of causal estimates and validation of these estimates using RCT data, and iii) finally, I study how these elements affect treatment decision-making via a controlled user study of a decision support tool prototype."
2023,Computational Modeling of Elastic and Transformation Incompatibility at Grain Boundaries in Shape Memory Materials,https://hdl.handle.net/1721.1/147221,"Shape memory alloys (SMAs) and zirconia-based ceramics (SMCs) find a wide range of applications in various fields due to their unique properties such as superelasticity and shape memory effect. Desirable superelastic properties of shape memory materials are realized to their maximum extent in single crystalline structures due to the absence of internal constraints. By contrast, in polycrystalline forms, superelasticity is significantly compromised by severe premature intergranular fracture originated at grain boundaries. This limitation has drawn significant research interests in developing microstructures that can preserve the properties of single crystals while avoiding the production cost and manufacturing limitations of single-crystal processing.

The overarching goal of the thesis is to improve our understanding of the competition between martensitic transformation, grain boundary constraints, and intergranular fracture in shape memory materials through comprehensive computational modeling. To this end, we developed a finite-element based framework for modeling martensitic transformation at the continuum level incorporating details of the micromechanical information. A single-crystal model is implemented to provide a full mechanistic three-dimensional description of both the anisotropic elastic and martensitic transformation stress-strain response, including the non-Schmid behavior observed in some types of SMCs. We used the geometrically nonlinear theory of martensite to identify all possible transformation systems in SMAs and SMCs, based on the knowledge of lattice parameters of the single crystal. In the case of SMCs, the model was calibrated against data obtained from compression tests of zirconia micropillars in previously published literature. We conducted finite element simulations to obtain detailed information on the nucleation and evolution of martensite variants and stress distribution at grain boundaries in both SMAs and SMCs. The simulation results also provide insights on the competing mechanisms of elastic and transformation incompatibility leading to severe stress concentration at grain boundaries. We identified grain boundary configurations which result in very large stress 3 concentrations at very low deformations due to elastic incompatibility, as well as others where the elastic incompatibility is relatively low and stress concentrations only occur at large transformation strains. We also showed how this approach can be used to explore the misorientation space for quantifying the level of elastic and transformation incompatibility at grain boundaries in both SMAs and SMCs. In addition, we investigated the correlation between different types of incompatibilities and grain boundary characteristics. In the particular case of SMAs, we explored the role that a coincident site lattice (CSL) may have in affecting grain boundary incompatibilities. We demonstrated that grain boundaries with low CSL order exhibit low elastic incompatibilities in Cu-based SMAs, as previously suggested from experimental observations. However, high CSL order grain boundaries result in incompatibilities that are commensurate with those exhibited by random grain boundary configurations. This approach could be used to identify misorientations that reduce or minimize grain boundary incompatibilities, thus extend the superelastic range of the material."
2023,A Doppler Radar Lock-in Demodulation Algorithm for Machine Vibration Sensing,https://hdl.handle.net/1721.1/150125,"Data-driven predictive maintenance of modern machinery has the potential to increase equipment lifespan and decrease manufacturing costs. Among various condition monitoring techniques, vibration analysis can effectively diagnose potential problems in machines. Doppler radar can be used as a sensor that provides non-contact, inexpensive real-time data collection without necessitating line-down time. Conventional Fast Fourier Transformation based vibration analysis requires large amounts of data to achieve high spectral resolution necessary for fault detection especially with radio frequency sampling, which can be computationally too expensive for analysis. In this work, we propose to use a sweeping lock-in amplifier to achieve high frequency resolution with small amounts of data by processing windowed sections of Doppler-shifted radio signals. This algorithm can reliably measure the Doppler shift frequency corresponding to the travelling speed of a low frequency moving object and identify the oscillation frequency with small amplitude, with the latter widely present in machine vibration. The distinguishing condition of the two cases is mathematically derived. The proposed algorithms are verified in simulation with triangular displacement waveform for simplicity of analysis and sinusoidal waveform for generic applications. For experimental verification, speaker vibration at a known frequency is analyzed to achieve an accuracy of 0.025 Hz within the known vibration frequency. This method is robust to the presence of noise frequencies and capable of detecting multiple frequencies."
2023,Integrated Heteroepitaxial Photodetectors,https://hdl.handle.net/1721.1/150248,"Optical detection in the near-infrared and telecommunication bands has historically been performed using single-crystal bulk Ge, but the development of Ge-on-Si epitaxy reduced fabrication costs and opened doors for usage in applications including optical communications and infrared imaging. To reap the benefits of monolithic integration and incorporation in the back-end-of-line (BEOL) stack, low processing temperatures (< 450°) are required. Using novel processing methods and strategic anneals, we have demonstrated that low temperature Ge growths on silicon can achieve low defect densities required for high performance. In this work, Ge-on-Si p-i-n photodetectors illuminated under normal incidence have demonstrated comparable responsivity and dark current density to devices processed at high temperatures. Relatively low temperature anneals (500°C) increased performance, but as-grown diodes also showed a responsivity of 0.11 A/W and [formula]. Annealing conditions of 500°C 3 hr improved such performance to 0.15 A/W and [formula].

In the mid-wave infrared (MWIR), photodetection has been successfully implemented for decades using the II-VI material set, Hg₁₋ₓCdₓTe. Extensive research pushed HgCdTe to nearly reach its theoretical performance limit, while also highlighting its inherent shortcomings for commercialization. An upcoming material set, [formula]  has the potential to overcome such barriers while promising comparable performance. In this work, Lumerical simulations were performed to optimize a waveguide-integrated photodetector that incorporated an [formula] homojunction and was straightforward to fabricate, assuming successful epitaxy growths. The photodetector design promoted 30% light absorption after 20 𝜇m propagation into the detection region."
2023,"Between the Lines: Encoding Relations Through Body, Tool, and Algorithm",https://hdl.handle.net/1721.1/151991,"The tools architects use orchestrate the discipline in seen and unseen ways. In recent decades, we have swapped early forms of mechanical drawing instruments for digital tools with unimaginable computing power. While this increased level of computational literacy allows us to script and code architectural forms more efficiently, it has also created incongruities between the computationally described object and material constructions. At times the digital tools we depend on today go as far as defining the aesthetic of our buildings. To complicate this further, the digital tools most often solicited by the architectural practice are non-native imports adapted for their visual potential and practical uses. Meaning embedded within the programming of tools that shape our buildings are residual values of other disciplines. For example, we can trace the origins of CAD software back to engineers and mathematicians at Boeing and here at MIT, who sought to mechanize the construction of splines and irregular curved surfaces for the production of slipstream automobiles, toothbrushes, and even letterforms. And much like the hidden algorithms in the background of our digital tools, there is an apparatus of choreography surrounding our physical tools that encode instructions on how the body engages with the object. In other words, the machines we use produce not only drawings but gestures as well, keying us into the always-present yet rarely discussed embodied dimensions of tools. 

To expand upon the embodied dimensions of our tools today, we need to reconsider the machine as the site of intervention. Motion data and performance envelopes surrounding our tools extend beyond the projective reenactment of the machine and offer us a means to measure the derivative of what it takes to produce a drawing, a surface, or a construction. This thesis dislocates the spline from its formal geometry associated with slipstream construction and recasts it as a way to record the tumble-type inscriptions surrounding an object’s performance — a tactic to mutually mark and negotiate the activity between humans and machines."
2023,Technical and experimental design for electricity conservation policy : continuous information feedback,http://hdl.handle.net/1721.1/15003,"Thesis: M.S., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 1986"
2023,Model development based on discrete particle simulations of partially- and fully-saturated granular media,https://hdl.handle.net/1721.1/150048,"Granular materials are ubiquitous in industrial and geophysical scenarios. At a high computational expense, the discrete element method (DEM) simulates granular materials with a high accuracy by tracking individual particles. At the other extreme, empirical formulas based on dimensional analysis and continuum models are convenient to be applied to large scale problems, but calibrations may be needed. In this thesis, DEM simulations are carried out as virtual experiments to study the particle-scale physics and then guide the formulation of empirical relations or continuum models for two applications.

Dynamic similarity, commonly applied in fluid systems, has recently been extended to locomotion problems in granular media. Our previous research was limited to locomotors in cohesionless, flat beds of grains under the assumption of a simple frictional fluid rheology. However, many natural circumstances involve beds that are sloped or composed of cohesive grains. Expanded scaling relations are derived and DEM simulations are performed as validation, with inclined beds and cohesive grains using rotating “wheels” of various shape families, varying size and loading conditions. The data show a good agreement between scaled tests, suggesting the usage of these scalings as a potential design tool for off-road vehicles and extra-planetary rovers, and as an analysis tool for bio-locomotion in soils.

In the bedload sediment transport process, the variability in the relation between sediment flux and driving factors is not well understood. At a given Shields number, the observed dimensionless transport rate can vary over a wide range in controlled systems. A two-way coupled fluid-grain numerical scheme has been validated against physical experiments of spherical sediment particles. It is used to explore the parameter space controlling sediment transport in simple systems. Examination of fluid-grain interactions shows fluid torque is non-negligible near the threshold. And the simulations guide the formulation of continuum models for the bedload transport and the creep flow. Furthermore, a numerical scheme has been developed to simulate the transport of natural shaped sediment particles. Conglomerated spheres, approximating the real shapes from CT scanning, are constructed in DEM and coupled with the fluid solver. Agreement with the corresponding flume experiments is observed."
2023,Data Science in Investment Management,https://hdl.handle.net/1721.1/152807,"In this thesis, titled ""Data Science in Investment Management,"" we aim to explore the applications of data science and artificial intelligence across various dimensions of investment management, offering innovative solutions and insights to the industry. This thesis is composed of several parts, each addressing a different aspect of investment management and leveraging data science techniques to deliver valuable insights.

In first part, for industries and crypto-currencies, we develop a dynamic classification system that groups stocks according to quantified similarities from a wide variety of structured and unstructured data features. With the availability of big data, we were able to use artificial intelligence (AI) methods to extract relevant information about companies from various data sources and learn about their similarity in the future, according to market perception. In second part, we study ways of creating capital and portfolio management for fusion energy and biopharmaceutical investments. By leveraging computational techniques like portfolio approach, we provide novel insights into the optimal financing strategies for high-risk, high-reward ventures like fusion research and biopharmaceutical investing. We also quantify the impact of clinical trial results on the stock prices of the companies, that can aid biopharma investors in risk management. Given the increasing interest in ESG investing, we study the excess-returns of the ESG investing. We also develop the measure of the impact on patient lives due to the products of the biopharmaceutical companies that can attract ESG funds for biopharmaceutical companies. Next part of the thesis investigates the real-time psychophysiological analysis of financial risk processing, offering a deeper understanding of human behavior in the context of investment decision-making using a data driven approach. In the next part, we focus on the use of explainable Machine Learning for an important problem of consumer credit risk. In the final part, we conclude with the discussion about the future of Artificial Intelligence and Data Science in Finance."
2023,zk-Sigstore: System for Anonymous Certificate-Based Software Signing,https://hdl.handle.net/1721.1/151609,"Most software developers get their software dependencies from online repositories, allowing for greater efficiency during the development process. However, downloading software from the internet comes with security concerns, and issues with open source software security have led to several high-profile attacks. In order to combat the problem, many repositories have implemented digital signatures for packages to verify the contributor’s identity, but with limited success due to well-documented usability issues surrounding key management. The digital signature primitive itself also does not provide an answer to which signers have the authority to sign which artifact. Proposals like Sigstore aimed at fixing the usability problems with digital signatures come with privacy concerns that have limited uptake, and though they provide some answers to the signing authority question, these come with scalability, verifiability, and privacy concerns.

This thesis presents zk-Sigstore, a system for usable (certificate-based) and anonymous digital signatures for software. zk-Sigstore is a certificate-based signature system, but instead of publishing identities in the clear, identities are obfuscated with a cryptographic commitment. Techniques from key transparency verifiable key directories inform a scalable, verifiable, and private authorization record for mapping digital artifacts to the maintainers with the authority to sign them.

Using zk-Sigstore for software signing, signing and verifying times are on the order of hundreds of microseconds even for the largest of software repositories, and deployment of zk-Sigstore requires minimal changes to existing infrastructure, making it a practical solution to this real-world problem."
2023,"Visualization and Behavioral Testing Of Common
Sense Generative Programs",https://hdl.handle.net/1721.1/152834,"Probabilistic generative programs are powerful tools that allow for modeling complex 3D worlds containing objects and agents. Recent advances in these programs have resulted in creation of rich models whose traces represent 3D scenes, but there exist challenges in using visualizations and simulation tools for practical implementations. In this thesis, I describe the development of infrastructure to accelerate research in this area. Specifically, I present a pipeline for synthetic data generation with physics simulation capabilities and a suite of rendering options. By leveraging existing scene graph generators and multiple visualization engines, photorealistic datasets can be produced to evaluate probabilistic generative programs and create stimuli for gathering information on human behavior. This framework allows fine-grained temporal tracking of object poses and velocities, both with and without occlusion, facilitating the collection of rich human behavioral data on dynamic object tracking. More broadly, the tools developed here provide visualization, debugging capabilities, and configurable synthetic datasets to benchmark future progress in 3D scene understanding. Development of this infrastructure is an investment in improved synthetic data generation and analysis frameworks is an important step toward robust probabilistic generative programs for 3D world modeling."
2023,Optimization of Throughput in Sheet Metal Manufacturing by Tuning the Sheet Metal Nesting Strategy Based on Sheet Utilization and Downstream Part Handling Costs,https://hdl.handle.net/1721.1/152702,"Sheet metal fabrication has become a fundamental process in modern engineering due to its versatility and is used across a wide range of industries. Nesting a given set of sheet metal blanks onto raw material sheets is a major cost driver as it determines the amount of usable metal and the rest of the sheet is thrown away as scrap. Nesting algorithms are very effective at identifying the most efficient layout of a given set of parts to maximize the sheet utilization. Hence, material utilization of the sheet is mainly defined by the number of parts being nested and their geometries. On one hand, nesting algorithms would prefer having a large number of grouped parts that allow them to make more efficient sheet metal nests due to more possible combinations of parts on a given sheet. On the other hand, the downstream sorting process which sends the parts to their respective further processing stations would prefer having fewer number of grouped parts as the parts get nested randomly which increases the time spent on the non value add activity. Therefore, an effective nesting strategy between the two extremes is necessary to balance the sheet utilization with the intensive sorting requirements to make the process cost effective and meet the required throughput. In this thesis, a sheet metal nesting strategy is identified for a manufacturing operation with a wide variety of products and plant locations across the globe. Cost and throughput models are produced which inform the selection of a globally optimized nesting strategy. Regional differences in cost drivers such as varying labor rates and raw material costs are considered, and an optimized nesting strategy is validated for deployment across global plant locations. This work provides a detailed approach to optimizing sheet utilization in sheet metal manufacturing through selection of an optimized nesting strategy."
2023,Permutation-based Significance Tests for Multi-modal Hierarchical Dirichlet Processes with Application to Audio-visual Data,https://hdl.handle.net/1721.1/152853,"Complex underlying distributions in multi-modal data motivate the need for data fusion methods that integrate observations of different modalities in a meaningful way. We explore the multi-modal hierarchical Dirichlet process (mmHDP) mixture model as a Bayesian non-parametric approach to data fusion. In particular, we elaborate on its censored-data perspective, which aligns groups of observations at a group level to accommodate for missing data in any modality. To explore the model behavior, we develop a processing pipeline that applies the mmHDP to audio-visual data, a common and practical multi-modal system. We apply this pipeline to musical data with known audio-visual relationships and provide in-depth qualitative analyses on the learned model parameters. Because of its non-parametric and unsupervised clustering nature, it can be difficult to quantify the significance of the learned mmHDP structure. We propose a novel permutation testing framework that empirically measures the significance of the mmHDP structure and demonstrate its viability using both synthetic and real audio-visual data. The results convey that the mmHDP model captures meaningful structure in the audio-visual data and that the permutation testing framework is a viable method for quantifying model significance."
2023,Integral Equation-Based Inverse Scattering and Coil Optimization in Magnetic Resonance Imaging,https://hdl.handle.net/1721.1/152694,"One trend in Magnetic Resonance Imaging (MRI) over the years has been to steadily increase the static magnetic field strength and hence the frequency of operation, resulting in higher available signal-to-noise ratio that could be traded for shorter scan times and increased image quality. In the ultra-high field regime (≥7T), since the radiofrequency wavelength is comparable to the dimensions of body, quasi-static approaches cannot be used to simulate the interactions between electromagnetic field and biological tissue, which can result in unwanted energy deposition hot spots and in decreased image quality. The electrical properties of tissue (permittivity and conductivity) influence these interactions and the RF field distributions inside of the body. Although undesirable from the point of view of coil and pulse design, this dependence on EP opens the door to new imaging modalities using the same MR data. In this thesis, I detail how we applied highly accurate integral equation formulations to the tasks of 3D electrical properties estimation (inverse scattering) and parallel transmit (pTx) coil array optimization. I also present novel regularization strategies that are ideally suited for inverse problems. I also discuss how we validated these approaches with numerical examples, and the efforts that we undertook to estimate electrical properties of a phantom using data from an MR scanner."
2023,Magnetothermal Modulation of Nerve Growth,https://hdl.handle.net/1721.1/150562,"Magnetic nanoparticles (MNPs) provide several mechanisms for wireless neuromodulation. MNPs under applied AC magnetic fields (AMFs) exhibit hysteresis loss, which can activate heat-sensitive ion channels such as TRPV1. This magnetothermal modulation requires AMFs with peak field strengths of up to 40 kA/m and frequencies of up to 580 kHz. While air–gap magnetic cores can achieve the necessary field parameters, their small size limits them to in–vitro experiments. A 10 cm coreless solenoid design generates the desired field parameters and is suitable for in–vivo experiments but requires several kilowatts of power. Here, we show the construction of a resonant tank inverter capable of delivering 6000 Watts of power at 600 V and 10 A to the tank circuit and generating the requisite AMF field strength and frequency inside the coil.

As a first experiment, we use the apparatus to demonstrate wireless, magnetothermal modulation of dorsal root ganglia (DRG) explants, sensory neuronal structures that are a critical target for nerve therapy. Calcium influx into neurons plays a key role in many processes necessary for axonal regeneratiton. Using magnetothermal modulation, we stimulate calcium uptake into DRG cells via TRPV1 ion channels, which are endogenously expressed and heat sensitive. By adjusting the pulse pat-tern of magnetic stimulation, we find the optimal conditions for inducing neurite outgrowth in DRG cultures."
2023,Data Augmentation and Conformal Prediction,https://hdl.handle.net/1721.1/151275,"Conformal prediction is a popular line of research in uncertainty quantification. Conformal predictors output sets of predictions accompanied by a guarantee that the set contains the true label. Conformal prediction is particularly promising because it makes no distributional assumptions and requires only a black-box classifier to produce sets with this type of guarantee. Unfortunately, existing conformal predictions can produce uninformatively large prediction sets for certain examples, which limits their applications to real-world contexts. In this thesis, we explore the impact of data augmentation, a popular computer vision technique, on the performance of conformal predictors. In particular, we present multiple ways of combining data augmentation with conformal prediction by introducing five methods of test-time-augmentation-enhanced conformal prediction (TTA-CP). We find that certain TTA-CP methods can improve upon the size and stability of prediction sets created by traditional conformal prediction. Using ImageNet and Fitzpatrick 17k, two datasets differing in size, complexity, and balance, we reveal dataset-dependent decisions that are key to improving performance in conformal prediction."
2023,Defio: Instance-Optimized Fusion of AWS Database Services,https://hdl.handle.net/1721.1/151479,"Building large-scale data infrastructures is hard: There are often more than a single type of workloads and business requirements, but unfortunately, “one size does not fit all”. Modern database systems tend to specialize towards a specific type of workload, and thus organizations are left to integrate these differently-specialized database systems in order to achieve sufficient performance for all of their use cases and workloads.

This kind of hybrid architecture—also known as Data Mesh architecture—often leads to the increasing complexity of maintaining and utilizing database services, both for the data engineers and the end users. However, we believe that some of this complexity can be abstracted away from the end users, in particular with respect to query routing, i.e. determining where to execute each individual SQL query among the multiple database engines.

To overcome this challenge, we propose Defio, a unified interface to multiple specialized database engines that can intelligently handle myriads of workloads without having the end users think about the underlying execution of each query. Specifically, this thesis focuses on the design and implementation of an instance-optimized query router, which ultimately enables Defio to take advantage of the performance benefits of each specialized database in a Data Mesh architecture—resulting in what we call a fusion of database services."
2023,Multi-Modal Transit Time Prediction for E-Commerce Fulfillment Optimization and Carbon Emissions Reduction,https://hdl.handle.net/1721.1/151341,"Consumers are purchasing an increasing amount of goods through digital channels as compared to brick and mortar and expect fast, reliable delivery. At the same time, society is facing the urgent challenge of reducing carbon emissions to limit global warming to levels considered safe by climate scientists. A global sportswear retailer is investing in improving the digital consumer experience while meeting its aggressive 2030 carbon reduction goals. This work studies how machine learning can be used to both improve the retailer’s digital fulfillment operations and reduce their carbon emissions footprint. It focuses on enhancing the decision-making used to select a distribution center to fulfill a consumer’s order from, and aims to do so by increasing the accuracy of a key input into that process. Specifically, the work targets accuracy improvement of transit time estimates, which quantify the number of days between a parcel’s carrier induction and delivery.

Machine learning techniques are leveraged to develop a model for predicting transit times. Model development begins with data preparation, which is inclusive of sourcing, cleaning, sampling and feature engineering. It then continues with a series of experiments to provide insights into favorable model design elements. A final model is created under consideration of experimentation results. This model is associated with an accuracy of 67%, which is a improvement beyond the current state accuracy of 45%. A counterfactual analysis is conducted to assess the impact of improved transit time estimates on key fulfillment metrics. On a one month sample, the model enables improved fulfillment decisions; namely ones that are associated with a 4.5% decrease in lead time, a 3% reduction in CO2 emissions, and a 1.5% reduction in cost."
2023,Provably near-optimal algorithms for multi-stage stochastic optimization models in operations management,http://hdl.handle.net/1721.1/77827,"Many if not most of the core problems studied in operations management fall into the category of multi-stage stochastic optimization models, whereby one considers multiple, often correlated decisions to optimize a particular objective function under uncertainty on the system evolution over the future horizon. Unfortunately, computing the optimal policies is usually computationally intractable due to curse of dimensionality. This thesis is focused on providing provably near-optimal and tractable policies for some of these challenging models arising in the context of inventory control, capacity planning and revenue management; specifically, on the design of approximation algorithms that admit worst-case performance guarantees. In the first chapter, we develop new algorithmic approaches to compute provably near-optimal policies for multi-period stochastic lot-sizing inventory models with positive lead times, general demand distributions and dynamic forecast updates. The proposed policies have worst-case performance guarantees of 3 and typically perform very close to optimal in extensive computational experiments. We also describe a 6-approximation algorithm for the counterpart model under uniform capacity constraints. In the second chapter, we study a class of revenue management problems in systems with reusable resources and advanced reservations. A simple control policy called the class selection policy (CSP) is proposed based on solving a knapsack-type linear program (LP). We show that the CSP and its variants perform provably near-optimal in the Halfin- Whitt regime. The analysis is based on modeling the problem as loss network systems with advanced reservations. In particular, asymptotic upper bounds on the blocking probabilities are derived. In the third chapter, we examine the problem of capacity planning in joint ventures to meet stochastic demand in a newsvendor-type setting. When resources are heterogeneous, there exists a unique revenue-sharing contract such that the corresponding Nash Bargaining Solution, the Strong Nash Equilibrium, and the system optimal solution coincide. The optimal scheme rewards every participant proportionally to her marginal cost. When resources are homogeneous, there does not exist a revenue-sharing scheme which induces the system optimum. Nonetheless, we propose provably good revenue-sharing contracts which suggests that the reward should be inversely proportional to the marginal cost of each participant."
2023,First Step into A New Physics Realm: Search for the Majorana Nature of Neutrinos in the Inverted Mass Ordering Region,https://hdl.handle.net/1721.1/150760,"The search for neutrinoless double-beta decay (0𝜈𝛽𝛽) is the only way to prove the Majorana nature of neutrinos and is thus a major area of interest for neutrino physics. Discovering 0𝜈𝛽𝛽 and measuring its half-life will be the first solid evidence for physics beyond the Standard Model (BSM) and lead to a plethora of new theoretical and experimental investigations.

This dissertation contains both theoretical and experimental work. The theoretical calculation of the non-perturbative nuclear matrix element for 0𝜈𝛽𝛽 is done using lattice quantum chromodynamics (LQCD) to interpret the experimental data. The preliminary results of 𝑛⁰𝑛⁰ → 𝑝⁺𝑝⁺𝑒⁻𝑒⁻ process for both long-range (light Majorana neutrino exchange) and short-range (heavy Majorana neutrino exchange) contributions are obtained.

The experimental work focused on KamLAND-Zen 800. It is one of the leading efforts with data from an exposure of 970 kg·yr of ¹³⁶Xe. Machine learning methods are employed to discriminate background events in data and generate new simulations for future study. With no 0𝜈𝛽𝛽 signal excess over the background expectation, statistical properties are extracted by a Bayesian analysis utilizing Markov Chain Monte Carlo (MCMC) algorithm. The lower limit for the 0𝜈𝛽𝛽 half-life of [formula] at 90% C.I., corresponding to the effective neutrino mass range of 38.4-160.0 meV, which is the first search in the inverted mass ordering region."
2022,Quasistatic computing environments,http://hdl.handle.net/1721.1/38790,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1996."
2022,A plan for remodeling an industrial power plant,http://hdl.handle.net/1721.1/51558,"Thesis (B.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1930."
2022,Synthesizing Object Models from Natural Language Specifications,https://hdl.handle.net/1721.1/144829,"Program synthesis has traditionally excelled in tasks with precise specifications such as input-output examples and formal constraints by using structured and algorithmic approaches based on enumerative search and type inference. However, traditional synthesis techniques have no mechanism of incorporating real-world knowledge, which is commonplace in software engineering. Motivated by this, we introduce a new synthesis task known as specification reification: synthesizing concrete realizations of vague, high-level application specifications. We focus on a specific instance of this: generating object models from natural language application descriptions. Towards this goal, we present three approaches for object model synthesis that leverage domain knowledge from the GPT-3 language model. In addition, we design a scoring metric to evaluate the success of synthesized object models on seven sample tasks such as classroom management and pet store applications. We demonstrate that our language-model-based synthesizers generate object models that are comparable in quality to human-generated ones."
2022,Mathematical programming and electrical networks,http://dspace.mit.edu/handle/1721.1/13366,"Thesis (Sc. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1959."
2022,A demonstration of a formal specification & requirements language : a case study,http://hdl.handle.net/1721.1/86852,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2001."
2022,Dielectric Resonator Antennas : theory and design,http://hdl.handle.net/1721.1/36782,"Theoretical models for the analysis of Dielectric Resonator Antenna (DRA) are developed. There are no exact solutions to many of the problems in analytical form, therefore a strong focus on the physical interpretation of the numerical results is presented alongside theoretical models. I have used the physical interpretation of the numerical results to lay down some important design rules. A few new inventions associated with the DRA are also included. These are the elliptical DRA, the DRA with a rectangular slot, the adjustable reactance feed, the triangular DRA and the dual band DRA-patch antenna."
2022,Magneto-thermal Transport and Machine Learning-assisted Investigation of Magnetic Materials,https://hdl.handle.net/1721.1/142832,"Heat is carried by different types quasiparticles in crystals, including phonons, charge carriers, and magnetic excitations. In most materials, thermal transport can be understood as the flow of phonons and charge carriers; magnetic heat flow is less well-studied and less well understood.

Recently, the concept of the flat band, with a vanishing dispersion, has gained importance. Especially in electronic systems, many theories and experiments have proven that some structures such as kagome or honeycomb lattices hosts such flat bands with non-trivial topology. Even though a number of theories suggest that such dispersionless mode exist in magnonic bands under the framework of the Heisenberg spin model, few experiments indicate its existence. Not limited to these flat band effects, magnetic insulators can assume a variety of nontrivial topologies such as magnetic skyrmions. In this thesis, I investigate the highly frustrated magnetic system Y0.5Ca0.5BaCo4O7, where the kagome lattice could potentially lead to nontrivial thermal transport originated from its flat band. While we do not observe signatures of the flat band in thermal conductivity, the observed anomalous Hall effect in electrical transport and spin glass-like behavior suggest a complex magnetization-transport mechanism.

Motivated by the rapid advancement of artificial inteligence, the application of machine learning into materials exploration is recently investigated. Using a graphical representation of crystallines orginally suggested in Crystal Graphical Convolutional Neural Network (CGCNN), we developed the ML-asssited method to explore magnetic compounds. Our machine learning model can, so far, distiguish ferromagnet or antiferromagnet systems with over 70% accuracy based only on structual/elemental information. Prospects of studying more complex magnets are described."
2022,"Graphs, Principal Minors, and Eigenvalue Problems",https://hdl.handle.net/1721.1/140006,"This thesis considers four independent topics within linear algebra: determinantal point processes, extremal problems in spectral graph theory, force-directed layouts, and eigenvalue algorithms. For determinantal point processes (DPPs), we consider the classes of symmetric and signed DPPs, respectively, and in both cases connect the problem of learning the parameters of a DPP to a related matrix recovery problem. Next, we consider two conjectures in spectral graph theory regarding the spread of a graph, and resolve both. For force-directed layouts of graphs, we connect the layout of the boundary of a Tutte spring embedding to trace theorems from the theory of elliptic PDEs, and we provide a rigorous theoretical analysis of the popular Kamada-Kawai objective, proving hardness of approximation and structural results regarding optimal layouts, and providing a polynomial time randomized approximation scheme for low diameter graphs. Finally, we consider the Lanczos method for computing extremal eigenvalues of a symmetric matrix and produce new error estimates for this algorithm."
2022,Predicting Audience Tweet Engagement,https://hdl.handle.net/1721.1/143330,"Social media has become the ubiquitous infrastructure through which the world is connected. It allows people to interact not only with family members and friends but also with prominent figures like movie stars, presidential candidates, and even royalty. These celebrities have immense presences on social media, and each post they share has the potential to reach millions of people. As the sphere of social media influence grows increasingly large, it also becomes increasingly important to be able to understand how influencers on social media affect their audience. However, it is difficult for individuals with large social media platforms to gain insight into how their posts influence their followers. While social media platforms do provide influencers with some audience breakdowns and statistics, they are often not granular enough to be useful. In this thesis, we present methods to analyze an influencer’s tweets and audience. We then use these results to predict which segments of an influencers audience will interact with different types of posts. These insights can help determine which areas an influencer has the greatest potential to make an impact in and thus guide the direction and content of influencer campaigns."
2022,Design and Analysis of a Novel Wave Energy Converter With a Tension Leg Platform and Oscillating Proof Masses,https://hdl.handle.net/1721.1/143342,"A design of novel wave energy converter with an oscillating proof mass and an electromagnetic power takeoff mechanism was considered. The wave energy converter has two parts, a tension leg platform connected by tether lines to the sea floor and inside of it, proof mass oscillators with motions which are coupled to those of the tension leg platform. In order to simplify the analysis, the system was constrained to only oscillate in the direction of surge. Complex hydrodynamic forces caused by ocean waves will excite the system and the surge motion of the proof mass relative to the tension leg platform will generate power via the electromagnetic power takeoff mechanism. First a model of the system with a linear restoring force exerted on the proof mass is analyzed using linear theory. Following the development of the linear theory, a more complex model with a nonlinear restoring force was considered. Using both a frequency-domain approach and a time-domain simulation, the average power of these systems were calculated. To further maximize power, a control circuit and control law are introduced which increase the average power by multiple factors. By introducing nonlinear restoring force and a control law, the performance of the system was shown to be further improved."
2022,Uncertainty-Based Design Optimization and Decision Options for Responsive Maneuvering of Reconfigurable Satellite Constellations,https://hdl.handle.net/1721.1/145138,"There are many time-sensitive mission applications for persistent satellite coverage, including dynamic and unpredictable events such as natural disasters, oil spills, extreme weather events, or geopolitical conflicts, which may progress rapidly and require frequently-updated information to co-ordinate the ground response. Reconfigurable satellite constellations can provide on-demand regional coverage by maneuvering orbits to focus passes over the area of interest. In contrast, traditional satellite constellations cannot maneuver to pass over specific ground locations, meaning that achieving persistent coverage spanning all possible locations of interest globally results in a requirement for thousands of satellites. This would present prohibitive costs for many applications, as well as contributing to worsening issues of space traffic management and congestion in Low Earth Orbit (LEO).

Incorporating reconfigurability into constellation design allows for responsive maneuvering of satellites into repeating ground tracks (RGTs) over a location of interest, simultaneously reducing the required constellation size by improving the utilization of individual satellites and providing flexibility in the achievable ground coverage. Past work on reconfigurable constellations (ReCon) demonstrated average cost savings of 20-70% compared to iso-performance static constellations, although the complexity of the solution space for the design optimization process limited the maximum size of constellations that could be evaluated.

In this thesis, a probabilistic performance metric is developed to compare constellation designs, adopting principles of reliability-based design optimization to quantify the confidence level that reconfigurable designs will outperform iso-cost static alternatives and by what margin of performance. The results show that 74.2% of reconfigurable designs outperform iso-cost static designs with a confidence level of 90% or higher, and with a margin of at least 10% improvement in the level of performance achieved. Computational intensity of the model presents the major constraint upon the size and complexity of simulation cases that may be modelled, so variance reduction techniques are applied to lower the standard error of mean performance in the output, allowing for a reduction in optimization size and runtime while maintaining the same level of error in the predicted results. Decision options for the operational phase of a reconfigurable constellation are presented and assessed to characterize how satellite operators must weigh mission priorities to evaluate trade-offs between propellant conservation and improved coverage of high-value targets."
2022,Reducing intraday patient wait times through just-in-time bed assignment,http://hdl.handle.net/1721.1/99014,"Massachusetts General Hospital (MGH) is the oldest and largest hospital in New England as well as the original and largest teaching hospital of the Harvard Medical School. The neuroscience units experience patient flow issues similar to those observed throughout MGH, including high bed utilization and long intraday patient wait times. This project focuses on the neuroscience units as a microcosm of the hospital. MGH consistently operates near capacity. Patients from the emergency department, the perioperative environment, intensive care units (ICUs) and other sources compete for beds. The admitting department manages the bed assignment process across MGH. Assignments are often made without access to all relevant information, such as expected admission, surgery and discharge timing. As a result of common procedures, patients are frequently assigned to a bed before they are clinically ready to move. Our analysis reveals that suboptimal bed assignment and patient transfer processes are among the leading root causes of intraday patient delays. The primary objective of the project is to develop a bed assignment policy to reduce intraday patient wait times. The policy consists of a bed assignment algorithm and enabling bed management processes. To account for patient acuity, the algorithm segments patients by movement (e.g., ED-to-ICU). The target maximum wait for each segment is the acceptable wait length (AWL). The algorithm ranks patients based on their ready times and the AWLs, and assigns beds primarily on a just-in-time (JIT) basis. The enabling bed management processes include small-scale early discharge and early transfer interventions to better align the intraday timing of demand for inpatient beds with available capacity. A simulation of neuroscience patient flow is used to evaluate different approaches. The model shows that adoption of the JIT policy would increase the percentage of patients who experience bed waits within the AWL for all movement types. Predicted bed waits for patients who require ICU-level care would be 30 minutes or less for 90% of ED patients and 95% of OR patients (improvements from historical baselines of 44% and 91%, respectively). Predicted bed waits for transfers to floor beds would be two hours or less for 81% of ED patients and 93% of OR patients (improvements from historical baselines of 63% and 84%, respectively). The solution significantly reduces intraday patient wait times without a major increase in hospital capacity."
2022,Double-gated field emission arrays,http://hdl.handle.net/1721.1/30099,"There is a need for massively parallel, individually addressed and focused electron sources for applications such as flat panel displays, mass storage and multi-beam electron beam lithography. This project fabricates and characterizes double-gated field emission devices with high aspect ratio. One of the gates extracts the electrons while the second gate focuses the electrons into small spots. High aspect ratio silicon field emitters were defined by reactive ion etching of silicon followed by multiple depositions of polycrystalline oxide insulators and silicon gates. The layers were defined by a combination of lithography, chemical mechanical polishing and micromachining. We obtained devices with gate and focus apertures of 0.4[mu]m and 1.2[mu]m diameter. The anode current has very little dependence on the focus voltage and the ratio of the focus field factor to the gate field factor βF / βG is 0.015. Scanning electron micrographs of the devices, numerical simulation and spot size measurements on a phosphor screen confirmed these results. An e-beam resist, PMMA, was successfully exposed using the FEA device as an electron source."
2022,Optimal sizing of solar and battery assets in decentralized micro-grids with demand-side management,http://hdl.handle.net/1721.1/108959,"Solar-based community micro-grids and individual home systems have been recognized as key enablers of electricity provision to the over one billion people living without energy access to-date. Despite significant cost reductions in solar panels, these options can still be cost-prohibitive mainly due over-sizing of generation assets corresponding with a lack of ability to actively manage electricity demand. The main contribution shared is the methodology and optimization approach of least-cost combinations of generation asset sizes, in solar panels and batteries, subject to meeting reliability constraints; these results are based on a techno-economic modeling approach constructed for assessing decentralized micro-grids with demand-side management capabilities. The software model constructed is implemented to represent the technical characteristics of a low-voltage, direct current network architecture and computational capabilities of a power management device. The main use-case of the model presented is based on serving representative, aggregated, household-level load profiles combined with simulated power output from solar photovoltaic modules and the kinetic operating constraints of lead-acid batteries at hourly timesteps over year-long simulations. The state-space for solutions is based on available solar module and battery capacities from distributors in Jharkhand, India. Additional work presented also extends to real-time operation of such isolated micro-grids with requisite local computation. First, for load disaggregation and forecasting purposes, clustering algorithms and statistical learning techniques are applied on quantitative results from inferred load profiles based on data logged from off-grid solar home systems. Second, results from an optimization approach to accurately parametrize a lead-acid battery model for potential usage in real-time field implementation are also shared. Economic results, sensitivity analyses around key technical and financial input assumptions, and comparisons in cost reductions due to the optimization of solar and battery assets for decentralized micro-grids with demand-side management capabilities are subsequently presented. The work concludes with insights and policy implications on establishing differentiated willingness-to-pay, tiers of service, and dynamic price-setting in advanced micro-grids."
2022,Design and analysis of a 6/4-GHz receiver front end,http://hdl.handle.net/1721.1/37783,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2022,Representing troubleshooting information for a high-volume production line,http://hdl.handle.net/1721.1/35384,"Thesis (M.S.)--Massachusetts Institute of Technology, Sloan School of Management, 1994, and Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1994."
2022,Algorithms for simulating human pre-mRNA splicing decisions,http://hdl.handle.net/1721.1/37066,"In this thesis, I developed a program, ExonScan, to simulate constitutive human pre-mRNA splicing. ExonScan includes several models for splicing components, including splice sites, exonic splicing enhancers, exonic splicing silencers, and intronic splicing enhancers. I used ExonScan to test various aspects of human splicing, including correlation of splicing signal strength with tissue expression levels, the effectiveness of experimentally determined exonic splicing silencers, and splice site identification."
2022,Modeling of micro-electro-mechanical integrated test structures,http://hdl.handle.net/1721.1/36044,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2022,Multiple machine maintenance : applying a separable value function approximation to a variation of the multiarmed bandit,http://hdl.handle.net/1721.1/87269,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2002."
2022,Improving equipment performance through queueing model applications,http://hdl.handle.net/1721.1/38050,"Thesis (M.S.)--Massachusetts Institute of Technology, Sloan School of Management, 1995, and Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2022,A stacked full-bridge microinverter topology for photovoltaic applications,http://hdl.handle.net/1721.1/85805,"Previous work has been done to develop a microinverter for solar photovoltaic applications consisting of a high-frequency series resonant inverter and transformer section connected to a a cycloconverter that modulates the resonant current into a single-phase 240 VRMS utility line. This thesis presents a new stacked full-bridge topology that improves upon the previous high-frequency inverter section. By utilizing new operating modes to reduce the reliance on frequency control and allowing for the use of lower blocking voltage transistors, the operating frequency range of the HF inverter is reduced and efficiency is increased, especially at low output powers and lower portions of the line cycle. The design of an experimental prototype to test the stacked full-bridge HF inverter topology is presented along with test results that demonstrate the success of the topology. Future improvements to increase performance are also suggested."
2022,Modeling the appearance of cloth,http://hdl.handle.net/1721.1/14924,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1987."
2022,Real property portfolio management : a decision-support model,http://hdl.handle.net/1721.1/70166,"In the 1980's corporate real estate has assumed a more active role in the strategic planning of American corporations. However, the tools to accurately evaluate the performance of corporate real property portfolios are still at a very rudimentary stage in their development. This thesis concentrates on the space inventory system of a large corporation and presents a model for determining fair comparisons between buildings across the portfolio. A technique is devised for identifying ""outliers"", that is, buildings whose performance is significantly different from other buildings of the same type. This technique shows how to classify buildings into groups, so that building class standards can be determined and trends identified. Artificial Intelligence tools such as decision-support systems can be helpful to encode the expertise for evaluating buildings' performance levels. Through the design of two working demos the thesis illustrates how that is possible, and points towards future alternatives. The author spent an academic semester as a consultant/ intern in the real estate division of a multinational corporation. For anonymity purposes, the corporation is called the Star Corporation. The Star Corp. provided the data used in the research, as well as the supervision and training in their in-house systems operation."
2022,Frequency domain model-based intracranial pressure estimation,http://hdl.handle.net/1721.1/77016,"Elevation of intracranial pressure (ICP), the pressure of the fluid surrounding the brain, can require urgent medical attention. Current methods for determining ICP are invasive, require neurosurgical expertise, and can lead to infection. ICP measurement is therefore limited to the sickest patients, though many others could potentially benefit from availability of this vital sign. We present a frequency-domain approach to ICP estimation using a simple lumped, linear time-invariant model of cerebrovascular dynamics. Preliminary results from 28 records of patients with severe traumatic brain injury are presented and discussed. Suggestions for future work to improve the estimation algorithm are proposed."
2022,A multiscale framework for Bayesian inference in elliptic problems,http://hdl.handle.net/1721.1/65322,"The Bayesian approach to inference problems provides a systematic way of updating prior knowledge with data. A likelihood function involving a forward model of the problem is used to incorporate data into a posterior distribution. The standard method of sampling this distribution is Markov chain Monte Carlo which can become inefficient in high dimensions, wasting many evaluations of the likelihood function. In many applications the likelihood function involves the solution of a partial differential equation so the large number of evaluations required by Markov chain Monte Carlo can quickly become computationally intractable. This work aims to reduce the computational cost of sampling the posterior by introducing a multiscale framework for inference problems involving elliptic forward problems. Through the construction of a low dimensional prior on a coarse scale and the use of iterative conditioning technique the scales are decouples and efficient inference can proceed. This work considers nonlinear mappings from a fine scale to a coarse scale based on the Multiscale Finite Element Method. Permeability characterization is the primary focus but a discussion of other applications is also provided. After some theoretical justification, several test problems are shown that demonstrate the efficiency of the multiscale framework."
2022,Asynchronous distributed flow control algorithms,http://hdl.handle.net/1721.1/15388,"Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1984."
2022,Studies in discrete dynamic programming,http://hdl.handle.net/1721.1/31055,"Thesis (Sc. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1958."
2022,A strategic perspective on the commercialization of artificial intelligence : a socio-technical analysis,https://hdl.handle.net/1721.1/122431,"Many companies are increasing their focus on Artificial Intelligence as they incorporate Machine Learning and Cognitive technologies into their current offerings. Industries ranging from healthcare, pharmaceuticals, finance, automotive, retail, manufacturing and so many others are all trying to deploy and scale enterprise Al systems while reducing their risk. Companies regularly struggle with finding appropriate and applicable use cases around Artificial Intelligence and Machine Learning projects. The field of Artificial Intelligence has a rich set of literature for modeling of technical systems that implement Machine Learning and Deep Learning methods. This thesis attempts to connect the literature for business and technology and for evolution and adoption of technology to the emergent properties of Artificial Intelligence systems. The aim of this research is to identify high and low value market segments and use cases within the industries, prognosticate the evolution of different Al technologies and begin to outline the implications of commercialization of such technologies for various stakeholders. This thesis also provides a framework to better prepare business owners to commercialize Artificial Intelligence technologies to satisfy their strategic goals."
2022,Global and Robust Optimization for Engineering Design,https://hdl.handle.net/1721.1/143181,"There is a need to adapt and improve conceptual design methods through better optimization, in order to address the challenge of designing future engineered systems. Aerospace design problems are tightly-coupled optimization problems, and require all-at-once solution methods for design consensus and global optimality. Although the literature on design optimization has been growing, it has generally focused on the use of gradient-based and heuristic methods, which are limited to local and low-dimensional optimization respectively. There are significant benefits to leveraging structured mathematical optimization instead. Mathematical optimization provides guarantees of solution quality, and is fast, scalable, and compatible with using physics-based models in design. More importantly perhaps, there has been a wave of research in optimization and machine learning that provides new opportunities to improve the engineering design process. This thesis capitalizes on two such opportunities.

The first opportunity is to enable efficient all-at-once optimization over constraints and objectives that use arbitrary mathematical primitives. This work proposes a constraint sampling and learning approach for global optimization, leveraging developments in machine learning and mixed-integer optimization. More specifically, the feasible space of intractable constraints is sampled using existing and novel design of experiments methods, and learned using optimal classification trees with hyperplanes (OCT-Hs). OCT-Hs describe union-of-polyhedra approximations of intractable constraints, which are solved efficiently using commercial solvers to find near-feasible and near-optimal solutions to the global optimization problem. The constraints are then checked and the solution is repaired using projected gradient methods, ensuring feasibility and local optimality. The method is first tested on synthetic examples, where it finds the global optima for 9 out of 11 benchmarks, and high-performing solutions otherwise. Then it is applied to two real-world problems from the aerospace literature, and especially to a satellite on-orbit servicing problem that cannot be addressed via other global optimization methods. These applications demonstrate that decision tree driven optimization provides efficient, practical and optimal solutions to difficult global optimization problems present in aerospace design as well as other domains, regardless of the form of the underlying constraints.

The second opportunity is to optimize designs affected by parametric uncertainty in a tractable and deterministic manner, while providing guarantees of constraint satisfaction. Inspired by the wealth of literature on robust optimization, and specifically on robust geometric programming, this thesis proposes and implements robust signomial programming to solve engineering design problems under uncertainty. The methods are tested on a conceptual aircraft design problem, demonstrating that robust signomial programs are sufficiently general to address engineering design problems, solved efficiently by commercial solvers, and result in designs that protect deterministically against uncertain parameter outcomes from predefined sets. In addition, robust designs are found to be less conservative than designs with margins; robust aircraft demonstrate 9% better average performance than aircraft designed with margins over the same scenarios, while providing guarantees of constraint feasibility.

In anticipation of future aerospace design problems becoming increasingly coupled, complex and risky, this thesis provides a new perspective for dealing with design challenges using structured mathematical optimization. The proposed methods inject mathematical rigor into engineering design methods while keeping practical concerns for conceptual design in focus."
2022,"Dynamic responsiveness in the American states : legislators, constituents, and organized interests",https://hdl.handle.net/1721.1/121603,"In my second paper, I demonstrate the effectiveness of supervised machine learning methods in recognizing textual references to firms, organized interests, or any other political actors (an application of named entity recognition), and then resolving these references to real-world referents (an entity resolution task). Together, these methods make possible the large-scale measurement of political actors or their activity from sources such as diplomatic cables, transcripts, and administrative or legislative records. Organized interests are embedded in the legislative process in state capitols, writing bills and participating in committee meetings; they contribute stakeholder perspective and testify to the technical points of proposed legislation. Studying exactly which groups participate addresses a minimal standard for democratic governance. The third paper accomplishes this using the measurement strategy described in the second paper."
2022,Analysis of signal transduction networking using activation ratios,http://hdl.handle.net/1721.1/16606,"The molecular processes by which information is incorporated and distributed within a cell are termed signal transduction. These pathways allow cells to interact with each other and with their environments and are critical to the proper cellular function in a variety of contexts. Previously developed methods for analyzing signaling networks have been largely ignored, most likely due to their mathematical complexity and difficulty in application. A novel analysis framework was developed to assist in the examination of signaling networks, both to facilitate the reconstruction of previously undetermined pathways as well as to quantitatively characterize interactions between components. This approach, termed activation ratio analysis, involves the ratio between active and inactive forms of signaling intermediates at steady state. The activation ratio of an intermediate is shown to depend linearly upon the concentration of the activating enzyme. The slope of the line is defined as the activation factor, and is determined by the kinetic parameters of activation and inactivation. The mathematical functionality of the activation ratio changes for other signaling network arrangements. It is therefore possible to extract the original network structure from a set of measured activation ratios, with activation factors yielding a measure of activation potential between intermediates. This framework was tested using computational simulations of a small-scale interconnected network, cascades with feedback, and in the presence of experimental noise. In the process, additional tools were developed to automate and evaluate the analysis."
2022,Autonomous data collection techniques for approximating marine vehicle kinematics,http://hdl.handle.net/1721.1/100132,"Understanding vehicle kinematics is essential in allowing autonomous guidance algorithms to accurately assess short range encounters. Low cost, reconfigurable autonomous vehicles motivate using in-field online techniques rather than tow tank testing or Computational Fluid Dynamics (CFD). While the parameters of many physical dynamic models can be obtained using System Identification (SI) techniques, these models require knowledge of the vehicle actuators, which may not be the case in a ""backseat driver"" architecture using payload autonomy. Even when an identified physical model is available, using it to simulate trajectories requires insight into the design of the relevant controller, which may be proprietary or otherwise unknown to the back seat. This thesis develops a data collection procedure to obtain empirical kinematic trajectories for unmanned surface vehicles (USVs). A linear black box model of the USV yaw system is also developed, using only data available in the backseat. A prediction table for the M200 USV is developed with both techniques."
2022,Uncertainty-Aware Ensembling in Multi-Modal AI and its Applications in Digital Health for Neurodegenerative Disorders,https://hdl.handle.net/1721.1/140988,"Common neurodegenerative disorders such as Alzheimer's dementia and Parkinson's disease are increasingly recognised as leading causes of death and disability with debilitating symptoms such as progressive cognitive decline, communication breakdown, motor dysfunction and accompanying psychiatric disorders. However, factors such as unavailability of efficient and cost-effective assessments for conclusive diagnosis, time-consuming test protocols, poor prognostic capabilities, and inadequate treatment options with accompanying side effects are all barriers to progress in providing faster and more effective intervention to individuals living with these life-altering disorders. In this thesis, we take a step towards using digital health and machine learning to improve diagnostic and prognostic capabilities and to address remote care via telemedicine in Alzheimer's dementia and Parkinson's disease. Our goal is to provide more cost-effective, non-invasive, and scalable technologies for risk stratification of Alzheimer's dementia using speech. We also aim to monitor drug response and disease progression for Parkinson's disease via telemedicine, allowing real time symptom tracking through wearables alongside a patient's treatment status, which will help facilitate remote care and dynamic and adaptive treatment plans. In addition to addressing the challenges in diagnosis and treatment of neurodegenerative disorders, we further propose a novel uncertainty aware boosting technique for multi-modal ensembling and evaluate it on healthcare tasks related to Alzheimer's dementia and Parkinson's disease. This presents manifold benefits, such as reducing the overall entropy of the system, making it more robust to heteroscedasticity, and improving calibration of each of the modalities along with high quality prediction intervals."
2022,Design of Nuclear-Targeting Peptides for Macromolecule Delivery via Machine Learning,https://hdl.handle.net/1721.1/143244,"The effective design of functional peptide sequences remains a fundamental challenge in biomedicine. For example, cell-penetrating peptides (CPPs) are capable of delivering macromolecular cargo to intracellular targets that are otherwise inaccessible. However, design of novel CPPs with high activity and unique structure remains challenging. In this thesis, methods to design and characterize highly active CPPs for antisense oligonucleotide delivery were explored.

Machine learning is a promising method for de novo design of functional peptide sequences. A deep learning model inspired by directed evolution was used to optimize abiotic sequences that traffic antisense oligomers to the nucleus of cells. The model was able to predict activities beyond those in the training dataset, and simultaneously decipher and visualize sequence-activity predictions. The validated miniproteins (40-80 residues) were more effective than any previously known variant in cells. By augmenting the machine learning model to over-represent shorter sequence space, the model also predicted a short peptide (18-residues) with comparable activity to a positive control peptide. Empirical sequence-activity studies demonstrated reliance on the cationic residues as well as the C-terminal cysteine residue. These sequences were nontoxic, able to deliver other biomacromolecules to the cytosol, and efficiently delivered antisense cargo in mice.

A different approach to discover and characterize CPP sequences was also taken, by extracting peptides taken up into cells and analyzing their relative quantities or identifying their sequences by mass spectrometry. First, several mirror-image D-peptides had similar delivery activity to their native forms, while demonstrating complete proteolytic stability. Mixtures of fully intact antisense-peptide conjugates could be recovered from whole cell and cytosolic lysates, and relative concentrations were quantified by MALDI-TOF. This method was then extended to the discovery of de novo sequences from a combinatorial library of antisense-peptide conjugates containing unnatural residues. Following cell treatment with the biotinylated antisense-peptide library, the cytosol of cells was extracted and internalized peptides recovered via affinity capture. De novo sequencing was achieved by Orbitrap tandem mass spectrometry, and several unique, unnatural sequences were identified that could effectively deliver the antisense oligomer to the nucleus.

In summary, machine learning and mass spectrometry-based strategies to discover and characterize novel CPP sequences for antisense delivery were explored. In the future, we envision combining these methods in order to use lists of library hits to train a machine learning model to design sequences composed of fully unnatural amino acids."
2022,Optimal control of controllable switched systems,http://hdl.handle.net/1721.1/33201,"Many of the existing techniques for controlling switched systems either require the solution to a complex optimization problem or significant sacrifices to either stability or performance to offer practical controllers. In [13], it is shown that stabilizing, practical controllers with meaningful performance guarantees can be constructed for a specific class of hybrid systems by parameterizing the controller actions by a finite set. We extend this approach to the control of controllable switched systems by constraining the switching portion of the control input and fixing the feedback controller for each subsystem. We show that, under reasonable assumptions, the resulting system is guaranteed to converge to the target while providing meaningful performance. We apply our approach to the direct-injection stratified charge (DISC) engine and compare the results to that of a model predictive controller designed for the same application."
2022,A framework for multi-modal input in a pervasive computing environment,http://hdl.handle.net/1721.1/16825,"In this thesis, we propose a framework that uses multiple-domains and multi-modal techniques to disambiguate a variety of natural human input modes. This system is based on the input needs of pervasive computing users. The work extends the Galaxy architecture developed by the Spoken Language Systems group at MIT. Just as speech recognition disambiguates an input wave form by using a grammar to find the best matching phrase, we use the same mechanism to disambiguate other input forms, T9 in particular. A skeleton version of the framework was implemented to show this framework is possible and to explore some of the issues that might arise. The system currently works for both T9 and Speech modes. The framework also includes potential for any other type of input for which a recognizer can be built such as graffiti input."
2022,Active pixel sensors for X-ray astronomy,http://hdl.handle.net/1721.1/34370,"An active pixel sensor array, APS-1, has been fabricated for the purpose of scientific x-ray detection. This thesis presents the results of testing the device. Alternate design architectures are explored. Recommendations are made for a next-generation sensor. CCDs have been the dominant x-ray sensor in astronomy for over ten years. Limitations inherent to CCDs are starting to become important. Active pixel sensors (APS) provide an alternate architecture that may solve these problems. APS-1 is a first-generation sensor designed by Lincoln Laboratory's Advanced Silicon Technology Group. APS-1 is fabricated in a fully depleted silicon-on-insulator (FDSOI) technology. FDSOI is especially well-suited to produce a scientific x-ray imager. The device includes sixteen different pixel variations to determine the processing parameters that can produce the best imager. Dark current, noise, and responsivity of the various pixel designs was measured using an electronics system adapted from a CCD test system. X-rays were detected at room temperature. Ordinary active pixels have high noise levels ( 70 electrons). Many pixel designs capable of lower noise have been presented in the literature. Active reset, pixel-level CDS, and CTIA pixel designs are discussed in detail and simulated. A second-generation sensor from Lincoln Laboratory, using pixel-level CDS, is discussed. This device, APS-2, will be available for testing in 2006. APS-2 simulation results are presented. It is expected to have an input-referred noise of less than five electrons, near the performance of modern CCDs."
2022,Nonsmooth Methods for Process Integration,https://hdl.handle.net/1721.1/144971,"Process integration is a promising method to improve sustainability and reduce waste in chemical processes by recovering excess resources such as heat, water, or other materials. However, calculating the maximum amount of resource that can be reused is challenging because resource sinks can only take in resource if it is of high enough quality. As a result, most current integration methods are either limited and heuristic or use large superstructure formulations that must assess all possible matches between the resource sources and sinks. 

Therefore, this thesis presents new computational methods for maximizing resource recovery that use nonsmooth functions to compactly describe the resource that is available at different qualities. This work can be divided into three main contributions that improve process integration for systems with different resources and assumptions:

1. A generalized approach to process integration that uses a system of two nonsmooth equations to describe optimal reuse for a wide variety of resources, including multiple resources simultaneously,

2. An extension of this general approach to more complex mass and water systems with multiple contaminants that can limit their reuse,

3. A non-smooth optimization formulation that applies our integration approach to design variable-temperature cogeneration systems that convert process waste heat into electricity. 

By utilizing non-smooth equations, each of these contributions exhibits improved scaling compared to other integration methods and have numbers of equations or constraints that remain the same regardless of the size and complexity of the system. In addition, unlike other methods, our approaches have the flexibility to either determine resource requirements or the process variables that achieve a given target.

This thesis describes the formulation and implementation of each of these non-smooth approaches and applies them to a wide of range of example applications. These applications include carbon-constrained energy planning, hydrogen conservation networks, water recovery from petroleum refining with multiple contaminants, and designing improved cogeneration systems for sulfuric acid and cement production processes. The results from these examples show the flexibility and scalability of our approaches and the breadth of improvements they can provide. Together, our contributions increase the applicability of computationally efficient process integration methods to improve the sustainability of a wide range of chemical processes."
2022,The design and construction of a special purpose computer for speech synthesis-by-rule.,http://hdl.handle.net/1721.1/27481,Thesis. 1976. M.S.--Massachusetts Institute of Technology. Dept. of Electrical Engineering and Computer Science.
2022,Identifying and modeling unwanted traffic on the Internet,http://hdl.handle.net/1721.1/37100,"Accurate models of Internet traffic are important for successful testing of devices that provide network security. However, with the growth of the Internet. it has become increasingly difficult to develop and maintain accurate traffic models. While much internet traffic is legitimate, productive communications between users and services, a significant portion of Internet traffic is the result of unwanted messages sent to IP addresses without regard as to whether there is an active host at that address. In an effort to analyze unwanted traffic, tools were developed that generate statistics and plots on captured unwanted traffic to unused IP addresses. These tools were used on a four-day period of traffic received on an inactive IPv4 class A network address space. Each class B subnet in this address space received an average of 7 million packets corresponding to 21 packets per second. Analyses were performed on a range of class B and C subnets with the intent of discovering the types of variability that are characteristic of unwanted traffic. Traffic volume over time, number of scans, destinations ports, and traffic sources varied substantially across class B and C subnets."
2022,"A theoretical analysis of interstitial hydrogen : pressure-composition-temperature, chemical potential, enthalpy and entropy",http://hdl.handle.net/1721.1/78547,"We provide a first principles analysis of the physics and thermodynamics of interstitial hydrogen in metal. By utilizing recent advances in Density Functional Theory (DFT) to get state energies of the metal-hydrogen system, we are able to model the absorption process fairly accurately. A connection to experiment is made via Pressure-Composition-Temperature (PCT) isotherms, and thermodynamic molar quantities. In the model, we understand the excess entropy of absorbed hydrogen in terms of the change in its accessible microstates. A connection is also made between the entropy and electronic states of interstitial hydrogen. However, our model indicates that this connection is too small to account for experimental results. Therefore, a conclusion is made that the entropy of absorbed hydrogen is mostly (non-ideal) configurational in nature. To model the latter in a manner consistent with experiment, we have explored a new model that posits a weak binding between clusters of hydrogen atoms at neighboring sites. We have developed a formulation and fitted the results to experimental data. We find a least squares fitting of the model to the entropy and enthalpy results in model parameters which seem physically reasonable. The resulting model appears to provide a natural physical explanation for the dependence of the excess entropy on loading."
2022,Scheme for identifying and describing behavioral innovations embodied in computer programs,http://hdl.handle.net/1721.1/40608,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1996."
2022,Nickel-catalyzed Suzuki-Miyaura reactions of unactivated halides with alkyl boranes and planar-chiral borabenzene catalysts for Diels-Alder reactions,http://hdl.handle.net/1721.1/62101,"Part I describes the expansion in scope of a nickel-catalyzed coupling reaction of unactivated alkyl bromides and alkyl boranes to include unactivated alkyl chlorides. The new method is adapted for use outside of a glove box and is also found to be applicable not only to the coupling of primary chlorides, but also to the coupling of bromides and iodides, both primary and secondary. ... This coupling reaction of chlorides is further adapted to the of p-chloro aryl alkyl amines. This work constitutes an extension directing groups for the asymmetric Suzuki-Miyaura reactions halides. ... Part II details work towards an asymmetric Diels-Alder reaction between cyclopentadiene and methacrolein catalyzed by a planar-chiral boron Lewis acid. This system exhibits a level of turnover that is unprecedented in reactions mediated by planar chiral boron heterocycles. Computational studies shed light on the nature of the 7tsymmetry interaction between borabenzenes and complexed carbonyl groups. The selectivity of the borabenzene-catalyzed Diels-Alder reaction is also examined."
2022,Dynamic models for convective systems.,http://hdl.handle.net/1721.1/13456,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1966. Ph.D.
2022,Designing and Testing a Mobile Creative Coding Application for Children,https://hdl.handle.net/1721.1/139229,"Children are becoming increasingly engaged with applications on mobile phones. They use mobile apps to socialize, communicate, and play games. There is an opportunity to channel this familiarity and fascination with mobile phones in order to introduce children to computational thinking and creative expression. Towards that end, the Lifelong Kindergarten (LLK) Group is designing a free mobile application that will provide a motivating, creative, and accessible way for children to learn how to code through creating interactive animations that they can send to friends and family. This thesis identifies key design questions and challenges involved in the design of this new coding platform for children, reviews the strategies other mobile applications have employed in addressing related design challenges, and introduces a variety of solutions that our research team has developed, along with their affordances and limitations. This thesis also presents an analysis of data from conducting playtests and semi-structured interviews, and suggests lessons learned for other designers of mobile coding applications for children."
2022,A study of time-compressed speech.,http://hdl.handle.net/1721.1/13583,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1974. Ph.D.
2022,Similarity Metrics for Biological Data: Algorithmic developments for high-dimensional datasets,https://hdl.handle.net/1721.1/145044,"Advances in experimental methods in biology have allowed researchers to gain an unprecedentedly high-resolution view of the molecular processes within cells, using so-called single-cell technologies. Every cell in the sample can be individually profiled — the amount of each type of protein or metabolite or other molecule of interest can be counted. Understanding the molecular basis that determines the differentiation of cell fates is thus the holy grail promised by these data.

However, the high-dimensional nature of the data, replete with correlations between features, noise, and heterogeneity means the computational work required to draw insights is significant. In particular, understanding the differences between cells requires a quantitative measure of similarity between the single-cell feature vectors of those cells. A vast array of existing methods, from those that cluster a given dataset to those that attempt to integrate multiple datasets or learn causal effects of perturbation, are built on this foundational notion of similarity.

In this dissertation, we delve into the question of similarity metrics for high-dimensional biological data generally, and single-cell RNA-seq data specifically. We work from a global perspective — where we find a distance function that applies across the entire dataset — to a local perspective — where each cell can learn its own similarity function. In particular, we first present Schema, a method for combining similarity information encoded by several types of data, which has proven useful in analyzing the burgeoning number of datasets which contain multiple modalities of information. We also present DensVis, a package of algorithms for visualizing single-cell data, which improve upon existing dimensionality-reduction methods that focus on local structure by accounting for density in high-dimensional space. Lastly, we zoom in on each datapoint, and show a new method for learning 𝑘-nearest neighbors graphs based on local decompositions.

Altogether, the works demonstrate the importance — through extensive validation on existing datasets — of understanding high-dimensional similarity."
2022,Optical studies of super-collimation in photonic crystals,http://hdl.handle.net/1721.1/34677,"Recent developments in material science and engineering have made possible the fabrication of photonic crystals for optical wavelengths. These periodic structures of alternating high-to-low index of refraction materials allow the observation of peculiar effects, in particular, the propagation of optical beams without spatial spreading. This effect, called super-collimation (also known as self-collimation), allows diffraction-free propagation of micron-sized beams over centimeter-scale distances. This linear effect is a natural result of the unique dispersive properties of photonic crystals. In this thesis, these dispersive properties are studied in a two-dimensional photonic crystal slab. Both qualitative and quantitative descriptions are presented. The beam propagation method was used to simulate the evolution of a Gaussian beam inside such structures. The wavelength dependence of the super-collimation effect was studied, and it was observed that the optimum wavelength for this device was around 1500 nm. A precise contact-mode near-field optical microscopy technique was used to obtain high-resolution images of the beam profile at different positions along the photonic crystal, and showed that a 2 [micro]m beam width was conserved over 3 mm. In addition, high-resolution confocal measurements confirmed the size of the beam after 5 mm of propagation."
2022,Optimization of optical characteristics of travelling wave modulators,http://hdl.handle.net/1721.1/15299,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1985."
2022,Cost-optimal design of a household batch electrodialysis desalination device,http://hdl.handle.net/1721.1/111935,"This thesis investigates the pareto-optimal design of a household point-of-use batch electrodialysis (ED) system to provide a cost-effective replacement for existing reverse osmosis (RO) devices, for brackish water desalination of Indian groundwater, at lower energy consumption and higher recovery: 80-90% vs 25-40%. Target specifications derived from user-interviews, and RO products, guided the selection of a batch architecture, for which a coupled flow-mass transport model to predict desalination rate was developed, and validated using a lab-scale ED stack. The effects of varying the production rate (9-15 L/hr) and product concentration (100-300 mg/L) requirements on optimal selection of geometry, flow-rates, and applied voltage for total cost minimization were then explored using a multi-objective genetic algorithm. Given the low utilization of the system and the current cost of materials, the energetic cost was dominated by the capital-cost of the system. At a fixed feedwater concentration of 2000 mg/L, which is representative of the upper bound on groundwater salinity underlying much of India, and a recovery ratio of 90%, the capital cost sharply increased for systems targeted at 100 mg/L vs 200 mg/L and 300 mg/L: $141, $93, and $79, respectively averaged for systems that produced between 11.5 and 12.5 L/hr of desalinated water. Promising directions for additional cost reduction include voltage-regulation during the batch process and the development of inexpensive pumps. In addition, a candidate cost-optimal design was prototyped and tested to verify that the measured desalination performance agreed with the modeled expectations."
2021,Exploration of alternative algorithms for multi-channel acoustic echo cancellation,https://hdl.handle.net/1721.1/129887,"Multi-Channel Acoustic Echo cancellation (MCAEC) is a vital component of delivering clean speech to a virtual personal assistant through a smart speaker with multi-channel audio (stereophonic, etc). The use of the Kalman filter as an alternative adaptive filter methodology for this MCAEC application is explored in this work. The Normalized Least Mean Squares filter (NLMS) serves as a benchmark for the Kalman filter. Simulations using room recordings and measured room responses are employed in this exploration. Useful metrics such as the Word Error Rate (WER) and Echo Return Loss Enhancement (ERLE) help to distinguish performance among the two adaptive filter algorithms. For the single channel case, simulations confirm the cancellation and convergence rate advantage of the Kalman filter, in full-band, but the NLMS filter gives similar results in the sub-band domain, as measured by WER and ERLE. In the multi-channel case, both solutions achieve similar steady state cancellation, but the NLMS offers slightly faster convergence rates. In experiments where adaptation was not frozen, the Kalman filter effectively maintains high echo cancellation by tracking input signal statistics. In most cases, the Kalman filter does not present an appropriate alternative for the MCAEC application in this work."
2021,Developing software for compressed imaging transcriptomics,https://hdl.handle.net/1721.1/129086,"Modern-day biological experimentation often necessitates a scale of data that is exponential with respect to the number of genes that are being measured, and this in turn leads to high latency and monetary cost during hypothesis testing. In addition to such practical constraints, some biological experiments are just physically infeasible due to fundamental limitations on the throughput of current technologies. However, because nearly all biological data are highly structured and can be described in terms of relatively few components, it is not necessary to measure each data point individually. Instead, using the framework of compressed sensing, it is possible to take advantage of this structure to gather the requisite data for an experiment while collecting only a fraction of the original number of measurements. In previous work, we have applied compressed sensing for the particular purpose of generating spatial gene expression profiles using fluorescence microscopy (i.e. imaging transcriptomics). In order to make this technique more accessible and user-friendly, we built CISIpy, an open-source software system that implements the pipeline's computational aspects. This system is designed to enable efficient compressed sensing workflows that is highly portable across platforms and especially amenable to cloud computation. The end result is a well-tested, open-source software package replete with functionality, documentation and examples."
2021,Sensing and timekeeping using a light-trapping diamond waveguide,http://hdl.handle.net/1721.1/111878,"Solid-state quantum systems have emerged as promising sensing platforms. In particular, the spin properties of nitrogen vacancy (NV) color centers in diamond make them outstanding sensors of magnetic fields, electric fields, and temperature under ambient conditions. This thesis focuses on spin-based sensing using multimode diamond waveguide structures to efficiently use large ensembles of NV centers (> 10¹⁰). Temperature-stabilized precision magnetometry, thermometry, and electrometry are discussed. In addition, the precision characterization of the NV ground state structure under a transverse magnetic field and the use of NV-diamond for spin-based clocks are reported."
2021,ML-driven clinical documentation,https://hdl.handle.net/1721.1/129149,"Electronic health records (EHRs) have irrevocably changed the practice of medicine by systematizing the collection of patient-level data. However, clinicians currently spend more time documenting information in EHRs than interacting directly with patients, and have adapted to time-intensive note-writing by authoring free-text notes overloaded with jargon and acronyms. Clinical notes are therefore difficult to parse and largely unstructured. This negatively impacts the ability of EHR systems to convey information between different clinicians and institutions, to communicate medical findings to patients, and to allow for programmatic ingestion of data to derive further automatically-learned insights. In this thesis, we present a new EHR system that addresses these problems by using novel machine learning methods to streamline the processes by which clinicians enter in new information and surface relevant details from past medical records. Our intelligent interface aids physicians as they type, allowing for automatic suggestion and live-tagging of clinical concepts to alleviate documentation burden, while simultaneously enabling clinical decision support and contextual information synthesis. Furthermore, as clinicians craft notes we automatically structure and curate their free-text inputs, allowing for further data-driven innovation and improvement. This EHR can reduce physician burnout, decrease diagnostic error, and improve patient outcomes, all while collecting a corpus of clean, labelled clinical data. Our system is currently deployed live at the Beth Israel Deaconess Medical Center Emergency Department and is in use by doctors."
2021,Algorithmic intervention to mitigate inventory and ordering amplification in multi-echelon supply chains,https://hdl.handle.net/1721.1/129114,"The 'bullwhip effect' is a classic, yet persisting, problem with reverberating consequences in inventory management and refers to how forecast errors and safety stock builds yield increasing amplitudes in both orders and on-hand inventory positions the further one moves away from a source of order variability. The bullwhip effect is responsible for both excessive strain on real world inventory management systems, stock outs, and unnecessary capital reservation though safety stock building. In this paper, the author develops algorithmic approaches to mitigating bullwhip using simulation modeling, including cost minimization and amplification minimization, and then interprets the results in the context of existing models of human heuristics in ordering decisions. The algorithmic approaches are optimized as one member within a model of a human decision makers operating within a multi-echelon supply chain with imperfect information sharing and information delays."
2021,Efficient homomorphically encrypted privacy-preserving automated biometric classification,https://hdl.handle.net/1721.1/130608,"This thesis investigates whether biometric recognition can be performed on encrypted data without decrypting the data. Borrowing the concept from machine learning, we develop approaches that cache as much computation as possible to a pre-computation step, allowing for efficient, homomorphically encrypted biometric recognition. We demonstrate two algorithms: an improved version of the k-ishNN algorithm originally designed by Shaul et. al. in [1] and a homomorphically encrypted implementation of a SVM classifier. We provide experimental demonstrations of the accuracy and practical efficiency of both of these algorithms."
2021,Autonomous navigation of distributed spacecraft using intersatellite laser communications,https://hdl.handle.net/1721.1/128308,"Autonomous navigation refers to satellites performing on-board, real-time navigation without external input. As satellite systems evolve into more distributed architectures, autonomous navigation can help mitigate challenges in ground operations, such as determining and disseminating orbit solutions. Several autonomous navigation methods have been previously studied, using some combination of on-board sensors that can measure relative range or bearing to known bodies, such as horizon and star sensors (Hicks and Wiesel, 1992) or magnetometers and sun sensors (Psiaki, 1999), however these methods are typically limited to low Earth orbit (LEO) altitudes or other specific orbit cases. Another autonomous navigation method uses intersatellite data, or direct observations of the relative position vector from one satellite to another, to estimate the orbital positions of both spacecraft simultaneously."
2021,Loanwords and the perceptual map : a perspective from MaxEnt Learning,https://hdl.handle.net/1721.1/129120,"It will be shown that the patterns of consonant deletion and vowel epenthesis used by speakers of Cantonese to adapt English words are compatible with the PMap, and can be modelled through the MaxEnt learners mentioned above. It will also be shown through a series of computational simulations that Wilson's (2006) learner fails to acquire the grammar necessary to account for the patterns of loanword adaptation, while White's (2013) learner succeeds. This is a result of the way in which the PMap is encoded within these learners. While both encode the PMap as a series of asymmetrical Gaussian distributions on the weights of constraints, Wilson (2006) encodes this asymmetry through the variances, or plasticities, of the distributions, while White (2013) encodes it through the means, or target weights. A grammar which encodes the PMap through asymmetrical plasticities must encounter evidence from the phonology of the language in order to alter the weights of constraints."
2021,Predicting post-surgical opioid consumption using perioperative surgical data,https://hdl.handle.net/1721.1/130199,"Improper consumption of prescription opioids is a massive public health issue in the United States currently. Here, we propose one approach of tackling this issue through using machine learning techniques to predict opioid consumption post discharge for surgical patients. Through the data collected from surgical patients at BIDMC, relevant features will be identified and used to predict if patients high, outlier consumption. Using logistic regression and gradient boosted decision trees, model performance were evaluated at AUCs of 0.7270 and 0.7289 respectively."
2021,Identifying patterns of learning : a case study of MIT's Introductory Programming Course (6.000x),https://hdl.handle.net/1721.1/128986,"The ever-increasingly relevant introductory programming course offered at MIT presents a unique opportunity to uncover student learning patterns and common behavioral motifs. The course 6.0001/6.0002 harbors a wealth of student interaction data on its companion MITx platform as well as associated grades. Although this course has been offered for the last twelve years, since 2008, little has been done to identify aspects of the course that best aid or hinder student success. This thesis will focus on finding various learner subpopulations to elucidate those materials that best aid certain students to allow for a more tailored teaching mode for future iterations of the course. In addition, this thesis will define an 'effort' statistic that encompasses the holistic engagement of a given student in order to provide an additional statistic to use when determining final grades. I begin with a course specific analysis of enrollment demonstrating the significance of this type of analysis."
2021,Tiresias : a peer-to-peer platform for privacy preserving machine learning,https://hdl.handle.net/1721.1/129840,"Big technology firms have a monopoly over user data. To remediate this, we propose a data science platform which allows users to collect their personal data and offer computations on them in a differentially private manner. This platform provides a mechanism for contributors to offer computations on their data in a privacy-preserving way and for requesters -- i.e. anyone who can benefit from applying machine learning to the users' data -- to request computations on user data they would otherwise not be able to collect. Through carefully designed differential privacy mechanisms, we can create a platform which gives people control over their data and enables new types of applications."
2021,Using natural language to predict bias and factuality in media with a study on rationalization,https://hdl.handle.net/1721.1/130716,"Fake news is a widespread problem due to the ease of information spread online, and its ability to deceive large populations with intentionally false information. The damage it causes is exacerbated by its political links and loaded language, which make it polarizing in nature, and preys on peoples' psychological biases to make it more believable and viral. In order to dampen the influence of fake news, organizations have begun to manually tag, or develop systems to automatically tag, false and biased information. However, manual efforts struggle to keep up with the rate at which content is published, and automated methods provide very little explanation to convince people of their validity. In an effort to address these issues, we present a system to classify media sources' political bias and factuality levels by analyzing the language that gives fake news its contagious and damaging power. Additionally, we survey potential approaches for increasing the transparency of black-box fake news detection methods."
2021,Past price and trend effects in promotion planning; from prediction to prescription,https://hdl.handle.net/1721.1/129030,"Sales promotions are a popular type of marketing strategy. When undertaking a sales promotion, products are promoted using short-term price reductions to stimulate their demand and increase their sales. These sales promotions are widely used in practice by retailers. When undertaking a sales promotion, retailers must take into consideration both the direct and indirect effects of price promotions on consumers, and as a result, on the demand. In this thesis, we consider the impact of two of these indirect effects on the planning process of promotions. First, we consider the problem of the promotion planning process for fast-moving consumer goods. The main challenge when considering the promotion planning problem for fast-moving consumer goods is the negative indirect effect of promotions on future sales. While temporary price reductions substantially increase demand, in the following periods after a temporary price reduction, retailers observe a slowdown in sales."
2021,Design of a Phi-2 and a Class E inverter for underwater systems,https://hdl.handle.net/1721.1/129917,"In Autonomous Underwater Vehicles (AUVs), many potential failure modes exist due to pressure housing and the need for connections between different pressure housings. Waterproof connectors do exist but drive up the price and weight of underwater systems, a costly disadvantage as mass and volume are at a premium for an underwater system. If we can remove the necessity for physical connectors, we can design cheaper, more robust submarines. This can be done with wireless power transfer (WPT), which can transmit power efficiently across mediums within the submarine, therefore eliminating the need for physical connections and making underwater systems more compact and light-weight. The thesis presents two WPT systems for an AUV with two different inverters that convert DC power to AC power that drive the WPT system. The first system presented uses a Class E Inverter, a common topology for DC-AC conversion, and the second system utilizes a Phi-2 Inverter, a topology that uses the inherent parasitic capacitances to substitute for physical components. The WPT system utilizes magnetic resonance coupling to transmit power from transmitter coils attached to the inverters to receiver coils attached to a load through a rectifier. Simulations show that, when correctly tuned, the two designs can give comparable performance in power transfer efficiency and range. The choice of design is likely to be decided by a combination of the size and weight of the finished system, along with the ease of design."
2021,Machine learning in housing design : exploration of generative adversarial network in site plan / floorplan generation,https://hdl.handle.net/1721.1/129855,"Technology has always been an important factor that shapes the way we think about Architecture. In recent years, Machine Learning technology has been gaining more and more attention. Different from traditional types of programming that rely on explicit instructions, Machine Learning allows computers to learn to execute certain tasks ""by themselves"". This new technology has revolutionized many industries and showed much potential. Examples like AlphaGo and OpenAI Five had shown Machine Learning's capability in solving complex problems. The Architectural design industry is not an exception. Early-stage explorations of this technology are emerging and have shown potential in solving certain design problems. However, basic problems regarding the nature of Machine Learning and its role in Architecture design remain to be answered. What does Machine Learning mean to Architecture? What will be its role in Architectural design? Will it replace human architects? Will it merely be a design tool? Or is it relevant to Architecture at all? To answer these questions, this thesis explored with a specific type of Machine Learning algorithm called Pix2Pix to investigate what can and cannot be learned by a computer through Machine Learning, and to evaluate what Machine Learning means for architects. It concluded that Machine Learning cannot be a creative design agent, but can be a powerful tool in solving conventional design problems. On this basis, this thesis proposed a prototype pipeline of integrating the technology into the design process, which is a combination of Generative Adversarial Network (Pix2Pix), Bayesian Network and Evolutionary Algorithm."
2021,Expresso-AI : a framework for explainable video based deep learning models through gestures and expressions,https://hdl.handle.net/1721.1/130700,"We have developed a framework for Analyzing Facial Videos and applying it to Automatic Depression Detection. We also developed a video based models We have developed a framework to analyze the decisions of Deep Neural Networks trained on facial videos. We test this framework on Automatic Depression Detection. We first train Deep Convolutional Neural Networks (DCNN) pre-trained on Action Recognition datasets and fine-tune on the facial videos. We interpret the model's saliency maps by analyzing face regions and temporal expression semantics. Our framework generates both visual and quantitative explanations on the model's decision. Simultaneously, our video based modeling has improved previous single-face benchmarks of visual Automatic Depression Detection (ADD). We conclude successfully that we have developed the ability to generate hypotheses from a facial model's decisions, and improved Automatic Depression Detection's predictive performance."
2021,Structural and algorithmic aspects of linear inequality systems,https://hdl.handle.net/1721.1/128971,"Linear inequality systems play a foundational role in Operations Research, but many fundamental structural and algorithmic questions about linear inequality systems remain unanswered. This thesis considers and addresses some of these questions. In the first chapter, we reconsider the ellipsoid algorithm applied to solving a system of linear inequalities. Unlike the simplex method and interior point methods, the ellipsoid algorithm has no mechanism for proving that a system is infeasible (in the real model of computation). Motivated by this, we develop an ellipsoid algorithm that produces a solution to a system or provides a simple proof that no solution exists. Depending on the dimensions and on other natural condition measures, the computational complexity of our algorithm may be worse than, the same as, or better than that of the standard ellipsoid algorithm.. In the second chapter, we reduce the problem of solving a homogeneous linear inequality system to the problem of finding the unique sink of a unique sink orientation (USO) in the vertex evaluation model of computation. We show the USOs of interest satisfy a local property that is not satisfied by all USOs that satisfy the Holt-Klee property. This addresses an open question that is motivated by the idea that such local structure could be leveraged algorithmically to develop faster algorithms or a strongly polynomial algorithm. In the third chapter, we make progress on a conjecture about a particular class of linear inequality systems that have balanced constraint matrices. A balanced matrix is a 0-1 matrix that does not contain a square submatrix of odd order with two ones per row and column. The conjecture asserts that every nonzero balanced matrix contains an entry equal to 1, which upon setting to 0, leaves the matrix balanced."
2021,Scaling RFID positioning systems using distributed and split computing,https://hdl.handle.net/1721.1/129111,"Fine-grained tracking of objects in the physical world at scale has a broad potential impact in health care, retail, manufacturing, supply chain, and consumer product industry. In this thesis, I focus on using RFID-based technology for such applications due to its low-cost and growing prevalence of RFID tags. In contrast to current RFID systems that focus on a monolithic reader, I propose a distributed sensor node architecture that can scale by combining distributed and split computing techniques. On the distributed computing front, I introduce an architecture that enables extending the operation range and coverage from an end user's perspective while improving the manageability aspect via high-level semantic API. On the split computing front, I develop a framework to offload expensive tasks to the cloud or an edge server; the framework enables the use of small, cheap commodity compute devices as hosts at the edge while maintaining the high accuracy of fine-grained positioning. The thesis describes the design and implementation of these techniques. Moreover, through a hybrid evaluation of simulation and practical systems, the thesis demonstrates how these techniques enable us to design a scalable, manageable, and accurate RFID positioning system."
2021,Discrete mechanical metamaterials,https://hdl.handle.net/1721.1/130610,"Digital fabrication enables complex designs to be realized with improved speed, precision, and cost compared to manual techniques. Additive manufacturing, for example, is one of the leading methods for rapid prototyping and near net shape part production. Extension to full scale structures and systems, however, remains a challenge, as cost, speed and performance present orthogonal objectives that are inherently coupled to limited material options, stochastic process errors, and machine-based constraints. To address these issues, this thesis introduces new materials that physically embody attributes of digital systems, scalable methods for automating their assembly, and a portfolio of use cases with novel, full-scale structural and robotic platforms. First, I build on the topic of discrete materials, which showed a finite set of modular parts can be incrementally and reversibly assembled into larger functional structures."
2021,Temperature prediction using thermal fluctuations from wireless sensor networks in adaptive filter model,https://hdl.handle.net/1721.1/129903,"In many scientific experiments, it is imperative to minimize the unintended effects of variables other than the independent variables. Temperature, pressure, and gas levels are factors controlled to a certain extent using expensive climate-controlling technology, yet the resolution for monitoring their levels is generally low. The downward scaling of communication-enabled electronics in size, cost, and energy provides a potential toolset for tracking such data with high spatial and temporal resolutions. We establish a data collection methodology through a low-cost, small footprint distributed network system of modules that records data in a remote server. The system architecture allows for increased spatial resolutions, demonstrates high precision of measurements, and investigates room dynamics. Modules are fabricated using commercial sensors such as the ESP8266, BME680, and TCS34725. In this paper, we propose a temperature prediction model using adaptive filter methodologies to learn the relationship between thermal fluctuations at distinct locations within a lab environment."
2021,G-Network for outcome prediction under dynamic intervention regimes,https://hdl.handle.net/1721.1/129838,"Counterfactual prediction is useful in settings where one would like to know what would have happened had an alternative regime been followed, but one only knows the outcomes under the observational regime. Typically, the regimes are dynamic and time-varying. In these scenarios, G-computation can be used for counterfactual prediction. This work explores a novel recurrent neural network approach to G-computation, dubbed G-Net. Many implementations of G-Net were explored and compared to the baseline, linear regression. Two independent datasets were used to evaluate the performance of G-Net: one from a physiological simulator, CVSim, and another from the real-world MIMIC database. Results from the CVSim experiments suggest that G-Net outperforms the traditional linear regression approach to G-computation. The best G-Net model found from the CVSim experiments was then evaluated using the MIMIC dataset. The outcomes under a few different counterfactual strategies on the MIMIC cohort were explored and evaluated for clinical plausibility."
2021,Social and affective machine learning,https://hdl.handle.net/1721.1/129901,"Social learning is a crucial component of human intelligence, allowing us to rapidly adapt to new scenarios, learn new tasks, and communicate knowledge that can be built on by others. This dissertation argues that the ability of artificial intelligence to learn, adapt, and generalize to new environments can be enhanced by mechanisms that allow for social learning. I propose several novel deep- and reinforcement-learning methods that improve the social and affective capabilities of artificial intelligence (AI), through social learning both from humans and from other AI agents. First, I show how AI agents can learn from the causal influence of their actions on other agents, leading to enhanced coordination and communication in multi-agent reinforcement learning. Second, I investigate learning socially from humans, using non-verbal and implicit affective signals such as facial expressions and sentiment."
2021,Automatic modeling of machining processes,https://hdl.handle.net/1721.1/130833,"3 axis CNC milling is a ubiquitous manufacturing method in industry due to its versatility and precision. The fundamental parameters that dictate cutting performance (""speeds, feeds, and engagement"") must be manually set by the machine programmer; proper operation therefore relies heavily on operator skill. In this thesis, an intelligent CNC controller is presented that uses low-cost sensors to fit an analytical model of cutting forces. The analytical nature of this model allows for favorable convergence characteristics and low computational costs. This is used to optimize cutting feeds with respect to process constraints for future movements; as more data is collected, the model continuously reinforced. This intelligent controller therefore abstracts out some of the complexities of machining and makes the process more approachable."
2021,Measuring justice in machine learning,https://hdl.handle.net/1721.1/130203,"How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?"
2021,Investigating mechanisms of biophysical diversity between phasic and tonic motor neurons,https://hdl.handle.net/1721.1/130197,"Neurons exhibit striking diversity in core neuronal properties (intrinsic biophysical and synaptic properties), which are the building blocks of brain function and computation. Despite the central role of these properties in brain function, the underlying molecular and biophysical mechanisms which generate this diversity remain incompletely understood. In the Drosophila larval motor system, phasic (1s) and tonic (1b) motor neurons (MNs) differ in their intrinsic biophysical properties, providing an ideal system to examine electrophysiological diversity across neuronal populations. To address this question, we combined in vivo whole-cell patch-clamp physiology with biophysical modeling. First, we characterized biophysical diversity between 1s and 1b MNs. To explore molecular mechanisms underlying such diversity, single-neuron PatchSeq RNA profiling experiments were carried out to correlate biophysical properties with differences in ion channel gene expression profiles. These experiments suggest that cyclic nucleotide- gated like (CNGL) ion channels are upregulated in 1b MNs several folds, which indicates that CNGL could be a candidate ion channel that might specify diversity in electrical properties . To test this hypothesis, we misoverexpress CNGL in 1s MNs so that we could investigate how this ion channel contributes to the diversity between them. We developed an analysis toolset in MATLAB that can be used to analyze whole-cell patch-clamp physiology data and obtain excitability properties. Using the Izhikevich model, we were able to quantify and predict the spiking properties of 1s and 1b MNs. Using a ball and stick model, we were able to reproduce the tonic firing pattern of 1b neurons and tested tonic firing patterns in different compartments of 1b neurons. Taken together, this thesis work laid the foundation to begin characterizing biophysical mechanisms of intrinsic diversity of Drosophila neurons by combining experimental data with modeling."
2021,Inductive logic programming with gradient descent for supervised binary classification,https://hdl.handle.net/1721.1/129926,"As machine learning techniques have become more advanced, interpretability has become a major concern for models making important decisions. In contrast to Local Interpretable Model-Agnostic Explanations (LIME), this thesis seeks to develop an interpretable model using logical rules, rather than explaining existing blackbox models. We extend recent inductive logic programming methods developed by Evans and Grefenstette [3] to develop an gradient descent-based inductive logic programming technique for supervised binary classification. We start by developing our methodology for binary input data, and then extend the approach to numerical data using a threshold-gate based binarization technique. We test our implementations on datasets with varying pattern structures and noise levels, and select our best performing implementation. We then present an example where our method generates an accurate and interpretable rule set, whereas the LIME technique fails to generate a reasonable model. Further, we test our original methodology on the FICO Home Equity Line of Credit dataset. We run a hyperparameter search over differing number of rules and rule sizes. Our best performing model achieves a 71.7% accuracy, which is comparable to multilayer perceptron and randomized forest models. We conclude by suggesting directions for future applications and potential improvements."
2021,Learning causal graphs under interventions and applications to single-cell biological data analysis,https://hdl.handle.net/1721.1/130806,"This thesis studies the problem of learning causal directed acyclic graphs (DAGs) in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. The identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes, has previously been studied. This thesis first extends these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them, by defining and characterizing the interventional Markov equivalence class that can be identified from general interventions. Subsequently, this thesis proposes the first provably consistent algorithm for learning DAGs in this setting. Finally, this algorithm as well as related work is applied to analyze biological datasets."
2021,Contrasting contrastive and supervised models interpretability,https://hdl.handle.net/1721.1/130717,"In this thesis, we compare the representations of an unsupervised contrastive model to those of an equivalent supervised model using several deep neural network interpretability methods: network dissection, sparsity experiments, and saliency maps. Network dissections of self-supervised contrastive and supervised models show that the neurons of the contrastive model tend to learn about different parts of an object (ie. top-half of a dog or left-half of a person) while the neurons of the supervised model tend to learn about the entire object (ie. a dog or a person). Sparsity experiments show that the representations learned by the contrastive model are less sparse than the representations learned by the supervised counterpart model. Saliency maps show that the contrastive model focuses more on specific parts of the input image. Finally, we find that the contrastive model representations transfer better to finegrained classification tasks than the supervised model representations."
2021,Algorithms for learning to induce programs,https://hdl.handle.net/1721.1/130184,"The future of machine learning should have a knowledge representation that supports, at a minimum, several features: Expressivity, interpretability, the potential for reuse by both humans and machines, while also enabling sample-efficient generalization. Here we argue that programs-i.e., source code-are a knowledge representation which can contribute to the project of capturing these elements of intelligence. This research direction however requires new program synthesis algorithms which can induce programs solving a range of AI tasks. This program induction challenge confronts two primary obstacles: the space of all programs is infinite, so we need a strong inductive bias or prior to steer us toward the correct programs; and even if we have that prior, effectively searching through the vast combinatorial space of all programs is generally intractable. We introduce algorithms that learn to induce programs, with the goal of addressing these two primary obstacles. Focusing on case studies in vision, computational linguistics, and learning-to-learn, we develop an algorithmic toolkit for learning inductive biases over programs as well as learning to search for programs, drawing on probabilistic, neural, and symbolic methods. Together this toolkit suggests ways in which program induction can contribute to AI, and how we can use learning to improve program synthesis technologies."
2021,Phosphoproteomics analysis of Alzheimer's disease,https://hdl.handle.net/1721.1/130816,"Alzheimer's disease (AD) is a form of dementia characterized by the appearance of amyloid-[beta] plaques, neurofibrillary tangles, and inflammation in brain regions involved in memory. Despite numerous clinical trials, a limited understanding of disease pathogenesis has prevented the development of effective therapies. Several lines of genetic and biomolecular evidence indicate that AD progression involves cellular signaling through neuronal and glial protein phosphorylation networks. In order to understand which phosphorylation networks are dysregulated, I use mass spectrometry to characterize the phosphoproteome of post-mortem brain tissue from AD patients and multiple mouse models of AD. Using computational analysis, I identified several signaling pathways that are dysregulated before neurodegeneration occurs. Many of these signaling factors were expressed primarily in non-neuronal cell types, including microglia, astrocytes, and oligodendrocytes."
2021,Modeling and design of magnetic flux compression generators,https://hdl.handle.net/1721.1/129899,"The explosively-pumped magnetic flux compression generator (FCG) is a pulsed-power current amplifier powered by an explosion. This thesis surveys FCGs, demonstrating their general operation; develops a new magnetic-field-strength-based model for FCGs in the form of a generalized cylinder that more accurately captures losses to magnetic diffusion than commonly employed circuit models, but maintains simplicity in the form of a low order DAE; develops a simplified means of calculating the inductance of FCGs, providing a bridge between the field-based and circuit models; presents a design of a full loop FCG system (a topology underserved by existing literature) and an experimental setup to verify the designed loop generator; and proposes a class of non-explosive magnetic flux compression generators. The designs and models herein provide new tools and jumping-off points for further research into FCGs, particularly in the miniaturized systems gaining popularity and in the potential for reusable flux compression power sources."
2021,Supernumerary robotic limbs for human augmentation in overhead assembly tasks,http://hdl.handle.net/1721.1/111770,"Manufacturing tasks are highly demanding of work, and there is an especially high prevalence of injury associated with overhead tasks which are taxing to the shoulder and upper body. To assist workers completing these tasks, and to increase overall productivity, safety and effectiveness, we introduce a novel design of Supernumerary Robotic Limb (SRL). This is a robotic arm worn on the shoulder of the technician/- worker which extends the human capability with implicit force control algorithms that allow for intuitive control and interface of the extra robot arm. Affectionately dubbed Aucto, the robotic arm can lift an object and hold it while the wearer is securing the object using a tool with both hands. The worker does not have to take a laborious posture for a long time, reducing fatigue and injuries. Furthermore, a single worker can execute the task, which would otherwise require two workers. Two technical challenges and novel solutions are presented. One is to make the wearable robot simple and lightweight with use of a new type of granular jamming gripper that can grasp diverse objects from an arbitrary direction. This eliminates the need for orienting the gripper against the object with three-axis wrist joints, reducing the number of degrees of freedom (DOF) from 6 to 3. The other is an effective control algorithm that allows the wearer to move freely while the robot on the shoulder is holding an object. Unlike a robot sitting on a floor, the SRL worn by a human is disturbed by the movement of the wearer. An admittance-based control algorithm allows the robot to hold the object stably and securely despite the human movement and changes in posture. A 3 DOF prototype robot with a new granular jamming gripper and an ergonomic body mounting gear is developed and tested. It is demonstrated that the robot can hold a large object securely in the overhead area despite the movement of the wearer while performing an assembly work."
2021,A zero kernel operating system : rethinking microkernel design by leveraging tagged architectures and memory-safe languages,https://hdl.handle.net/1721.1/129858,"A secure kernel is the keystone upon which all software systems are built. Historically, memory corruption errors have accounted for a large portion of kernel bugs. These bugs are difficult to detect and avoid in memory-unsafe languages such as C. To mitigate such bugs, we build on top of an operating system written in a memory-safe language, Rust. Rust provides memory-safety guarantees while remaining as fast and flexible as other systems languages. Yet, some operations within operating systems, such as hand-written assembly for interrupt handling, do not fit within the scope of a language memory-safety model. To reduce the scope of these errors, microkernels isolate and reduce privilege by moving much of the traditional kernel into userspace services. However, their effectiveness is limited by the inflexibility of modern hardware. The Zero Kernel Operating System (ZKOS) emphasizes the high-level ideas of compartmentalization and least privileges on a tagged architecture. In particular, instead of relying on the Ring model and paging, which coarsely limit privilege and isolation granularity, a tagged architecture allows ZKOS to isolate at the memory word level and provide truly disjoint privileges. To this end, ZKOS slices kernelspace and userspace into fine-grained components based on function. Then, ZKOS defines specific entry and exit points between components and composes policies to limit component transitions and privileges. This increases the precision of isolation and privilege, and complements the local compile-time and runtime checks Rust performs to reduce the scope of bugs."
2021,Understanding microRNA targeting with high-throughput biochemistry,https://hdl.handle.net/1721.1/130661,"We therefore adapted a high-throughput biochemical platform utilizing random-sequence RNA libraries to obtain the vast quantity of affinity values required to predict miRNA targeting efficacy. Through a novel analytical approach, we assigned relative dissociation (K[subscript D]) constants to all binding sites </-12 nt in length, for six miRNAs. These analyses revealed unanticipated miRNA-specific differences in the affinity of similar sites, unique sites for different miRNAs, and a 100-fold influence of flanking dinucleotide context surrounding a site. These measurements informed a biochemical model of miRNA targeting that outperformed all existing models of miRNA targeting, which was extended to all miRNAs using a convolutional neural network (CNN) trained on both affinity and repression data. We also applied this high-throughput biochemical approach to understand the role of the miRNA 3' region using partially random RNA libraries."
2021,Leveraging machine learning to predict playcalling tendencies in the NFL,https://hdl.handle.net/1721.1/129909,"In this thesis, we apply four machine learning models to NFL play-by-play data from 2009-2018 to predict whether a team will run or pass the ball on a given play. We tested our models using league-wide and team-specific data in five different situations on the field. Our best league-wide models achieved a test accuracy of 80% and our best team-specific models achieved a test accuracy of 86%. Relative to the baseline of the run-to-pass ratio, the best league-wide models achieved an increase in accuracy of 25% and the best team-specific models achieved an increase of 27%. Our models showed that the Tennessee Titans, the New York Jets, and the Cincinnati Bengals have been the most predictable offenses in the NFL over 10 years. We found that a team's in-game run-to-pass ratio and their win and score probabilities are the driving factors for offensive play-calling. Additionally, our results show that teams are more predictable later in games, and that less predictable teams tend to experience greater success offensively."
2021,Pareto Gamuts : exploring optimal designs across varying contexts,https://hdl.handle.net/1721.1/129366,"Manufactured parts are meticulously engineered to perform well with respect to several conflicting metrics, like weight, stress, and cost. The best achievable trade-offs reside on the Pareto front, which can be discovered via performance-driven optimization. Objective functions used to define the Pareto front often incorporate assumptions about the context in which a part will be used, including loading conditions, environmental influences, material properties, or regions that must be preserved to interface with a surrounding assembly. Existing multi-objective optimization tools are only equipped to study one context at a time, so engineers must run independent optimizations for each context of interest. However, engineered parts frequently appear in many contexts: wind turbines must perform well in many wind speeds, and a bracket might be optimized several times with its bolt-holes fixed in different locations on each run. In this paper, we formulate a framework for variable-context multi-objective optimization. We introduce the Pareto gamut, which captures Pareto fronts over a range of contexts. We develop a global-local optimization algorithm to discover the Pareto gamut directly, rather than discovering a single fixed-context ""slice"" at a time. To validate our method, we adapt existing multi-objective optimization benchmarks to contextual scenarios. We also demonstrate the practical utility of Pareto gamut exploration for several engineering design problems."
2021,X-ray Micro-Computed Tomography and Deep Learning Segmentation of Progressive Damage in Hierarchical Nanoengineered Carbon Fiber Composites,https://hdl.handle.net/1721.1/138357,"Advanced composite laminates comprised of carbon (micro) fiber reinforced polymer (CFRP) have become widespread in modern high-performance aerospace structures, providing high, tailorable mass-specific stiffness and strength. However, while underpinning such performance benefits, CFRP microstructural heterogeneity and mechanical property anisotropy concomitantly give rise to complex damage mechanisms that lead to difficult-to-predict failure, limiting CFRP understanding. Progressive damage mechanisms in CFRPs generally encompasses a spectrum of modalities, interactions, and sequences across multiple scales, exhibiting broad sensitivity to loading conditions. Dominant damage mechanisms have been identified generally as polymer matrix cracking within (intralaminar) and between (interlaminar, termed ‘delamination’) plies, fiber fracture, fiber bundle microbuckling, and fiber/matrix interfacial debonding. Two emerging solutions aiming to suppress or delay such mechanisms toward enhanced strength and stiffness are considered in this dissertation: (i) aligned carbon nanotube (A-CNT) interlaminar reinforcement (termed ‘nanostitch’) that primarily targets delaminations, and (ii) thin-ply morphology that targets intralaminar cracking and delaminations. Both solutions have demonstrated significant mechanical improvements via standard ex situ tests that lack underlying progressive damage understanding. In view of these limitations, this dissertation advances understanding of composite progressive damage by modern ex situ and state-of-the-art in situ X-ray micro-computed tomography (µCT) studies, including advancing experimental techniques via artificial intelligence (AI), in the context of aerospace-grade CFRP strengthening and toughening effects of nanostitch, thin-ply, and their combination."
2021,Motional state engineering for continuous-variable quantum computation,https://hdl.handle.net/1721.1/130728,"The standard approach to quantum computation uses qubits, which are well-described as a two-level system. An alternative approach to quantum computation is continuous-variable quantum computation (CVQC), which uses physical observables, such as the strength of an electromagnetic field or the position of a particle in space, whose numerical values belong to continuous intervals. Trapped ions are well-developed for quantum computation, and they possess both qubit and continuous degrees of freedom that can be precisely controlled, making them a good candidate for a realization of CVQC. Although there exist software frameworks capable of simulating CVQC experiments, these frameworks do not incorporate realistic noise sources and cannot be tailored to a specific trapped-ion setup. In this work, we develop a computational framework for simulating CVQC operations using trapped ions in a realistic system with realistic noise sources. We do so first with ideal Hamiltonians and then with Hamiltonians generated directly from the electric potential and fields that can be applied to the trapped ion in a representative Paul trap. This allows for the direct simulation of a squeezing operation that can be implemented through application of voltages in trapped-ion experiments. These methods can be applied to other CVQC operations in order to allow for their direct simulation as well. We package these tools into a usable application with which we can load information about an experimental configuration and then use this simulation procedure to design and test experiments in CVQC achievable with an ion-trap setup, thus facilitating the experimental design process and eventually allowing for prediction of system behavior and comparison with experimental results."
2021,An optimizing C compiler for DSP architectures,http://hdl.handle.net/1721.1/47889,"Thesis (S.B. and M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999."
2021,Translational design computation,https://hdl.handle.net/1721.1/130836,"In Mediations, we present dynamic, synergetic, and emergent strategies for how computational mediations can occur within cocreation systems. The living and nonliving parts of any cocreation system may interact to form synergies. Combined, these synergies produce complexes that give rise to new macro-level organizations -- products of the synergies of the parts and not simply of the parts themselves. Thus, the mediation between physical, digital, and biological entities needs to address the design of dynamic relations guiding synergetic behaviors, the design of the synergetic behaviors themselves or ultimately, the design of emergent self-expression of the system. Throughout this thesis, the framework is developed theoretically and applied in practice. It is documented in publications such as Making Data Matter and Hybrid Living Materials and projects such as Wanderers, Living Mushtari, the Vespers Series, Rottlace, Lazarus, Totems, Fiberbots, and Silk Pavilion II."
2021,Observations of decision-making in the mechanical design process in a start-up company,https://hdl.handle.net/1721.1/131014,"This thesis examines the effect that working at a start-up company has on decisions and considerations during the mechanical design process, and is based on the experience of the author while interning at an Al robotics start-up as part of an MIT graduate students' team. An overview of the company is provided, the different stages of the product development are introduced and Miso's approach to the design of the modules for its product is discussed. Advantages and disadvantages of the approach are examined with examples, and suggestions for improvement are provided. In particular, the role of first-order-analysis (FOA) as a powerful tool to predict problems early is presented, the need for order as a necessary condition for growth is discussed, and next steps for the future production ramp-up stage are shared."
2021,On the use of switched-capacitor multi-level inverters for electro-aerodynamic thrust applications,https://hdl.handle.net/1721.1/130195,"A prototype HV DC-AC power converter was designed and tested for use on the dielectric barrier discharge(DBD) section of a decoupled thruster on an electro-aerodynamic(EAD) thrust plane application. The converter was a switched-capacitor multi-level inverter(SCMLI) capable of generating a variety of AC waveform shapes. The power draw of the DBD and the DC corona discharge current were measured for each waveform at a DBD voltage of 6kVpp and 10kHz, and used to estimate the thrust to DBD power characteristics of each waveform. The estimated thrust to DBD power for a sine wave and sawtooth wave were >76% higher than for a pulse wave, and >19% higher than for a square wave, indicating that a sine or sawtooth wave generating circuit such as a SCMLI may be a good choice for EAD flight applications."
2021,Deep learning methods for the design and understanding of solid materials,https://hdl.handle.net/1721.1/129054,"The trend of open material data and automation in the past decade offers a unique opportunity for data-driven design of novel materials for various applications as well as fundamental scientific understanding, but it also poses a challenge for conventional machine learning approaches based on structure features. In this thesis, I develop a class of deep learning methods that solve various types of learning problems for solid materials, and demonstrate its application to both accelerate material design and understand scientific knowledge. First, I present a neural network architecture to learn the representations of an arbitrary solid material, which encodes several fundamental symmetries for solid materials as inductive biases. Then, I extend the approach to explore four different learning problems: 1) supervised learning to predict material properties from structures; 2) visualization to understand structure-property relations; 3) unsupervised learning to understand atomic scale dynamics from time series trajectories; 4) active learning to explore an unknown material space. In each learning problem, I demonstrate the performance of the approach compared with previous approaches, and apply it to solve several realistic materials design problems and extract scientific insights from data."
2021,An explorable electrotactile display.,http://hdl.handle.net/1721.1/90639,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D.
2021,Made-up minds : a constructivist approach to artificial intelligence,http://hdl.handle.net/1721.1/77702,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1989."
2021,A Multi-Track Elevator system for E-commerce fulfillment centers,http://hdl.handle.net/1721.1/111754,"Fulfillment centers located in densely populated urban areas are an ever-growing need for leading online consumer websites. These urban fulfillment centers have limited land mass and must have innovative solutions to transport goods within the available vertical space. This work presents a Multi-Track Elevator (MTE) System, a competitive solution for rapid access and retrieval of goods in high-rise e-commerce fulfillment centers and warehouses. The MTE System consists of multiple vertical rails connected with angular traverse rails that allow multiple carriages to go up and down without collision. A novel turning point system switches track routes so that several carriages can move across the multiple rails for rapidly accessing many floors and collecting diverse goods. Unlike existing vertical-horizontal grid elevators and rail systems, the roller-coaster type, self-powered carriages on the MTE system do not have to stop at switching points, but can continually move across the network of rails. Further, this work describes the architecture of the rail network system and techniques for switching multiple rails, followed by the design of vertical turntables for smooth, continuous rail switching. Finally, outlining the use of a simple route optimization algorithm, diverse elevator systems are compared with respect to total traveling time and distance. A proof-of-concept prototype has been built and is presented."
2021,"Stimulation, speculation, simulation : the architecture of the captured city that the corporation gave us",https://hdl.handle.net/1721.1/129933,"For thousands of years, architecture built itself from a history of its own understanding of itself. A retracing over time, architecture was inherited from its past. Over time, sets of ideals were built collectively, with the city as its manifesto. It could be argued that the architecture built was in response to an architecture that already existed. the production of making architecture made cities, and further, showcased architecture as a way of thinking. Diagrammatic thinking produces diagrammatic architecture. Orthographic thinking produced orthographic architecture [1]. Machines have learned to compound a retracing of time through creating systems of algorithms that recursively produce patterns of understanding through their recordings of human behavior. The model is based on recording the past to predict the future. Increasingly, our cities have been non-stop recorded through various and ubiquitous sensing devices. The city is then re-represented afterward, through the eyes and interpretation of the machine. Or rather, by the eyes of the machine by the men who made the machine see the way they want the machine to see, building a proxy city, a representation of the real world, to help make choices in the distant and near future. How might we begin to imagine architecture as collective intelligence within this new system? Imagining architecture as a type of metadata. Mined through vision, camera, surveillance technology, connecting various strands of metadata produced surveillance capitalisms abilities. Imagine other connections mining this figural metadata could produce for a second just processing the knowns and unknowns, speculating on the possible city in Rumsfeldian ways, knowing from these systems, it is that they have the capabilities to find patterns and order not before seen. When past behavior is the basis of predicting future behavior, how might we revisit the city that was, to forge the city to come?"
2021,Improving the efficacy of teacher-facing analytics dashboards for game-based assessment and beyond,https://hdl.handle.net/1721.1/130702,"Since the early 2000s, more academic instruction has been moved to take place online, which has caused a growing demand for dashboards--websites that educators can access to monitor and understand their student's performance. However, there is a glaring lack of research dedicated to creating design principles that effectively meet instructor needs. In this work, I contribute to this field of study by utilizing feedback from teachers to build and evaluate a dashboard prototype for Shadowspect, a game-based geometry assessment, and construct an accompanying set of design recommendations. I conducted a small user study with a local high-school mathematics teacher to assess the potential of the prototype. From this evaluation, I found preliminary evidence that suggests that the design principles that guided the development of the prototype, which emphasized actionability and transparency, were successful in addressing long-existing problems in current dashboards."
2021,Passive enhancement of air flow at pedestrian level in built environments,http://hdl.handle.net/1721.1/111767,"The velocity profiles in the third to sixth canyons were measured with Acoustic Doppler Velocimetry. Compared with the reference case, void decks enhance near-ground flows in all measured canyons by up to a factor of two, but the enhancement effect weakens in downstream canyons. The wind catcher enhances the flow in the target canyon by 2.5 times with no significant effect in other canyons. The reversed wind catcher and the step-up/ step-down canyons reduce flows in the downstream canyons. The experimental data was used to validate computational fluid dynamics (CFD) models. CFD simulation results agree well with the experimental results for all cases. The validated CFD models were then used to study the void decks and the wind catcher in three-dimensional canyons. Void decks double near-ground flows in all canyons. The wind catcher increases near-ground flow in the target canyon by only 50% due to leakage at the sides."
2021,Scalable integrated screening tools for cardiovascular disease,https://hdl.handle.net/1721.1/129918,"We have developed Android mobile apps and hardware capable of performing pulse wave analysis (PWA) and measuring pulse wave velocity (PWV) using PPG techniques. The analysis algorithms are configured to run on a custom server that is able to handle large amounts of medical data. In this thesis, I describe the PWA and PWV algorithms, the mobile applications associated with these measurements, and their integration with a custom server. To validate these new algorithms, data was used from two separate clinical studies conducted by our group. For PWA, I analyzed PPG waveforms from young athletic people, young non-athletic people, old healthy people, and old CAD patients, which resulted in median PWA Scores of 3.51 (0.57), 3.19 (0.78), 1.98 (0.66), and 1.81 (0.5) respectively. From these results, the PWA tool demonstrated sufficient sensitivity to distinguish between the four different cardiovascular health classifications."
2020,Machine learning methods for targeting and new product development,https://hdl.handle.net/1721.1/123572,"We conduct two large-scale field experiments to evaluate seven methods widely used to design targeting policies. The findings compare the performance of the targeting methods and demonstrate how well the methods address common data challenges. The challenges we study are covariate shift, concept shift, information loss through aggregation, and imbalanced data. We show that model-driven methods perform better than distance-driven methods and classification methods when the training data is ideal. However, the performance advantage vanishes in the presence of the challenges that affect the quality of the training data. Chapter 3: Firms typically compare the performance of different targeting policies by implementing the champion versus challenger experimental design. These experiments randomly assign customers to receive marketing actions recommended by either the existing (champion) policy or the new (challenger) policy, and then compare the aggregate outcomes."
2020,Prescriptive analytics in operations problems : a tree ensemble approach,https://hdl.handle.net/1721.1/123709,"In chapter 3, we show how to solve optimization problems with random forest objective functions and general polyhedral constraints. We show how to formulate this problem using MIO techniques and show this formulation can be decomposed and solved iteratively using Pareto-optimal Benders cuts. We also provide analytical guarantees on an approach that approximates a large scale random forest optimization problem by optimizing over a smaller forest, and develop heuristics based on ideas from cross validation. In chapter 4, we study a new problem where nurse practitioners need to be dynamically routed to patients' houses as service requests are received. We show how to solve using Approximate Dynamic Programming and develop methods to solve ADP's with combinatorial action spaces and non-linear cost-to-go functions approximated using a tree or tree ensemble approximation."
2020,Congestion-dependent pricing for a service provider,http://hdl.handle.net/1721.1/9456,"A service provider who provides users access· to a communication network is constrained in the system by the limited amount of bandwidth that can be offered to users. We will discuss congestion-dependent pricing, a method of pricing connection access to the network that accounts for this bandwidth constraint. Different models for the system are analyzed and simulated. The case of multiple classes of users as well as the case of a probabilistic demand function are analyzed in detail. The dynamic congestion-dependent pricing policy that maximizes performance will be determined using dynamic programming. The steady state revenue generated for the service provider will be used as the measure of system performance. Additionally, approximation and estimation techniques to simplify analysis and implementation of different systems are analyzed."
2020,Accommodation-through-Bypassing : overcoming professionals' resistance to the implementation of algorithmic technology,https://hdl.handle.net/1721.1/126962,"While algorithmic technologies are rapidly changing how work is performed in professional organizations, professional workers are resisting the implementation of these technologies in their workplaces. Previous studies of the development and implementation of workplace technologies suggest that managers or technology developers respond to workers' resistance in a way that is intended to make workers more amenable to using the technology, and that professionals' recalcitrance can ultimately impede the adoption of new technology despite managers' or developers' efforts. In my a 21-month field study of the development and implementation of three cases of machine learning-based algorithmic technology, I find that that the development and implementation of algorithmic technologies can proceed in the face of professionals' resistance when developers bypass the professional workers and repurpose the technology for use by another group of actors that is present in professional work settings: managers, administrators, and other ""central"" actors. Using the post-humanist concept of tuning -- which views technology development as a dialectic between resistance and accommodation -- I show that in order to carry out accommodation-through-bypassing, developers engage in a series of practices whereby they strategically manage relations with the resistant ""local"" actors, the technology itself, and the ""central"" actors. These findings highlight that the dialectic of resistance and accommodation that characterizes the technology development process can occur even in the face of strong professional recalcitrance, that accommodation can be strategically geared toward workers who are not the originally-intended users of a technology, and that technology developers can play a key role in influencing workplace relations in professional organizations."
2020,Building blocks for regenerative medicine : vascularized models and immunomodulation to engineer hepatic cell therapies,https://hdl.handle.net/1721.1/124281,"By applying unsupervised machine learning techniques to scan the secretome in stimulated devices, we identified endothelial-derived mediators that can independently stimulate proliferation of human hepatocytes. Collectively, the data presented here underscore the importance of multicellular models that integrate tunable biochemical and fluid forces, and demonstrate that SHEAR devices can be used to discover and validate conditions that promote human liver regeneration. Limiting the allogeneic immune response is a major challenge in the implementation of cell-based therapies. To ameliorate this problem, we engineered an immune cloak around transplantable liver tissue by enabling trans-presentation of immune checkpoint pathways. This technology is called SHIELD (stealth hepatic immunotolerant ensembles for liver disease). SHIELD activates checkpoint pathways in supporting stromal cells and/or in endothelial cells lining the vasculature to induce immune cell exhaustion and anergy."
2020,The complexity of the future of work,https://hdl.handle.net/1721.1/123625,"Rapidly advancing cognitive technologies, such as artificial intelligence (AI), have the potential to drastically impact modern society and to shape the future of work. Accordingly, policy makers and researchers seek forecasts into technological change and labor trends, including growing job polarization and income inequality as well as decreasing career mobility and spatial mobility for workers. Although a given technology impacts demand for only a narrow set of workplace skills, modern empirical work relies on coarse labor distinctions between cognitive and physical or routine and non-routine work to explain employment trends. In this dissertation, I explore the complex ways that skills and employment undergird aggregate labor dynamics in the US. As a motivating example, I demonstrate how simple measures for skills within a labor market contribute to the differential impact of automation across US cities of different sizes."
2020,Molecular graph Self attention and graph convolution for drug discovery,https://hdl.handle.net/1721.1/124232,"Drug development is an important, but complicated and expensive process. By utilizing the power of deep learning, we aim to improve the current process of drug development. We model molecules as undirected graphs and use graph convolutions and self-attention to predict molecular properties. With a series of ablation studies, we demonstrate the added value of several key components in our network. We analyze two standard datasets: BBBP, which includes classication data on whether molecules pass the blood-brain barrier, and ClinTox, which includes toxicity information. Using our architecture, we are able to match state of the art performance on the BBBP prediction task."
2020,Development of designer-relevant Lattice-Boltzmann Wind Field Model for urban canyons and their neighborhoods,http://hdl.handle.net/1721.1/104293,"Wind field analysis is one of the most important components for designers to achieve a thermally-comfortable and energy-efficient building design. Designers need a fast and relatively accurate wind field model to get integrated into the design workflow, but current platforms to work on are either costly and time-consuming conventional Computational Fluid Dynamics (CFD) tools or over-simplified data correlation factors, which makes the workflow undesirable for designers' use. In this thesis, a novel Lattice-Boltzmann Wind Field Model (LBWFM) is developed and integrated in a designer-relevant Rhino-based environment. Lattice-Boltzmann Method (LBM) is introduced as the solver due to its open-source and parallelism natures, and coded in C# language for three-dimensional urban airflows. Results of the model are validated with experimental measurements as well as conventional CFD tools for both wind velocity and pressure fields. To further enhance the computational efficiency, proper settings of inlet wind profile and optimal modeling domain size are investigated for the LBWFM. And the relative wind pressure coefficient calculated out of the model is then applied in the analysis of wind-driven natural ventilation potential with the indicator of air exchange flow rate. Finally the limitation of the model is stated and future work is discussed on the modifications of buoyancy effect and potential extension is addressed in the application of LBWFM."
2020,Data-driven optimization with behavioral considerations : applications to pricing,https://hdl.handle.net/1721.1/123707,"In chapter 3, we study a retailer's optimal promotion strategy when demand is affected by different classes of customers' status in the rewards program and their heterogeneous redemption behavior. We formulate the retailer's problem as a dynamic program and prove a unique optimal threshold discounting policy. We also propose an approximation algorithm of the optimal price as a convex combination of the optimal prices for each class separately. Using data from a fast food chain, we assess the performance of the algorithm and the optimal pricing compared to current practice. In chapter 4, we are concerned with accurately estimating price sensitivity for listed tickets in the secondary market. In the presence of endogeneity, binary outcomes and non-linear interactions between ticket features, we introduce a novel loss function which can be solved using several off-the-shelf machine learning methods."
2020,Light source relighting for indoor scene photos with deep neural networks,https://hdl.handle.net/1721.1/127441,"We seek to use deep neural networks to develop a method to detect the light sources in a given image of an indoor scene, computationally adjust their lighting intensity, and re-render the edited scene as an image. By doing so, we can visually relight the image--effectively turning the light source ""on"" or ""off"" in the image. This thesis introduces such a method by using Generative Adversarial Networks (GANs) and intervention techniques to this end. The method is composed of a pipeline of processing stages, from detecting the light sources to reconstructing the scene in GAN representation space to performing edits on the GAN representation to fine-grained control over the edited lighting, and we present its results here. The thesis work has a wide range of applications in the field of content creation and image editing."
2020,New optimization approaches to matrix factorization problems with connections to natural language processing,https://hdl.handle.net/1721.1/127291,"The new approach builds uncertainty sets using one-sided constraints and two hypothesis tests, uses alternating optimization and projected gradient methods, including Adam and mirror descent, to find good local optima. In computational experiments, we demonstrate that these models are better able to avoid over-fitting than LDA and PLSA, and result in more accurate reconstruction of the underlying topic matrices. In Chapter 5, we develop modifications to latent Dirichlet allocation to account for differences in the distribution of topics by authors. The chapter adds author-specific topic priors to the generative process and allows for co-authorship, providing the model with increased degrees of freedom and enabling it to model an enhanced set of problems. The code for the algorithms developed in each chapter in the Julia language is available freely on GitHub at https://github.com/lauren897"
2020,An adaptive partitioning scheme for ad-hoc and time-varying database analytics,http://hdl.handle.net/1721.1/105961,"Data partitioning significantly improves query performance in distributed database systems. A large number of techniques have been proposed to efficiently partition a dataset, often focusing on finding the best partitioning for a particular query workload. However, many modern analytic applications involve ad-hoc or exploratory analysis where users do not have a representative query workload. Furthermore, workloads change over time as businesses evolve or as analysts gain better understanding of their data. Static workload-based data partitioning techniques are therefore not suitable for such settings. In this thesis, we present Amoeba, an adaptive distributed storage system for data skipping. It does not require an upfront query workload and adapts the data partitioning according to the queries posed by users over time. We present the data structures, partitioning algorithms, and an efficient implementation on top of Apache Spark and HDFS. Our experimental results show that the Amoeba storage system provides improved query performance for ad-hoc workloads, adapts to changes in the query workloads, and converges to a steady state in case of recurring workloads. On a real world workload, Amoeba reduces the total workload runtime by 1.8x compared to Spark with data partitioned and 3.4x compared to unmodified Spark."
2020,Coflow scheduling in data center networks,https://hdl.handle.net/1721.1/124265,"In this work, I analyze the problem of coflow scheduling on a multi-stage cross bar switch with a set of intermediate ports. Using a multi-stage abstraction for the data center network enables development of algorithms that reduce the network latency for certain traffic patterns. In this context, I compare the capabilities of the multi-stage network with the cross-bar switch via simulations and theoretical calculations. A theoretical upper bound of 1/e was shown for the ratio of the difference between the minimum total coflow completion times (TCCT) on these two networks and the minimum TCCT on the cross-bar switch. On the other hand, simulations using approximation algorithms gave values of up to 8 percent for the average of the same ratio under randomly generated trac patterns."
2020,Adaptive sampling of transient environmental phenomena with autonomous mobile platforms,https://hdl.handle.net/1721.1/124212,"is subsequently presented, which addresses the MSS problem and overcomes key technical challenges with planning in natural environments. Theoretical performance guarantees are derived for PLUMES, and empirical performance is demonstrated against canonical uniform search and state-of-the-art baselines in simulation and field trials. Ultimately, this thesis examines the challenges of autonomous informative sampling in the environmental and earth sciences. In order to create useful systems that perform diverse scientific objectives in natural environments, approaches from robotics planning, field design, Bayesian optimization, machine learning, and the sciences must be drawn together. PLUMES captures the breadth and depth required to solve a specific objective within adaptive sampling, and this work as a whole highlights the potential for mobile technologies to perform intelligent autonomous science in the future."
2020,Deep learning with physical and power-spectral priors for robust image inversion,https://hdl.handle.net/1721.1/127013,"Conversely, if more restricted examples are used as training examples, the training can be guided to undesirably ""remember"" to produce the ones similar as those in training, making the cross-domain generalization problematic. Next, we also propose to use deep learning to greatly accelerate the optical diffraction tomography algorithm. Unlike previous algorithms that involve iterative optimization algorithms, we present significant progresses towards 3D refractive index (RI) maps from a single-shot angle-multiplexing interferogram. Last but not least, we propose to use cascaded neural networks to incorporate the system physics directly into the machine learning algorithms, while leaving the trainable architectures to learn to function as the ideal Proximal mapping associated with the efficient regularization of the data. We show that this unrolled scheme significantly outperforms the End-to-End scheme, in low-light imaging applications."
2020,Dynamic and robust network resource allocation,https://hdl.handle.net/1721.1/123565,"We formulate a dynamic robust optimization model that addresses this decision question, apply a tractable solution heuristic, and prove theoretical guarantees of the heuristic's performance. Our model is calibrated with publicly available data to generate insights on how the policymakers should balance investment between medical inventory and personnel training. The World Health Organization and regional public health authorities decide on the influenza (flu) vaccine type ahead of flu season every year. Vaccine effectiveness has been limited by the long lead time of vaccine production - during the production period, flu viruses may evolve and vaccines may become less effective. New vaccine technologies, with much shorter production lead times, have gone through clinical trials in recent years. We analyze the question of optimal vaccine selection under both fast and slow production technologies. We formulate the problem as a dynamic distributionally robust optimization model."
2020,A Comparison of artificial intelligence algorithms for dynamic power allocation in flexible high throughput satellites,https://hdl.handle.net/1721.1/127074,"Although multiple AI approaches have been proposed in the recent years, most of the analyses have been conducted under assumptions that do not entirely reflect the new operation scenarios' requirements, such as near-real time performance or high-dimensionality. Furthermore, little work has been done in thoroughly comparing the performance of different algorithms and characterizing them. This Thesis considers the Dynamic Power Allocation problem, a DRM subproblem, as a use case and compares nine different AI algorithms under the same near-real time operational assumptions, using the same satellite and link budget models, and four different demand datasets. The study focuses on Genetic Algorithms (GA), Simulated Annealing (SA), Particle Swarm Optimization (PSO), Deep Reinforcement Learning (DRL), and hybrid approaches, including a novel DRL-GA hybrid. The comparison considers the following characteristics: time convergence, continuous operability, scalability, and robustness."
2020,"Urban computing using call detail records : mobility pattern mining, next-location prediction and location recommendation",http://hdl.handle.net/1721.1/104156,"Urban computing fuses computer science with other fields, such as transportation, in the context of urban spaces by connecting ubiquitous sensing technologies, analytical models and visualizations to solve challenging problems in urban environment and operation systems. This paper focuses on Call Detail Records, one widely collected opportunistic sensing data source for billing purposes, to understand presence patterns, develop mobility prediction methods and reduce traffic congestions with location recommendations. Understanding human mobility and presence patterns at locations are the building blocks for behavior prediction, service design and system improvements. In the first part, this thesis focuses on 1) understanding presence patterns at user locations with a proposed metric Normalized Hourly Presence, 2) extracting common presence patterns across the population with Principal Component Analysis; 3) and infer home and workplaces using K-means Clustering and Fuzzy C-means Clustering. The proposed method was implemented on MIT Reality Mining data, by which we demonstrate that with inference rates of 56% and 82%, the method can improve 79% and 34% in accuracy respectively in home and workplace inference comparing to the baseline model. In addition, it was implemented on the CDR data collected in a crowded city in China to prove its scalability and applicability in real-world applications. With Fuzzy C-means Clustering, we could flexibly trade-off between inference rate and accuracy to understand the interplay between the two and apply it for various purposes. With an understanding of mobility patterns, the next crucial foundation in urban computing is mobility prediction, enabling transportation practitioners to take actions beforehand and commercial organizations to send location-based advertisements, etc. Specifically, this paper focuses on next-location prediction from Call Detail Records. Mobility traces was analogized to language models, mapping cell towers to words and individual location traces to sentences. Recurrent Neural Network is a successful tool in natural language processing, which is applied in mobility prediction due to its acceptance of sequential input, variable input length and ability to learn the 'meaning' of cell towers. By implementing the method on Call Detail Records collected in Andorra, we show that the method improved more than 40% over the baseline model, with 67% and 78% accuracy in next location at cell tower and merged cell tower level respectively. The 'meanings' of the cell tower could also be inferred, the same as learning the meanings of words in sentences, from the embedding layer of Recurrent Neural Network. The last project aims at tackling the challenge of severe traffic congestions with location recommendations. The availability of large-scale longitudinal geolocation data, such as Call Detail Records, offers planners and service providers an unprecedented opportunity to understand location preferences and alleviate traffic congestions. Location recommendation is a potential tool to achieve these two objectives. Previous research on location recommendations has focused on automatically and accurately inferring users' preferences, while little attention has been devoted to the constraints of service capacity. The ignorance may lead to congestion and long waiting time. We argue that Call Detail Records could help planners and authorities make interventions by providing personalized recommendations given the comprehensive urban-wide picture of historical behaviors and preferences. In this research, we propose a method to make location recommendations for system efficiency, defined as maximizing satisfactions toward recommendations subject to capacity constraints, exploiting travelers' choice flexibilities. We infer implicit location preferences based on sparse and passively-collected Call Detail Records. We then formulate an optimization model the defined system efficiency. As a proof-of-concept experiment, we implement the method in Andorra, a small European country heavily relying on tourism. By extensive simulations, we demonstrate that the method can reduce the travel time increased by congestion during peak hour from 11.73 minutes to 5.6 minutes with idealized trips under full compliance rates. We show that the average travel time increased by congestion is 6.17, 6.98, 8.37 and 10.98 minutes with 80%, 60%, 40% and 20% compliance rates. Overall, our results indicate that Call Detail Records can be used to make locations recommendation while reduce traffic congestion for system efficiency. The proposed method can be applied to other large-scale location traces and extended to other location or events recommendation applications."
2020,Design of an advanced sEMG processor for wearable robotics applications,https://hdl.handle.net/1721.1/124074,"This thesis presents the design and evaluation of a surface electromyography(sEMG) acquisition platform specialized for wearable robotic applications. While sEMG is one of the best ways to interpret human muscle and neural activity, the research field in wearable robotics has limitations in utilizing sEMG signals with commercially-available sEMG acquisition platforms. These limitations include a large and bulky electronics package, poor portability, insufficient electrode versatility, and limited reconfigurability. This thesis aims to present an effective design that provides solutions to these many difficulties while insuring robustness and acquisition signal quality. The thesis reasons and explains in detail every design decision and process among the system development and manufacturing. The evaluation of the manufactured system compared to a benchtop state-of-the-art sEMG recording platform is demonstrated. Practical utility of the developed sEMG measurement system is also demonstrated with a real world wearable robotic application."
2020,Mechanistic modeling of bacterial nutrient uptake strategies,https://hdl.handle.net/1721.1/124116,"Bacteria have developed a variety of strategies to nd and consume the substrates necessary for both the cell's energy-consuming processes and for the additional biomass needed to replicate. A greater understanding of the diversity and regulation of these strategies can provide us with a number of insights relevant for a variety of applications, from predicting bacterial population dynamics and thus carbon-cycling rates in the ocean to bio-engineering bacteria into microscale robots. Here I use toy, mechanistic models of single-cell metabolism that allow me to quantify the costs and benefits of various nutrient uptake strategies. I find that: (i) a sensing-uptake trade-off governs E. coli's regulation of maltose uptake and chemotaxis to maltose; (ii) a rate-affinity trade-off in nutrient transport systems governs the speciation of marine oligotrophic and copiotrophic heterotrophs; and (iii) an exploration-conservation trade-off governs the prevalence of motility in the marine microbial world. This work thus provides new understanding of how both phenotypic diversity and cellular regulation are governed by trade-offs for maximizing growth rate in dierent environments."
2020,Non-contact ultrasound,https://hdl.handle.net/1721.1/123779,"This thesis explores the design, development, and evaluation of two novel non-contact ultrasound imaging methods: immersion ultrasound and optical ultrasound. Immersion ultrasound utilizes traditional piezoelectric elements in a tomographic framework to develop new algorithms and acquisition methods for quantification of tissue geometry and properties in human proximal limbs. Bone is uniquely challenging for ultrasound due to the high impedance mismatch between bone and soft-tissue in the imaging domain. New imaging algorithms are necessary for both geometric and quantitative reconstruction of subjects with bone. Multiple immersion systems were designed and constructed using the framework presented in this thesis. Mechanical systems include a 4 degrees of freedom single element system and a fully flexible 36 degrees of freedom robotic system abbreviated MEDUSA (Mechanically Discrete Ultrasound Scanning Apparatus)."
2020,Long-wave infrared frequency combs based on quantum cascade lasers,http://hdl.handle.net/1721.1/113922,"Ever since the invention of quantum cascade laser (QCL), the performance and the flexibility in design has made it a desirable source for a wide range of applications, such as trace-chemical sensing, health monitoring, frequency metrology, noninvasive imgaing and infrared countermeasures. The LWIR region (or mid-infrared region), roughly ranging from 2-20 [mu]m, is of particular importance to spectroscopy applications, since many molecular species have their strongest rotational-vibrational absorption bands in that area. Infrared laser spectroscopy began about 40 years ago and has been using a variety of different tunable laser-based sources, particularly lead salt diodes, color center lasers, difference frequency generation and optical parametric oscillators. The large tunabilitiy in the design (lasing frequency, tunability, power, material system, etc.) and the compactness in fabrication and packaging has made QCL an ideal source for laser-based spectroscopy. Traditional spectroscopy systems suffer from problems like large physical dimensions, long data-processing times and spectral resolution restrictions. Therefore the development of a simple, robust, compact and inexpensive optical source/system like QCL frequency combs can largely benefit spectroscopy systems. In the past few years, QCLs have proven to be able to form comb radiation in both LWIR and THz regions. And dual comb spectroscopy has been demonstrated using QCL frequency combs with very short acquisition time ([mu]s). The development of a broadband, high power, narrow linewidth and stable LWIR frequency comb based on quantum cascade laser is the key to realizing such broadband ultrafast spectrometer in the mid-infrared range. This thesis explores the design, fabrication and characterization techniques towards the development of LWIR QCL frequency comb devices for spectroscopic purposes. A complete wet etch epi-up fabrication process is reported, with preliminary results on the dry-etch technique to incorporate dispersion compensation strucutre and epi-down fabricaiton for high power CW mode QCL device. Formation of comb(-like) regime has been observed in two devices, with the Gires-Tournois Interferometer (GTI) mirror providing dispersion from the rear facet. In order to improve the comb performance of these devices, dispersion of the device is measured to provide essential information for the design of chirped top cladding for dispersion compensation. This thesis provides an important step towards the realization of a room temperature, broadband, CW mode LWIR QCL frequency comb device for spectroscopic purposes."
2020,Preventing IPC-facilitated type confusion in Rust,https://hdl.handle.net/1721.1/128627,"Type-safe languages undertake to prevent the type confusion vulnerabilities that arise in type-unsafe languages such as C++. One such type-safe language is Rust, which provides powerful type safety guarantees [1]. However, these guarantees are valid only for a single compilation unit. That is, they may not hold when multiple separately compiled processes communicate. In this work, we explore how type confusion vulnerabilities can still arise when multiple separately compiled, internally type-safe processes share information through inter-process communication (IPC). We propose safeIPC, a tool for eliminating IPC-facilitated type confusion in Rust. safeIPC is a Rust compiler extension that detects communications over IPC and inserts runtime checks to ensure that type safety is maintained. Programs instrumented with safeIPC throw a runtime error if the type of any data received over IPC is not equivalent to the type expected. Our analysis shows that safeIPC is effective in preventing type confusion vulnerabilities not prevented by Rust alone."
2020,A stereo vision system with automatic brightness adaptation,http://hdl.handle.net/1721.1/9438,"This thesis describes the development of an automatic brightness adaptive imaging system for use in stereo vision algorithms implemented for a variety of processing architectures. A 256 x 256 array of wide dynamic range pixels with on-chip A/D converters provides the digital data path for a feedback network which controls the charge integration parameters at each pixel. The first goal of the project was to build a real-time demonstration of the imager with configurable compression functions. Secondly, electronic irising was employed by controlling the global charge integration time based on the average intensity of the image. In addition to electronic-irising, the imaging system employs a linear or a logarithmic compression scheme based on the image data. The controller fits the compression function to the image by comparing the average intensities of many different regions within the image. Finally, a 3-camera stereo-vision system was developed with data transfer to a PC through the PCI bus at 60fps. The imagers are synchronized and controlled based on the center imager's data which allows for consistent object correlation in stereo vision algorithms."
2020,Infrastructures for secure multiparty computation,https://hdl.handle.net/1721.1/127006,"We devise information theoretically secure protocols that allow additional pairs of parties to establish secure OT correlations using the help of other parties in the network in the presence of a dishonest majority. Our main technical contribution is an upper bound that matches known lower bounds regarding the number of OT channels necessary and sufficient for MPC. In particular, we characterize which n-party OT graphs G allow t-secure computation of OT correlations between all pairs of parties, showing that this is possible if and only if the complement of G does not contain the complete bipartite graph Kn-t,n-t as a subgraph. Finally, we study the problem of building an infrastructure for fair secure computation, where we guarantee that if any party receives the output of the secure computation, then all honest parties do as well."
2020,Modeling and tradespace exploration of a space suit hip bearing assembly using multi-degree-of-freedom range of motion analysis,https://hdl.handle.net/1721.1/124205,"Space suits are crucial to human spaceflight, but can restrict motion, require additional energy, and increase injury risk. Previous planetary suits were largely based on flexible components, which generate additional forces on the occupant as they resist volumetric changes from flexing components. The NASA Mark III suit addresses this problem using a Hip Brief Assembly (HBA), composed of rigid, constant-volume sections connected by bearings. However, due to the rigid components and fixed degrees of freedom (DoFs), the HBA and other hard-component joint assemblies (HCJAs) have stricter bounds on motion. For example, previous analysis shows that the hip multi-DoF range of motion (ROM) for an HBA occupant is not well-aligned with the nominal hip ROM during gait (gait NHROM). In this thesis, a set of methods for describing HCJA geometry and the effect on occupant ROM is presented."
2020,Tracking engagement : a machine learning framework for estimating affective engagement,https://hdl.handle.net/1721.1/127333,"Globally, construction fatality counts remain among the highest of all industries. As part of efforts to improve workers occupational health and safety, most companies provide workers with ongoing safety training. Yet accidents continue to take place, as there is a lack of understanding on how to increase the knowledge transfer that would help improve safety. The goal of this thesis is to automate and improve manual observation methods, presently used to determine construction workers' engagement during training courses by applying machine learning techniques to video images. This thesis proposes a framework to measure construction workers' engagement during training courses by unobtrusively analyzing engagement through body and pose estimation, codifying who is speaking and understating the predicted emotional state of a given worker through their facial expressions of emotion at specific lectures times through stateof- the-art computer vision techniques. The framework was prototyped on fifteen graduate and undergraduate students from a private university in the United States during four class sessions in a stadium set up classroom by three high definition cameras. The proposed system can enhance our understanding of learning processes within classroom contexts, while reducing the labor-intensive process of traditional observations methods, and allowing for the observation of a full class simultaneously. Further, the repeatability and standardization of objective observations will be improved as it will no longer depend on the skills of the observer and on his or her ability to capture and make sense of what was observed."
2020,Designing structures with tree forks : mechanical characterization and generalized computational design approach,https://hdl.handle.net/1721.1/127284,"The thesis presents results that systematically test this methodology by studying how matching quality varies depending on the number and species of tree forks available in the library and relates this back to the mechanical properties of tree branches found through physical testing. Second, mechanical laboratory testing of tree fork nodes of various tree species (available locally in the area) is presented to quantify the structural capacity of these connections and observe the behavior under tree fork load transfers. A structural score is developed to characterize the tolerance of tree fork nodes to imperfect matches in terms of structural capacity; these resulting geometries are compared to the previous matching-based scoring system. The resulting approach is projected forward as a framework for a more general computational approach for designing with existing material systems and geometries that can also be expanded beyond tree forks."
2020,The effects of fuel volatility and operating conditions on sprays from pressure-swirl fuel injectors,http://hdl.handle.net/1721.1/9427,"Optimal design of modern direct injection gasoline engines depends heavily on the fuel spray. Most of the studies published regarding these fuel sprays involve cold bench tests or motored optical engines, neglecting the roles of the fuel volatility and temperature. This study, therefore, was designed to describe changes in the spray properties due to fuel volatility and operating conditions using a firing optically-accessible engine. Planar laser-induced fluorescence and planar Mie scattering imaging experiments were performed to show changes in the spray structure, including its radial and axial penetration. Phase-Doppler particle analysis experiments were included to track the droplet diameter and velocity at various points throughout the spray. A computational fluid dynamics model was also used to study the physics leading to the observed changes. The results show that the spray structure changes with not only ambient gas density, which is often measured, but also fuel temperature and volatility. The mean droplet diameter was found to decrease substantially with increasing fuel temperature and decreasing ambient density. Under conditions of low potential for vaporization, the observed trends agree with published correlations for pressure-swirl atomizers. As ambient density decreases and fuel temperature increases, the volatile ends of multi-component fuels evaporate quickly, producing a vapor core along the axis of the spray. Beyond a certain point, evaporation is violent enough to cause additional breakup of the droplets. A fit to this volatility-induced breakup data provides an additional correlation for determining the mean diameter of volatile sprays. Coincident with the volatility-induced breakup trend is an increase in the initial cone angle of the spray. However, the reduced droplet diameter and rapid vapor generation under these superheated conditions result in a narrow spray with increased axial penetration. In the process of performing these experiments, insights were found regarding the operation of these diagnostics in high-density sprays."
2020,Prediction and analysis of degree of suicidal ideation in online content,https://hdl.handle.net/1721.1/127664,"Machine learning (ML) has increasingly been used to address the growing burden of mental illness and lack of access to quality mental health care. Recently such models have been applied to online data, such as social media postings to augment mental health screening. Despite the potential of these methods, online ML classifiers still perform poorly in multi-class settings. In this thesis, we propose the usage of novel document embeddings and mental health based user embeddings for triaged suicide risk screening. Machine learning to infer suicide risk and urgency is applied to a dataset of Reddit users in which the risk and urgency labels were derived from crowdsource consensus. We show that the document embedding approach outperforms count-based baselines and a method based on word importance, where important words were identified by domain experts. We examine interpretable features and methods that help to discern and explain risk labels. Finally, we find, using a Latent Dirichlet Allocation (LDA) topic model, that users labeled at-risk for suicide post about different topics to the rest of Reddit than non-suicidal users."
2020,Investigating the influence of biosphere-atmosphere interactions on atmospheric chemistry and composition,https://hdl.handle.net/1721.1/124187,"The interactions between the biosphere and the atmosphere are an important controlling factor for regional to global atmospheric chemistry and composition. This ultimately has wide impacts on issues like air quality and climate change. However, there are still substantial uncertainties in the biosphere-atmosphere interaction processes that drive the global abundance and variability of many critically important atmospheric constituents, including ozone, aerosol, and Volatile Organic Compounds (VOCs). This thesis aims to address these uncertainties through a multifaceted approach, combining theory and data-driven models with observations. The scope of the research completed herein is introduced and described in Chapter 1. Chapter 2 is a case study of biosphere atmosphere interactions where the air quality impact of large-scale agricultural deforestation in Southeast Asia is investigating using global models."
2020,Applying Critical Chain to improve the management of uncertainty in projects,http://hdl.handle.net/1721.1/9399,"In an ever intensifying global competitive market, the management of projects, particularly product development efforts, increasingly is one of the few areas which can produce a sustained competitive advantage. Firms that can bring products to market faster can extract higher initial margins, can be more responsive to their customers, and will have products with longer sales lives. Critical Chain is a new methodology that applies Eli Goldratt's Theory of Constraints to project management in order complete projects faster and with greater predictability while simultaneously making more efficient use of resources. The Critical Chain method accomplishes this by building project networks with average task durations, aggregating buffer at the end of projects where it can absorb unplanned iterations and other delays, and de-conflicting resources, both within and across projects. This new project management methodology was researched by spending seven months on site with ITT Night Vision and applying the method to two product development projects. In addition, benchmarking studies of previous product development efforts at the same site and of another lead user of the tool were conducted to provide both qualitative and quantitative comparison data. Critical Chain appears to minimize schedule risk while simultaneously minimizing project duration, and has the potential to improve both communication and employee morale."
2020,Precision measurement of and search for dark matter in the transverse momentum spectra of Z bosons,https://hdl.handle.net/1721.1/124575,"A measurement of the differential Z boson production cross section in proton-proton collisions is presented. It furnishes a precision test of the Standard Model, and constrains the parton distribution functions of the proton. Moreover, it is a building block for future measurements of the mass of the W± boson. A study of the efficiency of lepton identification algorithms is performed which drives the precision of the measurement at lower values of transverse momentum. In tandem, a search for new physics in events with a Z boson produced in association with large missing transverse momentum is presented. The results of this search are interpreted in the context of several dark matter models: generic spin-0 or spin-1 mediators, invisible decays of a Higgs-like boson, unparticles, and large extra spatial dimensions. A multivariate analysis was developed to enhance the sensitivity of the invisible Higgs interpretation. The theoretical uncertainty on the irreducible background from electroweak diboson processes is constrained by emulating the missing energy using pure control samples in the fully leptonic final states. The data were collected with the Compact Muon Solenoid detector at the Large Hadron Collider and correspond to an integrated luminosity of 35.9 fb-1. No significant deviations from the Standard Model are found."
2020,Steps towards proof construction using reinforcement learning : environments and models for hypothesis-posing as subtask creation,https://hdl.handle.net/1721.1/124245,"Despite recent advances in reinforcement learning (RL) that have allowed AI algorithms to master games such as Go from scratch, scant progress has been made on applying RL to one of the first tasks seen as susceptible to automation: theorem proving. I present steps towards training agents to construct proofs through utilizing the ability to pose hypotheses as a way to uncover information and break tasks down into subtasks. To do so, I create a novel bitstring problem that retains many of the challenges posed by proof construction while dispensing with the need to parse grammars. I then assess the performance of well-known RL algorithms on tasks derived from this problem, demonstrating that it is non-trivial. Finally, I alter a model that successfully learns one of the bitstring tasks in order to acquire results on possible mechanisms for theorem-proving prototypes."
2020,Optimization of Patterned Surface Structures,https://hdl.handle.net/1721.1/127323,"This thesis advances a recent work on the optimization of patterned surface structures used for architecture and structural engineering. On their own, well-designed surface structures--such as plates and shells--can be highly efficient, but by introducing specific aperture patterns, designers can further improve their potential for structural efficiency. Used as a way to invigorate an otherwise homogeneous architectural environment, even intricately patterned surfaces with highly complex geometries can be realized thanks to recent advancements in digital fabrication technologies. Most recent work on the integration of pattern design and structural optimization lacks general structural engineering applicability and does not address the incompatibility of traditional analysis methods with contemporary CAD environments."
2020,Lexical and Language Modeling of Diacritics and Morphemes in Arabic Automatic Speech Recognition,http://hdl.handle.net/1721.1/87941,"Arabic is a morphologically rich language which rarely displays diacritics. These two features of the language pose challenges when building Automatic Speech Recognition (ASR) systems. Morphological complexity leads to many possible combinations of stems and affixes to form words, and produces texts with high Out Of Vocabulary (OOV) rates. In addition, texts rarely display diacritics which informs the reader about short vowels, geminates, and nunnations (word ending /n/). A lack of diacritics means that 30% of textual information is missing, causing ambiguities in lexical and language modeling when attempting to model pronunciations, and the context of a particular pronunciation. Intuitively, from an English centric view, the phrase th'wrtr wrt n thwrt with 'morphological decomposition' is realized as, th wrtr wrt n th wrt. Including 'diacritics' produces, the writer wrote in the writ. Thus our investigations in this thesis are twofold. Firstly, we show the benefits and interactions between modeling all classes of diacritics (short vowels, geminates, nunnations) in the lexicon. On a Modern Standard Arabic (MSA) corpus of broadcast news, this provides a 1.9% absolute improvement in Word Error Rate (WER) (p < 0.001). We also extend this graphemic lexicon with pronunciation rules, yielding a significant improvement over a lexicon that does not explicitly nodel diacritics. This results in a of 2.4% absolute improvement in WER (p < 0.001). Secondly, we show the benefits of language modeling at the morphemic level with diacritics, over the commonly available, word-based, nondiacratized text. This yields an absolute WER improvement of 1.0% (p < 0.001)."
2020,Interpretable machine learning methods with applications to health care,https://hdl.handle.net/1721.1/127295,"With data becoming increasingly available in recent years, black-box algorithms like boosting methods or neural networks play more important roles in the real world. However, interpretability is a severe need for several areas of applications, like health care or business. Doctors or managers often need to understand how models make predictions, in order to make their final decisions. In this thesis, we improve and propose some interpretable machine learning methods by using modern optimization. We also use two examples to illustrate how interpretable machine learning methods help to solve problems in health care. The first part of this thesis is about interpretable machine learning methods using modern optimization. In Chapter 2, we illustrate how to use robust optimization to improve the performance of SVM, Logistic Regression, and Classification Trees for imbalanced datasets. In Chapter 3, we discuss how to find optimal clusters for prediction. we use real-world datasets to illustrate this is a fast and scalable method with high accuracy. In Chapter 4, we deal with optimal regression trees with polynomial function in leaf nodes and demonstrate this method improves the out-of-sample performance. The second part of this thesis is about how interpretable machine learning methods improve the current health care system. In Chapter 5, we illustrate how we use Optimal Trees to predict the risk mortality for candidates awaiting liver transplantation. Then we develop a transplantation policy called Optimized Prediction of Mortality (OPOM), which reduces mortality significantly in simulation analysis and also improves fairness. In Chapter 6, we propose a new method based on Optimal Trees which perform better than original rules in identifying children at very low risk of clinically important traumatic brain injury (ciTBI). If this method is implemented in the electronic health record, the new rules may reduce unnecessary computed tomographies (CT)."
2020,Excitonic spin engineering for solar cells and organic light-emitting diodes,https://hdl.handle.net/1721.1/128410,"This result highlights the general applicability of dihedral angle tuning, a molecular design strategy that can be used to improve the performance of donor-acceptor type TADF emitters without significantly changing their emission spectra. In contrast, contemporary solar cell technologies are dominated by silicon, an inorganic semiconductor. But when absorbing photons, silicon (like other semiconductors) wastes energy in excess of its bandgap. Reducing these thermalization losses is possible by sensitizing the silicon solar cell using singlet fission, a carrier multiplication phenomenon that occurs only in organic semiconductors. In this process, two triplet excitons are generated from a singlet exciton. In tetracene, those triplet excitons are energetically matched to the silicon bandgap. Transferring triplet excitons to silicon creates additional electron-hole pairs, promising to increase cell efficiencies from the single-junction limit of 29% to as high as 35%."
2020,The design and implementation of a new wide-range frequency detector,http://hdl.handle.net/1721.1/9471,"In this thesis, I designed and implemented a wide range frequency detector for use in clock recovery and data retiming applications. The new detector works in conjunction with the existing ""mid-range"" frequency detector to accurately lock the VCO to the incoming data rate. The new detector consists of two halves: one to detect when the VCO is too fast, and one to detect when the VCO is too slow. The design and analysis of the new frequency detectors, in addition to a method for integrating it with the existing detector, is discussed. Simulation data of the new and original frequency detectors are used to support the concepts upon which the new detector is designed. Some topics for future work are suggested at the end of this thesis."
2020,Optimizing vaccine dosing kinetics for stronger antibody response,https://hdl.handle.net/1721.1/124586,"What are the aspects of affinity maturation being altered by various temporal patterns of antigen dosing? Certain extended-duration dosing profiles increase the strength of the humoral response, with exponentially-increasing(EI) dosage providing the greatest enhancement. While this is an exciting result, it is necessary to establish a mechanistic understanding of how immune response be enhanced to further engineer and optimize the temporal patterns. From our computational model, the effect is driven by enhanced capture of antigen in lymph nodes by evolving higher-affinity antibodies early in the GC response. We validate the prediction from independent experimental data, where EI dosage result in promoted capture and retention of the antigen in lymph nodes. To our knowledge, this work is the first to demonstrate a key mechanism for vaccine kinetics in the response of B cells to immunization, and may prove to be an effective method for increasing the efficacy of subunit vaccines. 3."
2020,Topology hiding computation on all graphs,http://hdl.handle.net/1721.1/113966,"A distributed computation in which nodes are connected by a partial communication graph is called topology-hiding if it does not reveal information about the graph beyond what is revealed by the output of the function. Previous results have shown that topology-hiding computation protocols exist for graphs of constant degree and logarithmic diameter in the number of nodes [Moran-Orlov-Richelson, TCC'15; Hirt et al., Crypto'16] as well as for other graph families, such as cycles, trees, and low circumference graphs [Akavia-Moran, Eurocrypt'17], but the feasibility question for general graphs was open. In this work we positively resolve the above open problem: we prove that topology-hiding computation is feasible for all graphs under the Decisional Diffie-Hellman assumption. Our techniques employ random or deterministic walks to generate paths covering the graph, upon which we apply the Akavia-Moran topology-hiding broadcast for chain-graphs (paths). To prevent topology information revealed by the random-walk, we design multiple graph-covering sequences that, together, are locally identical to receiving at each round a message from each neighbor and sending back a processed message from some neighbor (in a randomly permuted order)."
2020,"Relationships between functionality, security, and privacy for multiparty computation, hashing, and encryption",https://hdl.handle.net/1721.1/127024,"One of the fundamental goals of cryptography is to be able to offer security and privacy without sacrificing functionality. Cryptographers have been able to achieve the best of all three by exploiting the assumed hardness of some problems (e.g. discrete log), and have been able to build protocols for secure multiparty computation, collision-resistant hash functions, public key cryptography, and much more. This thesis explores three facets of this balance. First, we delve into Topology-Hiding Computation, which is multiparty computation where we also hide the communication network, strengthening the notion of privacy. Second, we study Property Preserving Hashing, which can be thought of as an extension of collision-resistant hashing where we add functionality. Finally, we explore Fine-Grained Cryptography, and develop a public key cryptosystem. In this model of cryptography, security takes on a much less restrictive role (e.g. adversaries must run in O(n¹⁰) time), but the protocols and security reductions must run in ""fine-grained"" time (e.g. less than O(n⁵))."
2020,Induction heating of a metallic cylinder,http://hdl.handle.net/1721.1/16249,"Thesis (B.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science; and, (B.S.)--Massachusetts Institute of Technology, Dept. of Mechanical Engineering, 1977."
2020,Efficient THz lasers and broadband amplifiers based on quantum cascade gain media,http://hdl.handle.net/1721.1/93073,"One of the most important applications for Terahertz (THz) quantum cascade (QC) lasers is to provide compact and powerful frequency-stabilized solid-state sources as local oscillators in heterodyne receivers for astronomical studies. The first part of the thesis is dedicated to the device cavity design, fabrication and characterization of the microstrip antenna coupled third-order distributed feedback QC lasers aimed for 2.060 THz atomic oxygen line. THz travelling-wave QC amplifiers are highly desired to achieve broadband amplification of THz radiation in free space. The second part of the thesis focuses on the development of 4.3 THz travelling-wave QC amplifier by monolithically integrating horn antennas and attaching silicon lenses at the metal-metal waveguide facets."
2020,Improved packing strategy for distribution centers to reduce freight cost,https://hdl.handle.net/1721.1/123714,"In this thesis, we designed and implemented a data-driven packing strategy for distribution center outbound packing activities to reduce freight cost and carbon footprint. The strategy consists of two parts. First, I proposed an Carton Combination method, an algorithm that can select any predetermined number of distinct cartons from a large carton pools (over 1000 options) to be used for outbound shipment packing such that the annual total wasted air content inside the shipment is minimized. Second, I proposed an Carton Selection algorithm, which can determine the best carton, from the carton options chosen by the Carton Combination method, for an incoming or- der with known dimensions. The entire packing strategy prototype was implemented by MATLAB R2019a; the prototype was tested with the 2018 outbound shipment data from Waters Corporation Global Distribution Center (GDC) and the simulation showed that the annual averaged shipment air percentage was reduced from 60% to 40%, which projects to an annual operation cost saving of 83,000 USD and carbon dioxide emission reduction of 20 ton. The data-driven packing strategy has a potential to be scaled up and implemented via an industrial environment such as SAP ABAP."
2020,Creating a shipboard power simulation tool using electrical load behavior modeling,https://hdl.handle.net/1721.1/127040,"Trends in power system simulation that demand computationally-intensive, physics-based models may impede the acquisition of useful results for applications like condition-based maintenance [1], electrical plant load analysis (EPLA) [2], and the scheduling and tasking of finite generation and distribution resources. A tool that can quickly evaluate many scenarios, as opposed to intense, high fidelity modeling of a single operating scenario, may best serve these applications. This thesis presents a behavioral simulator that can quickly emulate the operation of a relatively large collection of electrical loads, providing ""what-if"" evaluations for more complete exploration of a design or plant operating envelope. Comparisons to field data collected from a microgrid on-board a 270 foot US Coast Guard ""Famous"" Class medium endurance cutter demonstrate the utility of this tool and approach. The usefulness of this tool is further demonstrated by showing simulated EPLA load factors within 10%of observed load factors over comparable mission sets, both inport and underway. Finally, this thesis will discuss the lessons learned during SPS development and testing, specifically, the need to expand its modeling capability so it can support direct current (DC) electrical distribution systems. The SPS, in its current form can only model alternating (AC) electrical distribution systems."
2020,StreamJIT : a commensal compiler for high-performance stream programming,http://hdl.handle.net/1721.1/91094,"There are domain-specific libraries for many domains, enabling rapid and cost-effective development of complex applications. On the other hand, domain-specific languages are rare despite the performance advantages of compilation. We believe the reason is the multiple orders-of-magnitude higher cost of building a compiler compared to building a library. We propose commensal compilation, a new strategy for compiling embedded domain-specific languages by reusing the massive investment in modern language virtual machine platforms. Commensal compilers use the host language's front-end, use an autotuner instead of optimization heuristics, and use host platform APIs that enable back-end optimizations by the host platform JIT. The cost of implementing a commensal compiler is only the cost of implementing the domain-specific optimizations. We demonstrate the concept by implementing a commensal compiler for the stream programming language StreamJIT atop the Java platform. The StreamJIT commensal compiler takes advantage of the structure of stream programs to find the right amount of parallelism for a given machine and program. Our compiler achieves 2.4 times better performance than StreamIt's native code (via GCC) compiler with considerably less implementation effort."
2020,Data-driven pricing,http://hdl.handle.net/1721.1/45627,"In this thesis, we develop a pricing strategy that enables a firm to learn the behavior of its customers as well as optimize its profit in a monopolistic setting. The single product case as well as the multi product case are considered under different parametric forms of demand, whose parameters are unknown to the manager. For the linear demand case in the single product setting, our main contribution is an algorithm that guarantees almost sure convergence of the estimated demand parameters to the true parameters. Moreover, the pricing strategy is also asymptotically optimal. Simulations are run to study the sensitivity to different parameters.Using our results on the single product case, we extend the approach to the multi product case with linear demand. The pricing strategy we introduce is easy to implement and guarantees not only learning of the demand parameters but also maximization of the profit. Finally, other parametric forms of the demand are considered. A heuristic that can be used for many parametric forms of the demand is introduced, and is shown to have good performance in practice."
2020,Textile precision for customized assemblies,https://hdl.handle.net/1721.1/123603,"With the potential to configure patterns and materials with stitch-level control, textiles are becoming an increasingly desirable method of producing mass customized items. However, current textile machines lack the ability to transfer three-dimensional information between digital models and production with the same level of control and accuracy as other machines. Designers are accustomed to generating three-dimensional objects in a digital model then converting these into instructions for machines such as 3D printers or laser cutters, but current design interfaces and production machines for textiles provide no comparable workflow for producing items that rely on precise control of physical size and fit. Customized assemblies-such as footwear or architectural projects with complex geometries--increasingly integrate textile components with parts produced through a variety of other industrial processes. Furthermore, there is growing interest in the use of three-dimensional data, such as 3D body scanning, to aid in the production of custom-fit products. As mass customization becomes more widespread as an alternative to mass production, general-purpose machines are increasingly capable of generating customized items with high efficiency, relying on design-to-machine workflows to control geometric changes. However, current textile machines are unable to adapt to changing geometric information with the same efficiency. The challenges to dimensional precision in textiles are wide ranging, affected by computational interfaces, production machines, and material technique. Addressing these problems, this thesis demonstrates a design-to-fabrication workflow that enables the transfer of three-dimensional information directly to a device for textile production. The proposed workflow seeks a solution to the material, mechanical, and computational bottlenecks related to spatial accuracy in textile production."
2020,Omnidirectional obstacle detection using minimal sensing,https://hdl.handle.net/1721.1/124595,"An integrated approach to visual obstacle detection for aerial multi-rotor vehicles (drones) is introduced. The approach achieves omnidirectional detection of obstacles via suitable synergy of hardware and software. The drone requires a specific arrangement of two cameras, opposing each other, and placed below and above the drone. A total coverage of the drone's surroundings is achieved by fitting each camera with a fisheye lens whose field of view is significantly greater than 180 degrees. The combined field of view of the cameras is omnidirectional, and may be conceptually subdivided into three regions: the monocular portions of each camera (centered at the north and south poles of the drone) and the stereo portion common to both cameras (circling the drone's equator). To use both the stereo and monocular data, a special image projection is developed, based on a model of the world as a 'capsule'."
2019,Optical frequency domain imaging of human retina and choroid,http://hdl.handle.net/1721.1/38556,"(cont.) The 1050-nm OFDI system developed for this thesis comprised a novel wavelength-swept laser that delivered 2.7 mW of average power at a sweep rate of 18.8 kHz, representing a two-order-of-magnitude improvement in speed over previously-demonstrated lasers in the 1050-nm range and below. The system, with an optical exposure level of 550 gW, achieved resolution of 10 gm in tissue and sensitivity of >92 dB over a depth range of 2.4 mm. Two healthy volunteers were imaged with the OFDI system, with 200,000 A-lines over 10.6 seconds in each imaging session. In comparison to results from a state-of-the-art spectral-domain OCT system, the OFDI system provided deeper penetration into the choroid. This thesis demonstrates OFDI's capability for comprehensive imaging of the human retina, optic disc, and choroid in vivo. The deep penetration power of the system enabled the first simultaneous visualization of retinal and choroidal vasculature without the exogenous dyes required by angiography. The combined capability for imaging microstructure and vasculature using a single instrument may be a significant factor influencing clinical acceptance of ophthalmic OFDI technology."
2019,The reduction of acoustic noise emissions from a hard disk drive,http://hdl.handle.net/1721.1/34324,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1994."
2019,"Probabilistic latent variable modeling for predicting future well-being and assessing behavioral influences on mood, stress and health",http://hdl.handle.net/1721.1/120421,"In recent years, there has been a shift in the psychological research literature from an emphasis on dysfunction to a focus on well-being and positive mental health. As a result, enhancing well-being in individuals has become a viable approach to improving health, in addition to treating disorders when present. Also, the availability of rich multi-modal datasets and advances in machine learning methods have spurred an increase in research studies assessing well-being objectively. However, most of these studies tend to primarily focus on using data to estimate or detect the current state of well-being as opposed to the prediction of well-being. In addition, these studies investigate how stand-alone health behaviors and not a combination of health behaviors influence well-being. Furthermore, these studies do not provide data-backed insights and recommendations to individuals seeking to improve their well-being. In this dissertation, we use a real-world dataset from a population of college students and interpretable machine learning methods to (1) predict future mood, stress and health, (2) uncover how combinations of health behaviors work together to influence well-being, and (3) understand how to make evidence-based recommendations to individuals looking to improve their well-being. The use of these methods contributes to the development of objective techniques that can help individuals monitor their wellbeing. In addition, insights from this study contribute to knowledge advancement on how combinations of daily human behaviors can affect well-being."
2019,Low variance methods for Monte Carlo simulation of phonon transport,http://hdl.handle.net/1721.1/69799,"Computational studies in kinetic transport are of great use in micro and nanotechnologies. In this work, we focus on Monte Carlo methods for phonon transport, intended for studies in microscale heat transfer. After reviewing the theory of phonons, we use scientific literature to write a Monte Carlo code solving the Boltzmann Transport Equation for phonons. As a first improvement to the particle method presented, we choose to use the Boltzmann Equation in terms of energy as a more convenient and accurate formulation to develop such a code. Then, we use the concept of control variates in order to introduce the notion of deviational particles. Noticing that a thermalized system at equilibrium is inherently a solution of the Boltzmann Transport Equation, we take advantage of this deterministic piece of information: we only simulate the deviation from a nearby equilibrium, which removes a great part of the statistical uncertainty. Doing so, the standard deviation of the result that we obtain is proportional to the deviation from equilibrium. In other words, we are able to simulate signals of arbitrarily low amplitude with no additional computational cost. After exploring two other variants based on the idea of control variates, we validate our code on a few theoretical results derived from the Boltzmann equation. Finally, we present a few applications of the methods."
2019,Distributed cooperative control architectures for automated manufacturing systems,http://hdl.handle.net/1721.1/11295,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1996."
2019,Analysis and transfer of photographic viewpoint and appearance,http://hdl.handle.net/1721.1/55085,"To make a compelling photograph, photographers need to carefully choose the subject and composition of a picture, to select the right lens and viewpoint, and to make great efforts with lighting and post-processing to arrange the tones and contrast. Unfortunately, such painstaking work and advanced skill is out of reach for casual photographers. In addition, for professional photographers, it is important to improve workflow efficiency. The goal of our work is to allow users to achieve a faithful viewpoint for rephotography and a particular appearance with ease and speed. To this end, we analyze and transfer properties of a model photo to a new photo. In particular, we transfer the viewpoint of a reference photo to enable rephotography. In addition, we transfer photographic appearance from a model photo to a new input photo. In this thesis,we present two contributions that transfer photographic view and look using model photographs and one contribution that magnifies existing defocus given a single photo. First, we address the challenge of viewpoint matching for rephotography. Our interactive, computer-vision-based technique helps users match the viewpoint of a reference photograph at capture time. Next, we focus on the tonal aspects of photographic look using post-processing. Users just need to provide a pair of photos, an input and a model, and our technique automatically transfers the look from the model to the input. Finally, we magnify defocus given a single image. We analyze the existing defocus in the input image and increase the amount of defocus present in out-of focus regions."
2019,Development of multimodal spectroscopy for the detection of vulnerable atherosclerotic plaques,http://hdl.handle.net/1721.1/44709,"The combination of reflectance, fluorescence, and Raman spectroscopy - which is termed multimodal spectroscopy (MMS) - provides complementary and depth-sensitive information about tissue composition. As such, MMS can provide biochemical and morphological information useful in detecting vulnerable atherosclerotic plaques, that is, plaques most prone to rupture and causing sudden death. Early detection of these vulnerable plaques is critical to reducing patient mortality associated with cardiovascular disease. In developing MMS into a clinical diagnostic modality, several scientific and engineering directions are explored in this work: the physical motivation for MMS, the framework of quantitative extraction of spectral parameters, the spectral probes that enable the efficient collection of data, a clinical instrument able to provide real-time diagnosis, and, finally, a clinical implementation of the entire methodology. The motivation for MMS is shown through a pilot in vitro study using carotid artery specimens, which shows the promise for MMS to detect features of vulnerable plaque. Having established the motivation, the next step describes the mathematical tools used to extract quantitative spectral parameters and, moreover, to assess the uncertainty and confidence of the spectral information. In order to implement MMS, the development of an efficient, specialized MMS probe for data acquisition and a compact and practical clinical MMS instrument are described. Lastly, in vivo and ex vivo results from a relatively large clinical study of vulnerable plaque in humans show excellent agreement between MMS and histopathology. Specifically, MMS is shown to have the ability to detect a thin fibrous cap, necrotic core or superficial foam cells, and thrombus."
2019,Multiresolution statistical modeling with application to modeling groundwater flow,http://hdl.handle.net/1721.1/10749,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1997."
2019,File system unification using LatticeFS,http://hdl.handle.net/1721.1/53145,"LatticeFS is a namespace unification system designed to merge multiple source file systems into a single working file system. atticeFS can be used to merge multiple software package directories, work with multiple file systems as if they are one, and share a single storage medium among multiple machines. On a high level, LatticeFS takes as input an arbitrary number of file system paths, and mounts a new virtual drive that will appear to the user as a union of the input file systems. Of course, attempting to combine multiple file systems will inevitably be met with conflicts. Situations in which multiple input file systems contain files/directories with the same name will be common in large systems; which file/directory should the user be exposed to in this case? Previous work such as UnionFS solved the problem by giving each input file system a strict priority value, and when a conflict occurred, the file/directory with the highest priority was the one shown to the user. In LatticeFS, we have introduced a plug-in system in which different strategies for resolving conflicts can be easily swapped in and out; additionally, handlers for special file types can also be ""plugged"" into the system. This paper describes and evaluates all aspects of LatticeFS in detail."
2019,Theoretical models for microwave remote sensing of earth terrain,http://hdl.handle.net/1721.1/84193,"Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1984."
2019,An implementation of a 5.25 GHz transceiver for high data rate wireless applications,http://hdl.handle.net/1721.1/34360,"The desire for transmission of high data rate information across wireless channels has grown immensely over the past decade. Wireless devices available today including mobile phones, wireless local area networks (WLANs) and Bluetooth radios have realized a wide variety of applications at data rates ranging from 10s of kbit/s to 10s of Mbit/s. Mobile telephone design strives for large transmit distances, Bluetooth technology enables communication between two close range devices, and wireless LAN strives to achieve a high data rate wireless link within an office or home environment. This link is traditionally implemented through a central access point that communicates with one or more workstations. Due to the large number of applications demanding high speed wireless links, the aspiration for even higher data rates is prevalent."
2019,Toward a compact underwater structured light 3-D imaging system,http://hdl.handle.net/1721.1/83705,"A compact underwater 3-D imaging system based on the principles of structured light was created for classroom demonstration and laboratory research purposes. The 3-D scanner design was based on research by the Hackengineer team at Rice University. The system is comprised of a low-power, open-source hardware single-board computer running a modified Linux distribution with OpenCV libraries, a DLP pico projector, camera board, and battery module with advanced power management. The system was designed to be low-cost, compact, and portable, while satisfying requirements for watertightness. Future development and applications may involve navigation systems for an autonomous underwater vehicle (AUV). An initial study of 3-D imaging methods is presented, and the strengths and drawbacks of each type are discussed. The structured light method was selected for further study for its ability to produce high-resolution 3-D images for a reasonable cost. The build of the 3-D imaging system was documented for reproducibility, and subsequent testing demonstrated its functions and ability to produce 3-D images. An instruction guide for operation of the device is provided for future classroom and laboratory use. The 3-D imaging system serves as a proof-of-concept for utilizing structured light methods to produce 3-D images underwater. Image resolution was limited by the output resolution of the pico projector and camera module. Further exploration in obtaining ultra high-resolution 3-D images may include use of a more powerful projector and a higher resolution camera board module with autofocus. Satisfactory 3-D scanning validated the performance of structured light scanning above water. However, contaminants in the water hindered accurate rendering by the system while submerged due to light scattering. Future development of a on-the-fly mapmaking system for AUV navigation should include algorithms for filtering light scattering, and hardware should based on an instantaneous structured light system utilizing the Kinect 2-D pattern method. Autofocus and increased projector brightness would also be worthwhile additions."
2019,Probabilistic modeling of planar pushing,http://hdl.handle.net/1721.1/118739,"This work studies the problem of data-driven modeling and stochastic filtering of complex dynamical systems. The main contributions are GP-SUM, a filtering algorithm tailored to systems expressed as Gaussian processes (GP), and the probabilistic modeling of planar pushing by combining input-dependent GPs and GP-SUM. The main advantages of GP-SUM for filtering are that it does not rely on linearizations or unimodal Gaussian approximations of the belief. Moreover, it can be seen as a combination of a sampling-based filter and a probabilistic Bayes filter as GP-SUM operates by sampling the state distribution and propagating each sample through the dynamic system and observation models. Effective sampling and accurate probabilistic propagation are possible by relying on the GP form of the system, and a Gaussian mixture form of the belief. In this thesis we show that GP-SUM outperforms several GP-Bayes and Particle Filters on a standard benchmark. To characterize the dynamics of pushing, we use input-dependent GPs to learn the motion of the pushed object after a short time step. With this approach we show that we can learn accurate data-driven models that outperform analytical models after less than 100 samples and saturate in performance with less than 1000 samples. We validate the results against a collected dataset of repeated trajectories, and use the learned models to study questions such as the nature of the variability in pushing, and the validity of the quasi-static assumption. Finally, we illustrate how our learned model for pushing can be combined with GP-SUM, and demonstrate that we can predict heteroscedasticity, i.e., different amounts of uncertainty, and multi-modality when naturally occurring in pushing."
2019,Verification of d-wave pairing symmetry by microwave intermodulation distortion measurements in yttrium barium copper oxide,http://hdl.handle.net/1721.1/39732,"We report measurements of the temperature and power dependence of the microwave frequency intermodulation distortion (IMD) in high quality pulsed laser deposition (PLD) Yttrium Barium Copper Oxide (YBCO) on LaAlO3 substrate. A low-temperature (T < 30 K) increase in IMD is the observation of an upturn of the nonlinear coefficient of the quadratic field dependence of the penetration depth. This IMD upturn is limited by the nonlinear Meissner effect that has been predicted for d-wave high-T, superconductors. Various amounts of IMD increase are observed for different films with impurity (Ni, Zn and Ca) doping and other defects. The demonstration of the IMD upturn and the nonlinear Meissner effect were possible because the IMD measurement is an extremely sensitive method to detect the penetration depth change at even less than 0.01 nm. IMDs from various samples tend to merge at a single universal value at 0 K regardless of disorder, defects, and impurities due to the node singularity at 0 K. There is a similar converging trend in IMD towards the transition temperature T, due to the quasiparticle thermal excitation and depletion of superelectrons. It is most likely that IMD has both intrinsic and extrinsic contributions."
2019,Learning with generalized negative dependence : probabilistic models of diversity for machine learning,https://hdl.handle.net/1721.1/122739,"This thesis establishes negative dependence as a powerful and computationally efficient framework to analyze machine learning problems that require a theoretical model of diversification. Examples of such problems include experimental design and model compression: subset-selection problems that require carefully balancing the quality of each selected element with the diversity of the subset as a whole. Negative dependence, which models the behavior of ""repelling"" random variables, provides a rich mathematical framework for the analysis of such problems. Leveraging negative dependence theory for machine learning requires (a) scalable sampling and learning algorithms for negatively dependent measures, and (b) negatively dependent measures able to model the specific diversity requirements that arise in machine learning. These problems are the focus of this thesis."
2019,Optimization of electron optics in a resonator cavity using Nelder-Mead simplex search for the quantum electron microscope,http://hdl.handle.net/1721.1/101580,"The Quantum Electron Microscope (QEM) is a proposed imaging modality that aims to reduce or eliminate the effects of radiation on living cells compared to traditional electron microscopy techniques. In recent years, an interaction free measurement scheme was proposed by Putnam and Yanik [1], and an implementation of this idea is being developed by an international collaboration. The current implementation foresees an electron cavity, which can be installed into a regular scanning electron microscope, to allow multiple passes of two electron wavefunctions over the specimen. In order to implement this idea, multiple different electron optical designs were proposed. Extensive simulation work is required to test and validate these designs. This work outlines the simulation work done for QEM, and proposes a general framework for optimizing electron trajectory simulations using Nelder-Mead search. It also provides a library of MATLAB wrapper functions and optimization methods to be used with the Integrated Lorentz-2E software."
2019,Biologically-plausible six-legged running : control and simulation,http://hdl.handle.net/1721.1/29695,"This thesis presents a controller which produces a stable, dynamic 1.4 meter per second run in a simulated twelve degree of freedom six-legged robot. The algorithm is relatively simple; it consists of only a few hand-tuned feedback loops and is defined by a total of 13 parameters. The control utilizes no vestibular-type inputs to actively control orientation. Evidence from perturbation, robustness, motion analysis, and parameter sensitivity tests indicate a high degree of stability in the simulated gait. The control approach generates a run with an aerial phase, utilizes force information to signal aerial phase leg retraction, has a forward running velocity determined by a single parameter, and couples stance and swing legs using angular momentum information. Both the hypotheses behind the control and the resulting gait are argued to be plausible models of biological locomotion."
2019,Diffractive optics for maskless lithography and imaging,http://hdl.handle.net/1721.1/17963,"Semiconductor industry has primarily been driven by the capability of lithography to pattern smaller and smaller features. However due to increasing mask costs and complexity, and increasing tool costs, the state-of-the-art technology in lithography is accessible only to a select few. Zone-plate array lithography (ZPAL) is a novel method of maskless lithography that aims to alleviate some of these issues while offering a solution that can be extended to the limits of nanolithography. In ZPAL, an array of diffractive lenses is used to form an array of spots on the substrate. Each spot is modulated independently by means of spatial-light modulators. This essentially creates a ""parallel laserwriter"". In addition, this lithography system can be converted into a parallel-confocal microscope, which enables fast, high-resolution imaging. This thesis addresses the performance of diffractive lenses, particularly high-numerical aperture zone plates for lithography and imaging using a combination of experimental and theoretical studies. A novel proximity-effect correction algorithm that was implemented effectively in a ZPAL system is also described. Variations to another diffractive lens known as the photon sieve are proposed. The first ever lithography results performed using these new elements are presented in this thesis."
2019,Constitutive equations for superelasticity in crystalline shape-memory materials,http://hdl.handle.net/1721.1/8141,"A crystal-mechanics-based constitutive model for polycrystalline shape-memory materials has been developed. The model has been implemented in a finite-element program. Finite-element calculations of polycrystal response were performed using two methods: (1) The full-finite element method where each element represents a single crystal chosen from a set of crystal orientations which approximate the initial crystallographic texture; (2) A simplified model using the Taylor assumption (1938) where each element represents a collection of single crystals at a material point. The macroscopic stress-strain responses are calculated as volume averages over the entire aggregate. A variety of superelastic experiments were performed on initially-textured Ti-Ni rods and sheets. The predicted stress-strain curves from finite-element calculations are shown to be in good accord with the corresponding experiments. For the Ti-Ni sheet, strain-temperature response at a fixed stress was also experimentally studied. The model was also shown to accurately predict the results from these important experiments. Further, by performing superelastic experiments at moderately high strain rates, the effects of self-heating and cooling due to the phase transformations are shown to be captured well by the constitutive model. The thermo-mechanically-coupled theory is also able to capture the resulting inhomogeneous deformations associated with the nucleation and propagation of transformation fronts. Finally, an isotropic constitutive model has also been developed and implemented in a finite-element program. This simple model provides a reasonably accurate and computationally-inexpensive tool for purposes of engineering design."
2019,Real-time path-planning using mixed-integer linear programming and global cost-to-go maps,http://hdl.handle.net/1721.1/35290,"With the advance in the fields of computer science, control and optimization, it is now possible to build aerial vehicles which do not need pilots. An important capability for such autonomous vehicles is to be able to generate their own path to navigate in a constrained environment and accomplish mission objectives, such as reaching waypoints in minimal time. To account for dynamic changes in the environment, perturbations, modeling errors and modifications in the mission scenario, the trajectory needs to be continuously re-optimized online based on the latest available updates. However, to allow for high update rates, the trajectory optimization problem needs to be simple enough to be solved quickly. Optimizing for a continuous trajectory of a dynamically-constrained vehicle in the presence of obstacles is an infinite-dimension nonlinear optimal control problem. Such a problem is intractable in real-time and simplifications need to be made. In this thesis, the author presents the mechanisms used to design a path-planner with real-time and long-range capabilities. The approach relies on converting the optimal control problem into a parameter optimization one whose horizon can be reduced by using a global cost-to-go function to provide an approximate cost for the tail of the trajectory."
2019,Attitude control via structural vibration : an application of compliant robotics,http://hdl.handle.net/1721.1/111922,"We review and present techniques for effecting and controlling the reorientation of structures ""floating"" in angular-momentum-conserving environments, applicable to both space robotics and small satellite attitude control. Conventional orientation control methods require either the usage of continuously rotating structures (e.g. momentum wheels) or the jettisoning of system mass (e.g. hydrazine thrusters). However, the systems proposed herein require neither rotating structures nor mass ejection; instead, orientation is controlled by the imposition of a bounded cyclic shape change-the canonical example of such a system is a cat righting herself while falling, thereby always landing on her feet-coupled with the conservation of angular momentum, which acts analogously to a nonholonomic constraint on the system dynamics. Further, by considering the reduced system dynamics, we extend the concept to consider the class of structures where the requisite cyclic shape change is attainable via dynamical effects, such as the normal modes of structural vibration for structures with finite stiffness. This is the central novel result of this thesis and has implications for the design of space structures where the attitude control hardware is integrated directly into the preexisting structure, the development of orientation control techniques for soft robots in space and underwater, and the design of MEMS attitude control actuators for very tiny satellites. We apply mathematical tools drawn from differential geometry and geometric mechanics, which can be intimidating but which provide a comprehensive and powerful framework for understanding a wide range of locomotion problems fundamental to robotics and control theory. These tools allow us to make succinct statements regarding gait design, controllability, and optimality that would be otherwise inaccessible."
2019,Dielectric elastomer actuators for binary robotics and mechatronics,http://hdl.handle.net/1721.1/35305,"Future robotics and mechatronics applications will require systems that are simple, robust, lightweight and inexpensive. A suggested solution for future systems is binary actuation. Binary actuation is the mechanical analogy to digital electronics, where actuators ""flip"" between two discrete states. Systems can be simple since low-level feedback control, sensors, wiring and electronics are virtually eliminated. However, conventional actuators, such as DC motors and gearbox are not appropriate for binary robotics because they are complex, heavy, and expensive. This thesis proposes a new actuation technology for binary robotics and mechatronics based on dielectric elastomer (DE) technology. DE actuators are a novel class of polymer actuators that have shown promising low-cost performance. These actuators were not well understood and, as a result, faced major reliability problems. Fundamental studies conducted in this thesis reveal that reliable, high performance DE actuation based on highly viscoelastic polymers can be obtained at high deformation rates, when used under fast, intermittent motion."
2019,A highly configurable software bug tracking system,http://hdl.handle.net/1721.1/80130,"Thesis (S.B. and M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999."
2019,The second skin approach : skin strain field analysis and mechanical counter pressure prototyping for advanced spacesuit design,http://hdl.handle.net/1721.1/32443,"The primary aim of this thesis is to advance the theory of advanced locomotion mechanical counter pressure (MCP) spacesuits by studying the changes in the human body shape during joint motion. Two experiments take advantage of three-dimensional laser scan technology to measure the shape changes of the human body. The first experiment is an analysis of the surface area and volume of the thigh, knee, calf, and entire leg during knee flexion. The second experiment is an analysis of the full-field strain on the skin surface of the leg during knee flexion. A repeatable and quantitative technique for mapping the leg skin strain field is developed. The results of the algorithm indicate the magnitude of strain over the entire surface of the leg, as well as the direction of minimum leg skin stretching during knee flexion. For 88% of the leg surface, knee flexion causes skin strain between -0.3 and 0.3 (less than 30% contraction or extension). However, just below the patella, longitudinal strain is as high as 0.7, and at the knee hollow, it is as low as -0.6. Circumferential strain values are as high as 1.0 and 0.5 just below the patella and over the calf muscle, respectively, and along the anterior surface of the lower leg, they are as low as -0.7. The leg area, volume, and skin strain results lead to quantitative design requirements for highly mobile second skin spacesuits, and they inspire two prototype MCP leg sleeves: a hybrid urethane-foam bladder garment and a skintight nylon fiber lines of non-extension garment. These two prototypes are constructed and tested for mobility and skin surface pressure. Pressurization of the hybrid foam prototype inhibits leg mobility."
2019,Development of high fidelity methods for 3D Monte Carlo transient analysis of nuclear reactors,http://hdl.handle.net/1721.1/119034,"Monte Carlo is increasingly being used to perform high-fidelity, steady-state neutronics analysis of power reactor geometries on today's leadership class supercomputers. Extending Monte Carlo to time dependent problems has proven to be a formidable challenge due to the significant computational resource and data processing requirements. In this thesis, a transient methodology is proposed and implemented to enable accurate and computationally tractable time dependent Monte Carlo analysis. The frequency transform method has been described and implemented in Monte Carlo for the first time. The attractiveness of this method lies in its ability to accurately capture the space and time dependent distribution of the delayed neutron source throughout a transient. Nuances to the algorithmic implementation are described and validated through a series of simple analytical test problems. Comparison with the adiabatic method currently employed for Monte Carlo transient analysis shows significant improvement in the spatial distribution and magnitude of the power for a negative reactivity insertion transient in the 2D and 3D C5G7 geometry. To aid in understanding the effect of statistical uncertainty in the tallied quantities on the time dependent flux solution, a simplified point kinetics model was developed and used for insightful analysis on simple transient test problems. This revealed how the time dependent flux profiles for a series of independent trials can be approximated by a normal distribution at low uncertainties in the tallied reactivity, but deviates from a normal distribution at relatively modest uncertainties in reactivity. Given the compuational constraints of solving large problems, having a simple model that can provide insight on the expected behavior and flux distribution can be very valuable. The frequency transform methodology belongs to a class of indirect space-time factorization methods that perform high-order calculations (e.g. Monte Carlo) over long time steps and low-order, computationally-efficient calculations (e.g. Point Kinetics) over short time steps as an approach to balance performance and accuracy. The coarse mesh finite difference (CMFD) diffusion operator is employed as the low-order solver in Monte Carlo transient analysis for the first time. The CMFD diffusion operator is attractive due to its potential to increase the time step size between the computationally expensive high-order solves. Implementing this methodology is important as continuous energy Monte Carlo is reactor-agnostic and able to treat complex geometries without difficulty, opening up the possibility of solving transients on new experimental geometries for which there is little data."
2019,Algorithms for Subset Sum using linear sketching,https://hdl.handle.net/1721.1/122750,"Given n positive integers, the Modular Subset Sum problem asks if a subset adds up to a given target t modulo a given integer m. This is a natural generalization of the Subset Sum problem (where m = + [infinity symbol]) with ties to additive combinatorics and cryptography. The non-modular case was long known to be NP-complete but to admit pseudo-polynomial time algorithms and, recently, algorithms running in near-linear pseudo-polynomial time were developed [9, 211. For the modular case, however, the best known algorithm by Koiliaris and Xu [21] runs in time 0̃ (m⁵/⁴). In this thesis we tackle this problem by devising a faster algorithm for the Modular Subset Sum problem, running in 0̃(m) randomized time, which matches a recent conditional lower bound of [1] based on the Strong Exponential Time Hypothesis. Interestingly, in contrast to most previous results on Subset Sum, our algorithm does not use the Fast Fourier Transform. Instead, it is able to simulate the ""textbook"" Dynamic Programming algorithm much faster, using ideas from linear sketching. This is one of the first applications of sketching-based techniques to obtain fast algorithms for exact combinatorial problems in an offline setting."
2019,Alterations in cardiovascular regulation and function assessed using cardiovascular system identification,http://hdl.handle.net/1721.1/86525,"Thesis (S.B. and M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2000."
2019,Delivering real-time holographic video content with off-the-shelf PC hardware,http://hdl.handle.net/1721.1/17966,"We present a PC based system to simultaneously compute real-time holographic video content and to serve as a framebuffer to drive a holographic video display. Our system uses only 3 PCs each equipped with an nVidia Quadro FX 3000G video card. It replaces a SGI Onyx and the custom built Cheops Image Processing System that previously served as the platform driving the MIT second-generation Holovideo display. With a prototype content generation implementation, we compute holographic stereograms and update the display at a rate of roughly 2 frames per second."
2019,Sequence-structure correlations in the MaSp1 protein of N. clavipes dragline silk,http://hdl.handle.net/1721.1/67610,"Silk is a hierarchically structured protein fiber with exceptional tensile strength and extensibility, making it one of the toughest and most versatile biocompatible materials. While experimental studies have shown that the molecular structure of silk has a direct influence on the stiffness, toughness, and failure strength of silk, few molecular-level analyses of the nanostructure of silk assemblies, in particular under variations of genetic sequences, have been published. Here, atomistic-level structures of wildtype as well as modified MaSp1 protein from the N. clavipes spider dragline silk sequences are reported, obtained using an in silico approach based on replica exchange molecular dynamics (REMD) and explicit water molecular dynamics. In particular, the atomistic simulations discussed in this parametric study explore the effects of the poly-alanine length of the N. clavipes MaSpi peptide sequence, solvent conditions, and nanomechanical loading conditions on secondary and tertiary structure predictions as well as the nanomechanical behavior of a unit cell of 15 strands with 900-1000 total residues used to represent a cross-linking 7-sheet crystal node in the network within a fibril of the dragline silk thread. Understanding the behavior of this node at the molecular scale is critical for potentially bypassing strength limits at this length scale and vastly improving silk for medical and textile purposes as well as synthetic elastomers and polymer or aramid fiber composites with a similar molecular structure and noncovalent bonding for aerospace, armor, and medical applications. The main hypothesis tested is that there exists a critical minimum length of the poly-alanine repeat that ensures the formation of a robust cross-linking the [beta]-sheet crystal. Confirming earlier experimental and computational work, a structural analysis reveals that poly-alanine regions in silk predominantly form distinct and orderly [beta]-sheet crystal domains while disorderly regions are formed by glycine-rich repeats that consist of 310-helix type structures and 7-turns. These predictions are directly validated against experimental data based on dihedral angle pair calculations presented in Ramachandran plots combined with an analysis of the secondary structure content. The key results of this study are: e A strong dependence of the resulting silk nanostructure on the poly-alanine length. The wildtype poly-alanine repeat length of six residues defines a critical minimum length that consistently results in clearly defined [beta]-sheet nanocrystals allowing for misalignment. For poly-alanine lengths below six residues, the /-sheet nanocrystals are not well-defined or not visible at all, while for poly-alanine lengths above six the characteristic nanocomposite structure of silk emerges with no significant improvement of the quality of the sheet nanocrystal geometry. A simple biophysical model is presented that explains the minimum length scale based on the mechanistic insight gained from the molecular simulations. The efficient stacking of the [beta]-sheets of a well-defined crystal reinforces local hydrophobicity and prevents water diffusion into a crystal above a critical size. Nanomechanical testing reveals that the combination of the 12-alanine length case and central pull-out loading conditions results in delayed failure by employing a hierarchy of strong [beta]-sheets and soft, extensible semi-amorpous regions to overcome a predicted H-bond saturation. This work constitutes the most comprehensive study to-date of the molecular structure prediction and nanomechanical behavior of dragline silk. Building upon previous computational studies that used similar methods for structure prediction and mechanical analysis, e.g. REMD and force-control loading, this work presents: the first results of the near-native structures determined by REMD after equilibration in TIP3P explicit solvent, the first parametric study of the effects of modifying the wildtype poly-alanine segment length to values outside the range naturally observed for MaSp1 on structure prediction and nanomechanical behavior, and, the first comparison between previously published loading conditions, i.e. the Stretch test, and the novel Pull-out loading conditions that are hypothesized to be more appropriate for modeling of the in situ loading of the cross-linking [beta]-sheet crystal. Further parametric studies in peptide sequence to optimize bulk fiber properties must involve changes in simulated nanomechanical loading conditions to properly assess the effects of the changes in peptide sequence. These findings set the stage for understanding how variations in the spidroin sequence can be used to engineer the structure and thereby functional properties of this biological superfiber, and present a design strategy for the genetic optimization of spidroins for enhanced mechanical properties. The approach used here may also find application in the design of other self-assembled molecular structures and fibers and in particular biologically inspired or completely synthetic systems."
2019,VLSI microdisplays and optoelectronic technology,http://hdl.handle.net/1721.1/11401,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2019,Tenant-level network performance isolation in Flowtune,http://hdl.handle.net/1721.1/112821,"Performance isolation is a major concern for multi-tenant datacenters. Many service level agreements include a specification on the allotment of resources. For tenants, these resource guarantees are critical to the availability and efficiency of their services. While CPU, disk, and memory isolation are well-understood, network performance isolation is less straightforward. In this thesis, I investigate methods for enforcing bandwidth fairness guarantees for logical networks in a datacenter and implement network performance isolation in Flowtune. Flowtune is a datacenter network architecture which introduces a centralized arbiter to enforce congestion control at the flowlet level. Flowtune achieves rapid convergence to a desired allocation of network resources in addition to reducing tail latencies in various settings. However, Flowtune currently does not provide tenant-level network performance isolation."
2019,Neuromuscular modularity and behavioral correlates of motor control,http://dspace.mit.edu/handle/1721.1/33215,"I studied organizational principles that may subserve the control and learning of forelimb movements. Among these principles, I focused on muscular coordination patterns, motor cortical excitability, and sensorimotor interactions. I found that muscle activity in grasping and reaching behaviors could be reconstructed by linear combinations of a small number of time-varying muscle synergies, each fit with coefficients unique to the behavior. However, the generalization of these synergies between behavioral conditions was limited, in part by the sensitivity of the extraction algorithm to stereotyped muscular relations within contrasted conditions. In reaching studies designed to assist or resist different movement directions, I found a gradual change in the structure, as well as recruitment, of synergies. When a perturbation was targeted to the activity within a single muscle, I found a transient, relative suppression of this muscle in response to descending motor commands. In other motor cortical microstimulation experiments, I confirmed that long-train microstimulation is able to evoke complex, convergent movements. Even during highly-trained reaching movements, I found that there was relatively little invariance of the muscular patterns in relation to kinematic variables coding for the hand's displacement and velocity."
2019,Iterative decoding of codes defined on graphs,http://hdl.handle.net/1721.1/9949,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1998."
2019,Automated detection of frequency specific fluctuations in ECG morphology,http://hdl.handle.net/1721.1/72282,"Thesis (B.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1991."
2019,Early word learning through communicative inference,http://hdl.handle.net/1721.1/62045,"How do children learn their first words? Do they do it by gradually accumulating information about the co-occurrence of words and their referents over time, or are words learned via quick social inferences linking what speakers are looking at, pointing to, and talking about? Both of these conceptions of early word learning are supported by empirical data. This thesis presents a computational and theoretical framework for unifying these two different ideas by suggesting that early word learning can best be described as a process of joint inferences about speakers' referential intentions and the meanings of words. Chapter 1 describes previous empirical and computational research on ""statistical learning""--the ability of learners to use distributional patterns in their language input to learn about the elements and structure of language-and argues that capturing this abifity requires models of learning that describe inferences over structured representations, not just simple statistics. Chapter 2 argues that social signals of speakers' intentions, even eye-gaze and pointing, are at best noisy markers of reference and that in order to take advantage of these signals fully, learners must integrate information across time. Chapter 3 describes the kinds of inferences that learners can make by assuming that speakers are informative with respect to their intended meaning, introducing and testing a formalization of how Grice's pragmatic maxims can be used for word learning. Chapter 4 presents a model of cross-situational intentional word learning that both learns words and infers speakers' referential intentions from labeled corpus data."
2019,Integrating statistical and knowledge-based methods for automatic phonemic segmentation,http://hdl.handle.net/1721.1/80127,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999."
2019,A frequency synthesizer for a 2.4 GHz spread spectrum communications application,http://hdl.handle.net/1721.1/10271,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1997."
2019,Extracting parallelism from sequential programs,http://hdl.handle.net/1721.1/14752,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1988."
2019,Tradeoffs of the use of SiGe buffer layers in tandem GaAsP/Si solar cells,http://hdl.handle.net/1721.1/107289,"III-V multi-junction solar cells currently have the highest reported theoretical and experimental energy conversion efficiency but their cost, mainly attributed to the use of expensive substrates, limits their widespread use for terrestrial applications. Successful integration of III--V's on a Si substrate to enable a III-V/Si tandem cell can lower the cost of energy by combining the high-efficiency of the III--V materials with the low-cost and abundance of the Si substrate. A maximum theoretical efficiency of 44.8% from a tandem cell on Si can be achieved by using a GaAsP (Eg=1.7 eV) as the top cell. Out of several possible integration routes, the use of a linearly graded SiGe buffer as interfacial layer between the two cells potentially yields the highest quality for the epitaxial GaAsP layer, an essential requirement for realization of high-efficiency solar cells. In this thesis, the impact of the SiGe buffer layer on the optical and electrical characteristics of the bottom Si cell of a GaAsP/Si tandem solar cell was assessed via experimental work. The growth of a SiGe buffer layer was shown to increase the threading dislocation density and as a result the leakage current of the bottom Si cell by about 10x. In addition, the low-bandgap SiGe absorbs more than 80% of the light that is intended for the Si sub-cell, reducing the short-circuit current of the Si cell from 33 mA/cm² to only 6 mA/cm². By using a step-cell design, in which the SiGe was partially etched to allow more light to reach the bottom cell, the current was increased to 20 mA/cm². To quantify the merits of the studied approach as well as evaluate other approaches, we have carried out a theoretical study of absorbed irradiance in a Si single-junction cell, a bonded GaAsP/Si tandem cell, a GaAsP/SiGe/Si tandem cell as well as the step-cell design. The GaAsP/Si bonded tandem cell showed 24% relative improvement in light absorption over a single-junction Si cell. The addition of a SiGe graded buffer was shown to reduce the total absorption by 25%, bringing the efficiency of GaAsP/SiGe/Si tandem cell under that of the Si single-junction cell. The step-cell design, even though successful in increasing light absorption, was not found effective in achieving a higher absorbed power density than that of the Si cell. These results suggest that any future work on integrating GaAsP cells on Si towards a high-performance tandem cell should be focused on using a higher-bandgap material as a graded buffer or using a wafer bonding technique."
2019,Making machines that make : object-oriented hardware meets object-oriented software,http://hdl.handle.net/1721.1/107578,"Rapid prototyping has been in the limelight for the past decade. 3D printers have an evocative name that promises production of complex parts on demand. Yet current practice doesn't quite deliver on these promises of advanced manufacturing. Existing digital fabrication tools enable repeatability and precision by using codes to describe machine actions. But the infrastructure used for digital fabrication machines is difficult to extend, modify, and customize. It is very difficult for the end-user to incorporate more forms of control into the workflow. Machine design today is largely the same as it was 50 years ago, despite decades of progress in other fields such as computer science and network engineering. I argue that we need to transition from rapid prototyping to rapid prototyping of rapid prototyping. To make diverse goods, we need diverse tools. To develop diversity in digital fabrication tools, we need reconfigurable and extensible infrastructure for machine building. Using insights from object-oriented programming, end-to-end principles in network design, and the open system interconnection model, I propose a new paradigm for machine building called object-oriented hardware. In this paradigm, software objects and hardware objects are peers that have procedures, methods, ports, and presentations. Machine building modules are available as software libraries are to programmers. A machine instantiation is an assembly of objects situated in a particular context. Using this approach, a thing together with the machine that makes it becomes an application. This method transcends the additive versus subtractive manufacturing comparisons by considering both types of rapid automation. Development work is divided into infrastructural engineering, which develop modules for use in any machine, and application development, which develop specific machine instantiations. Here I present technical implementations of machine building infrastructure first. These include distributed networked controls, reconfigurable software interfaces, and modular mechanical machine components. Then I present machine instantiations that use this infrastructure to demonstrate its capability. Finally to evaluate the object-oriented hardware paradigm in the wild, I observe machine building novices using these tools in both a workshop format and in the Fab Lab network for machine building. To make the modular components for machine building accessible in this context, I developed an extensible toolkit for machine building-the Cardboard Machine Kit. Using this toolkit, novices were able to make a wide range of machines, demonstrating the power of this method."
2019,Fast interceptor of a dynamic object,http://hdl.handle.net/1721.1/54605,"This thesis presents a path planning and control strategy that enables an unmanned non-holonomic vehicle to intercept a fast moving object. The path planning is performed under model uncertainty, with respect to the vehicle's maneuverability, as well as uncertainty in the estimation of the object's future trajectory and position. This problem involves the tracking of the dynamic object in a cluttered environment and the accurate estimation of its future position in the presence of noisy measurements. The ground vehicle (interceptor) is required to intercept the dynamic object at a predicted (catch) location in a finite amount of time. This time restriction presents quite a challenge given the inherent limitation in the vehicle's steering and maneuverability. The solution strategy is divided into three sub-problems: 1) prediction, 2) path planning and 3) control. The prediction of the parameters that describe the dynamic's object in space is accomplished via Kalman Filtering which, in conjunction with an impact predictor, provide the waypoints needed to construct a reference path that will place the interceptor on a collision course with the dynamic object (target.) A pure pursuit algorithm was used to steer the interceptor along a reference trajectory, which was designed to make the vehicle engage the dynamic object on a near tail-on aspect. In the endgame, the pure pursuit algorithm was modified to ensure arrival to the catch point while a position controller was added to ensure timely arrival to the predicted catch location. The problem statement was then augmented to include obstacle avoidance."
2019,Teaching computer science principles using StarLogoTNG,http://hdl.handle.net/1721.1/85510,"This thesis outlines the development of a 3-module set of lesson plans implemented using StarLogoTNG. The purpose of these lesson plans are to serve as a vehicle for teaching and reinforcing specific learning objectives of the CollegeBoard's Advanced Placement Computer Science Principles course, which has 7 main themes. Each lesson plan has as its focus a subset of learning objectives from one of the themes of Creativity, Data, or Internet, while simultaneously incorporating additional learning goals from the themes of Abstraction, Programming, Algorithms, and Impact. These interactive lesson plans go beyond the use of StarLogoTNG to complete specific tasks by integrating meaningful class discussions and occasional peer instruction and peer review activities. Such activities become catalysts for students to develop a deeper understanding of the course materials. By connecting learning goals from different themes of the course and packaging them in cohesive lesson plans that utilize methods of teaching for understanding, this thesis aims to provide a useful and effective set of a materials for the instruction of computer science principles."
2019,Experimental and computational studies of electric thruster plasma radiation emission,http://hdl.handle.net/1721.1/40306,"(cont.) Second, spectral measurements of a TAL type laboratory mini-Hall thruster, MHT-9, were presented. Third, radiation emission measurements of an experimental Helicon plasma source being studied to assess the possibility of using Helicon discharge as a propulsive system are presented and the trends are discussed. Two collisional-radiative (C-R) models are developed for Argon and Xenon plasmas to analyze the experimental spectra. In the C-R models, electron induced excitation, deexcitation and ionization collisions, and spontaneous radiative de-excitation transitions are simulated for neutral and singly charged ion species. The models are validated against measured spectra obtained using different experimental setups. The BHT-200 Hall thruster has insulator ceramic annular walls made of Boron-Nitride (BN). Erosion of ceramic walls is one of the major life limiting factors for Hall thrusters. Emission spectroscopy is used as a means to determine the trends in the thruster wall erosion rate by measuring the radiation emission of the Boron neutral 249.68nm and 249.77nm lines. Discussion about the spectral measurements and relevant analysis are presented."
2019,Design of the control logic for StarT-Voyager,http://hdl.handle.net/1721.1/47612,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1998."
2019,Modeling and estimation of space-time stochastic processes.,http://hdl.handle.net/1721.1/69685,Thesis. 1977. Ph.D.--Massachusetts Institute of Technology. Dept. of Electrical Engineering and Computer Science.
2019,An autonomous forklift research platform for warehouse operations,https://hdl.handle.net/1721.1/121621,"Autonomous vehicle technology has seen transformative change over the past decade, enabling products such as autonomous automobiles. In the field of warehouse automation, autonomous vehicles have a long history, with automatic guided vehicles (AGVs) existing since the 1950s. Early vehicles were inflexible and relied on costly infrastructure. However, advances in technology have enabled a much greater level of sophistication. Yet, currently only 16% of companies operating warehouses make use of AGVs. Additionally, modern AGVs available today, while quite sophisticated, are still relatively inflexible and costly. In this project, we develop a prototype forklift AGV research platform capable of operation in an indoor warehouse environment. The aim of the project is to provide researchers with a vehicle platform with which to experiment with advanced autonomy and push the boundaries of AGV capability. The vehicle is a fully functional 3-wheel counterbalance fork truck with 4000lb load capacity. The vehicle is equipped with cameras, laser scanners, an IMU and a powerful onboard computer. A reference software implementation is also developed and tested, which allows a base level of full autonomy, enabling the vehicle to perform autonomous pick and place tasks. In it's role as a research platform, it is anticipated that the vehicle will enable investigation into research areas such as fully autonomous operation using inexpensive sensors, manipulation of overhead loads, operation in unstructured cluttered environments, and operation in collaboration with human operators."
2019,Temporal texture modeling,http://hdl.handle.net/1721.1/11210,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2019,Learning by learning to communicate,http://hdl.handle.net/1721.1/42056,"Human intelligence is a product of cooperation among many different specialists. Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the ""high-level"" agile cooperation that permeates our daily lives. I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of communication bootstrapping, in which shared experiences form a basis for agreement on a system of signals. In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence. I then demonstrate progress along the course of investigation laid out in my roadmap: * I establish a measure of developmental cost that allows me to eliminate many possible designs * I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using failure simplification. * I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system. * I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping."
2019,Micro-optic elements for a compact opto-electronic integrated neural coprocessor,http://hdl.handle.net/1721.1/97800,"The research done for this thesis was aimed at developing the optical elements needed for the Compact Opto-electronic Integrated Neural coprocessor (COIN coprocessor) project. The COIN coprocessor is an implementation of a feed forward neural network using free-space optical interconnects to communicate between neurons. Prior work on this project had assumed these interconnects would be formed using Holographic Optical Elements (HOEs), so early work for this thesis was directed along these lines. Important limits to the use of HOEs in the COIN system were identified and evaluated. In particular, the problem of changing wavelength between the hologram recording and readout steps was examined and it was shown that there is no general solution to this problem when the hologram to be recorded is constructed with more than two plane waves interfering with each other. Two experimental techniques, the holographic bead lens and holographic liftoff, were developed as partial workarounds to the identified limitations. As an alternative to HOEs, an optical element based on the concept of the Fresnel Zone Plate was developed and experimentally tested. The zone plate based elements offer an easily scalable method for fabricating the COIN optical interconnects using standard lithographic processes and appear to be the best choice for the COIN coprocessor project at this time. In addition to the development of the optical elements for the COIN coprocessor, this thesis also looks at the impact of optical element efficiency on the power consumption of the COIN coprocessor. Finally, a model of the COIN network based on the current COIN design was used to compare the performance and cost of the COIN system with competing implementations of neural networks, with the conclusion that at this time the proposed COIN coprocessor system is still a competitive option for neural network implementations."
2019,Model-based compressive sensing with Earth Mover's Distance constraints,http://hdl.handle.net/1721.1/82391,"In compressive sensing, we want to recover ... from linear measurements of the form ... describes the measurement process. Standard results in compressive sensing show that it is possible to exactly recover the signal x from only m ... measurements for certain types of matrices. Model-based compressive sensing reduces the number of measurements even further by limiting the supports of x to a subset of the ... possible supports. Such a family of supports is called a structured sparsity model. In this thesis, we introduce a structured sparsity model for two-dimensional signals that have similar support in neighboring columns. We quantify the change in support between neighboring columns with the Earth Mover's Distance (EMD), which measures both how many elements of the support change and how far the supported elements move. We prove that for a reasonable limit on the EMD between adjacent columns, we can recover signals in our model from only ... measurements, where w is the width of the signal. This is an asymptotic improvement over the ... bound in standard compressive sensing. While developing the algorithmic tools for our proposed structured sparsity model, we also extend the model-based compressed sensing framework. In order to use a structured sparsity model in compressive sensing, we need a model projection algorithm that, given an arbitrary signal x, returns the best approximation in the model. We relax this constraint and develop a variant of IHT, an existing sparse recovery algorithm, that works with approximate model projection algorithms."
