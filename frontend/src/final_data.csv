year,title,college,author,supervisor,matched_domain,similarity,abstract
2023,面向复杂场景的交互式图像分割,计算机学院,林铮,程明明,CV,0.3815,"交互式图像分割指的是一种用户在图像上不断添加交互以获得目标对象精确掩膜的分割任务。该任务对于基于深度学习的计算机视觉领域所需的大规模数据标注具有重大意义，同时也作为图像编辑、目标标识等工作的基础。由于用户的任务需求不同，交互式图像分割需要面对各类复杂场景，其可能存在不同的图像类型、物体分布、目标结构等。在此情况下，该任务主要存在以下四个逐步递进的难点与挑战：（1）目标定位不准确；（2）局部区域精度低；（3）细小结构交互难；（4）医学图像分割差。面对这些难点，如何设计高性能的网络模型以及高效率的交互模式成为了面向复杂场景的交互式图像分割的关键问题。

      对应上述挑战，本文由主到次提出了四个研究目标，从网络模型和交互模式两个维度提供了解决办法，并在多个数据集上取得了领先性能。具体内容如下：

      1. 为了实现针对目标定位的全局物体分割，本文提出了基于初始交互点注意力的交互式分割。该工作突出了初始交互点的目标定位作用，并提出了初始交互点注意力网络。该模型利用初始交互点的指导信息解决了目标的定位问题，并使其他交互点能更好地实现修复目的。

      2. 为了实现针对精确细节的局部区域分割，本文提出了深入聚焦视角的交互式分割。该工作提出了聚焦分割的流程框架。该框架从交互点的聚焦视角出发，在整体分割的基础上，对交互点周围的局部分割进行精细化修复，有效地提升了交互式分割方法对于细节的分割性能。

      3. 为了实现针对细小结构的复杂拓扑分割，本文提出了修复细小结构的切割线交互式分割。该工作针对细小结构物体提出了新的切割线交互模式并设计了相应的网络模型。该模型利用细小区域的相似性，让用户自由地对局部或全局的细小结构分割进行修复，有效地减轻了用户的交互负担。

      4. 为了实现针对低对比度的医学图像分割，本文提出了面向医学图像的多模式交互式分割。该工作设计了一个多模式的交互式医学图像分割框架。该框架不仅集合了多种初始及修复的交互模式，而且通过一个共享网络使这些交互相互协作，对低对比度医学图像中的目标进行高效分割。"
2024,边缘计算场景下的任务分发和资源分配方法研究,计算机学院,于朝阳,刘晓光,CV,0.253,"随着网络通信和物联网技术的快速发展，终端设备数量呈指数级增长，海量的数据需要实时处理。同时，自动驾驶等新一代物联网应用对服务质量也提出了更高的要求。边缘计算作为新兴的计算范式，将云计算能力下沉到数据源附近，不仅减轻了远端云数据中心的带宽压力，降低了数据传输延迟，也缓解了终端设备的计算和存储压力，能够满足不同应用领域对计算的多样化需求。人工智能技术的发展进一步推动了边缘计算平台的部署，开启了万物智联新时代。


尽管边缘计算提供了更高效、安全和实时的计算环境，边缘任务分发和资源分配问题仍然存在尚未解决的重要挑战。首先，在单边缘云场景下，非欧式结构数据难以进行有效特征提取，不合理的任务分发策略会降低边缘系统的资源利用率。如何高效进行任务分发，并维持边缘服务器集群的负载均衡是一个难题。其次，多边缘云协作任务分发能够实现资源共享，缓解单边缘云计算压力。然而，瞬时高并发任务会导致边缘服务器负载具有高度不确定性。如何准确预测负载，并实现任务计算延迟和卸载成本的多目标联合优化，从而提高多边缘云协作的效率，是一个重要的挑战。最后，边缘服务器执行的资源分配策略同样会影响边缘系统整体性能，资源分配不当会增加任务延迟，加剧任务间的资源竞争。如何精确模拟边缘服务器的资源利用追踪数据分布，优化其资源分配和调度，同时保证边缘服务商利益和用户服务质量，又是一项重要挑战。


国内外学者已经对边缘场景下的任务分发和资源分配问题进行了探索，在提高任务执行效率和优化边缘系统性能方面取得了一定的成果。然而，前人的研究对象缺乏对非欧式结构数据的关注，对环境状态的感知和建模不够完善。此外，已有的研究方法缺乏主动探索环境的能力，难以在动态复杂的边缘环境下实现任务分发和资源分配的自适应决策和持续优化。本文引入人工智能技术，基于深度强化学习进行边缘计算场景下的任务分发和资源分配。首先，归纳四个模型设计原则，以指导任务分发和资源分配系统框架的设计，确保边缘环境模型开发和部署的可靠性，适应物联网多样化的应用需求。接着针对上述挑战，分别在不同边缘场景下，基于不同的研究问题和优化目标，建立高效和自适应的边缘任务分发和资源分配决策机制。具体来说，本文包含以下三个工作：


第一，针对单边缘云下任务特征难提取、任务分发负载不均衡的问题，提出了一种基于深度强化学习的单边缘云任务分发模型。该模型使用图注意力神经网络对有向无环图类型的作业进行有效特征提取与聚合，动态学习任务本身及子任务之间的隐含信息；基于高维抽象任务特征与边缘服务器资源特征建模环境状态，通过深度Q 网络在线进行任务分发决策，保证了任务分发位置的公平性。从长期角度来看，该模型实现了单边缘云下多个边缘服务器之间的负载均衡。相较于GRL 算法，提出的模型最大平均性能提升可达到6.22%。


第二，针对多边缘云下负载预测难、任务分发协作效率低的问题，提出了一种基于双分支强化学习的多边缘云安全协作任务分发模型。面对大量的瞬时高并发任务，通过多边缘云协作方式降低了任务计算延迟，缓解了单边缘云的计算压力。该模型采用双向长短期记忆网络预测边缘服务器负载，降低了瞬时高并发任务引发的边缘服务器负载的不确定性；通过图神经网络对有向无环图作业和无向加权图边缘网络拓扑进行特征提取和聚合，以完善环境状态的建模。该模型基于双分支强化学习方法，从长远角度实现任务计算延迟和卸载成本的多目标联合优化，提出的模型相对于GRAM 算法的最大性能提升达到10.13%。


第三，针对多边缘云下的资源追踪分布模拟不准确、系统资源利用率低的问题，提出了一种基于多经验回放强化学习的多边缘云资源分配模型。将研究问题从任务分发扩展到资源分配，旨在优化边缘系统性能。该模型基于生成对抗网络进行半监督学习，对边缘服务器历史资源利用追踪数据分布进行精确模拟，预测边缘服务器未来资源利用率。该模型建立带有优先级的多经验回放强化学习机制，提高了采样效率，降低了样本之间的相互影响，从长期角度来看提高了边缘服务器的资源利用率，同时保证了边缘服务提供商的利益和用户服务质量。提出的模型相对于MMRA 算法的最大性能提升可达到48.21%。


综上，本文基于深度强化学习方法解决了单边缘云场景下的任务分发、多边缘云场景下的安全协作任务分发及多边缘云场景下的资源分配问题。研究场景从单边缘云扩展到多边缘云，研究目标从单目标优化扩展到多目标联合优化，研究问题从任务分发扩展到资源分配，建立了边缘计算场景下高效和自适应的任务分发和资源分配决策机制。本文研究工作所涉及的问题及提出的解决方案为未来边缘计算领域的任务管理和资源调度研究提供了新的方向和思路。"
2024,基于神经网络的医学图像篡改检测与定位研究,计算机学院,王南,王刚,CV,0.3125,"数字化信息系统在医疗领域得到广泛应用，发挥着日益重要的作用。通过实现医学图像等诊断信息的传输，远程医疗诊断逐渐成为现实，同时为学术界的教学和科研也提供了便捷。然而，随着这一数字化趋势，医学图像篡改问题成为学术界和医疗界亟待解决的安全隐患。与自然图像不同，受限于医学图像视觉质量和检测精度的特殊性要求，针对医学图像的篡改检测与定位的相关研究仍处在探索阶段，现有算法在精确性方面仍显不足。


本文针对医学图像的精细纹理结构与灰度变化特点进行深入剖析，研究针对医学图像的篡改检测与定位。本文针对复制—粘贴和拼接篡改问题，提出了一种新颖的医学图像篡改检测与定位网络模型，命名为MemAU-Net。在这一模型中，本文引入了记忆增强的注意力门（Memory-Enhanced Attention Gate, MAG），通过整合到U-Net的跳层连接中，增加记忆补充模块（Memory Supply）和记忆聚焦模块（Memory Focus）有效融合了浅层和深层特征，优化了特征表示，使得注意力机制更适用于医学图像篡改定位任务。同时，本文引入全连接条件随机场（Dense CRF），以平滑检测结果边界、减少误检和漏检。实验证明，MemAU-Net分别在眼底和肺部CT两个医学图像篡改数据集上，相较基线模型均达到最优表现。添加Dense CRF进一步提高了精确度并增强了可视化效果。此外，MemAU-Net对于不同篡改后处理和噪声攻击都表现出了良好的鲁棒性。


鉴于扩散模型在图像生成领域的卓越性能和潜在的强大特征提取能力，结合对于实际医疗场景的时效性和资源受限程度的考虑，本文提出了一种基于扩散模型和知识蒸馏的医学图像篡改检测与定位算法，命名为 DDFL（Diffusion Distillation Forgery Localization），由扩散模块和蒸馏模块两部分构成。算法通过扩散模块强化医学图像篡改特征，同时引入知识蒸馏技术，在保持性能的前提下减小模型规模，解决了扩散模型采样慢、参数多、模型大等问题。实验结果表明，DDFL在混合不同模态医学图像的篡改定位任务上取得了优越性能，用轻量级网络实现了医学图像的精确篡改定位，并对于旋转、缩放篡改后处理和噪声攻击表现出鲁棒性，更适用于存储和计算能力有限的医疗设备。"
2023,复杂场景的自适应视觉感知,计算机学院,高尚华,程明明,CV,0.4161,"视觉感知旨在理解视觉场景中的语义要素。面对复杂场景进行鲁棒的视觉 感知，需要感知系统具有强大的自适应处理能力，进而需要在模型架构和表征 学习策略两个方面处理如下挑战:(1)面对复杂多变的场景，模型架构需要具备 多尺度特征自适应处理能力和相应的感受野。(2)复杂场景产生的大规模和多 样化的数据使人工标注过于昂贵，因而要求模型能够在尽可能少的人工干预下 完成对数据的表征和理解。(3)复杂场景导致训练数据和模型体积的激增，因 此要求新模型能够有效利用现有模型的知识储备自适应地学习更强的表征，从 而降低训练学习的计算开销。


针对以上问题，本文着重研究在复杂场景下增强网络架构的多尺度和感受 野自适应能力，以及表征学习策略的数据和模型学习的自适应能力。具体而言， 本文对以下几个方面的自适应感知能力进行研究。


1)本文提出了基于残差递进多尺度模块的主干网络架构，其多尺度特征的 自适应表征能力得到组合爆炸式的提升。所提出的主干网络架构在分类、检测 等十多种典型视觉任务中取得了显著性能提升。


2)为了克服传统方法手工指定感受野大小的局限性，本文针对复杂场景实 际需求，提出对卷积神经网络感受野进行从全局到局部的自适应搜索的有效算 法。该感受野搜索算法可以有效提升多种应用中的模型性能。


3)为了避免昂贵的数据标注，本文提出了大规模无监督语义分割问题，并 设计了有效算法。该算法通过自我表征学习从百万量级数据中学习丰富的语义 特征，并将自适应总结出的上千个语义类别分配给大规模数据中的每个像素。 本文验证了无需人工标注的自适应大规模视觉感知是可行的。


4)本文提出绿色可持续视觉感知模型学习的新概念。通过构建基于掩码重 建的目标增强条件化自监督预训练机制，本文方法可以自适应地学习并超越特 性各异的已有视觉模型，避免了现有神经网络基础模型从头开始训练导致的计 算量和能耗过高的问题。"
2023,边缘辅助下基于 SVC 的文本视频超分方法研究,网络空间安全学院,张海玉,张建忠,CV,0.3315,"近年来，随着5G与人工智能等技术的不断发展，远程教育快速普及，线上教学逐渐成为教学的主要形式。在线上教学形式中，教学视频的学生端观看质量是影响教学效果的重要因素。当下广泛使用超分辨率技术、更有效的编码方式和边缘计算等技术来提升视频传输与播放的质量。因此本文通过相关工作的调研，分析上述方法中存在的问题与挑战，并针对性地开展研究，以提高在线教学中的视频质量。论文的主要工作如下：

      首先，本文提出基于SVC可伸缩视频编码的文本视频超分方法。该方法有效结合SVC可伸缩视频编码技术与视频超分辨率技术，能够提升在线教学视频的观看质量。首先分析了SVC可伸缩视频编码用于视频超分辨率的可行性：SVC可伸缩编码的特点能够满足视频实时内容自适应的训练需求，不断调整的 模型参数使模型能够更好适应当前视频内容，超分效果更好。其次针对在线教学视频内容大多以文字为主的特点，设计了文本视频超分模型，改进其预处理方法及损失函数，使模型在字体轮廓及色彩方面有更好的超分效果。最后对基于SVC的文本视频超分方法使用数据集进行实验验证，结果证明此模型能满足视频超分的实时性要求，且相对于基准方法在视频观看质量上有较好的提升。

      其次 ，本文设计并实现了边缘云辅助的在线教学视频超分系统。系统由核心云、边缘云、端用户三层网络架构组成。在核心云中设计实现了部署调度模块与模型同步模块。其中设计服务编排算法，实现工作节点的动态编排与调度；设计超分模型同步方法，实现超分模型共享。在边缘云中设计实现了资源监控模块、超分任务模块及视频传输模块。其中设计监控方法实现对工作节点的资源监控；设计合理的超分任务训练推断流程和视频传输  网络架构，有效发挥SVC编码优势，降低网络带宽消耗。在端用户中提出了SVC自适应分辨率算法，并在此基础上提出了符合在线教学场景的QoE评价标准。本文设计API实现模块之间的通信，并在设定的网络波动情况下使用在线教学视频数据集对系统稳定性进行评估，实验结果证明，本文系统对网络波动有较高适应性，能够有效提升视频用户观看质量。"
2023,基于安全多方计算的隐私保护特征选择协议研究,网络空间安全学院,韩叶,刘哲理,Security,0.2858,"多方数据协作的价值日渐凸显，在数据安全和隐私保护的必然需求下，隐私计算技术正在受到越来越多的关注。作为代表性的基本密码技术，安全多方计算成为了解决跨组织的数据分享、流通和使用中面临的数据安全和隐私问题的重要手段。在此基础上构建而来的隐私保护机器学习技术是隐私计算协作数据分析领域的代表性方案，作为密码学和机器学习的交叉领域，该技术在解决隐私问题方面取得了重大进展。本文重点关注协作数据分析场景下，隐私保护机器学习在数据利用中的数据安全问题。目前，该领域的大部分研究工作主要集中在隐私保护的模型训练过程，而忽略了数据准备阶段的以特征工程为主要代表预处理步骤。由于数据来源于互不信任的多方，而各方需要协调统一进行特征选择等操作，任何数据泄漏都会影响原始数据的机密性，从而使得后续基于隐私计算的模型训练变得毫无意义。同时，安全数据处理协议的效率是开展实用数据合作的重要前提。


本文基于安全多方计算技术，提出了一个新的高效隐私保护的特征选择框架。该框架允许数据协作参与方在保护数据隐私的前提下完成对数据目标特征的选择，同时在协议结束后任何一方都无法获知被选择的特征的具体信息。为了进一步提升效率，在该框架的基础上，本文对协议设计的子模块进行了效率优化，给出了该场景下更为高效的安全top-k查询协议，用于完成对于特征评分排名的茫然查询，而不披露任何额外信息。本文所提出的隐私保护特征选择协议独立于具体的模型训练过程，具有良好的扩展性，能够与各种特征评分指标结合使用。作为一个示例，本研究探索了安全多方计算环境下皮尔森相关系数的使用，并实现了一个基于皮尔森相关系数的隐私保护特征选择协议。本研究对所设计的协议进行了理论和实验的评估，在三个公开真实数据集上进行了测试，结果显示，本文所提出的隐私保护特征选择协议是当前最高效的方案，在计算复杂度和通信开销上都有显著的提升，同时其扩展性可以良好地和现有技术相结合，以促进数据协作分析任务的开展。"
2023,基于Hopfield能量函数的OOD检测研究,计算机学院,张劲松,王刚,CV,0.3613,"近年来，深度神经网络的迅速发展极大的改善了人们的生活水平，但伴随而来的便是神经网络的安全问题，其中，OOD (Out-of-Distribution) 检测便是神经网络安全性问题的一大研究难点。在实际生产应用中，神经网络只有应对与训练集同分布（记作ID数据: In-Distribution）的测试用例时，其输出才是有意义的，反之，若测试用例与训练集属于不同分布（记作OOD数据），则其对应的输出毫无意义且存在安全隐患。


现有的神经网络，在面对与训练集同分布的ID数据，表现出越来越精确的预测准确性。然而当神经网络输入的样本为OOD数据时，神经网络同样会表现出和ID数据一样的高置信度，这使得神经网络无法轻易的识别出OOD数据。并且，由于OOD数据属于未知数据，具有不可预测的特点，通过训练的方式主动疏远OOD数据并不可行。


本文提出一种基于Hopfield能量函数的“存储-比较”的方式来进行OOD检测。该方法选取神经网络的倒数第二层输出特征图作为研究对象，首先选择了训练集中样本对应的特征图进行存储。其次，当网络进行在线推理时，该方法利用Hopfield能量函数作为衡量标准，将测试样本对应的特征图与提前存储的特征图进行比较，并计算出对应的Hopfield能量函数，进而判断测试样本是否属于ID数据。此外，为了降低Hopfield网络所带的存储开销和计算复杂度的成本，本文简化了传统的Hopfield能量函数，并提出一种免除超参数的OOD检测方法，并从实验和理论角度分析了简化的可行性。本文分别在CIFAR10，CIFAR100和ImageNet数据集进行了实验，选取了三种常用神经网络和九种常用的OOD检测数据集，大量的实验结果表明我们的方法具有简单且有效的特性。"
2023,基于带内遥测的网络拥塞控制方案设计与实现,网络空间安全学院,蔺雪莹,张建忠,Network,0.3521,"近年来，随着互联网产业的发展和规模的增长，多种新兴应用层出不穷，用户在使用网络应用的过程中对网络服务质量提出了更高的要求。网络运营商希望通过提升实时性、可靠性进一步满足用户体验。网络拥塞是影响网络服务质量的重要因素之一，传统的拥塞控制机制主要通过路由调度或控制发送方的发送速率等方式实现。随着可编程网络等新型网络架构的出现，在数据平面调整交换机缓冲区大小以解决网络拥塞问题，成为新场景下改善网络性能的有效方案。


       基于上述思想，本文在可编程数据平面的基础上，结合带内遥测技术和深度强化学习模型，实现了自适应调整交换机缓冲区大小的拥塞控制系统。本文设计的系统可以实时收集交换机内部的网络状态信息，利用深度强化学习模型对交换机缓冲区大小进行动态调整，达到保障网络服务质量、改善网络性能的目的。本文重点研究了带内遥测技术的设计与实现，主要包括的研究内容如下。


       首先，分析了带内遥测技术中的关键点，并在可编程数据平面实现带内遥测方案，对遥测模式、数据包格式等细节进行设计，以实现收集交换机内部实时、细粒度的网络状态信息的功能。相比于其他的网络测量方案，该方案在降低通信成本的同时，在使用上也更加高效。同时，基于带内遥测的设计方案，在硬件环境中对主机、交换机和遥测服务器的功能进行实现和验证。其中，主机用于发送和接收数据包；交换机用于对接收到的数据包进行处理，使其携带遥测信息；遥测服务器用于对数据包中的遥测信息进行提取。


       其次，在带内遥测技术研究的基础上，本文搭建了相应的原型系统，通过调整交换机缓冲区大小实现对网络的拥塞控制，并对系统性能进行测试分析。系统由带内遥测、数据训练、数据库、前后端交互等4个模块组成。由于硬件环境的限制，带内遥测模块移植在软件仿真环境下再次实现。实验结果证明，系统方案在多种网络服务质量的优化目标下，均能够达到良好的性能。例如，在UDP的低延迟场景下，相比于固定缓冲区大小为100的方案，系统实现的MAABS策略在效果上提升了176.6%。"
2023,基于口令的多因素认证协议研究,计算机学院,徐美佳,汪定,Security,0.3223,"身份认证是保障信息系统安全的第一道防线，往往也是唯一的防线。在身份认证中，口令作为最简便的认证方法，在可预见的未来仍是最主要的身份认证方法。在基于口令的单因素认证协议中，口令通常被存储在服务器中，一旦服务器被攻击导致口令文件泄漏，可能会引起口令被暴力破解等问题。因此对安全需求高的无线传感器网络应用，采用口令与其它认证因素相结合的方式进行认证，基于口令的多因素认证协议因时而生。然而研究表明，大多数多因素认证协议在被提出不久即被攻破。如何设计一个基于口令的多因素认证协议，使之避免陷入“破坏-修补-破坏-再修补”的恶性循环仍是一个亟待解决的问题。此外，为保障协议的安全性，还应对所设计的协议进行安全性分析。最新研究指出，协议的安全性应包含两个层面：（1）协议“独立”执行时的安全性；（2）协议作为一个模块与其他协议并发执行或被调用时的安全性（即通用可组合安全，又称UC安全）。协议独立执行时的安全性分析往往采用形式化的证明方法（如随机预言机模型和自动化分析工具），而基于口令的多因素认证协议的通用可组合安全问题鲜有研究。为解决上述问题，本文进行了以下工作：


（1）对新近提出的三个具有代表性的基于口令的多因素认证协议进行分析，以这三个协议为例，指出大多数协议中存在的共性问题，例如不能抵抗离线口令猜测攻击和节点捕获攻击，针对各类问题分别提出了相应的解决办法。


（2）基于Ma等人提出的协议设计原则和Wang等人提出的12条协议评价指标，提出一个基于口令的多因素认证协议，在随机预言机模型下证明了协议的安全性，并与新近提出的15个协议进行对比。对比结果显示，本文提出的协议在保障安全性的前提下，性能明显优于其他协议。


（3）基于通用可组合模型提出一个新协议并证明其通用可组合安全性，其中首次设计了基于口令的多因素认证UC理想功能函数，并采用当前最先进的自动化分析工具Tamarin建模并分析协议的安全属性。对比分析了自2009年以来提出的具有代表性的66个多因素认证协议，分析结果显示，本文提出的协议是唯一可满足Wang等人的12条协议评价指标并保证通用可组合安全的协议。"
2023,一种提升去中心化程度和共识算法安全性的区块链系统,网络空间安全学院,袁也,李雨森,Security,0.2913,"去中心化、安全和高效率构成的不可能三角一直是区块链研究的重要方向。越来越多的学者围绕着区块链的去中心化、安全和高效率展开研究，试图打破不可能三角。本文提出两种技术，分别在去中心化特性和安全性方面做出提升。


在去中心化方面，本文首先梳理了区块链系统的三类主要参与者：合格参与者，机构类参与者，投机性参与者；分析了这三类参与者对去中心化的影响并给出了去中心化程度的衡量变量；然后提出一种新的区块链经济系统，包含激励机制和货币机制，该系统实现了以下目标：


（1）设计激励机制使机构类参与者在参与共识的过程中可流动收益期望小于其最低接受值，从而主动退出系统，同时合格参与者与投机性参与者保持较高参与程度，提升区块链的去中心化程度。


（2）设计新的双币并行的货币机制，使货币总流通量在项目初期和发展期均能匹配货币需求，从而起到稳定币价、推进区块链生态稳定发展的作用。


实验过程将未使用本文经济系统和使用该系统的两个模型形成对照，在本文系统中投机性参与者的参与程度相较于对照模型提升了100%以上，机构类参与者在项目初期相较于对照模型参与度降低60%，很快其参与度降低为0，合格参与者的参与程度保持在50%以上，系统的去中心化程度提升了50%以上，在不同的代币消耗速度下货币机制均能调节代币数量，使代币供给匹配需求。


在安全性方面，随着近年来部分哈希算法安全性受到威胁，哈希算法在区块链共识中的应用也为区块链系统带来了安全风险。本文在区块链系统中分别采用Legendre序列、累加器技术和CMAC技术替换哈希算法挖矿、成员证明和摘要方面的应用，基于Bitcoin源代码实现了无哈希的区块链系统，给出了安全性证明，实验结果表明新的区块链系统能够完整实现挖矿、摘要、成员证明功能。"
2023,面向边缘智能的AI推理服务优化与隐私保护研究,计算机学院,张健丰,徐敬东,CV,0.3055,"万物互联的时代中，无人机、自动驾驶汽车、智能摄像头等物联网（Internet of Things，IoT）设备配备的高清摄像头、激光雷达等传感器所产生的数据量日益增多。与此同时，物联网设备通常具有受限的计算能力、带宽资源、存储空间，因而难以单独完成数据的分析处理任务。由于借助传统的云计算模式对这些数据进行处理有着较高的延迟，而且会对运营商核心网造成较大的传输压力，学术界以及工业界普遍认为需要利用边缘智能的理念，即把边缘计算的范式与人工智能（artificial intelligence，AI）相结合，运用边缘服务器的计算能力与AI模型的分析能力，对数据完成及时有效的处理。因此，在物联网设备产生数据量日益增长、AI模型的复杂度与计算量日益提高的背景下，论文从优化AI模型推理服务质量与用户数据隐私保护两个方面展开工作。论文的主要研究成果包括以下三个方面：


第一，为了降低模型推理服务的传输延迟并提高带宽利用效率，提出了适用于大数据量场景的由模型推理任务驱动的高效数据压缩、传输框架TORC与相应的算法。考虑到实际服务部署场景下的模型设计、更新、维护的复杂度，该框架具有较低的耦合度，在不对现有推理任务模型做出改动的情况下，由推理任务引导轻量级编解码器的编解码行为，进而在提高带宽利用率的同时，使边缘计算服务提供商能够对来自不同类型用户的请求进行批处理，提高硬件资源的利用率。TORC架构维持了AI模型推理任务与传统非AI任务的兼容性，并能够满足实际应用在单数据流上完成多个任务的需求。在仅需部署单一版本编解码模型的情况下，TORC能够适应于动态可变的带宽预算，以及不同模型推理任务重要性的变化，从而进一步提高带宽的利用效率。由于码率控制与模型推理任务之间存在依赖关系，论文提出了不同于现有多任务学习相关工作的算法，有效地完成编解码器模型的训练。实验评测结果表明，TORC编解码器具有较低的模型复杂度，能够较好地适应于带宽与任务重要性的变化，在相同的带宽下实现更高的推理任务准确率，最高可将带宽消耗与总延迟分别降低48%与26%。


第二，为了从边缘计算服务器的角度优化推理服务质量，论文考虑到了用户应用具有准确率、延迟等方面的不同需求，而且其效用函数具有差异性以及非线性。因此，服务器上需要为同一类型的推理任务部署多种不同的模型，这些模型在吞吐量、延迟特性、准确率等方面存在差异。在此基础上，需要解决用户请求与服务器上模型之间的调度问题。为此，论文首先基于已有工作提出了新的服务质量定义方式，它包含了用户非线性的效用函数，此外基于实际测试得出的请求延迟概率分布特性，引入了金融领域中下侧风险的概念，对用户请求经历高处理延迟的风险度进行描述。随后，在服务器扩容行为之间较短的时间尺度上，考虑到用户请求速率存在随机性，论文借助随机优化中样本均值近似的理论，对系统总的服务质量进行优化。实验结果表明，尽管存在着来自多个方面的误差，论文所提出的方案能够较为准确地估计实际系统的服务质量数值，降低高处理延迟的下侧风险，并提高推理服务的质量。


第三，由于边缘智能场景中用户数据隐私保护问题具有较高的重要性，且得到了越来越多的关注，因此论文考虑AI模型切分推理（即将其划分为用户部分与服务器部分）这一常见模式下的隐私保护问题。在该模式下，用户使用由服务提供商所提供的特征提取器对数据进行处理，所得到的特征随后发送给服务器完成后续处理，从而得到效用任务的推理结果。用户期望在得到效用任务正确处理结果的同时，避免如下两种对于隐私的威胁：恶意方（例如不被信任的边缘服务提供商）基于用户所传输的特征尝试复原出用户原始数据，以及恶意方从获取的特征中提取用户所不知情的隐私属性。为此，受到图像隐写术以及基于欺骗理念的网络防御思想的启发，论文提出StegEdge方案，对用户所需传输至服务器的特征进行隐私保护处理。在比相关工作更加宽松的威胁模型下，StegEdge利用欺骗的理念，在特征层面上将用户真实图像与不涉及隐私的图像（称为“容器图像”，例如随机下载的图片）融合。因而，恶意方较难发现用户隐私保护行为，与此同时所获得的隐私信息接近于容器图像中的信息。StegEdge方案维持了用户侧模型与服务器侧模型的兼容性，且具有较低的复杂度与计算开销，适用于计算资源受限的物联网设备。在两个数据集上的实验结果表明，相比于其他方案，StegEdge借助其欺骗性能够在相同效用任务准确率的情况下，更为有效地降低隐私信息的泄漏，进而抵御上述的两类隐私侵犯行为。"
2023,基于卷积注意力机制的视觉识别和语义分割骨干网络设计,计算机学院,陆承泽,程明明,CV,0.4162,"作为计算机视觉领域中基础任务之一，视觉骨干网络设计任务旨在提取高语义、多尺度的视觉特征。先前的骨干网络大多由纯卷积模块或自注意力机制构建，针对卷积神经网络无法捕捉全局信息和基于自注意力的Transformer网络训练成本较高的问题，本文主要研究使用卷积注意力机制来简化自注意力机制，使用卷积注意力机制作为基础模块搭建骨干网络，并在多个下游任务的基准数据集上进行骨干网络的评估。对于特定的下游任务语义分割，本文基于卷积注意力提取多尺度信息，并在多个语义分割数据集进行评估。

本文的主要贡献可概括如下：


（1）针对自注意力机制的计算复杂度高达n2的问题，本文提出了卷积注意力机制。使用Hadamard积作为融合方式，对输入特征进行重新加权，在降低计算复杂度的同时保证了对输入的自适应性。利用卷积注意力机制，本文从多个维度精心设计了Transforemr风格的骨干网络，包括模型深度与宽度的权衡，并且探索使用更大卷积核时骨干网络的性能变化。本文在三个不同的基准任务上进行了广泛的实验，包括图像识别、目标检测和语义分割。实验结果表明，本文提出的骨干网络在三个基准数据集上超越了当前基于卷积的和基于Transformer的最先进的方法。此外，本文还指出了该骨干网络存在的局限性和未来研究方向。


（2）针对特定的下游任务语义分割，本文针对性地拓展了所提出的卷积注意力机制，并简化了前有语义分割网络的设计理念，在减少计算量的同时避免性能的退化。考虑到多尺度特征的提取在语义分割任务中的重要作用，本文使用Hadamard积作为融合方式，在融合权重中利用不同大小的卷积核以引入多尺度信息。同时，本文使用大核卷积以扩大视觉感受野，有效提高了模型表现。本文在五个语义分割数据集以及一个遥感分割数据集上对本文提出的语义分割骨干网络进行了评估与实验结果分析。实验结果表明，相较于现有的语义分割模型，本文所提出的骨干网络在计算量相当的情况下性能大幅领先。"
2023,基于零知识证明的区块链可扩展性优化,计算机学院,乔悠扬,卢冶,Network,0.2788,"随着区块链技术的发展，其应用范围在逐渐拓宽，各个行业领域对区块链系统的性能需求也在不断提升和变化。但是传统区块链系统的共识效率、通信效率，以及数据安全性和隐私性等很难适应当前的应用场景需求。因此，本文以联盟链为例，针对区块链在实际应用时面临的共识效率低、可扩展性差，以及加入门槛高等问题进行研究。论文从区块链系统整体架构出发，对节点身份验证机制、共识流程冗杂以及节点间通信效率低等几个问题展开研究。本文的主要贡献如下：


（1）本文对三种区块链类型以及多种共识算法原理进行分析对比，选择应用范围较广且性能较好的联盟链进行研究，提出了基于零知识证明技术的实用拜占庭容错共识算法（Zero-Knowledge Practical Byzantine Fault Tolerance，ZKPBFT）。算法采用加入验证者节点的方法，一方面对共识过程中节点投票信息进行安全验证，简化共识通信复杂度；另一方面配合主节点完成共识过程，提升共识效率。通过仿真实验设计，从系统容错率和共识效率两个维度对比算法的改进效果。实验结果表明，算法对拜占庭节点的容错率由33%提升至50%，节点集群共识效率在总节点数超过100时提升达90%。


（2）本文根据区块链数据特性，选取四种主流的压缩率较高的文本压缩算法。分析并对比了压缩算法原理，在节点通信过程中引入数据压缩方法和解压缩处理。通过设计仿真实验，模拟区块链节点间数据传输，使用XBlock-ETH以太坊区块数据集评价改进效果。实验结果表明，压缩后节点间数据传输效率提升最高可达87.75%，对区块链系统数据传输有显著优化效果。


       本文提出的ZKPBFT共识算法和节点间数据压缩传输方法能够提升区块链系统的运行效率和可扩展性，拓宽区块链系统的应用场景，在区块链可扩展性研究方向具有一定的应用价值。"
2023,基于算力网络的联邦拆分学习多元调度问题研究,网络空间安全学院,杨梅娟,张建忠,Network,0.3166,"在计算和网络深度融合发展的大趋势下，算力网络已成为当前研究热点。通过全面监控计算和网络资源，算力网络能够实现云、边、网高效协同，提高计算和网络资源的利用效率。随着未来对大规模数据的收集和分析需求不断增加，分布式机器学习将成为算力网络中不可或缺的服务之一。联邦拆分学习作为一种新的分布式学习框架，通常只采用一个训练服务器，与客户端合作进行模型拆分训练。然而，现有联邦拆分学习框架忽略了客户端的异构性，将其直接部署在算力网络中并不能充分利用不同节点的计算资源。因此，本文在对国内外相关工作进行调研的基础上，完成以下工作：


首先，本文提出基于算力网络的分布式机器学习框架。为提升分布式机器学习的效率并保证数据隐私性，本文提出一种基于算力网络的联邦拆分学习框架CAN-FedSL，该框架可以根据不同客户端和服务器的计算资源，以及激活值的参数量，决策模型拆分的位置，同时决策客户端及服务器之间的路由和带宽以保证模型在有限时间内完成收敛。


其次，本文对框架中的多元调度问题进行建模并求解。本文基于 CAN-FedSL 框架，对训练任务时产生的训练效用和系统开销建模，形式化多元调度问题后，引入资源使用效率对问题目标进行转化。为求解该多元调度问题设计求解算法，首先将问题目标和非凸约束线性化，之后通过构建基于舍入的贪心算法进行迭代求解，直到问题目标达到收敛。本文通过严格的理论证明对算法的收敛性和复杂性进行分析。


最后，本文通过仿真实验进行性能评估。在两种真实的网络拓扑（ NSFNET 和 USNET）下，本文构建了不同网络规模场景中的CAN-FedSL 框架。通过在框架中运行两种训练任务（ MobileNet和 DenseNet）进行仿真实验，结果表明， CAN-FedSL 优于现有学习框架；通过控制算法中的不同变量，验证了多元调度问题的每部分对资源使用效率都有显著影响；与其他启发式算法相比，本文提出的求解算法在稠密网络场景下的资源使用效率明显优于其他启发式方法。"
2021,日志异常检测关键技术研究与应用,计算机学院,谢学说,黄申为,OS,0.2884,"日志作为物联网系统中广泛存在的数据资源，忠实地记录应用行为和系统状态。利用日志数据进行异常检测，可以辅助系统管理员实现非侵入式的异常定位和错误诊断，对于保证物联网的安全平稳运行具有重要的意义。近些年，基于日志的异常检测技术取得了长足的发展，尤其是将机器学习算法应用于海量日志数据分析的检测效果优异。日志异常检测模型主要分为日志收集、日志解析、特征提取和异常检测等四个步骤，其中日志收集负责提供可信的训练和检测数据，日志解析将非结构化的文本数据转换为结构化的事件模板，特征提取利用窗口技术生成特征矩阵，异常检测利用机器学习或数据挖掘方法进行事件分类。但是，具有高准确率和高可靠性的异常检测模型依赖于一定的限制条件：训练和检测的日志数据不能被篡改、日志数据的分布随着时间和系统的运行保持稳定、单条日志记录的长度和参数数量保持不变、存在足够可用且标记良好的训练数据等，影响了日志异常检测模型的性能和应用。因此，本文面向日志异常检测模型在物联网安全管理系统中实际部署应用时，从日志数据安全存储、日志解析算法评估、异常检测模型可靠性等方面入手，对日志数据篡改与信息泄露、日志解析算法过拟合、变长日志消息、以及异常检测模型概念漂移等问题开展深入和广泛研究。现将本文的主要研究内容总结如下：

       提出了一种基于置信度的日志解析评估方法，解决了传统的准确率、精度等指标难以有效评估出日志解析算法生成事件模板之间差异性等问题。首先，选择字符串编辑距离衡量日志消息与事件模板之间的相似程度，作为不一致性度量计算统计p值。其次，利用保形评估引入了两个统计指标：可信度和置信度。其中，可信度反映日志消息与日志解析算法生成事件模板的一致性，而置信度反映日志消息与所有其它事件模板的非一致性。最后，在多个数据集上评估了13个日志解析算法，并利用t-SNE技术在二维空间上可视化每个日志解析算法的可信度和置信度分布。实验结果表明，与日志解析精度评估相比，该评估方法能够更加细粒度地反映出日志解析算法生成事件模板的普适性与特异性；

       提出了一种基于区块链的日志安全存储方案，解决了日志数据在存储时的数据篡改问题和信息泄露风险。首先，设计了一个包含日志解析和Hash映射的日志预处理算法FLE，在实现消除数据冗余、减少信息泄露的同时，可以为异常检测模型保留足够多的信息。其次，利用以太坊Merkle Patricia Trie和四元组等链上存储结构，以及链下Key-Value存储结构，实现防篡改检测和事件级的篡改信息恢复。最后，在理论上分析了方案可以实现日志数据防篡改存储且不影响异常检测的性能。实验结果表明，FLE算法相比于3种最新的日志解析算法，在7个数据集上取得了不低于80.9%的准确率和99.61%的F_measure，处理2000条日志时最快为0.125s，算法置信度与基准值仅相差2%，远优于13种日志解析算法；

       提出了一个基于置信度的多算法联合异常检测模型Multi-CAD，解决了异常检测模型在动态日志中出现准确率下降等概念漂移问题。首先，通过统一的日志预处理算法生成相同的特征矩阵，避免日志解析过程对模型性能的影响。其次，选取多个机器学习算法构成非一致性度量模块，计算统计p值衡量日志消息之间的不一致性，利用先验知识保证单算法预测结果的可靠性。最后，利用预测集合、多数决策原则和反馈机制等实现多算法联合决策，利用置信度引导模型的参数调整并更新对应标签的非一致性得分集合。实验结果表明，Multi-CAD模型可以达到98.2%的准确率、95.2%的召回率和96.7%的F_measure，相比于构成非一致性度量模块的单个算法，召回率可以提高近39.1%，F_measure提高近25.7%；

       提出了一种基于物联网层次化体系结构的日志异常检测框架HADS，解决了利用日志数据进行物联网安全管理时的数据安全存储和管理策略安全可信等问题。首先，在资源受限设备上部署日志预处理算法FLE实现将原始日志消息序列转换为Hash序列。其次，在边缘层部署一个许可区块链，设计一系列智能合约，实现数据传输与存储、合约注册与查询、异常检测与反馈等全流程安全管理。然后，在服务层部署异常检测模型可以训练工作流，以邻接矩阵和合约全局参数等方式转换为边缘可执行的智能合约。最后，在应用层开发部署了一个可以监测边缘区块链和物联网接入设备状态的可视化应用平台。实验结果表明，HADS在设备层具有较低的CPU利用率，预处理速度可以提高3.6至7.3倍；在不影响日志异常检测模型性能的前提下，边缘区块链的账本压缩率可以达到7.1%。"
2021,深度卷积神经网络鲁棒性的研究,计算机学院,林怡,白刚,Robotics,0.3065,"近年来，深度卷积神经网络在许多领域得到了广泛应用，并取得了突出的成绩。然而相关研究发现，深度卷积神经网络虽然是模拟人类视觉系统设计的，但其决策结果的鲁棒性却远远不如人类观察者。这一现象的存在，严重阻碍了深度卷积神经网络在高安全性要求领域中的应用。为了探索深度卷积神经网络的鲁棒性问题，本文展开了深度研究。


      本文首先从深度卷积神经网络架构及训练流程入手，提出了“点激活”决策方式的概念，并认为由于深度卷积神经网络所采用的“点激活”决策方式，导致了其决策在语义上不可解释且低鲁棒性的结果。为了改善“点激活”决策方式带来的负面影响，论文设计并实现了基于知识蒸馏的多决策途径深度卷积神经网络模型，不增加网络参数规模的情况下，提升了深度卷积神经网络的鲁棒性。


      本文的主要贡献有：


      (1)  通过对深度卷积神经网络架构和训练过程的研究，定义了“点激    活”决策方式的概念，分析了其对决策鲁棒性和可解释性的影响，并通过所设计的一系列实验对结论进行了验证。


      (2)  通过分析卷积操作在深度卷积神经网络与数字图像处理中的具体应用，指出深度卷积神经网络中的卷积核函数与传统计算机视觉中的卷积核函数本质上并不相同。


      (3)  根据“点激活”决策方式的特点，设计且实现了两种多决策途径深           度卷积神经网络模型，具有网络规模小，训练速度快，参数利用率高等特点，相关实验验证了有关算法和结论。"
2019,基于视觉注意机制的语义分割自主学习,计算机学院,侯淇彬,程明明,ML,0.269,"近年来，作为计算机视觉领域中的热门研究方向之一，语义分割已经取得了长足进展，尤其在深度卷积神经网络的出现之后。另外，随着含有成千上万张图像的大规模数据集的出现，基于深度学习的语义分割方法的分割精度也在不断提升。然而，基于深度卷积神经网络的全监督语义分割模型严重依赖于大量人工标注数据，因而在学习新的类别信息时仍然需要庞大的人力成本来标注新数据。弱监督语义分割技术，由于仅依赖图像类别标签等轻量级标注数据，也因此正在成为一大学术研究热点。


早期的弱监督语义分割模型主要利用注意力模型等工具来生成带有语义信息的种子区域进而训练语义分割网络。此外，显著性物体分割模型，由于其检测出图像前景区域的能力，也逐渐被大多数算法所采用。将检测到的显著性区域与注意力模型生成的类别激活图像相结合可以给类别无关的显著性区域加上类别标签进而用来训练语义分割模型。以上讨论都基于一个假设方案，即所有图片的类别标签都是已知的且准确的。


考虑到人工标注的成本，当给定一个类别标签集合，如何利用免费的网络大数据来索引相关图像进而学习语义分割模型具有重大研究意义。针对上述难题，本文将从两种不同的视觉注意机制（显著性物体检测与注意力区域检测出发，提出一种有效的方案来解决这一更为通用的弱监督语义分割问题。与此同时，针对现有显著性物体检测与注意力区域检测模型的不足之处，本文也将提出改进方案。本文的具体贡献如下：


1. 提出一种自顶向下的基于短连接结构的显著性物体检测模型。通过在已有的分类网络的不同阶段后接入侧向路径并在不同侧向路径之间引入一系列自顶向下的短连接，可以使得卷积神经网络高层含有的高级语义特征被传递到低层侧向路径，同时低层特征含有的边缘信息可以进一步丰富高层语义特征模糊的边缘信息。5个被广泛使用的数据集上的实验结果表明该方法已明显优于现有方法。


2. 提出了一种基于自擦除策略的注意力模型。该方法在现有的对抗擦除策略的基础上引入了背景先验知识，并设计两种不同的自擦除策略，可以有效地解决基于对抗擦除策略的模型在训练过程中难以控制可辨别区域不断扩散的弊端。该方法生成的类别激活图像不但具有较高的质量，并且在弱监督语义分割任务中可以取得较高的分割精度。


3. 提出了如何智能地从互联网资源中挖掘有用知识来自主地学习语义分割模型。为了解决互联网数据中大量的类别噪声以及复杂的背景，该方法提出了噪声擦除网络的概念。通过从可辨别区域中学习语义知识，可以对显著性物体检测模型提取的前景物体的类别进行推断并擦除其中与检索关键词不相关的区域。"
2021,方面级别情感分析与领域自适应,计算机学院,胡梦婷,程仁洪,NLP,0.3897,"随着互联网的发展，人们已经习惯在网络中发表评价和意见，这些文本信息呈现爆炸式的增长，如何准确识别文本中包含的情感信息一直是学术界和工业界研究的热点。根据文本的粒度，可以将情感分析区分为文档级别、句子级别以及方面级别。其中，方面级别情感分析旨在识别文本中的细粒度情感信息，可以应用在多个实际应用场景，例如了解用户对产品的评价，同时也在推荐系统、对话系统等自然语言处理任务中具有重要作用。


近年来，随着机器学习、深度学习技术的快速发展，方面级别情感分析相关研究取得了令人瞩目的进展，但仍然存在诸多挑战。模型在识别句子中给定方面的情感极向性时，需要综合考虑句子与方面的交互关系，生成特定方面的句子表示用于预测。但是对于包含多个方面的句子，则忽略了多个方面之间的关系，同时多个方面容易互相误导，影响模型效果。如何有效的解决多方面句子仍然是需要解决的问题。此外，方面级别情感分类任务需要大量标注数据，实际应用中大量数据都是无标注的，标注方面级别情感这样细粒度的数据集耗费更大的人力物力。为了应对上述挑战，本文聚焦方面级别情感分类，从约束项、强化学习、领域无关信息蒸馏、多源多目标领域自适应等方面开展若干技术研究与实证分析。本文的主要贡献总结如下：


第一，针对方面类级别情感分类任务中的多方面问题以及注意力机制引入噪声的挑战，本文提出了约束注意力网络(Constrained Attention Network, CAN)。该模型为单方面句子引入稀疏正则项来约束注意权重分配到个别的情感词上，为多方面句子提出正交正则项来约束多个方面类的注意力权重互相正交，帮助注意力机制学习更好的方面类特定的句子表示。此外，识别上下文中的方面类描述能更好的预测方面类级别情感分类，因此本文在模型中引入方面类检测作为辅助任务，也将两个约束项扩展到该多任务学习的框架。在两个公共数据集上的实验结果说明了该方法的有效性。


第二，针对方面项级别的情感分类任务中的多方面问题，以及注意力机制容易被噪声词或来自其他方面的情感描述所分散的缺点，本文提出了一种硬选择方法(BERT-Hard)，它决定了意见片段的开始和结束位置，选择这两个位置之间的单词进行情感极向性预测。该方法利用预训练的BERT模型来学习句子和方面之间的深层关联以及句子中的长期依赖关系，使用强化学习对预训练模型编码后的句子表示进一步检测意见片段。在两个公开的数据集上进行了多项实验分析，与多个基线方法比较的结果表明了该方法在处理多方面句子方面明显优于软选择方法。


第三，针对方面项级别跨域情感分类任务，本文提出一个领域无关特征蒸馏模型(Domain-Invariant Feature Distillation, DIFD)，该模型借助正交领域相关任务以提取领域不变情感特征。大多数现有的跨领域情感分类方法都集中在学习源领域和目标领域中的领域不变表示，而很少有人关注特定于领域的信息。尽管特定于领域的信息具有不可迁移性，但同时学习领域相关表示可以促进领域不变表示的学习。在三个公共数据集进行了广泛的实验，实验结果展示了该方法的有效性。


第四，针对方面类级别情感领域自适应任务，本文提出一个多源多目标迁移网络(Multi-source Multi-target Transfer Network, MMTN)。过去的方法大多数都是粗粒度的，在领域迁移过程中考虑了源领域或目标领域作为一个整体，本文认为这些方法是低效的，它们忽略了不同方面类别之间的差异，因此提出了一种细粒度的领域自适应方法，通过考虑方面组的自适应来解决方面类级别情感分析任务。根据方面类别将源/目标领域划分为多个子域，模型可以实现了细粒度的迁移。广泛的实验结果展示了细粒度域自适应方法在方面类别情感分析中的有效性。


总的来说，本文研究方面级别情感分类中的方面类以及方面项两个典型任务，以及特定于每个任务中遇到的多方面问题和领域自适应问题。"
2022,密态数据库安全查询关键技术研究,计算机学院,吕思艺,刘哲理,Security,0.3494,"随着大数据时代的到来，网络数据呈现爆发式增长，数据拥有者将海量数

据外包到云存储以减轻本地的存储压力。近年来，频发的数据安全事故使得云存储安全问题得到广泛关注。云存储需要保证外包数据的机密性、完整性和可用性，尤其是机密性和可用性。因此，通常会将数据加密之后存储在密态数据库中以保证数据机密性，但是传统的加密操作破坏了数据的可用性，用户无法直接在密文上进行关键词查询、范围查询等操作。为保证外包数据的可用性，密态数据库一般采用保序加密（order-preserving encryption）、揭序加密（order-revealing encryption）和可搜索加密（searchable encryption）等方式来实现安全查询（范围查询和关键词查询），此类技术的安全性和效率是研究者们的重点研究目标。

本文以保证数据安全为核心，围绕安全性和效率问题，对揭序加密、可搜

索加密等密态数据库安全查询关键技术开展了深入研究。为解决密态数据库系统的范围查询问题，本文设计了减少泄露保证效率的揭序加密方案；为解决密态数据库系统的关键词查询问题，本文设计了两个可搜索加密方案。具体的研究内容如下：

1. 低泄露的高效揭序加密方案：针对揭序加密的密文长度和安全性

问题，提出了 HybridORE混合模型，并以该模型为基本框架设计了

EncodeORE方案。揭序加密的密文在比较操作之前并不泄露密文的顺

序信息，虽然它比保序加密方案的安全性高，但是仍存在信息泄露和

密文过长等问题。HybridORE混合模型采用了编码策略对明文进一步

进行混淆。通常揭序加密分为密钥生成算法、加密算法和比较算法，

HybridORE混合模型在其基础上加入了编码步骤。因此 HybridORE混合

模型包括密钥生成算法、编码算法、加密算法和比较算法。编码算法将

明文编码为两部分，分别为范围部分和真值部分。在比较算法中，对两

部分分别进行比较，先比较范围部分，当范围部分相等时再比较真值部

分。当比较不同长度的明文时，EncodeORE方案不泄露信息；当比较相

同长度的明文时，EncodeORE方案泄露真值部分的第一个不相等位出现

的位置。实验表明 EncodeORE 方案在比较不同长度的明文时，平均比

较时间为1.9ms，平均编码时间为12.7ns。

2. 应用块链技术的前向安全可搜索加密方案：针对可搜索加密的前向

安全及效率问题，提出了基于密钥的块链技术（key-based blocks chain,KBBC），并应用该技术设计了满足前向安全的可搜索加密方案 FFSSE。前向安全限制添加操作时泄露的信息，其有效抵御适应性文件注入攻击。本文提出的 FFSSE方案满足前向安全，并且其采用的 KBBC技术不依赖于特定的索引生成规则，使得索引块可以存储在 B树、B+树、表、链等多种结构中。相比于传统陷门技术，本方案采用了对称加密原语

进行改进，使得 KBBC技术具有高效性。FFSSE方案的更新操作代价为

O(1)，并且具有高效的令牌生成算法。FFSSE方案的更新操作时间比

∑ofoV -B 方案快300倍，FFSSE方案的查询操作时间比 ∑ofoV -B 方案

快4倍，FFSSE方案的令牌生成时间比 ∑ofoV -B 方案快300倍。

3. 支持安全删除的后向安全可搜索加密方案：针对可搜索加密的后向安全

及安全删除问题，提出了混合索引结构，并应用混合索引结构和可信硬

件技术设计了满足后向安全的可搜索加密方案 LUNA。前向安全限制数

据插入数据库时泄露的信息，但这并不足以抵御所有的攻击，后向安全

进一步限制了对已删除数据的信息泄露，提高可搜索加密方案的安全

性。本方案考虑密态数据库中删除操作带来的问题，解决了传统可搜

索加密方案因删除加密数据而导致无法满足后向安全的问题。安全删

除强调删除索引和密态数据库中的记录，并且保证方案的后向安全性。

该方案被封装在 MySQL数据库中，实验结果表明，其更新操作和搜索

操作均具有较高的效率，LUNA方案的平均插入时间比 MAIDEN方案

快65倍，LUNA方案的删除时间比 MAIDEN方案快8倍，并且 LUNA方

案的搜索时间比 MAIDEN方案快36-89倍"
2023,面向代码混淆场景的反汇编技术研究,网络空间安全学院,陈启源,贾春福,Security,0.3017,"随着计算机技术的发展，恶意程序和病毒为不法分子所大量使用，给计算机安全带来了严重威胁。为了识别、对抗恶意软件，安全公司的研究人员需要及时地通过各种渠道收集各类恶意软件样本并进行逆向工程。反汇编算法作为一种将目标程序从机器语言解析成汇编语言的技术，是逆向工程最基础的组件。为了对抗逆向工程，代码混淆技术开始被大量运用于恶意软件，其中抗反汇编技术是一种专门针对反汇编算法进行攻击的技术，它可以扰乱反汇编算法的工作流程，使其解析出错误的汇编指令。


针对现有反汇编算法无法有效处理抗反汇编混淆的问题，本文从代码混淆场景下现有反汇编算法的缺陷出发，将Superset Disassembly技术与混淆指令控制流特性、N-gram概率模型相结合，提出了一种面向代码混淆场景的反汇编技术。本文首先根据抗反汇编混淆技术的特点和原理，选用了Superset Disassembly技术作为基础的反汇编算法，其次依据抗反汇编混淆的指令控制流特性设计了基于控制流的冲突解决算法用于去除反汇编结果中的冗余指令，最后结合N-gram概率模型的思想，通过计算指令序列概率的方法辅助反汇编器高效、准确地筛选正确的指令序列。


本文与4种常用反汇编器，在一种正常程序数据集和两种混淆程序数据集上进行了对比测试，结果表明该方法能在不使用任何启发式策略的前提下，在正常程序中达到了和大多数常用反汇编器相近的覆盖率和准确率，而在使用了抗反汇编混淆技术的程序中可以得到显著优于常用反汇编器的覆盖率和准确率。"
2023,图像智能感知与编辑方法探索,计算机学院,崔梦瑶,杨愚鲁,CV,0.3508,"图像编辑一直是广受关注的研究热点，在常用的图像编辑方法中，用户输入颜色点、颜色线、调色板、参考图或文本对已有的图像进行编辑。随着深度学习的兴起，图像编辑算法向智能化的方向发展。例如，一些方法通过学习真实世界中的物体颜色分布，实现灰色图像的自动上色等。智能化、多样化的图像编辑方案对提升用户体验，提高工作效率有着重要的意义。


图像智能感知与编辑旨在通过深度学习方法进一步优化现有图像编辑工作 的生成结果存在的感知上的欠缺，并主要关注了两方面的欠缺。其一，现有的基于调色板的图像重着色方法让用户根据喜好编辑图片的颜色，然而，随意地修改颜色可能导致不真实的颜色编辑结果。其二，现有的文本-图像合成算法主要使用明确的文本描述来准确地生成/修改图像内容，这样的算法很少关注图像风格和文本的一致性，因此比较难被直接应用于粗糙匹配的文本。


针对以上问题，本文开展了对应的工作。为了实现保留真实感的图像编辑，本文提出了一种基于调色板的实例级别的图像重着色方法。整个方案包含一个离线的学习部分，用于学习现实世界中不同物体的颜色分布，以及一个在线的重着色部分，该部分首先识别物体的类别，而后推荐在离线部分根据物体类别学习的有真实感的候选颜色。在颜色选择后，使用蒙版抠图技术保证物体边缘光滑的融合效果。此外，本文还提供了一个直观的用户交互界面以进行有效的颜色编辑。为了对粗糙匹配的文本有效合成图像，本文提出了使用粗糙匹配的文本作为指导对图片进行风格化的任务，并提出了一个两阶段的生成对抗模型：第一个阶段使用句子特征生成图片的整体风格，第二个阶段使用多模态风格合成模型优化阶段一生成的风格。此外，本文还提供了基于粗糙匹配文本的图像风格化任务在图文对齐、故事可视化等任务上的应用的丰富样例。总的来说，本文工作的主要创新点在于：（1）提出一种新的基于真实物体进行调色板推荐 的图像编辑方式；（2）针对粗糙匹配文本提出一种新的基于文本的风格生成任 务以及用于该任务的一种新的基于注意力机制的生成式对抗网络；（3）提出了本文算法在实时图像颜色编辑，文本-图像对齐，以及故事可视化等任务上的应用方案。全面的评估表明，本文方法在自然重着色和基于粗糙匹配文本的图像生成任务方面优于现有的先进算法。"
2023,面向数据分布的持久化B+树优化,网络空间安全学院,罗依婧,李忠伟,Database,0.2525,"数据密集型应用的高速发展使得数据库中重要的索引结构得到了广泛研究。 随着数据量的不断增大，基于内存的索引结构无法满足大数据量存储的需求。 新型非易失性内存(Non-Volatile memory, NVM)有着数据持久化、读写性能高、 存储容量大等特性，有望成为下一代存储系统中的重要存储介质。近年来涌现 出许多利用 NVM 设备来优化广泛使用的 B+ 树索引的工作。虽然这些优化工作 考虑了 NVM 设备的特性，但却没有利用索引中的数据分布来进一步提升性能。


为了进一步提升持久化 B+ 树的性能，本文提出基于数据拟合模型的部分持 久化 B+ 树(Modeled B+ Tree, MB+Tree)。MB+Tree 通过线性模型拟合索引节点 内的数据分布，有效降低了对 NVM 设备的访问次数，在保证写性能的同时对读 性能有较大提升。MB+Tree 主要包括动态数据拟合模型、基于模型的易失性内 部节点和基于模型的持久化数据节点三个部分。动态数据拟合模型使用分段线 性拟合模型(Piecewise Linear Approximation，PLA)以及基于冲突阈值的模型


(One-Pass Conflict Bounded，OPCB)，分别对分布复杂的数据集和分布简单的数 据集进行拟合。动态数据拟合模型使用线性模型拟合数据，得到均衡的分片数 量和模型平均误差。基于模型的易失性内部节点使用分段线性模型减少二分查 找开销。对于简单数据集，基于模型的持久化数据节点使用稀疏存储以减少插 入带来的节点内移动和模型偏差，并且在分裂时使用分裂一半的策略以减少节 点分裂带来的数据重写及模型重建开销。对于复杂数据集，持久化数据节点使 用紧密存储，利用位于内存的缓冲数组副本实现有序插入并使用追加写的日志 保证数据持久化。


实验结果表明，MB+Tree 在写性能上与传统持久化 B+ 树接近，并且在读 性能上有较大的提升。MB+Tree 的点查询比传统持久化 B+ 树 Fast-Fair Tree 和 FPTree 平均有 50.35% 的提升，范围查询有 115.3% 的提升。相比于持久化 B+ 树变体 uTree，MB+Tree 的点查询有 29.02% 的提升，范围查询性能有 395.1% 的提升。MB+Tree 的点查询比持久化后的基于模型的 B+ 树变体 FITing-Tree 有 14.85%的提升，范围查询性能有8.3%的提升，而写性能最高有26.35%的提升。"
2023,深度学习框架OneFlow与昇腾处理器的适配与效能优化,计算机学院,杜承昆,李涛,CV,0.3195,"近年来，深度学习得到了爆发式的增长，不断涌现出高质量的深度学习模型。这一趋势与深度学习处理器和深度学习框架的快速更新、迭代密切相关。深度学习框架能够支持不同的深度学习处理器，屏蔽其差异，为用户提供统一、高效的接口。深度学习处理器为模型的训练和推理提供了强大的算力支持，然而，不同深度学习处理器之间硬件结构和软件栈差异巨大，编程模型难以统一，导致框架与处理器之间的适配十分复杂。因此，如何针对处理器与框架的架构、特性进行适配，实现软硬件兼容，使框架高效地利用处理器的算力，是一个意义重大且极具挑战的任务。


       针对上述问题，本文以华为昇腾（Ascend）处理器和国产深度学习框架OneFlow为目标实现适配框架Ascend-OneFlow，完成由底层交互至上层应用的功能适配与性能优化，最终在Ascend-OneFlow上完成了多个模型的训练。本文将按照由底向上的顺序进行组织：首先，介绍OneFlow与昇腾计算库CANN的系统交互适配，包括设备抽象、内存池设计与一致性内存拷贝。其次，设计一种算子适配框架NpuCmd，能够自动完成OneFlow张量到CANN数据结构的转换，为Ascend-OneFlow添加50多种昇腾算子的支持，并提出一种量化算子适配误差系数的方案。然后，针对昇腾处理器的特性实现低精度损失的混合精度训练算法；适配通信库HCCL，为新算子设计SBP Signature，实现基于SBP机制的分布式训练。最后，提出一种评价效率和充分性的二维框架评测方式，并从参数更新、冗余梯度等方面进行框架层的性能优化。实验结果表明：Ascend-OneFlow在算子误差、资源占用、分布式训练等方面的功能均达到预期；选取ResNet、VGG和Bert三类主流模型进行完整训练，以GPU平台上单精度训练得到的模型精度为基准，Ascend-OneFlow使用混合精度和分布式训练得到的模型精度均达到基准精度的98%以上；在选取的主流模型中，Ascend-OneFlow的性能最高可以达到华为官方适配框架Ascend-PyTorch的2.0倍。"
2023,高自由度物理模拟的控制方法,计算机学院,叶骁寒,任博,CV,0.2616,"近年来，随着图形学技术的快速进步，元宇宙、VR/AR等产业取得了重大进展。这些图形学产业的发展对于丰富人类的娱乐生活以及促进工业生产都有着重要作用。其中，物理模拟扮演着非常重要的角色。如今，实时的、真实感的物理模拟已经取得了非常重要的进步，人们也对物理模拟的控制问题越来越重视。这类模拟控制问题有着非常重要的应用，例如使用户在VR等应用中根据自身的意愿编辑虚拟场景中的对象，或者利用模拟中的控制策略帮助解决实际的规划任务。然而，解决物理仿真中的控制问题，尤其是高自由度系统的控制，仍面临非常大的挑战，因为这些系统的可控性处理尚有不足，且相关的高复杂度计算使得控制处理的求解很困难。


本文围绕两个经典的高自由度物理系统，流体模拟和群体模拟系统，研究了如何控制这些复杂系统并进一步利用它们解决具体问题。具体地说，本文研究了使用强化学习实现流固耦合系统运动控制以及使用可微分模拟解决多智能体系统无碰撞导航的方法。本文的主要贡献如下：


•提出了一种基于元强化学习的通用型流体-固体耦合系统的控制算法。该方法通过一个环境编码模块推断运行中的流体模拟器的动力学特征，并基于此训练可以在不同模拟器间迁移的流体控制器，解决了传统基于学习的流体控制方法不稳定、不可复用的问题。本方法同时使用一种高效的多任务学习技术解决离散多目标控制问题，实现了控制策略任务空间上的迁移。该方法在一系列高精度2D/3D模拟器上实现了复杂的流体控制效果。


•提出了一种基于可微分模拟的多智能体导航算法。该方法设计了新颖的可微分碰撞处理模块，并通过紧凑参数化的全局策略来指导多智能体的导航运动，并允许使用自监督学习高效地搜索导航策略。该方法可以实现障碍物繁多地形下的多智能体导航任务，训练速度比基于强化学习的基准提升一个数量级，并且有效缓解了多智能体模拟中常见的死锁现象。"
2022,基于启发式算法及插桩策略的混合式页面预取器,网络空间安全学院,王理治,宫晓利,Security,0.301,"高速缓存的使用以及针对高速缓存的缓存行预取策略已经从一定程度上有效缓解了传统体系结构中无法避免的内存墙问题。近年来以分离式内存、非易失内存及Intel SGX为代表的新硬件的出现带来了新的存储结构，同时也随之产生了对基于虚拟内存的页面预取技术的迫切需求。与传统高速缓存行预取器不同的是，基于虚拟内存的页面预取器通常不具备用以提供诸如缓存命中、缺失情况等辅助信息的额外硬件结构，仅能被动地统计缺页异常等事件的发生情况来作为预取的原始信息。同时内存访问中大量的时间局部性特征已经被高速缓存过滤，进行页面预取则需要进一步寻找程序在页面级的访存特征。另外传统的缓存行预取器都在私有缓存中进行，预取器只需要考虑单个核心中较强的局部性特征，而内存页面会被计算机多个核心下的各个程序共同访问，不同类型的混合访存特征会进一步增加页面预取器的预测难度。

本文基于较为成熟的SGX硬件进行研究，并针对SGX中的主要性能瓶颈——缺页异常处理流程进行了细致地分析，确定了页面预取技术在SGX硬件中应用的潜在价值及可行性。为了解决上述问题，本文提出了一种基于启发式算法和插桩技术的混合页面预取器。前者是一种启发式在线预取器（Brig-DFP），Brig-DFP在线预取器使用了基于趋势检测技术的多流线性预取算法，可以动态地调整预取步长和预取窗口大小，能够有效解决页间连续访存指令的缺页异常问题。后者则是一种基于源代码分析的离线预取器（Brig-SIP），Brig-SIP离线预取器通过模拟目标程序在SGX Enclave中的访存行为信息进行访存指令的插桩价值评估，并基于插桩技术与共享内存通信实现准确的预取，进而有效解决页间随机访存指令的缺页异常问题。

最后本文通过实验验证了混合页面预取器的有效性，在SPEC2017基准测试程序下，Brig-DFP在线预取器和Brig-SIP离线预取器分别获得了平均16.9%和11.7%的性能提升，平均降低了55.9%和66.1%的缺页异常数量。另外针对真实场景下典型的图像处理程序，混合预取器最终取得了平均13.8%，最高17.1%的性能提升，平均降低了68.6%的缺页异常数量。"
2024,端到端无偏场景图生成关键技术研究,计算机学院,李伟,袁晓洁,CV,0.3056,"视觉感知是人类获取外界信息的主要途径，随着互联网上图像和视频数据量的爆炸式增长，视觉语义理解技术已经成为计算机视觉和人工智能领域的研究热点。场景图作为一种对图像中实体和关系语义信息的抽象表示，凭借其结构化语义表达能力，在图像语义理解和视觉推理方面表现出显著效果。场景图生成也随之成为连接低层次视觉感知和高层次视觉推理的重要技术，并在视觉问答、图像生成等领域展现出广泛的应用潜力。


随着基于场景图的视觉理解推理应用研究不断深入，相关下游任务对场景图生成技术的语义表达能力和生成效率提出了更高的要求。通过深入调研国内外相关研究现状发现，现有的场景图生成方法面临长尾数据分布下关系谓词预测偏见的挑战，端到端场景图生成方法提升效率的同时也存在性能瓶颈问题。


为了进一步提升场景图在视觉理解推理应用中的表现，本文就无偏估计方法和端到端生成架构这两个场景图生成关键技术进行了研究。本文首先提出了基于谓词概率分布的无偏场景图生成方法。该方法通过谓词概率分布对关系谓词的潜在语义关联进行建模，进而设计了一种基于谓词概率分布差异的关系实例预测偏置检测方法，从而实现对长尾关系谓词预测损失的自适应重加权，在确保头部关系谓词预测性能的同时，加强了模型对尾部关系谓词的训练强度，提高了场景图关系谓词的语义丰富性和多样性。与现有先进场景图生成方法相比，本文提出的方法显著提升了长尾关系谓词预测的性能。


针对现有端到端架构场景图生成方法的性能瓶颈问题，本文提出了一种基于扩散模型的端到端场景图生成方法。该方法采用了基于位置框的关系建模方法，将场景图生成任务转化为基于位置的关系对预测任务和实体检测任务，有效结合了扩散模型与端到端场景图生成架构的优势。在通用场景图数据集上的大量实验结果表明，本文提出的基于扩散模型的端到端场景图生成方法展现出更优秀的场景图生成性能，并具备更强的可拓展性和泛化能力。"
2023,基于位置感知的离线手写数学公式识别方法,网络空间安全学院,宋春颖,李涛,CV,0.333,"数学公式是科研论文和学科教材的重要组成部分，通常以图像或 LaTeX 代码的形式表示。在智慧课堂、信息化教学等实际场景中，将离线条件下保存的静态手写数学公式由二维图像转化为 LaTeX 代码可以高效地实现公式数据的记录与传输，具有重要的应用价值。随着深度学习在各个领域取得重要进展，基于深度学习的离线手写数学公式识别方法备受关注，尤其是基于覆盖注意力机制(Coverage-based Attention)的编码器-解码器结构成为了现有工作的主流。然而由于覆盖注意力机制引起的错误累积问题一直被忽略，导致难以识别较长的公式，并且基于字符序列的解码器缺乏对空间结构信息利用，导致公式结构识别不准确。因此，针对上述问题，本文分别从序列位置感知和空间位置感知两个角度提出解决方法。


首先，针对由覆盖注意力机制引入的错误累积问题，设计序列位置加权注意力机制 SPWCA。基于序列位置无关的标准注意力机制和序列位置相关的覆盖注意力机制构造双分支结构；设计深度卷积置信度预测子网 ConfNet 从全局编码特征提取各序列位置的错误累积程度相关信息，通过置信度对双分支注意力做加权融合，在抑制错误累积和保留历史对齐信息间取得平衡；并提出加权覆盖更新模块降低错误累积在不同序列位置间的传递。基于 SPWCA 设计覆盖注意力机制修正网络 CACNet，实验结果表明，CACNet 在三个广泛使用的评测数据集的多个评测指标上，相比同类领先算法取得了最优的表现，相比次优结果公式识别准确率最高提升 3.31%。将评测数据集根据公式长度划分为公式数相对均衡的不同子集，SPWCA 的引入使 CACNet 相比基准模型在最长公式区间中的公式识别准确率最高提升 5.41%，证明了其在抑制错误累积上的有效性。


其次，针对公式结构识别不准确的问题，设计结合空间位置关系的多任务识别模型 MTMS。定义包含关系和相邻关系描述字符空间结构信息，以这两种空间位置关系为预测目标，在字符类别预测任务的基础上引入字符空间位置关系预测任务。通过设计单编码器-双解码器模型并结合多任务损失，使两种任务间共享语义特征，实现高效协同学习。分别利用包含关系、相邻关系和双向相邻关系对应得到 MTMS-I、MTMS-N、MTMS-N-Bi 三种模型。实验结果表明，在三个广泛使用的评测数据集的多个评测指标上，三种模型相比领域内领先算法都取得了最优结果，相比次优结果公式识别准确率最高提升 6.81%。相比基准模型，三种模型在同一数据集的公式结构识别准确率上分别取得了 3.34%、5.07%、6.59% 的大幅提升，直接证明了空间位置关系预测任务的引入可以显著提升模型对公式空间结构的识别能力。"
2022,多模态图文篇章智能理解关键技术研究,计算机学院,郭文雅,袁晓洁,CV,0.3198,"多模态智能理解旨在利用包含文本、图像、音频等多种模态的信息载体实现内容分析、观点挖掘等目标。微博、推特等生活场景中包含图片和文本的图文多模态数据获得了越来越多的关注。然而，这类图文多模态数据只包含少量文字和独立于文本之外的图片，组织形式相对简单，难以深入研究图文多模态语义交互。因此，本文研究更加复杂的多模态图文篇章数据。从语言学的角度，篇章指的是大于句子的语言单位。多模态图文篇章按照一定的排版组织图片和文本内容，并将图片穿插在与其内容密切相关的上下文中，图文语义交互复杂多变。多模态图文篇章广泛存在于电影影评、在线新闻等场景中，具有重要的研究意义和广泛的应用价值。


篇章长文本具有“单词—句子—篇章”的层级结构，多模态篇章包含的图片和文本在不同层级具有复杂的语义交互，为多模态智能理解带来了一系列亟待解决的科学问题：（1）单词级关键信息抽取，单词作为文本最基本的组成单位，语义丰富的同时，也存在大量的信息冗余，需要利用单词级图文关联进行凝练；（2）句子级图文对齐，图文之间存在语义鸿沟，句子作为篇章基本的语义单元，可以用来描述图像区域，实现句子级图文对齐可以辅助多模态篇章理解过程；（3）篇章级多模态特征融合，多模态篇章包括图文语义信息，以及图片与其上下文之间复杂的语义交互，实现智能理解要求综合这些信息并获得篇章级统一特征表示。本文的具体研究内容包括：


第一，提出一种基于注意力机制的单词级关键信息抽取方法，解决多模态图文篇章信息冗余的问题。利用注意力机制为信息量丰富的单词和图片元素分配更高的权重，在此基础上进行特征综合，实现图文篇章的信息凝练。针对现有方法只关注文本和图片全局信息，对图文内部语义关联建模不充分的问题，本文首次提出基于细粒度图文语义交互的关键信息抽取方法。该方法考虑单词以及图片中物体区域的特征，根据每一个<单词，图片区域>对之间的相似性，构造图文多模态相似性矩阵，并从中分别计算出各个单词及图像区域的注意力权重。这种细粒度的语义建模提升了注意力权重的质量，帮助模型得到更加精确的特征凝练结果。本文利用视觉问答任务对该单词级关键信息抽取技术的效果进行验证，三个大规模数据集中的实验结果显示，该方法得到的提升均超过了1%。此外，本文还通过引入视觉问答中的答案语义信息来优化学习到的图像注意力权重，实验结果得到了进一步的提升。


第二，提出一种基于实体关联建模的方法实现句子级图文多模态对齐。利用文本句子可以描述图像区域的特性，实现文本句子到图像物体之间的映射。将复杂的图像内容形式化建模为图像包含的物体属性以及图像物体之间的关联。基于物体区域视觉特征、在原始图像中的位置以及区域尺度信息，建立图像物体之间的关系，并将其与文本描述的关系进行对齐，使得模型可以根据给定的句子找到被描述的区域。但是以这种形式输入的尺度信息会对模型的结果造成混淆，使得模型倾向于认为尺度更大的区域更有可能是句子描述的正确物体。为此，本文设计了一种尺度不敏感的孪生网络，在保证样本正确性不变的前提下，随机改变相应区域的尺度信息，利用孪生网络的两个分支分别对两种尺度进行处理，在两个分支输出结果中引入完备的一致性损失，训练模型降低对尺度信息的依赖。本文设计了一系列实验，使用专门用来评估图文对齐效果的数据集验证模型性能，实验结果表明，本文提出的方法在小尺度物体定位中得到了超过5%的提升。


第三，提出一种排版驱动的多模态特征融合方法，考虑篇章的排版特征，计算文本和图像的篇章级统一特征表示。根据篇章文本的层级结构，建立分层注意力模型，首先利用单词层注意力抽取句子中重要的单词，得到句子的特征表示；然后利用句子层注意力抽取出关键句子。句子的注意力权重不仅来自于句子本身特征和上下文句子之间的语义关联，还与蕴含在篇章排版中的图文语义交互密切相关。本文将篇章排版形式化表示为图文位置关系，为每一张图片建立位置向量，以此标记篇章的排版特征。引入距离系数将图片的位置向量进行细化，用以辅助句子注意力学习。最终，将学习到的关键句子和图片视觉特征进行融合，作为整个图文篇章的统一特征表示。本文将得到的特征表示在情感分析任务上进行测试，结果显示，提出方法在使用的数据集中取得了最佳性能，典型样例的可视化结果证明获得的特征具有丰富的语义信息。"
2023,面向知识图谱扩充的属性抽取与知识融合技术研究,计算机学院,刘仡男,沈玮,Database,0.2744,"知识图谱是人工智能的基石，尽管很多大型知识图谱已被构建，但它们并不完备。为了提升知识图谱的覆盖率，以更好地支撑上层智能应用和服务，亟需对知识图谱进行扩充。万维网是一个大规模的信息源，为了利用其中蕴含的海量知识扩充现有知识图谱，属性抽取和知识融合作为两个关键任务被提出。属性抽取旨在从万维网数据中抽取出事实三元组，知识融合目标是将抽取到的三元组与现有知识图谱进行集成整合，二者紧密关联。本文聚焦属性抽取与知识融合，提出多种技术从实体、属性和关系三个层面来挖掘万维网海量数据中的有用信息，以实现知识图谱扩充的目标。具体研究了以下三方面技术。


第一是基于社交数据和网页数据的实体位置属性抽取技术，即通过融合社交数据和网页数据中的地理位置知识，为实体抽取其地理位置属性值。现有实体位置属性抽取研究通常利用单一类型的数据资源，抽取精度无法令人满意。本文提出一个基于期望最大化的无监督实体位置抽取框架。在期望步骤中，考虑到不同类型数据对于位置抽取任务的重要度和置信度不同，本文提出一个无监督排名聚合算法以整合每个实体对应的多个排名，从而生成一个更优排名。实验结果表明，本文框架优于基准方法且具有一定的扩展性，有助于解决知识图谱中实体地理位置属性值缺失的问题。


第二是基于对话数据的用户属性抽取技术，即挖掘对话数据中蕴含的用户属性知识，为用户预测属性值。以往基于对话数据的用户属性抽取研究通常依赖于外部数据或标注的用户话语，并没有充分利用蕴含在未标注用户话语中的用户属性知识，针对一些难以预测的用户属性，模型效果往往不太理想。本文提出了一个低资源用户属性值预测框架，该框架无需标注的用户话语和外部数据，利用更新的先验用户属性知识，以迭代的方式改进双词主题模型的吉布斯采样过程，将双词语义信息与单词共现信息无缝融合，进而利用未标注用户话语中丰富的用户属性知识来预测用户属性值。实验结果表明，本文框架既优于主流用户属性值预测方法，又优于主流弱监督文本分类方法，有助于解决用户知识图谱中用户属性值缺失的问题。


第三是基于开放信息抽取三元组数据的规范化与链接联合技术。开放信息抽取三元组规范化（即将名词短语和关系短语转换为规范化形式）和开放信息抽取三元组链接（即将名词短语和关系短语与它们在知识图谱中对应实体和关系建立映射）是将开放信息抽取三元组数据与现有知识图谱进行知识融合的过程中所涉及的两个重要任务。尽管这两个任务之间具有很强的互补耦合关系，现有研究只是独立地解决这两个任务，而没有对它们之间的互补耦合关系加以深入挖掘利用。本文提出一个基于因子图模型的规范化与链接联合框架，使得这两个任务互相促进、联合增强。实验结果表明，本文框架既优于主流规范化方法，又优于主流链接方法，能够利用开放信息抽取三元组数据，实现知识图谱的扩充融合。


总的来说，本文提出了实体位置抽取框架、低资源用户属性值预测框架和规范化与链接联合框架，其中，前两个框架从属性层面解决知识图谱扩充问题，第三个框架从实体和关系层面解决知识图谱扩充问题。"
2023,基于BERT掩码语言模型的日志异常检测研究,网络空间安全学院,周娜,刘晓光,CV,0.3158,"随着我国民航业的快速发展，空管自动化系统在运行过程中会产生大量记录系统执行操作与性能状态的日志信息（简称空管日志）。这些日志信息是捕获系统异常的重要数据参考，因此基于空管日志进行异常检测研究是辅助空管自动化系统安全与稳定运行的重要手段。这项研究符合当前民航空管部门的实际需求，并且具有广阔的应用前景。


研究日志异常的重点在于日志解析、特征提取和异常检测。其中，日志解析是通过分析日志源代码并保留源代码中常量部分作为日志模板信息来实现的。然而，主流的日志解析方法仅考虑源代码中的常量部分而忽略变量部分，这容易导致日志模板丢失关键信息，从而影响异常检测结果的精确度。关于特征提取，现有的方法大多基于循环神经网络结构来捕捉日志语句之间的联系，但对于长文本日志数据的表征能力有所欠缺。此外，当前的异常检测方法在面临正负样本分布不均的问题时容易出现性能瓶颈，从而导致异常检测效果不佳。针对当前面临的挑战，本文研究了空管日志数据的结构以及异常场景特征，并提出了一种基于BERT（Bidirectional Encoder Representation from Transformers）掩码语言模型的空管日志异常检测方法。该方法改进了三个部分：首先提出了语义增强的日志解析方法，通过构建模板树优化模板匹配所需的时间，并对模板集合进行语义上的拆分与合并，从而提升日志模板的准确度。其次，使用增加时间和频率编码的扩展式掩码语言模型（Extended Masked Language Model， EMLM），借助双向Transformer结构以及预训练任务来捕捉长日志序列的特征，从而获得更好的语义表征信息。最后，研究生成式对抗网络（Generative Adversarial Network，GAN）模型拟合数据分布的能力，创新性地将GAN与EMLM相结合，学习空管正常日志的数据分布。


为验证本文提出的空管日志异常检测方法在实际应用中的可行性和有效性，本文基于异常检测模型构建了空管日志异常检测原型系统，并在空管日志数据集FDP和MMI上进行了相关的实验与分析。实验结果表明：本文提出的日志解析方法不仅具有更快的时间效率，而且在检测准确率上也有最佳表现，在两个数据集中均获得了95%以上的F1分值，比其他方法至少提高3%；此外，将EMLM和GAN结合起来的异常检测模型，实验结果相较于基准实验也有一定提升，在FDP数据集和MMI数据集中分别获得了93.1%和96.4%的F1分值。最后，经过多方面的评估实验，本文提出的空管日志异常检测方法在各项评价指标中表现更加优异，验证了本文方法的有效性。"
2020,K12教学环境下纸笔物联体系及关键问题研究,计算机学院,曹珩,李庆诚,SE,0.2753,"教育信息化改革着眼于教学资源和活动的数据化、智能化。由于基础教育涉及群体广泛、传统性强，导致教育信息化改革未能深入到主流核心的教学过程中，全面信息化的教学模式和已有的教学模式之间仍然缺乏有效的衔接。


实现基础教学核心环节的信息化改革面临以下三大问题：


（1）主流常态教学工具难以改变，信息技术难以插足传统教育；


（2）基础数据的缺乏难以使工厂化的教学模式向个性化迁移；


（3）缺乏示范性应用场景，难以将教育装备大规模推广。


针对上述问题，本文完成基于K12版面的数据基础物联生态体系构筑，借助教育复合手势指令集解决书写UI不统一的介质联通问题，实现了以回滚增强的方式对主流常态教学工具进行改造，将教育核心流程中的行为数字化。具体地，对学习的载体，如试卷、作业等完成规范化设计，以符合教育场景的手势指令集联通教育介质和教学场景。并以批改作业这一核心场景为应用示例，展示了批改流程的数据采集流程和数据标注过程，萃取纸笔物联数据。最后，提出了以学习数据节律促进群组共振互助互学场景的方案，提高学生参与活动的积极性和互助互学的有效性，以纸笔物联数据环境基础助力以立德树人为根本任务的教育改革和课程改革。


具体地，本文的研究内容和成果如下：


（1）以介质联通为目标，从主流常态书写场景出发，设计适合基础教育场景使用的点阵交互纸张，并通过实现对纸张的位置信息标识和语义信息标识，使用智能笔K12-pen记录数据，完成基础教育核心书写数据的基础采集工作。


（2）提出教育复合手势指令集。从基础教学场景常用手势出发，提出以标准的带有教育语义的指令体系完成教育场景数据联通，实现了教学过程中手势数据的实时采集和响应。具体地，从指令的使用介质、指令符号以及使用场景出发，实现了对指令进行输入、识别、响应的流程，为构建介质联通的教学环境提供了手段支持。


（3）实现作业精准批改示例系统。基于对纸笔交互系统的改造升级，在作业核心场景下，用数据增强作答、批改、订正核心环节体验，设计并实现了作业数据分类众包标注工具精准批改系统，建立作业批改数据生态，验证教育行为数据化带来的教育交互模式的效率提升。


（4）提出基础教育数据管理与可视化方案。对教师、学生在教学活动中产生纸笔交互数据进行管理，提出基础教育数据节律的概念，从活动数据记录出发，设立衡量学生在学习中的个性化属性。采用链式结构对学生在任务不同阶段的过程数据进行存储和索引。同时通过数据可视化的方式展示了个性化的教育行为数据。


（5）实现群组共振互助互学的示范创新教育模式。学习者按周期发布带有奖励的众包任务，在数据节律的基础上，提出对任务奖励进行加权分配的方法，借助博弈论中位势博弈理论最大化每一名参与完成众包任务的学生的效用，建设竞争、协作型众包学习生态，提升群组学习积极性。


本文从传统教育出版视角出发，面向K12主流常态教学环境，创建了相对完整的纸笔物联生态体系，完善教学理论，提供了一种全新的工程架构和示范方案。"
2022,口令认证系统中口令强化服务问题研究,网络空间安全学院,武少强,贾春福,Security,0.3478,"随着物联网、移动互联网和云计算等技术的发展和应用，人们与网络的关系越来越紧密。在日常生活中，人们频繁登录各种Internet应用，时刻与网络服务交互，来获取信息、资源和服务。口令认证是占据主要地位的身份认证机制。但是，口令存储记录被攻击者视为入侵的主要目标，在泄漏后极易遭受离线口令猜测攻击。这是口令泄漏事件频发的主要原因。一种有效的解决方法是口令强化服务( Password Hardening Service, PHS )，它使用外部密码服务器持有的服务密钥强化口令记录，以抵抗离线攻击。本文发现，现有PHS方案缺少服务退出机制，并且存在单点故障问题。为了解决这两个问题，本文提出了两种PHS新方案PW-Hero和ButuPHS。本文主要工作如下： （1）提出可选择退出的PHS方案PW-Hero，解决了现有PHS方案一经使用就无法退出的问题。PW-Hero利用退出协议来实现退出功能，客户端通过身份核查后，可利用服务器返回的退出令牌安全地将其口令记录迁移到传统的加盐哈希状态。另外，PW-Hero充分考虑一些重要属性，例如，可验证性、细粒度速率限制在线攻击以及用户隐私。即使服务器是恶意的，也不能干扰口令验证结果，或窃取用户口令和隐私。PW-Hero被形式化定义为一组协议，并配备形式化安全定义。此外，本文基于伪随机函数和乘法盲化的双层密码机制构建了PW-Hero实例，并在随机预言模型下基于安全游戏证明了实例的安全性。 （2）提出多服务器设置下的PHS方案ButuPHS，解决了现有PHS方案在单服务器设置下的单点故障问题。ButuPHS基于本文提出的可更新的门限备份函数( Back-Up and Threshold Update, BUTU )来克服单点故障问题。具体而言，BUTU设置一组持有各不相同且相互独立的备份密钥的备份服务器，门限分割从当前密钥到备份密钥的更新令牌，并分配给所有备份服务器。当服务发生故障时，客户端收集阈值数量的共享份额来重构更新令牌，然后更新口令记录。更新后的口令记录可在备份服务器的协助下完成口令验证，以此完成故障恢复。此外，本文的安全和效率分析表明，ButuPHS不因引入BUTU函数而损失基础方案的安全性和效率。"
2023,基于机器学习的海洋观测数据异常检测方法研究,计算机学院,陈萱,苏明,CV,0.3215,"高质量的海洋数据集是推动海洋及其交叉学科创新发展的前提，而数据质量控制工作是构建高质量数据集的基础，其中，数据异常检测是海洋数据质量控制任务的重点与难点。随着观测手段的不断进步，传统的自动化质量控制方法逐渐无法满足效率需求。而由于海洋观测数据的复杂性，机器学习相关技术难以直接运用于海洋观测数据的异常检测任务。

首先，海洋观测数据体量庞大，人工标注代价高昂，而机器学习通常需要大量带标签数据作为训练样本。其次，已知的异常数据极少，正负样本不平衡，给机器学习模型的训练带来挑战。最后，海洋观测数据关联关系复杂，不仅在时间维度上存在长、短期关联现象，各要素间也存在复杂的关联关系，难以构建准确的检测模型。基于上述背景，本文的相关研究内容总结如下：


对于任意的单一海洋要素，本文提出统计分析和单分类算法相结合的多层次异常投票模型。结合海洋数据特点提取相应特征并构建数据样本，利用统计分析方法快速过滤大量正常样本，减小异常候选集规模，单分类模型对少量可疑样本进一步判决。本文在海洋数据集和公开数据集上，验证了本文提出的方法相对于其他方法具有更好的检测效果，$F_1$分数平均提高了10%。本文还在海洋数据集上开展消融实验，验证了统计分析层和单分类层的结合可以有效提升异常检测效果。


对于同一观测站点的多种海洋要素，本文提出基于无监督学习的变分Transformer模型。该模型有效结合了VAE和Transformer，在编解码过程融合了变分推断以提升模型对噪声的鲁棒性。无需人工标注样本即可完成模型训练，无需人工干预即可检测异常。V-Transformer模型利用自注意力机制同时提取时序关联和特征关联，基于重建概率判断数据的异常程度。本文在公开数据集和海洋数据集上验证了V-Transformer模型相较于现有模型具有更优的性能表现，消融实验结果表明V-Transformer模型各模块的设计是合理有效的。"
2023,基于略图的流模式下异质图条件频繁项挖掘研究,计算机学院,曾依玲,张莹,Network,0.2874,"流模式下图数据模拟了大数据时代中现实网络的演变特性，是包含丰富信息的网络数据，在各类现实场景中具有广泛的应用。爆炸式增长的数据量导致 网络中重要的信息很难被发现，频繁项和条件频繁项致力于提取网络中具有重大流量的实体及相关属性，从而揭示网络的数据特征。然而，流模式下的图数据具有庞大的容量和过快的变化速率，如何在流模式下图数据，尤其是带有标签信息的流模式下异质图上进行频繁项和条件频繁项的挖掘是一项重要的研究工作。


     在空间限制下，处理流模式下图数据需要先解决该数据的存储问题。使用概要对流式数据进行汇总是一种有效的技术。目前，流模式下同质图的概要研究已经较为充分，但流模式下异质图的概要研究还未深度推进。进一步而言，针对流模式下异质图的挖掘研究同样稀少。流模式下异质图包含丰富且复杂的标签信息，在概要中存储这些信息能够促进现实场景下的行为模式分析和推荐任 务。因此，本文旨在提出流模式下异质图的高效概要构建方法，并使用该概要 进一步完成频繁项和条件频繁项的挖掘。


     本文首先提出了基于略图的流模式下异质图概要构建方法。该方法通过矩阵分块等策略在略图中高效引入了流模式下异质图的标签信息，能够解决以往研究中忽略标签信息造成信息损失的问题。其次，考虑到数据的价值衰减问题， 该方法引入子窗口滑动机制自动进行过期数据的删除操作，保证了略图的时效性。本方法在四个真实异质数据集上进行了充分的实验。结果表明，本文提出的略图方法相较于领域内的先进方法至多提升了近两个数量级的精度。


     为进一步挖掘流模式下异质图的行为模式和数据特征，本文提出了两种基于略图的流模式下异质图条件频繁项挖掘方法。这两个方法均使用前文提出的略图为黑盒得到对应的频数估计。第一个方法以双向映射器 Bimap为存储结构，通过维护所有候选来优化查询响应时间。第二个方法以最小堆为存储结构，通过维护最小的候选集来优化存储空间。在三个真实异质数据集上的实验结果表明，本文提出的方法能够显著地提升查询准确度，实现了接近 1 的精确率和接近 0 的平均相对误差，能满足现实查询的需求。"
2023,面向持久内存的键值数据库Memcached优化方法研究,网络空间安全学院,李先铎,宫晓利,Network,0.2483,"随着物联网、云计算和分布式计算等技术的发展，数据规模爆发式增长，内存键值数据库Memcached成为数据中心合理组织和存储数据的关键技术。持久内存作为一种新型存储介质，提供字节寻址、非易失和大容量特性，为Memcached的应用带来新的机遇。但持久内存写寿命受限、读写带宽有限和写放大等问题，使得当前Memcached无法直接高效适配到持久内存。


本文首先分析了面向DRAM和持久内存的Memcached实现机制及访存行为，以及持久内存的架构特性，总结了Memcached直接适配到持久内存时存在的瓶颈问题，即（1）多次细粒度写入降低持久内存使用寿命；（2）未绕过Cache层次的持久化方式浪费硬件带宽，引入额外的Cache刷新开销；（3）以较小粒度写入持久内存，触发写放大问题。针对上述问题，本文提出了Hybrid-Memcached，一种基于 DRAM和持久内存 混合存储的Memcached 设计方案，具体如下：（1）通过使用DRAM缓存，将键值对缓存过程中的数据更新和持久内存写入过程分离，将原方案中多次细粒度写入转移到DRAM中，仅将更新后的DRAM数据一次写回持久内存，延长持久内存使用寿命；（2）基于非临时存储NTSTORE指令将DRAM中的数据绕过Cache层次实现持久化，避免读取到Cache、修改、刷新Cache过程，最大化带宽利用，减少Cacheline刷新开销；（3）基于更新和写入分离的机制，提出键值对聚合优化方案，将DRAM中多个键值对的缓存聚合写入持久内存，避免写放大问题并进一步减少持久内存访问。


本文将Hybrid-Memcached部署在傲腾持久内存上，相比baseline，启用优化一二三分别获得49.7%、18.9%和7.1%的吞吐量提升，证明优化方案的有效性。本文对三点优化实现并整合，形成Hybrid-Memcached系统，并验证了功能正确性。本文设计Benchmark程序生成持久内存访问占比不同的场景，在本地和多个模拟场景中，使用不同读写比例、访问分布、键值对尺寸、线程数量的负载测试时，Hybrid-Memcached平均有41.2%、54.7%、77.6%和66.3%的吞吐量提升。本文进一步使用YCSB中反映实际工业场景特性的多个负载进行测试，完全写密集和读写均衡负载中本方案分别降低了21.2%和11.8%的执行时间。"
2023,基于情感感知的多模态对话图像检索方法,计算机学院,刘胜哲,杨巨峰,CV,0.3412,"随着即时通讯应用的发展，在线聊天在人们的日常交流中占据了越来越大的比重。在在线聊天中，用户在调控对话中的情感时往往依赖于视觉信息，其中表情包图像因其自身表达语义和情感的多样性，成为了最流行的聊天视觉元素。多模态对话图像检索任务旨在利用对话的历史上下文信息，检索最适合当前语境的表情包图像。其具有广泛的应用场景，例如人机交互、聊天机器人、个性化推荐等。用户在聊天过程中发送的表情包图像在语义和情感上都和聊天上下文紧密相关，这要求多模态对话图像检索模型能够理解图像蕴含的情感，并准确把握模态之间的情感关联。本文从对话图像情感感知以及跨模态情感交互两个方面对以上问题进行研究。


对于对话图像的情感感知，本文探讨了表情包图像的情感识别任务。表情包图像自身的主题组织结构使同一主题下的图像内容相似，但由于局部细节的差异导致它们的情感发生变化。因此在提取表情包图像特征时，应给予这些局部细节更高的权重。本文设计了一个基于局部重注意力机制的多模态模型，结合金字塔自注意力编码器，在不同的视觉阶段显式地选择出对情感最关键的局部特征。为了解决表情包图像情感识别数据匮乏的问题，本文还搜集构建了第一个大规模表情包情感识别数据集SER30K。在SER30K以及其他图像情感分析数据集上的结果表明本文提出的模型具有最优性能。


对于跨模态的情感交互，本文具体针对基于对话的表情包图像检索任务，设计了一种“先感知后回复”的检索方法。其包含基于情感极性的难样本挖掘模块，结合表情包图像和对话上下文的语义与情感相似度进行难样本选取。为了提升图像特征质量，该方法设计了情感知识蒸馏模块，将SER30K中蕴含的情感知识迁移到图像编码器中。同时，为了有效利用表情包的主题信息，设计了主题层次语义对比学习模块，利用自监督的方式提升图像特征的区别性。在大规模多模态对话图像检索数据集上的实验验证了本文方法的有效性以及泛化能力。"
2023,复杂场景的三维信息估计和重建算法研究,计算机学院,朱一凡,任博,CV,0.346,"随着计算机视觉技术的发展，元宇宙、AR/VR 等产业迅速发展壮大，使得三维信息估计和场景重建技术的应用日益广阔，已在娱乐业、制造业以及影视业等方面取得了很大的成功。但如何在计算机中准确地重建和估计现实场景的三维信息，依然具有很大的挑战，是当前计算机视觉领域的一个热点研究课题。


对于三维信息估计和三维重建，有两种问题很难处理，即透明物体的三维重建和室外远距离处物体的深度估计。透明物体中的光线传播复杂，室外远距离物体目标对象面积较小，他们都属于三维信息估计和重建当中较难处理的场景。现有的三维信息估计和重建算法在处理这些复杂场景时还存在很多问题。本文围绕现实场景的三维信息估计和重建问题，研究了基于RGB-D输入的透明物体重建算法和室外远距离处物体深度估计这两种复杂情形下的三维信息重建和估计算法，通过改善三维信息估计和重建算法在这些复杂情形下的表现，拓展了三维重建算法在现实世界中的应用场景。本文的主要贡献如下：


• 本文提出了一种基于RGBD输入的SLAM 算法。该算法可以支持透明物 体重建，并可以在场景种含有透明物体的情况下进行准确的相机位姿估计 和场景重建。该算法由两部分组成，首先检测分割环境当中存在的透明物 体，然后利用该分割结果改善相机位姿的估计算法。然后对环境和透明物 体进行分别重建，在重建不透明物体的时候使用传统方法，在重建透明物 体的时候使用基于可视外壳的方法。实验结果表明本文的方法在位姿求解 的定量化指标和三维重建的可视化结果上都优于目前主流方法上都有明显 的提升。


• 本文提出了一种基于消失点增强的单目深度估计方法。该算法结合了物体的语义信息和和场景的透视信息，使得现有算法可以更好地估计室外远距离处物体的深度信息。该算法首先使用消失点检测方法获取图像当中的透视信息，并使用预训练的语义分割模型得到图像的语义分割结果。然后使用透视关系计算物体级别的深度信息, 并使用深度融合和深度滤波获取最后的结果。实验结果表明本文算法的主要指标要优于目前国际主流方法。"
2023,基于位置感知自注意力机制的文本表示研究,计算机学院,王宇辰,卫金茂,NLP,0.2939,"文本表示是指将自然语言文本转换为计算机可以理解和处理的形式，有效的文本表示方法可以更好地服务于下游任务。如何有效提取文本特征，是文本表示任务中的关键问题。其中，文本间的位置关系对文本语义理解发挥着重要的作用。目前主流的Transformer特征提取器受到置换不变性的影响，虽然能够通过位置编码获取文本间的位置关系，但仍面临着词向量与位置编码的简单结合不能充分表达词语与位置的关联的问题。针对上述问题，本文引入了词语与位置的关联的补充项，使模型能够获取更充分的语义特征。同时，模型在自注意力机制的基础上将词语的注意力集中于序列中与其更相关的位置，从而提高了模型的语义理解能力。


基于位置编码和自注意力机制，本文提出了一种新的文本表示模型——PASA。首先，为了解决现有位置编码方式中词语与位置的关联表达不充分的问题，本文设计了可训练的词语与位置关联权重矩阵，从中获取词语与位置的关联特征并将其加入现有注意力权重矩阵中，使模型能够充分利用位置信息辅助文本的特征提取。其次，为了更好地关注表达文本语义的核心词语，本文提出了自适应位置感知的注意力计算模块，在保留全局信息的基础上，通过池化掩码方式提取与当前词语更相关的局部信息，并将局部信息和全局信息相融合，增强模型的上下文信息提取能力。在多个公开数据集上的实验结果表明，PASA所提取的文本特征优于基线方法，在情感分析、释义识别、机器阅读理解、机器翻译等下游任务中都取得了更好的表现。"
2023,语音到文本转换任务中的预训练技术研究,计算机学院,王程一,杨征路,CV,0.311,"语音是人与人、人与机器之间进行交流的重要方式，而语音到文本的转换是理解和分析语音中内容信息的有效手段，也是计算机领域的重要研究方向之一。根据语音和文本是否属于同一种语言，语音到文本转换任务又可以分为语音识别任务与语音翻译任务。随着深度学习技术的进步，深度神经网络模型广泛应用于这两个任务中，但是这些模型通常需要大量的标注数据进行训练，而在标注数据稀缺的场景下，神经网络模型的性能将大大降低。面对标注数据匮乏的挑战，本文深入研究了预训练技术在语音识别和语音翻译任务中的应用，开展了如下研究工作：


        第一，为了缓解语音识别任务对于标注数据的依赖，本文提出基于中间层监督和伪音素标签的语音识别预训练方法，提升了已有的自监督预训练模型对语言学信息的建模能力。其中中间层监督方法从预训练方式的角度出发，通过在模型中间层添加额外的预训练损失强制模型低层学习尽可能更多的语言学知识；伪音素标签方法从预训练目标的角度出发，为模型提供具有明确语言学含义的预测目标，显式地引导模型学习语言学知识。实验结果表明所提出方法能够有效提升语音识别性能。


        第二，针对已有的预训练方法对数据利用不充分的问题，本文提出基于多语种通用表示学习的语音识别预训练。该方法将有监督训练损失和自监督训练损失通过多任务学习的方式相结合，进而同时利用标注数据和无标注数据进行预训练。此外，本文进一步提出向量替换策略提升模型的性能。实验表明该方法在八种低资源语言的语音识别任务上超越了已有工作。


        第三，针对语音翻译任务，已有的技术使用预训练的语音识别编码器初始化语音翻译模型的编码器，这使得编码器对于语义信息的建模能力不足。针对这一问题，本文提出面向端到端语音翻译的串联编码网络结构。该网络使用语音识别编码器和机器翻译编码器串联组成语音翻译编码器，实现了声学知识建模和长距离语义知识建模的解耦。此外，本文进一步提出参数共享方法和去噪翻译预训练方法，改进了串联编码网络的性能。实验证实该网络结构在多个语音翻译任务上均超越了传统预训练方式。


        第四，为了提升编码器对语义信息的建模能力以及避免对额外的机器翻译模型的依赖，本文提出面向端到端语音翻译的课程预训练方法。该方法首次将课程学习的思想引入到预训练过程，将语音翻译任务拆解成多个子任务，并通过多阶段预训练引导模型由易向难地逐步学习转录、语义理解和跨语言单词映射知识。实验结果表明该方法有效提升了语音翻译任务的性能。"
2023,椭圆曲线计算加速在零知识证明计算中的应用,计算机学院,文周之,苏明,Security,0.2629,"近年来，得益于为通用计算设计的零知识证明协议，许多密码学应用可以 利用简洁非交互式零知识证明协议 (zk­SNARK) 实现安全的新型原语，例如区 块链应用中的 Layer­2 扩容方案。这些新型的密码学应用有着广泛的应用前景， 然而生成 zk­SNARK 证明的时间开销较大，这影响了相关密码学方案的性能和 zk­SNARK 在实际中的应用。


为了提升 zk­SNARK 计算证明的效率，本文针对计算过程中占比 70% 的多 项式承诺步骤，提出了一种计算多项式承诺的加速方案。针对多项式承诺计算 的主体任务，即椭圆曲线数乘，本文提出了一种基于双基数字系统计算椭圆曲线 数乘的算法。该算法利用双基数字系统来表示椭圆曲线数乘计算中的乘数，通 过双基数字形式更少的底层运算次数来减少计算数乘过程中椭圆曲线加法操作 运行的次数，从而实现椭圆曲线数乘的加速。相比于现有的基于双基数字系统 的算法具有求解双基形式耗时大的缺点，本文提出的算法综合考虑了求解双基 数字形式表示和利用双基形式求解椭圆曲线数乘的时间总和，通过在双基形式 的质量和求解速度中实现取舍降低整体计算时间。实验结果表明，在求解双基 数字形式表示阶段，本文提出的算法的时间消耗是现有方法的 1/35，而双在计 算数乘阶段本文提出的算法的时间消耗与现有方法相当。


针对多项式承诺的计算任务特性，本文提出了一种使用 GPU 硬件来计算多 项式承诺步骤的计算方案，为对其进行工程实现，本文利用 NVIDIA CUDA 实 现了一个基于 GPU 端的大整数计算库和椭圆曲线密码学计算库，并基于 SIMD 模式在每个线程上运行双基形式求解椭圆曲线数乘算法求解多项式中的一项。 相比于现有的密码学计算库在 CPU 端计算椭圆曲线多项式承诺，本文利用 GPU 的众核计算的架构优势，处理多项式承诺计算中次数较高的椭圆曲线多项式，提 升了计算效率。实验结果表明，相比于现有的密码学计算库，本文的椭圆曲线 数乘计算速度最快比 OpenSSL 的实现快 8.4%，较大规模的椭圆曲线多项式 (大 于 2 19 个点) 的计算速度是 libsnark 的 4 倍。最后，本文给出了一个使用 GPU 计 算区块链扩容技术中 Merkle 树路径的零知识证明作为样例。"
2024,非抢占式任务卸载调度系统性能优化研究,计算机学院,姜沛言,张建忠,OS,0.2386,"移动边缘计算（Mobile Edge Computing，MEC）是一种网络架构，通过将云计算服务下沉到网络边缘，提高任务执行的效率和性能。在移动边缘计算中，任务卸载是实现高效计算和节能的关键环节。现有研究多针对单一应用场景卸载性能提升，而对多应用、多租户场景表现不佳。在此背景下，本研究以一种现有的基于混合优先级的在线卸载算法为基础，设计边缘计算任务卸载系统，实现该算法可运行实例。进一步提出相应方法优化其在资源受限情况下的卸载稳定性、任务调度过程中的公平性，使其在复杂业务场景下表现出更好的性能，进而提出一个当前业务场景下的任务卸载解决方案。本文的主要贡献如下：


（1）设计并实现了相应的任务卸载调度系统。本文基于Linux环境下设计的任务分发系统，实现去中心化的在线任务卸载算法及基于贪心算法的对照基线。通过对照试验，验证了该算法在当前业务场景下的有效性。


（2）针对该在线调度算法提出优化策略。在本研究设计并实现的任务卸载系统中，基于该算法运行情况扩展了系统性能的评价指标，并提出相应的优化方法：使用截断二进制指数退避算法降低任务的丢弃率，在边缘服务器的任务队列中添加动态优先级机制以调整低优先级任务，并通过二级卸载优化了系统的任务执行时间。


（3）基于Google Cluster公开数据集进行了仿真实验。本实验基于在线任务卸载系统，从综合响应时间、任务丢弃率等方面基于开源现实追踪数据集进行了性能测试。结果显示，在系统繁忙时，使用TBEB算法将任务的命中率提升了最大10\%，通过调整动态优先级阈值可变地将低优先级任务平均响应时间相较于无公平性算法提升了30\%以上，并使用二级卸载优化云端访问次数使得系统的总响应时间与带权响应时间方面性能均表现出显著提升。"
2023,面向高质量视觉信息生成的图像增强及图像信息融合技术研究,计算机学院,赵琳,杨征路,CV,0.377,视觉信息在感知、认知和行为调节中扮演着关键角色，是人类获取和理解外部世界的主要途径之一。图像作为视觉信息的重要载体，在表达主题和信息 交互上扮演着至关重要的角色。因此，如何通过图像信息来生成高质量的视觉信息具有重要的意义，其中包含两个方面：一是如何利用图像自身内容生成高质量图像；二是如何应用图像信息融合技术将多媒体信息融合到图像信息中，进而生成内容丰富的高质量视觉信息。本文针对这两方面问题提出了两种新模型。 首先，针对生成高质量图像信息的问题，本文同时考虑对欠曝光图像和低光图像进行增强，并基于此提出了一种新的深度对称网络架构来生成高质量图像。该架构具有高度可逆的性质，因此支持正反双向传播。网络框架包含一个可逆特征转换器和两对预训练的编码器-解码器。其中，可逆特征转换器是该网络的核心部分，其任务是进行双向的特征学习，并且可逆架构的约束条件保证了特征学习的准确性。编码器-解码器则对网络性能的提升起到了关键作用，它们提供了深度特征来增强可逆特征转换器的性能。此外，本文还提出了循环残差注意力模型，使网络可以逐步学习复杂的图像颜色变换。本研究在两个数据 集上进行了广泛的实验以验证所提架构的有效性，并通过消融实验证明了每个模块的重要性。 其次，针对基于图像信息融合技术的高质量视觉信息生成问题，本文提出了一种全新的多媒体图像格式——超图像。超图像的视觉效果与人脸图像相同，但可以解码为包含更为丰富的视觉信息的讲话视频（包括音频）。 超图像系统由编码器和解码器两部分组成。编码器将语音信息嵌入人脸图像中，即输入的音频数据首先被转换为梅尔频谱格式，再嵌入到人脸图像中高效地生成超图像。 解码器则是从超图像中提取语音信息，并将人脸信息和语音信息结合生成讲话视频。此外，该系统可以将多个音频迭代地嵌入到超图像中，并解码出不同的讲话视频，以此来支持不同级别的的访问控制权限。实验结果表明，使用分辨 率为 160×160 的超图像可以呈现长达 80 秒的高质量讲话视频。 关键词：高质量视觉信息；图像增强；图像信息融合；图像生成
2022,低资源图像描述关键技术研究,计算机学院,吴一可,袁晓洁,CV,0.3921,"图像描述是计算机视觉与自然语言处理交叉领域的关键任务之一，该任务旨在为一幅图像生成恰当的描述语句。一方面，在为图像生成描述语句的过程中，模型需要正确识别出图像中的物体及其属性，并且精准捕捉到物体之间的空间关系与语义联系，最终实现对图像主要内容的全面理解，进而将其以正确流畅的自然语言描述出来。因此，图像描述模型的提升能够促进人工智能领域内图像内容理解、自然语言生成等相关研究的发展。另一方面，随着大数据时代的来临，互联网上的多模态信息迅猛膨胀，通过为图像生成描述语句可以为人们提供更加丰富的信息内容，从而有效增强图文检索能力。此外，图像描述还在人机交互、辅助诊断、智慧交通等领域具有广泛的应用场景。综上所述，对图像描述任务的研究具有重要的学术意义和巨大的实用价值。


近年来，随着深度学习技术的兴起，基于深度学习的图像描述取得了长足的进步与发展。深度学习模型的训练往往需要大量的训练数据，然而在图像描述领域，由于数据的标注过程更复杂、标注成本更高，现有图像描述数据集的规模和种类都是十分有限的，无法满足现实世界中复杂多变的应用需求。因此，基于深度学习的图像描述依然面临着不容忽视的数据瓶颈。本文将上述问题称为图像描述的低资源问题，并从物体层次、句子层次、语言层次等三种不同层次的具体场景入手，对低资源图像描述的关键技术展开了深入的分析与研究，有效提升了低资源场景下图像描述模型的性能。本文的主要研究内容包括：


第一，针对物体类别低资源的图像描述，本文提出了一种物体可扩展的图像描述训练框架，从数据生成的角度解决图像描述模型涵盖物体类别有限的问题。一方面，现有的大多数图像描述模型只能为原始训练集中包含的物体类别生成描述语句，而无法覆盖到现实世界中的其他物体类别（本文称之为扩展物体）。另一方面，以人工手段为扩展物体标注训练数据往往是费时费力的。因此，本文设计了一种基于替换机制的数据自动生成方法，通过同时替换图像中的物体区域以及句子中的物体词，实现了在无需人力参与的条件下为扩展物体自动生成训练数据。然而，如果替换前后的物体差异较大，会导致不合理的替换结果。为此，本文构建了多模态上下文词向量，用来衡量两个物体之间的视觉上下文及文本上下文的相近程度，以确保替换后的结果是合理且有意义的。最终，将原始训练集与新生成数据合并为物体类别更为丰富的扩展训练集，并在该数据集上训练图像描述模型。实验结果表明，本文提出的物体可扩展的图像描述训练框架能够为扩展物体生成正确的描述语句，并且相较于基准模型在自动评估和人工评估的标准上都取得了最好的效果。


第二，针对标注语句低资源的图像描述，本文提出了基于广义成对比较损失的图像描述模型，从损失函数设计的角度解决模型性能随着数据集中语句资源的减少而下降的问题。一个图像描述数据集的规模由图像数量与每幅图像标注语句的数量共同决定。本文将图像描述数据集中的标注语句统称为语句资源，并聚焦于通过减少语句资源的标注来降低数据集的标注成本。然而，当数据集中的语句资源减少时，利用交叉熵损失和自批评损失训练的图像描述模型都出现了不同程度的性能下降。本文深入分析了导致模型性能下降的关键因素，并据此提出了一种新的广义成对比较损失，通过减小自批评损失中基线估计的方差有效提升了图像描述模型的性能，从而在不影响模型效果的前提下降低了数据集的标注成本。实验结果表明，在语句资源较少的情形下，相较于交叉熵损失或自批评损失，利用广义成对比较损失训练的模型都取得了更好的结果。


第三，针对低资源语言图像描述，本文分别从图文之间的语义对齐与数据资源的充分利用两个角度出发，提出了循环一致性约束与基于异构数据集的多目标优化策略，以解决某些语言由于训练数据不足而出现的模型性能不理想的问题。现有的图像描述数据集中大多数的语言都是英文，对于一些其他语言（例如德文），图像描述的数据还相对比较匮乏。本文将此类语言称为图像描述任务上的低资源语言，并围绕以英文数据作为辅助资源的跨语言设置，从两个角度分别研究如何提升低资源语言图像描述的性能。在视觉语义对齐方面，本文提出了循环一致性约束，进一步提升了图像区域、英文单词以及德文单词之间的细粒度语义对齐。在数据资源利用方面，本文提出了一种全新的训练设置，将三元数据集与大规模二元数据集同时应用于低资源语言的图像描述任务之中。此外，本文设计了一种基于异构数据集的多目标优化策略，以实现对多种数据集的充分利用。实验结果表明，本文提出的方法能够有效提升德文图像描述的性能，从而缓解了低资源语言图像描述数据匮乏的问题。"
2021,群组学习认知链与复合手势指令集体系及关键问题研究,计算机学院,王胜逵,李庆诚,SE,0.2665,"教育信息化强调使用现代科学技术实现教学的数字化、智能化、多元化，教育现代化强调以人为中心，注重人全面素养的提升。然而当前的信息化教学与现代化教学的目标存在一定差距，信息化教学依然以工厂化教学模式为主，难以融入传统的教学模式，也难以向个性化教学和提升素养教学迁移；信息技术改变教育的同时，也从使用方法和依赖性等方面束缚着教育的发展；教学行为基础数据的缺失，导致对学生的学习评价依然以结果评价为主，缺乏过程性和增值性评价。


        针对上述问题，本文从介质联通视角出发，首先提出K12交互纸张的概念，基于K12交互纸张和智能笔构建纸笔物联教学生态，通过教育复合手势指令集放大K12交互纸张的交互能力。其次，以实现对学生的综合评价、提升学生的整体素养为目标，提出群组共振研学理论，借助纸笔物联教学生态对基础教育核心场景下的群组学习行为基础数据进行采集。通过分析，发现了描述群组共振认知的多维数据节律，为群组过程存在性与增值性评价奠定基础。借助新型认知粒度、路径与工具——学科教学元和课程版面认知动态组织——页群，形成注入群组复合手势指令的认知数据结构链和课程数据空间，构造课程内容驱动的认知过程。通过本科嵌入式系统课程设计、基础教育核心最小场景实验展示，验证了基于群组共振研学和复合手势指令的认知链对OMO课程学习认知过程度量与数据生态环境改善的可行性。"
2019,认知规律启发的显著性物体检测方法与评测,计算机学院,范登平,刘晓光,ML,0.2683,"显著性物体检测技术起源于认知学中人类的视觉注意行为，即人类视觉系统能够快速地将注意力转移到视觉场景中最具信息量的区域而有选择性地忽略其它区域。该技术在现实生活中有着广泛的应用基础，如，自动驾驶、人机互动、视频分割、视频字幕、视频压缩等。除了其学术价值和实际意义之外，由于图像和视频数据（遮挡、模糊、运动模式等）自身的挑战以及人类在动态场景中注意行为（选择性注意分配和注意转移）固有的复杂性，使得显著性物体检测技术面临着巨大挑战。受制于采集设备，早期构建的显著性物体检测数据集表达真实场景的能力非常有限。同时，这一领域的评价指标也是基于像素级误差的, 完全忽略了人类认知规律的特性。上述问题，严重制约了显著性物体检测技术的。

       本文围绕图像视频显著性物体检测，研究了基于人类认知规律的数据集建立、模型建模、评价指标三个方向的问题。主要创新点包括：

1. 针对现有图像显著性物体检测公开测试存在的各种偏差问题，构建了一个富上下文环境下的图像显著性物体检测数据集SOC，并首次从属性层面对现有方法进行了大量评测和深入的分析。

2. 针对视频显著性物体检测中注意力转移的问题，构建了第一个高质量、稠密标注的视频显著性物体检测DAVSOD 数据集；提出了基于注意力转移的SSAV 模型，取得了国际领先的检测性能；提供了当前最大规模、最完整的视频显著性物体评测结果。

3. 针对非二进制显著性物体检测质量评价的问题，提出了符合人类认知规律的度量指标S-measure，使得评价方法从像素-级过度到结构-级，特别是与人的主观评价一致性性能从低于50% 提升到了77%。

4. 针对二进制显著性物体检测质量评价的问题，提出了符合人类认知规律的度量指标E-measure，使得评价方法在一个紧凑项中同时考虑了全局和局部信息，上述方法相比国际最先进算法的性能提高了19%。"
2019,基于知识图谱的图书馆咨询问答系统,计算机学院,兰格,师文轩,Security,0.2971,"中国国家图书馆馆室规模庞大，藏书丰富，活动多样，每天需要招待成千上万的访客。当这些访问者遇到困难时，往往需要向图书馆的咨询服务人员求助。一方面，对于有限的咨询服务人员而言，服务任务在节假日时期会格外繁重，另一方面，咨询服务人员经过培训后上岗，当一些知识更新后，服务人员如果没有及时获知，或者访问者的一些问题很少提及，导致服务人员遗忘相关知识时，访问者便不能获得满意的答复，很有可能会进行投诉。为了解决上述的问题，减轻图书馆咨询服务人员的工作负担，同时提高访问者的满意度，本论文构建了咨询问答系统，用于代替服务人员，为访问者提供咨询服务。


由于在图书馆的环境下，访问者的问题范围有限，问答系统并不需要针对开放域构建，但是系统的正确率需要很高。因此，为了解决知识过多以及答案的搜索效率低下的问题，本论文将国家图书馆的馆室、证件办理等咨询服务相关的知识构建为领域知识图谱，作为问答系统的数据基础，答案获取的数据源，而且知识图谱也用于辅助系统进行用户意图的理解。通过整理访问者的提问语句规则，本论文整理了大量问题模板，利用AIML语言构建了Libot问答引擎的基础，为了进一步提高问答引擎的问题识别范围，系统利用人工收集和编写的问答知识库，通过信息检索技术对问答引擎进行了辅助和强化。为了提高Libot的意图理解能力，即认知智能性，提高服务能力，系统利用人脸识别技术，通过对话管理功能实现了多轮会话，甚至在与用户的问答中会进行反问，从而引导用户进行问答。为了提高系统的用户体验度，Libot通过科大讯飞提供的语言模块与用户进行对话，获取用户的问题，而答案的呈现方式则不仅仅是文字和语音，还包括图片等多种媒体形式。由此，Libot可以学习培训知识，代替服务人员，理解用户的问题，为用户提供咨询服务，同时通过知识记录和更新，避免了人工服务时可能出现的知识过时和遗忘情况，从而有效地减轻了咨询服务人员的工作负担，提高了访问者的满意度。"
2022,代码实体与注释短语的关联关系识别研究,计算机学院,宾泽铭,沈玮,SE,0.3737,"在软件开发项目中，自然语言注释作为源代码的功能解释和描述，往往需要投入大量的人力对其进行更新维护，研究人员希望通过人工智能相关技术改变这一现状，实现注释自动更新的目标。显然，为了建立触发自动更新的指标，需要准确识别源代码中哪些代码实体与其对应的自然语言注释具有关联关系。在代码实体与注释短语的关联关系识别上，先前的相关研究进展迅速，但是也都面临着两个问题，分别是代码数据中信息含量有限和数据集之间存在分布差异与噪声干扰。这两个困难是该任务在当前研究阶段尚未解决的现实阻碍，影响着整体的研究进程。


为了解决这两个问题，本文提出了一种以丰富有效信息含量为目标的框架。本框架首先通过大规模预训练语言模型建立了代码数据与海量的无监督英文文本之间的联系，通过引入丰富的外部语义知识，在实质上提升了代码数据的有效信息含量。随后本框架借鉴了弱监督学习和多领域学习的思想，建立了一种显性融合与隐性融合相结合的数据融合方法，缓解了数据集之间的分布差异和噪声干扰带来的负面影响，进一步利用了不同数据集中代码数据所蕴含的有效信息。本文在一个公开数据集上设计了相关的实验来测试框架的性能，实验结果表面，本文提出的框架在代码实体与注释短语的关联关系识别任务上相对于七个基准方法都取得了显著的提升。"
2022,面向复杂场景的知识图谱推理技术研究,计算机学院,张瑶,杨征路,SE,0.2887,"知识图谱能够结构化表示、理解和互联人类知识，已经成为人工智能技术从感知智能向认知智能发展的核心驱动力之一。知识图谱推理技术通过对数据的深度分析和推理，使机器智能具有和人类一样的推理和决策能力，近年来得到了长足发展。然而，在知识图谱数据体量快速扩增以及与下游应用连接逐渐紧密的背景下，知识图谱推理技术开展所面临的场景也日益复杂。在复杂场景中，现有知识图谱推理技术面临着关系分布不均衡、推理距离长短混合和事实表达形式多元化的挑战。为了应对复杂场景中的这三个挑战，本文以两个代表性的知识图谱推理技术—–知识图谱表示学习技术和多跳推理技术为研究对象，提出广义关系学习、长短距离混合的多跳推理和多元知识图谱多跳推理三项研究目标。为了实现提出的研究目标，本文系统性地开展了三项研究内容： 第一，针对复杂场景下关系分布不均衡的挑战，提出了基于语义关联感知的广义关系学习方法，有效提升了知识图谱表示学习模型对关系类型的泛化性能。在关系分布不均衡的复杂推理场景下，现有知识图谱表示学习模型面临着对少样本关系向量表示的学习不充分和无法应对零样本关系的困难。为了实现广义关系学习的研究目标，本文提出了基于语义关联感知的广义关系学习方法。此方法通过感知并融合关系间的语义关联，实现对多样本、少样本和零样本关系的联合学习。通过在九个基准数据集上的实验验证，证实了此方法在链接预测、少样本关系学习和零样本关系学习任务上的优越性能。 第二，针对复杂场景下推理距离长短混合的挑战，提出了面向混合推理距离的通用多跳推理方法，有效提升了当前多跳推理技术的能力上限。在推理距离增长的复杂推理场景下，现有多跳推理技术大多应用于短距离推理场景，而对推理距离长短混合的场景缺乏关注。为了实现长短距离混合多跳推理的研究目标，本文提出面向混合推理距离的通用多跳推理方法。此方法通过融合局部-全局知识、差异化丢弃候选行为和自适应停止搜索，使多跳推理技术能够正确选择推理方向和判断推理距离。通过详尽的实验评估，此方法在短距离、长距离和长短距离混合三种推理场景下均表现出了优越的推理性能。 第三，针对复杂场景下事实表达形式多元化的挑战, 提出了针对多元知识图谱问答的事实树推理方法，突破了传统多跳推理技术在多元知识图谱问答任务上的局限性，所提出的首个多元知识图谱问答数据集有助于推动多元知识图谱的相关研究。在事实表达形式升级的复杂推理场景下，现有多跳推理技术只关注于二元知识图谱上，而忽视了多元知识图谱的研究。为了实现多元知识图谱多跳推理的研究目标，本文从多跳推理技术出发，基于多元知识图谱问答任务，有针对性地提出了事实树推理方法。事实树推理方法将多元知识图谱问答过程建模为事实树构建、事实定位和事实推理三阶段。通过详尽的实验评估，此方法在二元和多元知识图谱问答任务上均有出色的性能表现，同时此方法还具有良好的可解释性和可扩展性。 以上三项研究内容分别研究：1）关系的复杂化，即从以多样本关系为核心的传统场景，到以多样本、少样本和零样本关系为核心的复杂场景，2）路径的复杂化，即从以短距离路径推理为核心的传统场景，到以长短距离混合推理为核心的复杂场景，以及3）事实的复杂化，即从以二元事实为核心的传统场景，到以多元事实为核心的复杂场景。综上所述，本文工作以关系-路径-事实为研究主线，依照从传统到复杂的研究理念，形成了紧密的面向复杂场景的知识图谱推理技术研究网络。"
2022,联邦学习后门攻击防御方法研究,计算机学院,侯博禹,张莹,Security,0.3165,"在大数据时代，数据拥有者因为担心隐私泄露而不愿共享数据资源的现象愈发突出，导致越来越多数据孤岛的出现。联邦学习作为一种保护数据隐私的合作学习框架，成为解决数据孤岛问题的新思路。但是，联邦学习面临着来自内部恶意合作者的后门攻击威胁。后门攻击会导致模型对带有特定特征的数据输出特定错误结果，危害大且难以被发觉，大大削弱了联邦学习的安全性。因此，如何防御后门攻击成为提升联邦学习安全性的关键问题之一。


       现有的联邦学习中的后门攻击防御方法可分为基于攻击模型异常行为检测的防御方法和基于鲁棒性聚合算法设计的防御方法。但是，现有基于攻击模型异常行为检测的防御方法在获取数据的过程中常有违联邦学习对隐私的要求，即使引入额外的加密协议以保护隐私，也因极大地增加了计算开销而不具可应用性；现有的鲁棒性聚合算法虽然提升了联邦学习的鲁棒性，却往往不能应对典型的后门攻击。针对现有方法的不足，本文对联邦学习后门攻击防御技术展开研究，主要内容包括：


       第一，基于后门触发器检测的防御方法。为了提升计算效率、保护客户端数据隐私，本文在服务器端训练后门分类器，将分类器分发给参与者，参与者仅需在本地进行触发器的检测与清除。实验表明去除触发器区域后模型主任务在后门数据上的准确率可以从接近0提升到88%以上，显示出算法的有效性。


       第二，基于安全部分聚合协议设计的防御方法。本文为联邦学习设计能抵御典型后门攻击的部分聚合算法，PartialAvg，并针对该算法设计安全部分聚合协议。多个数据集上的实验结果表明，算法可以在几乎不影响联邦学习模型收敛速率的情况下，将一次性后门攻击的成功率从99%降至0，并能有效抵御多种后门攻击策略；同时该算法天然具有一定的隐私性，可以有效防御隐私推理攻击。"
2022,基于字形与上下文信息的中文文本表示方法的研究,计算机学院,朱亚朋,卫金茂,CV,0.3103,"文本表示是将文字转化成向量或者矩阵，通过特征信息的集合来表示原始文本数据的过程，是一系列自然语言处理任务的基础，其表示的好坏将直接影响模型的性能。文本表示的关键是应用特征提取方法对目标文本序列进行处理进而获取丰富的语义信息。在深度学习的背景下，现有的特征提取模型取得了不错的效果，然而在处理以中文为例的象形文字时，往往忽略文字本身所含有的结构化信息，即文字的视觉特征。另一方面，在提取文本的上下文语义信息时，多数模型都需要通过堆叠更多的网络层才能获取丰富的语义表示，这就限制了其在计算资源有限的条件下的使用。


      针对现有特征提取方法存在的上述问题，本文在BERT中文预训练语言模型获取文本向量表示的基础上引入字符本身的结构化信息，通过将文字转化成图像的方式，设计适用于字符图像的特征提取网络对其进行处理，并利用提取的视觉特征增强通过BERT预训练模型初始化的文本向量的表征能力。将融合后的向量作为上下文特征提取模型的输入。另外，在获取全局上下文信息时，本文提出了一种仅包含输入部分和两个子单元的单层体系结构，以学习全上下文语义表示。以输入文本序列中的字为目标字，该模型在三元组中搜索语义特征，该三元组包含目标字本身及其左右上下文向量。它试图在目标字及其上下文向量而不是其相邻字之间建立语义关系。因此，我们模型的存储成本随着序列长度线性增长，而不是平方增长。总的来说，我们通过整合象形文字本身的视觉特征以及其上下文信息得到最终的表示，在文本分类以及中文命名实体识别任务上的实验证明了本文提出的方法与现有的文本表示方法具有可比性或更好。"
2022,基于微服务架构的电子邮件检索系统设计与实现,计算机学院,徐礼承,温延龙,Security,0.316,"电子邮件在人们的通信交流中有着广泛的应用，但是其作为通信媒介也会被违法犯罪人员利用，如通过电子邮件推送违法广告、恶意病毒以及影响国家稳定的不良信息等。如何从海量的电子邮件数据中检索到关键信息并发掘出隐藏在邮件背后的人员和团体，逐渐成为当前信息检索与信息安全领域的一个重要研究课题。


在上述背景下，应国内某业务部门邀请，本文设计并实现了基于微服务架构的电子邮件检索系统。系统要求分批次导入电子邮件数据，且单批次邮件数量规模较大，并且不同批次的邮件分析需求存在差异，需要根据数据特点研发新的检索策略。因此系统基于微服务架构进行设计，可以灵活分配硬件资源，同时提升开发与运维效率。


基于微服务的开发背景和电子邮件检索的业务需求，系统分为服务注册与发现服务、配置中心服务、路由网关服务和应用监控服务等四个基础功能服务以及数据导入服务、电子邮件检索服务和用户与权限服务等三个业务功能服务，在分层开发的指导原则下分别完成了各个服务的设计与实现。系统经过功能与性能测试，达到了预期需求。系统已在该业务部门使用，提供了超过千万封邮件的检索服务，挖掘了潜在的人员和团体关系，收到了良好的效果。


针对系统在邮件导入和检索性能两方面遇到的瓶颈问题，本文提出了解决方案，并在系统中加以实现。第一，电子邮件文件体积小数量多，在读取和保存时需要大量的IO操作，影响数据导入性能，本系统采用基于多线程读取与批量保存的异步索引构建方案，有效地减少了数据导入的时间。 第二，电子邮件的检索需求包括关键词检索和人物关联检索，采用传统的单一数据存储方案和检索方式不能满足性能需求，本系统提出了面向多维查询的混合式存储检索解决方案，极大地提高了系统的检索性能和准确性。"
2022,联邦学习中数据增强与通信优化方法研究,计算机学院,张嘉超,徐敬东,CV,0.3285,"在数字经济时代，数据已经成为推动经济发展的重要因素。传统的机器学习借助云服务器强大的计算能力，将数据集中在云端进行训练，然而大规模的数据传输给网络带宽和隐私保护带来了挑战。联邦学习通过多客户端训练、服务器端模型聚合的方式以缓解网络带宽压力，保障数据隐私安全。但是，联邦学习中客户端的数据来源各不相同，形成的非独立同分布（non-IID）数据极大降低了模型精度。同时，客户端计算资源与网络资源差异导致训练等待问题，服务器需要等待所有客户端模型上传后才能进行全局模型更新，增加了训练时长。本文针对上述问题进行研究，具体工作如下：


      1) 针对non-IID 数据挑战，本文提出的FLGAN 算法采用两阶段训练。第一阶段利用联邦生成对抗网络进行数据增强，生成符合全局特征的新样本构建IID数据以提升模型准确率。第二阶段基于K-Means 算法对客户端模型分组，并根据客户端样本分布特征计算模型权重，完成组内和组间加权聚合，提升模型准确率的同时加快收敛速度，缓解带宽压力。


      2) 为解决联邦学习中的训练等待问题，本文基于DC-ASGD 算法，在FLGAN基础上实现AFLGAN 算法。在训练过程中，服务器采用异步通信方式减少训练时长，无需等待所有客户端模型进行全局模型更新，并借助泰勒展开降低梯度延迟造成的影响。


      3) 在保护用户隐私的前提下，边缘设备相较于端设备拥有更好的计算资源和网络资源。因此本文基于云-边-端三级架构实现边缘联邦学习系统，对客户端与服务器进行容器化封装。系统支持任务部署与训练，简化部署流程的同时加快部署速度。"
2022,基于引导特征感知的视觉显著性目标检测研究,计算机学院,方贤,邵秀丽,CV,0.4172,"视觉显著性目标检测，可以快速地辨别出一幅自然图像中最引人注目的对象或者区域，在图像分割、图像编辑、图像恢复、人机交互和路径规划等应用中发挥着重要的作用。然而，纷繁复杂的环境会存在遮挡、倒影、模糊、光照、小物体和低对比度等各类干扰，使得视觉显著性目标检测方法在性能上面临泛化性不足、实时性低下、鲁棒性较差和精准性欠佳的挑战。为了应对这些挑战，本文面向复杂的野外自然场景下目标的前背景分离任务，从引导充足特征和特征自主感知的方向探索特征的获取与利用，提出了基于引导特征感知的视觉显著性目标检测的研究方法。论文的主要研究内容如下：


      第一，针对图像目标的中央与边缘位置的有效信息量不均衡所导致的检测泛化性不足的挑战，本文通过以中央的对称发散方式增强边缘的标签量，提出了一种新颖的基于交互分支网络的RGB显著性目标检测方法。该方法依据距离因子来将真实标签进行剥离，形成伪身体和细节标签，以专注于对骨干和轮廓的信息反馈。进一步地，该方法运用设计的两个中心对称的分布函数对离散的伪标签进行像素级别的自适应增强与减弱，生成强化与弱化的身体和细节标签，从而获得更多具有区分性的标签信息。在此基础上，该方法构建四个分支解码器以获取不同类型的特征，并搭配一个交互编码器来可选择地参与二阶段的特征交互，进而完成迭代式的信息交换。同时，该方法设计以任务为导向的损失函数用以协作地监督网络的训练过程。多组的实验结果表明，此研究的泛化能力优越于同时期的其它方法。


      第二，针对图像中有用特征的缺乏以及冗余特征的污染容易造成算法收敛速度慢所导致的检测实时性低下的挑战，本文通过对庞大的网络结构进行优化设计，提出了一种高效的基于梯形上下文关联互补网络的RGB显著性目标检测方法。该方法首先通过在独立的分支中插入带有不同的内核大小或者膨胀比率的空洞卷积以动态地收集初始特征的多样化信息，然后通过整合邻居层与非邻居层的高级语义信息和低级细节信息以紧密地聚合不同等级的特征，最后通过利用多对交替的自顶向下和自底向上的特征交互流以逐步地收缩多尺度信息直至细化。通过多组的实验对比，此研究能够兼顾效益和效率，在保障了检测精度的前提下，也有效地节省了检测的时间成本。


      第三，针对图像的多模态特征融合的不兼容以及多尺度特征聚合的不充分所导致的检测鲁棒性较差的挑战，本文通过采取同步与异步的特征合并策略，提出了一种融合的基于多模态和多尺度细化网络的RGB-D显著性目标检测方法。一方面，该方法使用构造的通道注意力机制和空间注意力机制的串行有序的连接形式来促进颜色模态和深度模态的特征的多模态融合。另一方面，该方法使用构造的渐进的交互和跳跃的交互的并行无序的连接形式来促进较高层、中间层和较低层的特征的多尺度聚合。除此之外，该方法设计联合型的损失函数分别从单个或整体的层面施加对像素点重叠度的约束，来计算每个像素点的局部相关性和全局相关性，用于保障像素点的类间的区分性和类内的一致性。详尽的实验对比发现，此研究的鲁棒效果相比于同时期的其它方法有着明显的优越性。


      第四，针对深度卷积神经网络存在的特征表达能力的上限所导致的检测精准性欠佳的挑战，本文通过将Transformer以组群的结构引入特征编码，提出了一种新型的基于群Transformer网络的RGB-D显著性目标检测方法。为了更好地学习跨模态和各尺度特征的共同信息，该方法以三元组的形式对中三层和后三层的特征进行软分组。该方法首先将输入特征经过反复提纯与注意力机制增强以净化颜色模态和深度模态的跨模态特征，其次对产生的特征进行上下采样并拼接以确保不同组内的各尺度特征的大小统一且相互关联，接着构建多组共用能量权值的Transformer编码器以达到组群内部特征的信息共享效应，最后级联式地将所有特征聚类后按由高层传往低层进行结合以生成输出特征。通过详尽的实验证实，此研究取得了更为理想的检测精准度。


      综上所述，本文以基于引导特征感知的视觉显著性目标检测为研究点，针对复杂的野外自然场景下检测的泛化性不足、实时性低下、鲁棒性较差和精准性欠佳的研究挑战，从RGB单模态和RGB-D多模态两个角度出发，围绕在特征提取、特征挖掘、特征变换和特征融合过程中的充分性、关联性、互补性和兼容性等方面，深入探讨了基于交互分支网络的RGB显著性目标检测、基于梯形上下文关联互补网络的RGB显著性目标检测、基于多模态和多尺度细化网络的RGB-D显著性目标检测和基于群Transformer网络的RGB-D显著性目标检测的具体研究内容，形成了较为完整而紧密的研究体系。"
2022,序列图像的识别与检测算法研究,计算机学院,梅杰,程明明,CV,0.4207,"序列图像是一组按照特定时空域顺序排列的图像集合，与普通图像不同，序列图像不仅蕴含图像内部的语义信息，图像之间也存在着语义依赖关系（即序列图像的“序列性”）。对序列图像的处理是计算机视觉领域重要的研究课题，在医学影像辅助诊断、遥感图像处理以及视频分析等领域都有着广泛的应用。本文以序列图像的识别与检测算法为研究目标，将序列图像展开为空间和时间序列图像，致力于探索如何高效地挖掘图像内部和图像之间的深层次语义关联信息。从基于立体空间序列图像的肺结节检测、基于平面空间序列图像的道路提取和基于时间序列图像的变化检测等三个具体的视觉任务出发，对不同分布的序列图像进行分析，通过改进视觉注意力机制，建立了具有更强的序列关联性表达能力的深度网络模型。本文的主要研究内容和贡献包括以下几个方面：


1. 对于立体空间序列图像中的肺部 CT 图像，针对其立体空间维度较高的难点，提出了切片关联注意力网络进行肺结节的检测。受到医生临床诊断肺结节方式的启发，设计了一种切片分组非局部模块，将切片维度分组的思想引入自注意力机制中，来充分学习CT 图像中的立体空间序列信息。三维区域候选网络对肺结节的检测通常会带来大量的假阳性样例，设计了基于多尺度特征图的假阳性抑制模块来进一步优化检测的结果。此外，提出了医学影像领域目前为止规模最大的肺结节检测数据集 PN9，与之前的肺结节检测数据集相比，其在数据规模、种类多样性、图像丰富度和检测困难程度上都有了较大的提高。通过在不同数据集上的大量实验，充分验证了所提出的切片关联注意力网络能够有效提高肺结节检测的性能。


2. 对于平面空间序列图像中的道路遥感图像，针对其它地物会对道路进行遮挡的问题，提出了拓扑连通注意力网络从而直接提取出连通性较好的道路。考虑到道路在平面维度上连续分布并呈现出跨度大且细长的形状，设计了条形卷积模块，其利用水平、垂直、左对角线和右对角线等四种不同方向的条形卷积来学习道路的长距离依赖信息，同时抑制不相关区域对特征学习的干扰。此外设计了连通性注意力模块来探索相邻像素之间的连通关系，其能够缓解建筑物或树木等对道路的遮挡问题，提高道路的拓扑正确性。通过在两个公开数据集上的大量实验，验证了拓扑连通注意力网络在保证道路连通性方面的有效性。


3. 对于时间序列图像中的双时序高分辨率遥感图像，提出了差异感知注意力网络来同时进行建筑物分割和多级别变化检测。为了探索不同时序图像中能够反映出差异性变化模式的通道，设计了双时序聚合模块，其能够同时学习全局变化信息。此外，考虑到图像中存在的不同级别变化，进一步设计了差异注意力模块来探索多级别变化之间的局部联系，并提高对不同等级变化的判别能力。在大规模建筑物变化检测数据集上的大量实验表明，相比于其他的方法，本文提出的差异感知注意力网络具有较大的优越性。


4. 基于时间序列图像中的双时序高分辨率遥感图像，进一步提出了基于全局差异与局部注意力的网络模型进行变化检测。结合卷积神经网络能够更好的提取图像的低阶细节信息和 Transformer 可以对长距离依赖关系进行建模的优势，采用混合两者的架构作为编码器来提取图像特征。设计了全局差异模块，来学习全局变化信息并提高对图像中所有像素的整体理解。此外，设计了局部门控注意力模块，来学习局部变化差异并增强对双时序图像间多级别变化的判别能力，其利用门控自注意力机制来学习相邻变化敏感特征块之间的局部依赖性。通过大量实验，验证了此模型应对变化检测任务是有效的。"
2022,基于云原生5G网络的多源360°视频传输资源调度问题研究,计算机学院,苑新婧,徐敬东,Network,0.3117,"360°视频作为最典型、最普及的虚拟现实应用之一，在近年来快速发展，逐渐成为热门的新兴视频应用。与传统2D平面视频不同，360°视频需要具备更高的分辨率、帧率和动态范围。在相同的清晰度标准下，传输360°视频的下行带宽需求是传统2D平面视频的4–5倍。目前学术界通常采用基于切块的360°视频自适应流媒体传输方案，在保证用户体验质量的同时降低传输过程中的带宽需求。这种方案仍然受限于单源传输网络框架中有限且波动性较强的端到端下行带宽，以及用户视场预测的准确率。为了提升360°视频的用户体验质量，本文基于云原生5G数据中心网络架构部署360°视频流媒体传输服务。依据架构本身特性，利用5G承载网中的多个边缘数据中心或无线接入网中的多个微基站，以多源的形式向用户传输360°视频。在构建多源传输方案时，设计合理高效的资源调度算法来提高各类资源的利用率。主要研究成果包括以下三个方面：

   第一，针对源端带宽进行资源调度优化，提出一个基于切块技术的多源360°视频流媒体传输框架。用户向位于不同边缘数据中心的视频服务器请求一个视频片段中的不同切块，形成多个视频服务器共同服务一个用户的模式。在框架中客户端决策参与传输服务的视频服务器集合，以及向每个所选的视频服务器请求的视频切块数量，目标是在提升用户体验质量的同时尽量减少系统代价。同时考虑延迟硬约束和连接软约束，设计联合调度问题，并为问题提出了隐式限制方案和显式限制方案来制约服务器连接的数量。进一步地，两个方案的求解算法均能通过理论分析得出算法求解的近似最优性和运行的多项式时间复杂度。通过设计小型测试平台和大型仿真实验对算法进行验证评测，结果显示两个在线算法的运行时间均不超过视频片段持续时长的9.5%，保证了系统的轻量级实现。所提方案有效提升了源端总带宽，在产生可接受的系统开销的同时，保证了用户体验质量。

   第二，利用网内计算和存储资源对传输进行优化，提出一个基于缓存辅助超分辨率的多源360°视频流媒体传输框架。在该框架中，边缘数据中心向接入数据中心传输低质量的视频。在视频切块传输至接入数据中心后，利用接入数据中心的计算资源，采用超分辨率技术将低质量切块恢复为高质量切块。为了减少视频切块的传输时间和超分辨率的处理时间，利用接入数据中心处的存储资源，提前缓存部分高质量切块。框架中决策提前缓存哪些切块以减少传输和处理的整体延迟。在设计缓存问题时，同时考虑缓存效益、调度复杂性和视频内容动态变化等多方面因素。为了解决该问题，本文设计了一个基于分配竞争的最优决策算法。大量的实验评估验证了所提方案在各种系统设置中与其他方案相比的优越性能，如视频播放总卡顿时长最多为其他方案的84.2%。

   第三，针对无线接入网带宽进行优化，提出一个视频流信息感知的多源360°视频流媒体传输框架。在所提出的框架中，多个微基站利用协同多点联合传输技术，以功率放大的方式共同为一个视频用户传输数据；为了降低系统代价，将一段时间内不参与传输的微基站暂时设置为休眠状态。框架中需要决策将哪些微基站设置为休眠状态，以及如何为每个视频用户构建微基站分组，目标是最大化全局用户可用带宽的同时减少系统代价。同时，在设计问题时需要关注360°视频流媒体信息的影响，从而做出全局最优决策。本文设计了一个主从算法，使得问题能够在单时间片的理想设置下求出最优解；为了更贴近实际设置，将问题扩展至双时间片设置中，并求得近似最优解。大量的实验评估验证了所提方案在各种系统设置中与备选方案相比的优越性能，如视频播放总卡顿时长是其他方案的62.9%-92.5%。"
2022,云计算智能资源划分管理框架研究,计算机学院,陈若冰,刘晓光,Network,0.3252,"随着全球数字经济的发展和产业数字化的深入，企业上云成为大势所趋。 云计算数据中心也成为最重要的社会基础设施之一。当前，大型数据中心存在运营成本高和资源利用率低的行业痛点。多任务混部是提高资源利用率最有效的手段之一。但是，混部的多任务会竞争有限的共享资源，从而大致系统性能下降。为了提升整体系统性能，数据中心的资源划分成为该领域的重要研究热点。


尽管资源划分问题已经得到了广泛的研究，但是还存在一些尚未解决的重要挑战，使得设计一个足够好的资源划分框架变得非常困难。首先， 由于多个性能影响因素之间的复杂关系，很难精确分析混部在服务器上的任务的性能。 其次，云环境中经常会面临快速且不可预测的系统状态变化，这要求资源划分框架需要能够在线实时地做出适应状态的变化的智能决策。第三，为了避免落入局部最优，云计算数据中心要为多维资源分配设计最优划分方案，但寻找这一最优解需要巨大搜索量。第四，数据中心还需要处理黑盒任务以及黑盒优化目标等场景，黑盒任务对云服务提供商是不可见的，无先验知识可利用；而黑盒目标无法显式地被定义来直接进行优化。


本文提出了云计算智能资源划分管理框架的概念。本文的主要工作是将强化学习与深度学习技术引入资源划分问题，首先提出七个设计准则填补资源划分问题缺乏形式化定义的空白，为寻找符合实际需求的最优解提供指引；其次，设计出高效的资源划分调度算法，从而降低在各通用系统上运行的开销。具体来说，本文的工作如下:


第一，提出了面向通用云计算的智能化资源划分管理框架的七个形式化标准： 自治性、智能性、有效性、高效性、协调性、自适应性、可扩展性，并在后续四个具体的资源划分管理场景中贯彻执行。


第二，针对具有先验信息的已知任务，论文分别针对单服务器和数据中心的场景，提出了基于深度强化学习的服务器资源划分框架以及基于图神经网络的资源划分感知的数据中心任务放置方法，提升了单服务器任务执行效率和数据中心资源利用率。与当前最好的工作相比，本文工作可以将单服务器吞吐率提高13.3%~18.5% 。在考虑任务放置后，可以将数据中心中单服务器吞吐率提 高25.3%~47.7%。


第三，针对无先验信息的黑盒任务，论文提出了基于在线学习的动态资源划分框架，根据任务负载动态变化实现自适应的资源划分。与当前最好的工作相比，本文工作可以允许混部更多的延迟敏感型的黑盒任务，并将后台任务吞吐率平均提高1.7到12倍。


最后，针对黑盒多目标资源划分问题，论文提出了基于多智能体协同的优化方法，实现了在多个互相冲突的优化目标之间的最优权衡。与当前最好的工作相比，本文工作可以在提高吞吐率21.6%~60.7%的同时，将公平性提高13.4%~29.2%。


综上，面向云计算数据中心的智能化资源划分管理需求，本文提出基于强化学习和深度学习的优化方法，解决了面向已知任务场景下的服务器单目标、数据中心单目标的多资源划分问题；面向延迟敏感黑盒任务的单目标和面向带冲突的黑盒多目标多资源划分问题。本文研究工作中所提出的解决方案可以为其他资源、设备有限的调度问题或提供有益的启发。"
2023,基于校园数据的学生画像与动态行为分析技术研究,计算机学院,李文茹,张海威,CV,0.2885,"在大数据时代，高校校园信息化建设使得海量的教学数据和学生生活数据被记录下来，深入挖掘校园数据蕴藏的内涵和联系，已经成为新时代学生教育管理的重要研究课题。学生画像是指通过对学生静态属性和动态轨迹的挖掘分析，从海量原始校园数据中抽象出学生特征标签，进而对学生行为进行全面、客观的分析，帮助教育工作者掌握学生成长规律和行为模式，为高校学生教育管理科学化、精准化提供科学依据和数据支撑。因此，研究学生画像生成方法具有重要的应用价值和现实意义，本文分别从学生的静态属性数据和动态轨迹数据两个角度展开研究，主要内容如下：


第一，为了提高校园数据的利用效率，解决“信息孤岛”的问题，本文研究并实现了基于自适应特征交叉的学生画像生成方法。采用自适应特征交叉的两阶段神经网络模型，利用多源多维度的学生静态属性数据，通过特征拼接、特征映射、自适应交叉和特征融合，分别学习不同来源的学生静态属性信息特征及不同来源学生信息之间的关系构建出学生画像，同时得到不同来源的学生特征在学生画像生成过程中的贡献权重。


第二，为了解决高校无法获取有效学生静态属性数据的特殊情景下，对学生行为进行分析和预测的问题，本文研究并实现了基于动态轨迹分析的学生画像生成方法。采用基于长短时记忆网络的动态轨迹分析，从校园一卡通记录的学生动态轨迹数据出发，挖掘校园动态轨迹分布和变化规律与学生特征之间的关联，在无任何先验信息场景的情况下生成学生画像。


真实数据集上的实验结果表明，论文的两种方法均可以从专业学习、创新思维、实践参与、文艺体育、自立能力、发展潜力六个维度对学生的发展情况和能力进行评分，具有很强的可解释性。本文还结合了高校实际育人场景，对生成的学生画像进行样例分析、演化分析、应用示范和模型对比，在实际应用层面验证了学生画像的现实意义和应用价值。"
2023,分前缀ANN向量搜索引擎系统架构设计,计算机学院,潘宇,王刚,CV,0.2706,"随着自然语言处理和深度学习的高速发展，向量搜索引擎在语义检索中能够提供比基于词项匹配的传统搜索引擎更相关的查询结果和更好的用户体验。在分前缀向量搜索场景下，每个BS服务器需要管理和存储的数据量以及数据总访问热度都应该大致相同。BS服务器使用NVMe SSD作为外存保存原始向量可以大幅减少存储开销，但是由于NVMe SSD和DRAM的性能差距，引入NVMeSSD也会带来性能的损失。

为解决上述问题，本文提出了一种分前缀ANN向量搜索引擎架构。该架构设计了一种名为数量-热度前缀均衡分区算法对前缀数据集进行划分，它会先执行两阶段分区算法获得一个数据量和热度大致均衡的分区结果，然后使用分区迭代调优算法得到与BS服务器数目对应的前缀分区并且保证分区之间可以同时达到数据量均衡和热度均衡。对于单节点的BS服务器的向量搜索方案，本文针对前人工作的不足提出了与乘积量化结合的HNSW索引，它先使用量化距离执行HNSW搜索得到粗排结果，再使用原始向量计算精确距离进行rerank得到查询结果。它将原始向量保存在NVMe SSD中来减少存储开销。为了提升查询效率，本文设计了连续编号向量读取策略进行异步I/O读取并使用小容量向量Cache来减少读取次数、缓解高并发查询处理时NVMe SSD的读带宽瓶颈，从而降低读取延迟。为了保证连续编号向量读取策略的有效性，本文设计了双层聚类+哈密尔顿簇内排序算法来增加粗排阶段向量编号的连续性。

实验结果表明，对于不同规模前缀和不同目标分区数，数量-热度前缀均衡分区算法可以将分区之间的数据量差距和热度差距均控制在5%之内；对不同维度的百度真实向量数据集，双层聚类+哈密尔顿簇内排序算法能够有效降低I/O读取次数35%左右，连续编号向量读取策略和5%的小容量向量Cache整体可以至少将QPS提升50%以上，平均查询延迟降低33%以上。"
2023,面向智能合约字节码的静态漏洞检测方法研究,网络空间安全学院,杨金妮,卢冶,Security,0.3088,"区块链是一个去中心化的共享账本，能实现多个不互相信任主体之间的协作交易。而智能合约则是部署在区块链上能自动执行的程序代码。近年来，层出不穷的智能合约漏洞受到了工业界和学术界的重点关注。面向智能合约字节码的静态漏洞检测研究主要面临两个挑战：现有静态控制流图构建方法在构边完整度和时间开销上不能取得平衡；现有静态漏洞检测方法的误报漏报率较高，其表现容易局限于单个数据集的问题。


为了应对这些挑战、更好地防护智能合约安全，本文提出了一个可拓展的面向智能合约字节码的静态漏洞检测框架BYTEEYE。首先，BYTEEYE框架提出了一个边增强的控制流图构建方法，它结合了全局模式识别和局部符号执行方法。新的静态控制流图构建方法能够平衡时间开销和构边完整度，以较低的时间开销代价完成高构边完整度的控制流图构建，从而减少漏洞检测的漏报。其次，BYTEEYE框架提出了一个基于图神经网络的漏洞检测方法。通过对现有智能合约漏洞的大规模分析，BYTEEYE框架从边增强的控制流图中提取出包含通用和漏洞特有信息的四类字节码特征，从而减少漏报和误报。此外，论文应用图神经网络模型进行漏洞检测，让BYTEEYE框架能灵活适应不同数据集和智能合约漏洞。


论文用两个数据集评价BYTEEYE框架的有效性。实验结果表明，BYTEEYE框架在三类智能合约漏洞检测上超越了当前最优的方法。BYTEEYE框架也在真实世界智能合约中检测到了105个漏洞，其中99个漏洞是首次报告。BTYEEYE框架能在智能合约被实际部署之前，提前检测到潜在的智能合约漏洞，因此能有效防护到现实世界中智能合约安全。论文的研究成果在智能合约漏洞检测上具有极大的应用价值和研究潜力。"
2023,面向流调文本的时空信息识别及关联区域分析,计算机学院,王艺茹,温延龙,CV,0.2941,"流行病学调查过程中产生的流调文本是一种重要的信息来源，其中包含了患者的行为轨迹、活动区域等重要的时空信息。通过对这些信息进行提取和分析，能够得出流行病的传播规律，为疫情防控提供关键性的参考。


但是流调文本通常以非结构化的文本形式发布，且其中隐含的空间关联关系无法直观展现，导致大量有价值的信息难以有效利用，为后续的研究带来不便。


因此，本文首先对面向流调文本的命名实体识别方法展开研究，提出一种融合多特征的时空信息识别方法，可以高效的从流调文本中提取出有价值的时空信息。然后，本文对现有的时空数据分析方法进行研究，提出一种自适应参数的时空数据聚类算法，进一步挖掘出流行病传播过程中的关联区域。


本文的主要工作和贡献如下：


第一，本文提出一种融合多特征的中文命名实体识别算法。该模


型通过融合词汇特征、汉字结构特征等多种语义信息，弥补了中文命名实体识别在语义信息上的缺失，并通过改进的Transformer编码器，提升了模型对中文句子的上下文编码能力。为了证明算法在中文流调文本时空信息识别任务上的有效性，本文在两个公开个数据集和两个自建的疫情流调文本数据集上进行对比实验和消融实验，充分证明了模型的实体识别效果。


第二，本文提出一种自适应参数的时空数据聚类算法。该算法基于数据集自身的分布特征，自适应的确定出合适的聚类参数，排除了先验知识和人为因素的干扰。同时，算法引入了时空域内核心点和噪声点的快速判断定理，并采用HNSW（Hierarchical Navigable Small Word）索引提高时空聚类的效率。本文通过对比实验证明该算法在聚类的准确性和运行效率上具有一定的提升效果。


第三，本文基于上述研究，对天津新冠疫情中的关联区域进行了实例分析。通过采用本文提出的融合多特征的命名实体识别算法和参数自适应的时空数据聚类算法，对流调文本中的时空信息进行提取和分析，得出此次疫情传播过程中的三组关联区域，为之后在疫情防控中制定针对性的预防措施提供参考。"
2023,利用辅助数据的社区问答实体链接,计算机学院,李宇涵,沈玮,NLP,0.323,社区问答平台包含大量的社区问答文本（包括一个问题与其相对应的所有答案），其中命名实体无处不在。在本论文中，我们定义了一个新的社区问答实体链接任务，即把社区问答文本中检测到的实体提及链接到其在知识库中的相应实体。这项任务可以促进许多下游应用，包括专家查找任务和知识库扩充任务。传统的实体链接方法主要关注面向新闻文档的实体链接，在社区问答实体链接这个新任务上难以取得理想效果。这是因为这些传统实体链接方法不能有效利用社区问答平台所提供的各种辅助数据来帮助链接，例如并列答案，以及主题标签和用户两类元数据。为了解决这一问题，我们提出了一个新颖的基于Transformer的实体链接框架，其可以有效利用不同种类的辅助数据所提供的丰富知识提升社区问答实体链接性能。该框架包含基础模块和辅助数据模块两个模块，分别用于计算传统实体链接特征和辅助数据相似度特征。我们基于Quora平台为社区问答实体链接任务构建了新数据集QuoraEL，并在该数据集上将我们框架与目前先进的实体链接方法进行了对比，实验结果验证了我们框架的优越性。为了探索目前流行的大语言模型在当前任务上的表现，我们设计了一个大模型评估方法，在零次学习和一次学习两个场景下对当前任务进行评估。
2023,基于三维重建和无监督点云分割的地面物料盘点研究,计算机学院,张焜,王恺,CV,0.3449,"近年来，三维场景重建被广泛应用于数字孪生、元宇宙、自动驾驶等领域。这些行业的飞速发展促进了三维重建技术不断迭代。对于工业应用场景中的三维重建需求，其场地规模巨大，应用场景复杂，导致数据采集困难，成本高昂。因此，提出一种高效、便捷的三维重建技术并且将其应用于工业场景下是一个巨大的挑战。 本文采用无监督视觉的方法，一方面对基于稀疏图像序列的三维场景重建方法进行了探究与改进；另一方面针对工业物料管理问题，我们研究三维点云分割技术，实现物料管理需求，并在真实的工业场景下进行了应用部署。


第一，过去的三维重建研究的重建图像样本很容易被移动设备采集，密集的图像序列可以用于三维重建，并获得了不错的重建效果。然而，在一些需要实时监控和确保现场安全的智能工厂中，只能布置固定点位的数据采集设备，移动设备无法适用。因此，利用稀疏图像序列进行三维重建，对于减少所使用的设备数量，从而降低图像采集成本具有重要意义。在本研究中，我们提出了弱映射增强方法（WEmap），用于改进基于稀疏图像序列进行三维重建的结果。在初始重建后，通过将三维点云映射到二维图像来评估每幅图像的贡献。对低贡献图像和相应的匹配图像进行加权，以增强初始重建中的薄弱区域。据我们所知，这是首次利用稀疏图像序列进行三维重建的研究。在稀疏DTU和稀疏Tank and Temple数据集上的实验结果表明，WEmap可以有效地增强三维重建的结构完整性。更进一步，我们在实际工厂中采集了一个数据集进行实验，验证了WEmap在稀疏序列下可以更完整地重建场景结构。


第二，为了提高物料管理的效率，现代工厂需要实现全自动库存检查。目前，一些相关工作已经提出基于视觉三维技术或激光点云进行盘库的方法，然而，由于缺少从3D点云中提取出材料区域的高性能方法，全自动库存检查仍然是一项具有挑战性的任务。为避免大量的样本标记工作，本文研究了一种无监督的地面目标分割方法U3DSEG。我们首先利用点云中元素的法向量信息来完成大部分地面点的初始移除。然而，由于应用场景规模巨大，且真实的地面凹凸不平，单纯使用法向量信息无法过滤所有非地面目标元素。因此，我们提出了一种D-DBSCAN（Dynamic Density-Based Spatial Clustering of Applications with Noise）来细化地物边界。实验结果表明，U3DSEG达到了最先进的性能，满足了全自动库存检查的要求。


基于此研究，我们在实际的应用场景下搭建了一套完整的系统，实现了全自动库存检查系统在化工企业的管理应用。"
2023,一种用于交通预测的时空注意力网络,计算机学院,杨璐,蔡庆琼,CV,0.317,"随着全球城市化趋势加快，解决交通拥堵问题成为当前各国城市交通系统建设与管理面临的巨大挑战。分析和预测动态交通状况变化对新时期智慧城市建设和交通管理具有重要意义。交通预测是一个经典的时空预测问题，其目的是在空间不同位置上，基于历史交通观测值预测未来若干时间步的交通状况。然而由于不同道路之间复杂的空间相关性和动态时间变化模式，实现准确的交通预测是一项极具挑战性的任务。


在时间维度上，交通数据受到历史时间步交通状况的影响，包含短期和长期时序依赖关系，而现有模型大多缺乏同时建模这种短期和长期时间相关性的能力。同时，交通序列具有周期性，体现在一天中存在早晚高峰时段，以及工作日与休息日不同的交通变化模式。在空间维度上，受限于道路网络结构，交通数据受到相邻节点交通状况的影响。同时，交通网络中相距较远的节点也可能具有相似的交通变化模式。因此需要同时建模节点在空间位置上的邻近性和交通数据的相似性。此外，现有模型大多对所有时间段、所有空间位置使用共享参数，但不同时段的交通模式很可能不同（如早晚高峰时段与非高峰时段），不同空间位置的交通数据分布可能具有较大差异（如城市中心与城郊），使用共享参数可能会忽略这种时间和空间异质性。为了解决这些问题，本文提出了一种新的交通预测模型，称为时空注意力网络（STAN）。该模型利用门控循环单元（GRU）和异质性感知时间自注意力机制（HA-TSA）分别建模短期和长期时间相关性，使用图注意力网络（GAT）和异质性感知空间自注意力机制（HA-SSA）分别建模局部和全局空间相关性。HA-TSA和HA-SSA在传统自注意力机制基础上为不同节点、不同时间步学习不同的参数，以捕获时间和空间异质性。另外，为了建模交通数据中的周期性，STAN将时间戳作为辅助信息拼接到交通数据原始特征上。STAN还添加了基于节点度数的中心编码，以建模交通网络中节点的重要程度。


在两个基准交通数据集上的实验结果显示，与最新基线模型相比，STAN在平均绝对误差（MAE）指标上平均降低了7.48%和7.77%，在均方根误差（RMSE）指标上平均降低了1.89%和5.19%。"
2022,基于区块链的云存储审计关键技术研究,计算机学院,阎萌,王刚,Security,0.35,"近年来，云存储服务在全球范围内广泛普及。在我国随着医疗云、政务云等 行业的快速发展，云存储服务已经跟每一位国民生活的方方面面息息相关。与 此同时，多云存储服务因其高安全性、高灵活性、去厂商锁定等独特优势逐渐 得到越来越多企业和机构青睐。云上存储的很多数据涉及个人隐私、商业机密 或是组织机构的重要信息。这些数据一旦丢失或破坏，将造成巨大的经济损失 和不良的社会影响。因此，提高云存储服务的安全性、可靠性和隐私性成为十 分重要的研究问题。


针对大规模云存储系统，远程数据审计是十分有效的保护数据完整性和提 升存储可靠性的方法。审计过程无需下载原文件便可快速验证指定数据的完整 性，并且可以反复执行。多年以来，国内外学者在这一领域进行多方面研究，提 出了很多方案，但在审计的可信性和实用性方面一些难点始终尚未解决，具体 表现在如下三个方面。(1)审计可信性。目前采用的基于第三方的中心化审计 模型导致审计规则不公开、审计过程不透明、审计结果无法验证，因而无法从 根本上保证审计的安全可信。(2)审计可行性。目前提出的基于区块链的分布 式审计方案虽然可以部分实现审计可信性，但区块链本身存储可扩展性差的问 题成为阻碍审计区块链落地的一大难点。(3)审计机制评价。可靠性评价方法 能够量化分析审计机制对提升存储系统可靠性的贡献以及不同审计策略的实际 效果，从而为审计机制设计者提供理论指导。然而目前很少有学者关注对于云 存储审计系统的可靠性评价。


本文面向大规模云存储系统，基于区块链技术研究安全可信高效的远程数 据审计方案，主要工作包括:


第一，设计规则公开、过程透明、结果可验证的分布式审计方案，依靠算 法、协议而不是第三方保证审计的可靠可信。为彻底解决传统中心化审计的单 点故障问题，需要设计去中心化审计模型，构建分布式审计网络。而如何保证审 计频次的稳定并避免审计者恶意行为是其中两大难点。本文提出定期执行、不 可预测且公开可验证的审计者选举机制解决上述问题。针对传统审计规则和过 程不公开、结果不可验证的问题，基于智能合约技术实现审计算法和协议，使得审计规则以及审计运行过程全部记录上链，向所有用户公开，由此实现透明 可监管的审计。为避免审计记录上链引发的隐私泄露风险，基于零知识证明技 术设计具有隐私保护性质的审计协议，从而在去中心性、公开性和隐私保护之 间取得平衡。


第二，设计安全、高效、高可扩展的审计区块链系统。审计记录上链是保 证可信审计的关键，而如何维护链上数据成为新的难点。由云存储用户(即审 计者)作为区块链节点维护链上数据可以避免使用外部区块链网络引发的安全 隐患，保证审计记录可靠存储。但是如何降低节点存储开销，同时不损害区块 链数据可靠性和访问性能成为一大挑战。本文提出具有分层结构且实现存储模 式优化的审计联盟链系统。根据用户持有资源多少的不同在网络中划分出三类 区块链节点，并为每类节点分配不同等级的任务和权利。设计动态弹性存储分 片方案使得每个服务节点只需存储所有区块头以及一部分区块，并通过节点间 高效协作来访问本地未存储的区块。根据实时访问热度动态调整区块副本个数、 存储位置和存储模式，从而在节点存储开销、数据访问性能和数据可靠性之间 取得平衡。


第三，设计适用于大规模多云审计系统的可靠性评价方法。考虑到多云存 储系统结构的复杂性以及故障发生原因的多样性，本文从系统整体可靠性入手， 着重分析单个云服务商的可靠性机制失效导致多云存储系统发生数据丢失的情 况。为客观细致评价远程数据审计机制的运行效果，建立马尔可夫模型模拟系 统故障发生和修复的动态过程，对比分析不同参数下无审计机制的多云系统与 运行审计机制的多云系统可靠性，并对比分析不同审计策略对系统可靠性的提 升效果。设计蒙特卡洛仿真程序以验证和辅助模型分析结果。结合网络流量开 销、网络规模等多角度分析，给出恰当的审计方案设计建议，从而在系统可靠 性、数据安全性和审计机制开销之间取得平衡。


总之，本文提出了实现审计可信性的分布式审计框架以及实现隐私保护的 分布式审计协议，提出了审计联盟链系统模型以及区块链存储优化方案，提出 了适用于多云存储系统的审计机制评价方法。上述研究成果从不同角度加强了 远程数据审计机制的安全性和可行性，能够有效促进区块链审计技术在大规模 云存储系统的实际应用。"
2022,智能边缘系统中QoS优化关键技术研究,计算机学院,刘文文,王刚,Network,0.3258,"近年来，物联网及人工智能技术的不断发展与融合使万物互联的趋势越来越明显。无人驾驶、智能环境监测、智能健康管理等众多新领域的涌现对保证应用的QoS提出了更高要求。在此情况下，智能边缘计算系统将部分AI任务卸载到接近数据源的多个分布式计算体上运行，旨在解决云计算中服务延迟高、计算负载大、可靠性低等问题。相比于其他应用场景，智能边缘计算场景中的任务在QoS性能上对实时性、计算负载、可靠性的要求更高，这些特性为相关研究带来了诸多挑战。


    本文主要关注“云-边-端”三层架构中对低延时、低能耗、高可靠性要求的相关工作，分析了目前还需进一步解决的问题，具体表现为：第一，端层无线网络通信在实时性、可靠性、计算能耗方面效率不高。已有无线网络优化解决方案，特别在无线传感器网络中，在保障网络低能耗的同时，很难高效地降低数据传输延迟和提高数据传输率。第二，边缘层中基于干扰感知的任务分配策略在实时性、资源利用率上的效率有待提高。AI任务在计算体中运行时，一方面任务间的资源竞争会影响计算体的运行性能，另一方面边缘计算场景中不同任务的资源竞争情况在不同资源配置中是不同的，且资源竞争趋势也呈现出动态波动情况，因此探索资源竞争环境下实时保证较高的用户个性化体验质量的任务分配策略更具挑战。第三，“云-边-端”系统中边缘推理容错模型在实时性、准确性、高效性上有待提高。边缘推理 AI 任务在运行过程中时常会遇到网络中断、服务器故障等不可控状况，这些状况往往会导致响应结果不能及时准确地传递给用户，影响用户体验质量。因此，在实时性的要求下实现准确高效的容错计算成为智能边缘推理系统面临的又一挑战。


    为了解决上述问题，本文主要面向智能边缘计算应用场景进行了如下几方面的探索和研究。


 第一，在端层中的网络传输方面，提出了基于多目标优化的边缘无线传感器网络的高效传输方法。区别于典型的网络优化算法，本文在现有无线网络协议 IEEE 802.15.4 的基础上，推导出数据传输能耗、时延、传输成功率的量化计算模型。并以三者为优化目标构造了多目标优化模型。通过设定相关参数并求解多目标优化模型，可得出在满足时延和可靠性要求的前提下，最小化能耗的传输方案。测评实验验证了量化计算模型的准确性和多目标优化模型在传输能耗、时延、传输成功率之间权衡的有效性。


    第二，在边缘层中的任务分配方面，提出了基于强化学习的智能边缘系统中干扰感知任务分配方法。区别于典型的任务分配模型，本文首次探索了边缘环境中基于干扰感知的任务分配策略。首先，量化了性能干扰因素并构造了基于Prophet的任务性能下降率预测模型，实现了提前预测待分配任务与服务器内其他任务间竞争某一单独资源导致的性能下降率。然后，构造了关于时延、能耗和任务间性能干扰的多目标优化模型。最后，利用深度强化学习网络模型对目标优化模型求解，得出在资源竞争环境下能实时保证较高服务器利用率的任务分配策略。测评实验验证了预测模型的准确性，平均精确度为92.89%，最高精确度为98.96%；还验证了分配策略实现了任务性能下降率、执行时间和资源利用率间的有效权衡。


    第三，在边缘推理系统中的容错计算方面，提出基于计算编码技术和知识蒸馏方法的边缘AI任务高效容错解决方案。与典型的容错模型相比，本文以低延迟、低计算负载、高准确率的性能解决了边缘推理系统中多节点容错问题。该方案针对边缘AI推理任务因故障导致长尾延迟及现有故障恢复方法准确率不高的问题，首先利用计算编码方法建立了校验重构模型。然后，针对校验重构模型体积较大、计算成本较高的问题，利用知识蒸馏方法对其进行压缩和知识提炼。测评实验验证了优化后的校验重构模型在恢复时间、准确率和额外开销间达到了很好的权衡。


综上所述，本文为“云-边-端”三层架构中的QoS优化问题提供了低延时、低能耗、高可靠性的性能优化方案，总体上实现了端层无线网络高效数据传输、边缘层干扰感知任务的高效分配以及“云-边-端”中边缘推理请求任务的高效容错，在边缘智能场景中具有一定参考价值。"
2024,口令强度评估关键问题研究,网络空间安全学院,单轩,贾春福,Security,0.2847,"身份认证是阻止非法访问和保护用户信息安全的第一道屏障，如何对用户的身份进行认证是在开放环境下实现安全通信必须首要解决的关键问题。口令，凭借其便捷易用、容易修改、成本低廉等特性，在可预见的未来仍将是最广泛使用的身份认证方法。互联网和物联网技术的快速发展使得网络实体数量和类型迅速增长，越来越多的服务需要通过口令进行保护。与此同时，攻击者能力不断提高，催生了大量的新型攻击手段，使得口令安全面临更加严峻的威胁。


确保用户口令能够达到所需的强度水平，是保护用户账户安全的关键措施之一。几乎所有的互联网服务提供商都会在用户注册或者更改口令时执行口令强度评价（PasswordStrengthMeter，PSM），旨在协助用户选择安全的强口令。本文采用数据驱动方法，结合严格的理论分析和大规模实验评估，从口令强度评价的准确性、口令集安全性评估以及可适用于强度评估的口令树模型三方面，层层递进，对口令强度评估这一关键问题进行深入研究，主要完成了以下工作：


1. 口令强度评价器准确性评估框架。基于对口令在线猜测和离线猜测中各种策略特征的深入分析，本文首次提出了一个PSM评估框架，包含四个不同维度的评估指标，用以度量不同猜测场景和猜测策略下PSM的准确性。通过对12个领先PSM的大规模对比分析，本文发现现有的PSM均无法在所有的攻击场景（策略）下始终表现最佳，进一步提出了三条提高口令强度评价准确性的有价值建议，为网站服务商选择合适的PSM提供了指导，并为PSM的改进指明了可行方向。


2. 基于Levenshtein+距离的口令集安全性分析。本文提出了一个基于编辑距离的口令相似性度量指标：Levenshtein+距离。相比于原始的Levenshtein距离，Levenshtein+距离因特别考虑了子串换位而更贴近用户的真实编辑行为。利用Levenshtein+距离，本文将口令数据集转换为网络图，并通过实验说明了网络密度作为量化口令集安全性指标的合理性。进一步，本文构建了最优字典攻击者能力假设，确定了评估单口令抗字典攻击能力的关键指标PageRank，并揭示了不同猜测场景下单口令安全性的显著差异。


3. 基于树模型的口令猜测算法。基于对用户真实口令构造特征的深入分析，本文首次尝试将口令建模为树。相比于传统的序列模型，树模型能够更深层次地揭示口令内部的语义联系以及用户在构建口令时所遵循的思维逻辑。基于树模型，本文进一步设计了漫步猜测算法PassTree_Trawling与定向口令猜测算法PassTree_PII，并通过大规模实验评估说明了这两种猜测算法的有效性，进而证明了其应用于口令强度评估的可行性。"
2022,面向复杂场景的多层次目标检测与分割,计算机学院,吴宇寰,程明明,CV,0.4217,"目标检测与分割是计算机视觉领域最重要的研究领域之一，它是诸多下游应用的基础，它的任务是检测与分割感兴趣的目标。然而，目标的尺寸、形状、颜色、位置以及所处的环境都是复杂多变的，这些难题就使得目标检测与分割成为计算机视觉最具有挑战的问题之一。


设计面向复杂场景的目标检测与分割算法主要有以下挑战：（1）现有的算法对目标的定位能力不够高效，更多考虑了如何完美地恢复目标具体的细节，而忽略了对目标的定位能力；（2）现有的算法多通道特征融合的效率低，使得算法的实时性受到了限制，难以应用到体积小、功耗低的手机等移动设备上；（3）许多场景下的数据精细标注获取难，使得算法的泛化能力受到了限制。（4）现有的算法多尺度建模的效率低，难以胜任复杂场景下的目标检测与分割任务。


为了解决以上不足，本文提供了不同的改进方案来解决以上四大主要挑战难题。具体的研究内容和主要贡献如下：


1. 本文提出了极致下采样技术，聚焦于复杂场景中高效目标定位难的挑战。它不断地对特征下采样直至其变为一维特征向量，来学习图像的全局视图，从而使算法获得强大的全局先验，在消除模型对目标定位所需高分辨率要求的同时提升了目标的定位精度。该技术应用于显著性目标检测中，在五大主流数据集上进行了评测，并与目前主流的方法进行了性能对比，新算法取得了最佳性能。


2. 本文提出了用于多通道高效特征融合的隐式信息恢复技术，聚焦于现有算法多通道特征融合效率低的挑战。隐式信息恢复通过在输出端对感兴趣的信息进行重建恢复，在最粗糙的层次上进行融合就能达到较好的效果，大幅提升了特征融合的效率。该技术应用于RGB-D显著性目标检测中，在六大主流数据集上与其他方法进行了比较，新的算法在相比其他方法提速15~150倍的情况下取得了相当的性能。


3. 本文聚焦于复杂图像精细标注难以获取的挑战，提出了基于注意力融合的二元感知技术。注意力融合帮助算法使用分类、分割的二元感知来充分利用了更多的数据，特别是在低对比度的情况下，能够更精准地检测目标区域。


4. 本文聚焦于现有算法多尺度建模效率低的挑战，提出了基于金字塔池化的骨干特征提取技术。它非常高效，在降低模型计算复杂度的同时提升了多尺度特征表达能力。通过该技术，本文构建了一整套全新的骨干网络，并应用于语义分割、物体检测、实例分割等多个经典的目标检测与分割任务，在多个主流数据集上与其他骨干网络进行了对比，都取得了最佳性能，同时保持了较小的网络参数量和计算量。此外，该技术与本文提出的极致下采样、隐式信息恢复、二元感知等技术相结合，还能进一步大幅提升算法性能。"
2024,边缘辅助下的云游戏视频传输方法研究,网络空间安全学院,刘佳瑶,张建忠,CV,0.222,"近年来，在众多游戏种类中云游戏市场规模的增长尤为显著。云游戏使得游戏玩家在无需额外硬件配置的情况下，可以享受高质量的游戏体验。然而，不同于传统游戏及视频点播服务，云游戏服务提供商需要向玩家传输高分辨率、高帧率的游戏视频流，同时要满足玩家低延时体验需求。这对网络环境提出了更加严格的要求，即云游戏需要持续稳定的高带宽来负担高质量的游戏视频流。但在高度异构的现代互联网中，用户数量、移动性、网络拥塞、信道衰落等因素都会使网络发生波动。这导致了云游戏应用的卡顿、模糊等问题，进而导致游戏体验的下降。因此本文分析了当前实时交互视频应用的相关工作，提出了边缘辅助下的云游戏视频传输方法，以提高云游戏的游戏体验。论文的主要工作如下：


首先，本文提出了基于自适应比特率的两阶段编码决策算法。该算法在第一阶段根据实时的网络状态执行基于强化学习的自适应比特率算法，通过网络感知获得适合当前网络状态的最优比特率。在第二阶段根据上一阶段确定的比特率进行游戏视频流编码的时空决策，在细粒度上调整游戏视频流的帧率，在粗粒度上调整游戏视频流的分辨率，有效减少了传输数据量和引入新的关键帧个数。并且本文对两阶段编码决策进行了实验验证，通过性能评估和消融性实验证明了算法的有效性。


其次，本文提出了基于视频增强的帧恢复方案。帧恢复方案有效弥补了由于两阶段编码决策降低帧率和分辨率导致的游戏视频质量快速下降的问题。本文对现有的超分辨率模型和帧插值模型进行了优化，在保证模型推断质量的前提下，有效降低了模型推断时间，可以适应低延时的实时交互式视频应用。同时保证了稳定的高帧率、高分辨率的游戏体验，有效避免了由于网络波动导致比特率频繁切换所造成的视频质量不稳定的问题。并且本文对两种视频增强方法分别进行了实验验证，通过实验证明了超分辨率后插帧的可行性。"
2023,基于深度学习的医疗辅助诊断方法的研究及应用,计算机学院,杨婷婷,蔡庆琼,CV,0.3116,"数字医学领域有大量医疗数据，通过对临床数据进行分析归纳，并借助计算机辅助诊断，可以有效帮助医生做出准确的临床诊断、预后评估和治疗决策。深度学习凭借强大的特征提取能力，在医疗数据分析中起着关键作用。电子医疗数据通常包括数值特征和分类特征，具有多样的表现形式和特征属性。数值变量和类别变量的混合输入会导致深度学习模型难以提取数据中的特征信息从而影响模型精度。此外，大量的医疗数据需要采用适当的方法和工具来高效地提取和处理。现有的医院信息系统缺少数据分析和决策功能，难以借助深度学习算法进行辅助诊断。针对上述问题，本文进行了如下研究工作：

  首先，预处理数据集并设计基准模型。本文针对心脏病和慢性肾脏病原始数据进行清洗和预处理，并设计多类基准模型进行分类预测。

  然后，模型设计与预测分析。针对数值特征和类别特征混合的电子医疗数据，本文提出数值类别双路分类框架（NCBCF）。NCBCF 采用基于特征的自注意力机制和一维与二维卷积神经网络相结合的模型来提取数值变量的特征矩阵，用嵌入层来提取类别变量的特征矩阵，并将两个特征提取器提取得到的特征矩阵进行拼接，使用全连接神经网络进行分类。NCBCF 模型和基线模型相比，在心脏病数据集上准确率、精确率、召回率和F1 分数分别平均提升了6.8%、7.9%、5.9% 和7.3%，在慢性肾脏病数据集上四个指标分别平均提升了2.0%、5.0%、12.9% 和9.9%。此外本文还采用平衡类别的优化算法来提高找全少数类样本的准确率，并使用机器学习模型探究前两年临床参数平均水平和eGFR 变化率对终末期肾病发展的影响。

  最后，开发医疗数据管理及分析系统。基于本文提出的数据预测模型，设计并实现了医疗数据管理及分析系统，可用来管理患者数据信息、分析患者的指标、辅助临床诊断、关注临床变量间的因果关系。以数据库模块为支撑管理数据，以算法模块为技术核心提供数据分析预测的功能，以客户端GUI 模块为窗口与用户进行交互。系统功能完整，操作简便，数据管理规范高效，实时更新。"
2024,虚拟网络与物理网络融合的多主机容器网络优化及实现,计算机学院,达益鑫,徐敬东,Security,0.2461,"随着 AI 大模型等多元化复杂应用的快速兴起，高性能分布式云计算系统架 构的重要性愈发瞩目，各类应用对计算资源的高效利用提出了严苛的要求。一 系列的研究表明，容器网络隔离性所导致的资源浪费和性能损耗构成了一个严 峻的挑战，问题的核心是容器隔离性引发的数据包过长的传输路径以及繁多的 路由转发行为。尽管现有的工作从设备架构、调度算法、通信形式等方面对该问 题进行了研究，但难以应对云计算中扩张的集群规模和应用需求。为此，针对 该研究问题，本文在对国内外相关工作进行了研究的基础上，完成了以下工作。


本文首先提出了一种虚拟网络与物理网络融合的多主机容器网络优化方案 ——FOUD。针对容器网络与物理网络隔离性带来的控制流程分离及信息独立的 情况，该方案利用可编程硬件与内核可编程技术，通过非侵入式、可插拔式的设 计，实现了虚拟及物理网络组件模块来完成容器网络信息卸载到物理网络设备 的流程以及同步各个组件控制信息和状态信息的任务，在架构上融合了虚拟网 络及物理网络的控制逻辑，进而操控二者数据面组件进行转发。针对主机内核 中因容器隔离性引入的复杂路由转发行为所带来的资源消耗问题，FOUD 首先 将容器网络信息卸载到物理设备中，然后结合基于全局信息和 EWMA 的全局负 载均衡方案，让各个节点内核中大量的路由转发操作转移到可编程交换机，进 而降低了节点的资源消耗。接着针对容器网络传输过程因隔离性所带来的处理 路径过长的性能损失问题，FOUD 借助 eBPF 技术设计并实现了 CNI 插件，结合 全局网络信息所作出的决策，对单主机和多主机两个场景提出优化方案，缩短 了容器网络数据传输的路径长度，进而减少了网络性能的损失。


仿真实验表明，本文提出的优化方案能够有效减少单主机与多主机容器间 的数据传输路径长度，同时降低整个集群内部计算资源消耗，相较而言削减了 节点约 25% 的 CPU 占用率，并维持较低的内存消耗，在服务响应时间上缩减了 平均 50% 的时延。"
2023,基于自训练的糖尿病视网膜病变半监督学习方法研究,计算机学院,刘新慧,王恺,CV,0.3863,"眼底图像可以反映患者糖尿病视网膜病变（Diabetes retinopathy, DR）的具体情况，使用深度学习的方法对眼底图像进行分析可以辅助医生进行高效诊疗。现有眼底图像分析方法无法有效应对一些挑战：大量无标注数据无法得到有效利用；分类训练集中存在的类别不平衡问题；以及病变分割任务中存在的尺度不一致问题。本文针对糖网病变分类和分割任务分别设计两种基于自训练的方法，以解决上述问题。具体的研究工作如下：


首先，为了实现对无标签数据的利用并解决类别不平衡问题，本文设计一种基于一致性的类平衡自训练方法（Consistency based Class-Balanced Self-Training, CCBST）。使用一致性作为度量标准来衡量伪标签的可靠度，筛选可靠程度高的伪标签扩充训练集，同时采用类别依赖的筛选策略（Category Dependency Screening, CDS）来确定选择的伪标签数量，以达到平衡不同类别数据的效果。在三个数据集上进行实验，表明CCBST方法可以有效解决上述问题，提升DR分类模型性能。在DDR数据集上使用CCBST方法，以MobileNetV3-L、MixNet-M和EfficientNet-B0为骨干网络分别可以获得5.46%、6.1%和4.29%的性能提升。


其次，针对像素级分割标注图像不足以及病变尺度不一致问题，本文设计了基于尺度感知注意力的自训练方法（Scale-aware Attention based Self-Training, SA-ST）。SA-ST采用自训练的框架，包含两阶段重训练。在伪标签的筛选阶段，设计一种基于尺度信息伪标签可靠度预测方法（Scale Information based Reliability Prediction, SIRP），选取高可靠度伪标签扩充训练集并进行第一阶段的重训练。利用重训练得到的模型更新剩余伪标签，并再次扩充训练集进行第二阶段的重训练。实验结果表明，SA-ST方法在两个公开病变分割数据集上都可以有效提升模型的性能。在IDRiD数据集上，以MobileNetV3-L、MixNet-M和EfficientNet-B0作为骨干网，使用SA-ST方法与基准网络相比可以提升4.4%、3.64%和4.89%。与其他方法相比，SA-ST也可以获得更优的性能。"
2019,云环境下物联网数据安全的关键技术研究,计算机学院,闫红洋,贾春福,Security,0.343,"随着网络技术和信息技术的飞速发展，物联网（Ineternet of things，IoT）的概念从被提出，便受到人们的广泛关注。由于物联网终端运算能力存在瓶颈，物联网终端用户必须依托云平台对私有数据进行存储和处理，因此云平台对用户数据拥有绝对的控制权，这会对数据中隐私信息(如身份证号、医疗记录等)的完整性和机密性造成极大的威胁。保护用户数据最直观方法是在数据被外包存储在云平台之前对数据进行加密，但是这种方法给后续的数据可用性（如，数据检索和数据去重）带来困难。因此，在保护数据隐私的同时，不得不面临一个尖锐的矛盾，即在保护数据中敏感信息的同时确保数据在后续数据处理过程中的可用性。

    本文针对云环境下物联网数据外包时所面临安全威胁，研究如何构建物联网数据安全存储和处理技术。研究内容包括数据访问控制、数据检索和数据去重等方面的安全问题，并致力于设计能适应于物联网终端设备的数据安全存储方案。本文的主要研究成果如下：

1）云环境下物联网数据安全访问控制。针对物联网中恶意终端或应用对数据的越权访问问题，提出了基于功能的安全访问控制方案，即 IoT-FBAC 方案。该方案实现了对物联网终端的数据细粒度的访问控制。IoT-FBAC 将物联网中的终端设备按照功能划分，用户对注册的 APP 进行权限授权并设置访问策略，防止非授权的 APP 对用户数据进行越权访问。安全性分析表明，IoT-FBAC 在确保数据隐私的前提下，最大限度地实现了设备功能的灵活细粒度访问。性能评估表明平均每次访问时间将基于 ACL 访问控制方案的时间复杂度 O(log2N) 降为常量级，在物联网环境下更具有实用性。

2）云环境下物联网数据安全检索。针对检索外包图像数据时所面临的数据泄露和越权访问问题，提出了适应于物联网应用的安全相似图像检索方案，即SSIR 方案。在该方案中，图像集以加密向量的形式存储在云服务器中，云服务提供者无法获取有用的图像信息。借助 Intel SGX 技术所提供的隔离环境，实现了数据的明文检索，提高检索准确度的同时保证了明文数据不被云服务提供者或其他攻击者获取。实验结果分析表明 SSIR 方案在要求高精度检索应用中更具有优势。针对视频数据检索问题，本文第一次提出了安全的视频检索框架，使其在保证视频内容隐私的前提下，实现检索功能。

3）云环境下物联网数据安全去重。针对云平台中数据去重时所面临的数据隐私威胁，提出了适应于物联网环境的安全视频存储和去重方案。本研究工作通过分析统计信息泄露，并结合 SGX 技术、收敛加密技术等，提出了安全的块级视频去重方案，解决统计信息泄露问题。实验结果分析表明与现有的视频去重技术相比，该方案在多视频的去重操作下具有优势，能够更大限度降低存储空间消耗。"
2019,基于B/S的就业指导系统的开发,计算机学院,李思佳,刘晓光,Security,0.2872,"摘要


开发一套功能强大，性能稳定的就业指导管理系统，将大幅度提高学校对于毕业生信息管理的工作效率，促进学校和企业合作，给提升学校就业率提供良好的平台。本文主要是力求通过开发和使用该系统，达到提高信息的完整性和可靠性，提升学校管理水平和信息化建设的目的，满足学校、企业、学生三方面的需要。通过这个平台，一方面能方便广大学生了解用人单位信息，企业的招聘会信息，在同一时刻，学校也可以很好的了解就业的情况，这样毕业生的情况就能够更好的被了解，这些方面的进步可以提高学生管理水平、教务管理水平、对于实现就业指导的信息化具有重要意义。 本文详细阐述了就业管理系统的设计和实现，描述了基于ASP中的Web开发的相关技术，为基于B/S的系统开发提供了理论依据；系统的开发应用了MVC分层开发的思想和Ajax异步提交表单技术。在业务逻辑的分析上，合理的分析和科学的宏观调控，兼顾各个角色和各个部门的需求，保证系统的合理性和就业信息的安全性。


关键词：就业管理；B/S架构；MVC；异步交互


分类号：TP311.1"
2020,面向区块链的微支付通道路由研究与实现,计算机学院,蔺慧霞,张建忠,OS,0.2383,"区块链技术实现了匿名去中心化的应用，改变了传统支付系统需要第三方信任机构支持的现状。然而当前区块链系统处理交易的速度尚不能满足现实需要，闪电网络扩容方案将小额交易放在链下进行，减少用户与主链的交互次数，使系统能容纳更多交易，解决了区块链扩展性问题。用户在链下建立支付通道进行转账，没有直接建立通道的用户需要其他中间节点帮助转发交易，激励用户提供服务的机制是交易发送方向中间节点支付一定的费用。但目前闪电网络系统中产生的交易金额中服务费用占比达到5%，而这些费用都需要用户来承担。为此，研究如何在支付过程中降低服务费用有重要的现实意义。


本文分析区块链结构和闪电网络相关技术，结合链下支付路由原理，提出基于费用最小的微支付通道路由方法，模拟交易过程中数据的更新与传播，统计实验结果。本文的主要内容包括：


(1)  介绍了区块链及其相关技术。首先介绍了区块链的结构、技术架构、发展历程和不同类型区块链的特点，其次研究总结出现有的扩容方案涉及的方法和技术。


(2)  分析闪电网络的微支付通道技术，设计并实现支付路由系统。针对现有路由方法的不足并结合洋葱路由机制提出基于费用最小的路由方法，包括平衡服务费用、建立全局路由信息和减少路由开销。为用户创建路由表，结合全局路由信息和局部路由信息选择费用最小路径。按照用户发起交易请求、接收方作出应答、转发交易并广播更新路由信息这一过程完成整个交易。交易的验证在支付通道内进行，根据Schnorr签名算法完成点对点转账。


(3)  分析WS网络模型创建用户网络，通过仿真实验对比基于路径最短的传统路由与本文提出的费用最小路由方法，统计中间用户数目、服务费用和服务费占交易额的比例，分析实验结果表明文本提出的基于费用最小的微支付路由方法具有可行性。"
2020,视觉感知任务中神经网络损失函数设计研究,计算机学院,赵凯,程明明,NLP,0.2879,"卷积神经网络作为计算机视觉领域的一个核心工具，已经被广泛运用于图

像分类、语义分割、人脸识别、目标识别与跟踪、显著性物体检测等很多计算机视觉任务中。神经网络的训练高度依赖基于梯度的优化算法，在优化过程中，输出端在前向传播时计算神经网络预测的误差；在反向传播时计算误差对网络预测的梯度，然后通过反向传导算法将梯度逐层反传，并利用链式法则计算误差对各层参数的导数。最后用梯度下降法等算法将网络参数往误差下降的方向迭代更新，直至收敛。由于损失函数在神经网络优化中直接产生梯度，是梯度的来源，因此在网络的优化中处于十分重要的地位。基于交叉熵的多分类损失函数最开始主要用于图像分类任务中，后来逐渐被应用到多个任务中用作损失函数，因为很多任务例如人脸识别，显著性检测，语义分割等都可以视为特殊的分类任务。作为分类任务的损失函数，交叉熵损失并不考虑考虑样本标注的置信度，也没有考虑多个分类类别之间的语义关联性等因素。同时应用到其它非分类任务中时，交叉熵损失只寻求模型在训练集上的分类误差最小化，并没有针对性优化特定任务的评价指标。例如人脸识别旨在于最大化不同类样本特征之间的距离的同时最小化同类样本的距离；而检测结果的对比度和完整性对显著性检测十分重要，最小化交叉熵损失并不能针对性地提高这些和任务紧密相关的评价指标。因此，针对不同任务对性能指标的不同需求来个性化地设计神经网络的优化目标对模型性能十分关键。

本文的研究内容以神经网络的优化目标为中心展开，以计算机视觉任务为

落脚点，针对几个典型的计算机视觉任务对模型表现的不同期望，设计了任务相关的损失函数。针对显著性检测任务，本文针对当前主流显著性检测方法中物体边界模糊现象，设计了基于F-measure的损失函数。该损失函数在整个可行域具有显著梯度，可以迫使网络输出两极分化的预测值，显著增强显著性检测结果的对比度，有效解决物体边界模糊问题。针对人脸识别任务，本文提出了“互斥正则”损失函数，该损失项可以显式地增大不同类别间特征的离散程度，从而增强模型对人脸照片的辨识度。针对卷积神经网络压缩任务，本文提出了一种自适应的L1 稀疏正则损失，该正则损失函数可以在训练过程中自动控制模型稀疏性，以便在压缩剪枝阶段最小化对模型性能的影响。"
2021,基于分块技术的360°视频多源自适应流方案研究,计算机学院,云瑞琳,徐敬东,OS,0.2414,"近年来，360° 视频流传输研究吸引了各界的广泛关注。360° 视频是通过头戴式设备近距离观看，需要高码率、高分辨率的视频内容才能保证用户体验质量。一般来说，360° 视频流传输需要百兆及以上的带宽，而当前网络中端到端 带宽在几十兆左右，这给传统网络架构和单源自适应流媒体策略带来了压力和挑战。基于分块技术的自适应流传输是360° 视频流的主要技术方案，该方案通过仅传输用户视域内的高码率分块从而降低流传输的带宽需求。但是，用户视域预测的不准确性会导致用户视域出现黑块或者模糊画面。为保证用户体验质量，通常客户端会请求用户视域预测结果的超集，这导致单源流的带宽供应不能满足传输需求。随着5G网络和边缘计算的蓬勃发展，边缘云支持资源协同和提供更快的响应服务。因此，基于多服务器协同服务的多源流媒体架构具有很好的应用前景。基于5G边缘云网络架构，本文围绕 360° 视频多源自适应流传输方案展开研究，主要工作如下：


(1) 提出了一种联合码率选择和请求调度策略，该策略旨在最大化系统效用，即提高用户体验质量并降低网络服务开销，同时满足流服务完整性和传输延迟约束。为了有效地解决这个非线性整数规划问题，本文对问题的控制变量进行解耦，然后基于拟阵理论设计主从算法以求得问题的最优解。此外，本文实现了基于深度学习的码率选择算法，在保证系统效用的同时降低算法运行时间。本文通过大量数据驱动的仿真实验对上述两种求解方式的性能进行评估。


(2) 提出了一种具有成本效益的联合请求调度策略，该策略旨在最小化系统开销 (即连接成本、传输成本和切换成本)，同时满足服务完整性和等待时间约束。这是一个非线性整数规划问题，由于在目标函数中存在与视频段相关的切换成本项，该问题也是一个在线问题。为了有效求解问题，本文设计了一种新 颖的在线近似算法。首先对整数变量进行松弛并引入相关熵替换切换成本项， 并将问题解耦为多个与视频段无关的可独立求解的子问题；接下来利用随机依赖舍入算法对小数解进行舍入，最后得到近似解。本文通过数学分析对在线近 似算法的性能进行讨论，并展开大量实验对算法性能进行评估。"
2021,基于纹理特征的医学图像篡改检测与定位研究,计算机学院,史嘉琪,王刚,Robotics,0.2509,"如今，数字化时代已经到来，医疗信息管理也向数字化转型，远程医疗出现在大众视野中，给患者就医提供了便利。在远程医疗系统中，患者的个人信息、医学影像等敏感信息以数字化的形式传输。医学图像作为诊断的重要依据，真实性需要得到保障，视觉质量不能受到任何影响。如果病变位置被恶意遮盖或者扩散，都会导致医生误诊，带来难以挽回的损失。但医学图像在网络传输过程中，有可能会遭到恶意的拷贝和篡改攻击，目前图像编辑技术已经达到了以假乱真的程度，仅仅凭借肉眼无法识别图像的真实性。而且现在鲜有专门针对医学图像的篡改检测方法，现存算法的精确度也不够理想。因此本文充分挖掘医学图像的纹理结构，基于纹理特征对医学图像进行篡改检测和定位。


       本文提出了一种针对复制—粘贴篡改手段的医学图像篡改检测与定位算法。该算法包括特征提取、特征匹配和篡改定位三部分，结合基于图像块和基于特征点的技术，实现了对医学图像单复制—粘贴区域和多复制—粘贴区域的篡改检测与定位。算法重点关注医学图像的纹理特征，利用纹理描述符获得更准确的特征。为了在平滑区域中提取到足够数量的具有高辨别力的特征点，算法在纹理图像中提取SIFT关键点并降低了对比度阈值。同时通过减少特征维度和改变特征点相似性度量策略，加快了特征匹配速度。实验表明，该算法的精确度和鲁棒性都可以与现有算法相媲美。


       鉴于上述方法只能检测复制—粘贴篡改，本文提出了另一种普适性更高的算法，可以针对任意篡改攻击。算法首次采用主动篡改检测和被动篡改检测相结合的方式，主动篡改检测利用可逆水印实现对医学图像篡改区域的模糊定位。具体地，将医学图像划分为中心区域和边界区域，分别嵌入特征信息和辅助信息。水印检测到的篡改区域为后续的基于神经网络的被动篡改检测提供了一种特殊的注意力机制，让网络可以更加关注这些区域。被动篡改检测阶段还利用了知识蒸馏技术，其中教师模型和学生模型均以U-Net作为骨干网络，这样可以将低分辨率特征和高分辨率特征融合起来，使图像篡改定位的性能更加优异。算法通过“教师—学生”训练模式，用轻量级网络实现了医学图像的精确篡改定位。"
2021,区块链中面向隐私保护的用户身份管理研究,计算机学院,邵蔚,贾春福,OS,0.2396,"比特币已成为全球影响力最大的加密数字货币。它采用的区块链技术因其

安全性和其建立的全新的分布式信任模式受到广泛关注。区块链技术综合了一系列安全机制和策略，如数字签名、共识算法、哈希指针链等，以达到不可伪造、高效可信等安全特性。区块链上数据的最大特点之一在于它对其用户公开透明。然而，这一设定却为区块链用户的隐私带来挑战，也给区块链的管理提出了新要求。目前，区块链面临着链上数据可信度、用户隐私保护和系统监管之间的权衡。因此，如何让区块链技术兼顾三者成为研究热点。目前，在不同场景中这一问题还没有成熟高效的解决方法，因此，本文从区块链中的用户身份隐私问题研究、隐私保护的区块链用户身份管理设计和增强隐私区块链在实际场景中的应用三个方面全面剖析区块链中的用户身份隐私保护和管理问题。经研究，本文主要成果在于在准确了解区块链用户隐私和管理缺陷的前提下针对性地提出解决方案，并将之应用于实际，从而在根本上提高区块链技术的实用性。

1、基于深度学习的链上身份识别和隐私破坏。 大多公有区块链结构采用基于公钥的假名地址机制保护用户匿名性和交易隐私。然而，这种机制在用户隐私保护和管理方面存在缺陷。本研究提出了一个基于深度学习的地址­用户映射方法，它能通过学习寻找到用户地址的交易特征并准确地找到它们与其拥有者真实身份之间的联系，达到区块链用户识别的目的，破坏系统的隐私保护。该方法通过训练深度学习网络对地址的行为进行嵌入和表示，再通过地址验证、地址识别和地址聚类三个步骤进行地址拥有者的识别。相比于原有的基于经验性规律的地址聚类和用户身份识别方法，本方法兼具高效和良好的性能。经研究，现有区块链的用户身份隐私保护存在切实缺陷且对数据驱动的身份识别敏感，解决这个问题对于区块链实用性的提高至关重要。

2、隐私保护的区块链用户身份管理设计。 为了缓解现有区块链身份隐私的问题，特别是上述对数据驱动身份识别敏感的用户身份隐私问题，本文总结了区块链用户的三种身份，并从链上长期身份管理和交易所有权管理两个角度分别提供了有效的隐私保护的区块链用户身份管理设计。

1）从链上长期身份的角度，本文提出了基于可修改匿名证书的链上身份管理协议 BLCred，该协议为用户提供以用户为中心的和完全匿名的链上长期身份以进行链上交易和活动，在不泄露用户真实身份或属性的情况下进行有效的链上用户身份验证和管理。

2）从交易所有身份的角度，本文提出了准许区块链中支持交易发送者匿名性和阈值追踪性的交易所有身份管理架构 AttriChain。它实现了链上匿名的基于交易所有身份的访问控制和分布式的身份治理，为合法用户提供交易的不可关联性的同时对网络中用户都认为有问题的恶意交易提供分布式可追责性，从而增强用户在准许区块链中的隐私和自治权，也方便了网络审计。

3、隐私增强区块链在敏感数据场景中的应用。 在解决上述问题后，区块

链技术便可在处理、存储敏感数据的应用场景中发挥更大的作用。本文提出了LSC，一个支持隐私保护和智能合约自动更新的日志存储和分析框架，以基于区块链的日志系统为例，解决数据可信度、隐私保护和链上监管三个问题，完成自适应性的数据异常检测和系统防御。这个框架在增强隐私的用户身份管理的基础上，允许用户利用合约随着异常模型的更新而更新自己的异常检测逻辑，并在网络里快速共享，使节点间可以协同完成对网络的安全检测，提高系统安全性。以上成果均经过背景研究、理论证明、原型实现和效果测试等研究过程。经实验，本研究提出的方案在可接受的开销下具有实用性和创新性，可缓解面向隐私保护的区块链用户身份管理问题并提高区块链的适用性。"
2022,多GPU实时渲染系统及负载调度算法设计与实现,计算机学院,马秉正,李雨森,OS,0.2535,"由于人们对实时图形渲染需求的日益提高，单个GPU的计算能力逐渐难以满足如今图形程序的QoS。为解决这一问题，并行渲染成为一个主要思路，即通过多个GPU协同完成一个图形应用程序的渲染任务。并行渲染通常包括拆分渲染任务、执行渲染任务和合并渲染结果三个部分。根据GPU连接方式的不同，可将其分为集群渲染和多GPU渲染。在集群渲染中多个配置有GPU的机器通过网络进行通信，而在多GPU渲染中同一台机器上的多个GPU通过PCIe或硬件连接技术进行通信。由于网络延迟等问题，集群渲染很难适用于强交互式的实时渲染程序（比如游戏等），因此多GPU渲染成为这一类型图形程序的首选。


多GPU渲染存在诸多挑战。在系统设计层面，为保证多GPU渲染系统的易用性和可扩展性，其图形程序接口应尽量简洁，并且系统需要支持多个异构GPU协同渲染。在渲染任务调度层面，负载调度算法需要尽量保证系统的执行效率，然而存在以下几个问题需要解决：1) 由于渲染管线的复杂性导致难以实时预测渲染负载；2)系统的执行效率与参与渲染的GPU数量不一定成正比。在已有工作中，大部分并行渲染系统针对集群渲染系统设计，并且无法同时保证易用性和可扩展性。对于负载调度算法而言，大多数工作假设GPU有相同的性能，或者默认使用系统配置的所有GPU进行协同渲染，因此存在一定地局限性。


结合上述分析，本文实现了一个多GPU实时渲染系统，该系统同时保证了易用性和可扩展性。此外，本文为该系统设计了负载调度算法，该算法将渲染负载调度问题解耦为选择参与渲染的GPU集合以及集合内GPU的负载均衡问题，并为其分别设计了多GPU动态切换策略和基于模糊PID的负载均衡策略。实验表明，本文提出的负载均衡策略较前人最先进的算法，能够实现更优的负载均衡，在系统配置两个GPU时平均帧率提升在9.6%到16%，在系统配置三个GPU时平均帧率提升在14.9%到20.6%，并且在不同渲染场景下均有良好的鲁棒性；本文提出的多GPU动态切换策略通过分析系统当前状态，实时调整参与渲染的GPU集合，使得系统能够同时适用于低渲染负载和高渲染负载场景。"
2022,数据受限场景下图像分类算法研究,网络空间安全学院,陈松,张建忠,CV,0.4145,"依赖大量精心标注的数据，卷积神经网络在图像分类任务上取得了卓越的表现。然而，构建一个大规模精心标注的数据集需要大量的人力物力以及时间成本。此外，在许多真实场景下标注大规模的数据需要依赖专家知识，这限制了卷积神经网络在这些场景下的应用。因此，数据受限场景下的图像分类算法研究受到了广泛关注。本文以有标记图像数据数量受限为切入点，开展基于半监督学习与自监督学习的图像分类算法研究。


针对有标记图像数据数量部分受限的半监督学习场景，现有基于伪标签的半监督学习方法存在认知偏差问题。认知偏差源于使用模型的预测生成了不正确的伪标签，然后，基于这些不正确的伪标签进行模型训练，又增加了模型输出不正确预测的可能性。如此迭代的训练，会导致错误的伪标签累积和模型的表现下降。因此，本文首先从样本选择性角度提出了选择性对比损失来统一现有对比学习范式。然后，基于该损失设计了第一个可以端到端训练的、基于高置信度伪标签的对比正则化半监督学习框架。该框架利用样本之间的相关性来抑制伪标签方法中的认知偏差问题。选择具有高置信度伪标签的样本来构建正负样本对，通过对比正则化鼓励正样本对在特征空间中靠近，负样本对在特征空间中疏远。在多个公开的图像分类数据集上的实验结果证明了本文方法的有效性。


进一步，针对有标记图像数据数量完全受限的自监督学习场景展开研究。现有自监督学习方法专注于设计各式各样的代理任务，而忽略了对下游任务有利的样本语义性的利用，导致学习到的特征存在代理任务特定性问题。随着模型的深度增加，基于代理任务学习到的特征对下游任务的泛化性会下降。为了解决此问题，本文提出了一种基于语义伪标签的通用解耦自监督学习框架，将与代理任务无关的样本语义嵌入到表示学习过程中。该框架旨在利用代理任务学习到的底层通用特征与语义伪标签的高级语义特征之间的互补性，抑制特征的任务特定性并提高泛化性。在多个自监督学习的数据集上实验表明，本框架的表现优于现有最先进的方法。"
2023,面向移动设备的手臂动作识别系统关键问题研究,计算机学院,赵世坤,张金,CV,0.3392,"手势由于其自然、高效的特点成为人机交互的主要方式。智能手机中的惯

性测量单元（Inertial Measurement Unit，IMU）成为手势识别常用设备。智能手机在提供了简便的数据获取方式的同时提供了在移动端进行手势识别的推理环境，可实现更简便实用的人机交互。为在移动端实现实时高效的高精度手势识别推理系统，要充分考虑 IMU 的特性以设计合适的数据采集方案与恰当的分类算法，同时要充分考虑移动端的有限算力资源与系统泛化性，进行推理过程的优化以保证系统的高可用性。


首先，为了人机交互过程的有效性与人性化，本文利用更加自然、直观的

手臂动作作为交互媒介，设计了包含 12 种手臂动作的手势指令集，利用智能手机中的 IMU 模块获取实时数据并克服 IMU 漂移性带来的数据误差，确定了双IMU 的数据采集方案，并基于此构建了手臂动作数据集。其次，为了实现有效的动作识别，本论文设计了一种适配于双 IMU 数据采集方案的准确度高、泛化性好的双分支 1D-CNN 动作识别分类模型，并且兼顾了移动端算力有限的推理场景，在确保高精度识别的同时减少了模型的参数量。针对于移动端算力有限的推理场景，本论文优化了手臂动作识别的整体流程，首先使用了唤醒机制以减少系统实际使用过程中的算力消耗并保证了控制过程的有效性与安全性，同时为实现运动过程中精确的手臂动作识别以提高系统的可用性，提出了动作捕获与识别结合的动作识别机制。针对于系统对于新使用者的适配问题，提出了一种在移动端即可简单实现系统向新人适配的实时泛化策略，实现了新数据向模型的实时迁移，提高了系统的泛化性。


最终，基于以上优化方案本论文构建了一个面向移动设备的手臂动作识别

系统，兼顾了移动端有限的资源，实现了高准确度的手臂动作识别。同时，本论文利用该系统建立了自采集的手臂动作数据集，并验证了本文所提出的动作识别模型与推理系统的有效性与泛化性。手臂动作识别系统面对已知数据与陌生数据的实时推理分别达到了 98.39%、97.56% 的识别准确率。最后，通过实际使用测试与自开发交互游戏验证了系统的可用性与有效性。"
2023,基于动态链路推荐的过滤气泡存在性验证及改善方法研究,计算机学院,武海霞,宋春瑶,Network,0.3039,"互联网的高速发展导致了信息过载问题，推荐系统是解决该问题的有效手段，已被应用到社交网络推荐、商品推荐等各个领域。然而个性化推荐系统为了提高推荐的准确度，常以用户历史行为和兴趣作为推荐基础，经过用户与推荐系统的长期交互，推荐给同一用户的内容易变得越来越同质化，最终形成过滤气泡。本文对过滤气泡问题的国内外研究现状进行了调研，发现目前缺乏统一的标准来衡量过滤气泡是否存在；缺乏建立长期交互机制模拟现实状况来研究该问题；以及针对过滤气泡的解决方案效果不理想。本文基于链路推荐方法对推荐系统中的过滤气泡问题进行了以下研究。


       第一，图网络结构是推荐系统天然的组织形式，从中能够提取到节点的高阶信息；同时，网络拓扑结构演化能够揭示过滤气泡的形成过程，因此基于链路推荐的方法是研究过滤气泡问题的重要技术。然而目前对此类方法的调研仍不完善，因此本文对复杂网络上的链路推荐方法进行调研，对各类算法进行对比分析，为后续研究提供必要的基础。


       第二，针对过滤气泡的存在性争议，本文首先引入用户关联半径来衡量用户级别的过滤气泡问题。同时，提出了基于动态链路推荐的验证方法，通过多轮推荐机制建立起用户与推荐系统的长期交互模型。实验结果表明，基于准确度和基于多样性的方法都会导致用户陷入过滤气泡问题，基于多样性的方法通过牺牲一定的准确度能一定程度上抑制过滤气泡的形成。为了验证多轮推荐的可信度，本文在真实数据集上按照时间戳的顺序经验重演了真实的推荐过程，结果也进一步证实了真实推荐过程中存在过滤气泡的事实。

       第三，针对解决方案效果不理想的问题，本文搭建了基于注意力机制的动态链路新闻推荐模型，该模型融合了基于Transformer的编码模块、基于过滤器和分数融合器的重排模块等多个模块。实验结果表明，该方法在准确性指标、多样性指标以及衡量过滤气泡的用户关联半径指标上均优于六种基线方法，能够在保证推荐准确性的基础上缓解过滤气泡问题。通过设计两组消融实验，证实了模型重要组件的有效性。"
2023,针对函数行为的开源组件漏洞检测方法,网络空间安全学院,邱鹏达,贾春福,Security,0.3269,"开源组件的应用促进了软件行业的繁荣，也带来了诸多隐患。攻击者如果能提前掌握开源组件的漏洞情报，则可对所有使用它的软件发动大规模攻击。妥善维护软件工程内的开源组件并及时修复漏洞成为软件安全的关键环节。软件成分分析技术是解决该问题的利器，它能分析软件的组成部分并识别其中包含漏洞的组件，其实现原理是提取组件与漏洞的特征后再匹配未知程序。然而，相同源码生成的二进制程序会因编译环境变化而改变，且漏洞修复涉及的改动一般较小。现有工具提取的特征往往停留于代码表示层面，语义信息不足，当二进制代码变化时，它们很难捕捉到漏洞特征，进而难以准确识别漏洞。


本文创新性地提出了两阶段的开源组件漏洞检测方法，生成反映函数行为和漏洞特征的多粒度签名。首先该方法收集漏洞情报，分别生成包含漏洞和修复后的组件程序，再对相关函数分别提取粗粒度和细粒度特征。其中，粗粒度特征包括函数参数信息、函数调用和字符串等信息，用于初步过滤；细粒度特征包括包括内存操作、函数调用和返回值等与函数行为高度相关的信息，用于精准分辨漏洞。此外，还根据漏洞类型加入控制流分支或指令运算的代码信息。通过研究可知，所选细粒度特征受漏洞修复的影响明显，组合后的多粒度签名能有效应对编译环境的变化。检测过程分为两个阶段，第一阶段利用开源组件检测工具OSSPolice判断目标开源组件是否存在，若存在则使用函数签名定位漏洞相关的函数；第二阶段使用细粒度签名判断组件内是否存在漏洞。


使用本方法检测10种常用开源组件的共150个CVE漏洞，实验显示该方法应对4种不同编译环境时能达到99.33%的平均准确率，显著优于BinXRAY在面对单一环境时的96.87%。本方法检测无关程序时的假阳性率为0，且应对8种不同编译环境时的检测准确率最低达97.5%，说明本方法的函数签名具备独特性，且应对编译环境变化的健壮性较强。此外，本方法分析单个漏洞的平均时长为5.29秒。实验结果说明本检测方法准确且高效，具有较高的实用价值。"
2023,基于深度学习的图像去摩尔纹技术研究,计算机学院,盖立童,李岳,CV,0.3459,"摩尔纹是是一种由于两个透明介质之间存在微小的距离差而引起的光学干涉现象。由于折射率的不同，会形成明暗相间、难以与图像内容区分的干涉条纹。这些干涉条纹严重影响图像的清晰度，给后续处理任务带来困难。随着数码相机和智能手机的普及，图像摩尔纹问题受到人们广泛关注。本文面向图像去摩尔纹问题展开研究。


摩尔纹不均匀分布在图像上，会导致图像某些区域的细节和结构特征丢失或被掩盖，给图像复原带来困难。而现有方法对复杂摩尔纹区域缺少关注易出现失真、伪影等问题。本文提出了一种基于信噪比指导的双分支网络去摩尔纹方法SGMNet。具体来说，本方法提出全局注意力模块CGAM提取全局特征以复原复杂摩尔纹区域，以及局部注意力模块CLAM提取其他区域的局部特征，通过全局和局部特征相互补充，提高图像复原结果的质量。信噪比能够反映图像区域噪声水平和信息清晰度，因此本文使用信噪比作为先验，指导全局和局部信息的融合过程，减少低信噪比区域摩尔纹干扰。实验结果表明，本方法在PSNR和SSIM评价指标上优于现有方法，同时SGMNet方法的可视化效果也得到显著改善，能够更好消除复杂摩尔纹。


去摩尔纹任务在消除摩尔纹的同时需要保留更多图像细节。现有研究着重于使用多尺度特征提取方法来消除摩尔纹，但在消除摩尔纹过程中忽略了浅层特征的重要性，导致复原图像模糊或颜色失真等问题。因此，本文提出了一种基于边缘监督的多阶段去摩尔纹方法MENet，旨在消除摩尔纹的同时保留更多边缘信息等浅层特征。具体来说，通过在不同阶段间引入特征融合模块FFM，可以更好地保留更多的纹理和色彩等细节信息。本方法在前两个阶段的末尾引入边缘监督模块ESM，增强边缘区域的特征表达能力并减少传递误差。实验结果表明，本方法与现有方法和SGMNet方法相比获得最优的PSNR和SSIM指标，在可视化效果方面能够更好地保留图像的细节信息和结构特征。"
2023,基于可逆神经网络的鲁棒信息隐藏,网络空间安全学院,胥鸿博,杨愚鲁,CV,0.3236,"信息隐藏是在一种在媒体中隐藏某种形式信息的技术。随着数字通信和多媒体数据的日益普及，信息隐藏技术在现代变得至关重要，尤其在知识产权保护、数字资产认证、数字取证等领域得到广泛应用。最近一些基于深度学习的方法在信息隐藏领域已经取得了巨大进展。但仍存在以下两点问题：一是他们通常使用分离的“编码器-解码器”框架来分别完成信息隐藏中的嵌入和提取过程，存在信息丢失问题。二是他们对一些攻击的鲁棒性较差。针对这两个问题，本文通过使用可逆神经网络来缓解信息丢失问题，并通过模拟各种攻击来提升网络的鲁棒性。本文在数字水印和可逆灰度这两个具体的问题上进行了研究。

      数字图像水印算法旨在通过在数字图像中嵌入额外信息并在需要时提取此额外信息，以实现授权访问，保护数字图像的版权。本文提出了一种基于可逆神经网络的可逆水印网络（IWN，Invertible Watermarking Network），将水印的嵌入和提取视为一对可逆问题，利用可逆神经网络在解决这类可逆问题上的优势，提升水印算法的性能。为了增强水印算法的鲁棒性，本文在IWN 中引入噪声层模拟各种实际攻击。此外，本文还提出了一种比特信息归一化模块来压缩比特序列，同时提升了IWN 的鲁棒性和不可见性。大量实验表明，本文方法相较于基线方法在鲁棒性和不可见性方面有明显提升。

      可逆灰度旨在通过某种编码方法将颜色信息以不可察觉的方式嵌入到灰度图像中，同时可以通过相应的解码方法提取颜色信息以获得彩色图像。可逆灰度是一种非常有意义的灰度策略，可应对基于颜色信息处理的许多挑战。尽管现有的一些方法在这一领域已经取得了很多进展，但该领域仍有探索空间，特别是考虑到有损JPEG 压缩对各种现实应用的影响。本文将可逆神经网络应用于图像可逆灰度，提出了一种新的可逆灰度网络（IGN，Invertible Grayscale Network），该网络同时被用作编码方法和解码方法。此外，由于JPEG 压缩在各种实际场景中被广泛使用，本文在IGN 中引入了JPEG 模拟器，提升了算法对JPEG 压缩的鲁棒性。大量实验表明，在JPEG 压缩下，本文方法取得了优于先前可逆灰度方法的结果。"
2023,一种基于近边界数据的模型所有权推断方法研究,计算机学院,杨宗稳,蒲凌君,CV,0.3647,"深度神经网络(Deep Neural Network, DNN)训练代价昂贵，这是导致模型知识产权保护问题逐渐被重视的原因。近年来，模型盗窃行为频繁发生，攻击者非法复制、派生和发布DNN模型，严重侵犯了模型所有者的知识产权。因此，受到数字水印的启发，研究者提出了模型水印及指纹的方法，通过对模型提取水印或指纹进行匹配，从而验证模型所有权。然而，通过模型水印和指纹验证所有权具有较大的局限性，例如易被检测和清除。此外，攻击者还可以对模型发起歧义攻击，这是当前模型知识产权保护工作面临的重大挑战。为了解决上述问题，本文提出了一种新的思路，即推断模型所有权，代替以往基于模型水印和指纹验证模型所有权的方法。同时，本文提出了一种特殊的近边界数据，使用其对应的模型输出作为依据推断模型的所有权，解决伪造水印和指纹带来的歧义攻击问题，且不易被攻击者检测和清除。本文的主要工作如下：


       1） 本文揭示了以往验证模型所有权方案的脆弱性和局限性，并确认了数据驱动推断模型所有权的有效性。本文提出了一种新颖的推断模型所有权思路。与过去工作中利用模型水印和指纹验证模型所有权相比，本文方法使用数据在对应模型上结果作为所有权推断依据，结果的可比性和唯一性可以有效避免歧义攻击。为了保证不影响原始数据集和模型，且作为依据的数据不被伪造，本文设计了一种特殊的近边界数据。实验验证了该数据的特性可以继承到目前主流的盗窃技术派生出的模型上，从而作为推断模型所有权的依据。


       2）本文提出了基于近边界数据推断模型所有权的方法。该方法主要分为三个阶段：第一阶段通过改进的CW-L2算法，从原始训练数据生成初始近边界数据；第二阶段设计了基于DCGAN的特征提取器，提取原始近边界数据特征后，生成新的、私有化的近边界数据；第三阶段设计了新的损失函数并微调源模型，使私有近边界数据更加靠近分类边界。最后提出使用假设检验的方法对比结果的差异，以95%以上的置信度成功推断模型所有权。本文在三个开源数据集上进行了大量的实验，证明了本文方法在推断模型所有权时的有效性和鲁棒性。"
2023,基于慕课学习行为分析的在线课程推荐方法研究,计算机学院,张博健,温延龙,CV,0.303,"近年来，大规模开放式网络课程（简称慕课）蓬勃发展。特别是在新冠疫情流行期间，学习慕课已成为全世界数以亿计学生的主要学习方式之一。然而，由于慕课学习环境中缺少教师的直接指导和有效的监督机制，学生提前退出所选课程的现象普遍，慕课学习质量不佳。在慕课学习过程中识别有潜在退课风险的学生，并为学生推荐真正喜欢的课程，是提高学生学习质量的有效方法。因此，本研究具有很好的研究意义和应用价值。


本文充分调研了学生退课行为预测、在线课程推荐的国内外研究现状，研究了基于慕课学习行为分析的在线课程推荐，具体工作包括：


第一，提出了一个基于双重自注意力网络的学生退课行为预测方法。该方法采用时间相关的学生退课行为定义，使用时间序列建模学生的历史学习过程，并在影响因素和时间两个维度上分别挖掘不同因素之间的内在联系以及不同时间阶段之间的时序依赖，最终实现准确预测。在预测过程中，每种学习活动和每个时间阶段的实际意义均得到保留。得益于此，模型可以借助注意力分数指出影响学生退课的关键影响因素和关键时间阶段。真实数据集上的实验结果和案例分析表明本文方法不仅能够准确预测，而且可以个性化地解释预测结果，辅助慕课平台减少学生提前退课的发生。


第二，提出了一种基于学生学习行为的在线课程推荐方法。根据对学生学习行为的研究，本文注意到在慕课学习场景中：学生对所选课程具有差异化的兴趣；学生既可以顺序组织学习内容也可以随意安排。为此，本文设计了一个基于学习行为的混合注意力课程推荐模型，从学生学习行为中挖掘课程之间的潜在关联，区分学生对所选课程的个性化偏好，从而更好地推荐课程。该模型通过图卷积神经网络从“学生-课程”二部图中提取特征，并在此基础上从学生历史学习行为序列中进一步挖掘不同课程之间的联系。最终使用混合注意力网络来预测学生对候选课程的选择分数以完成推荐。真实数据集上的实验结果表明本文提出的方法在推荐效果上取得了一定的提升。"
2023,少样本虚假新闻检测关键技术研究,计算机学院,袁婧,陈晨,CV,0.3356,"自21世纪以来，互联网的迅猛发展加快了信息化进程，人们获取、传递和共享信息的方式也随之发生了变化。但是在网络技术的飞速发展和广泛应用下，其中存在的问题也逐渐暴露出来。虚假新闻以高度的话题性、广泛的影响力和指数爆炸式的传播速度，给社会的发展带来了不容忽视的挑战。


通过对国内外虚假新闻检测技术的调研，本文发现现有虚假新闻检测算法的效果依赖标签数据的数量。但标注数据的时间成本很高，且对标注人员的要求很高。为了克服这些问题，本文受到少样本学习和事实验证等工作的启发，对虚假新闻检测的关键技术进行了研究，并提出了以下两个虚假新闻检测方法。


第一，本文提出了一种基于知识蒸馏的少样本虚假新闻检测方法。该方法能够同时利用大量的无标签新闻样本和少量的有标签新闻样本联合训练，从微调后的预训练语言模型中提炼知识并表现出优异的性能。另外，该方法还提出了一种优化算法，通过设定阈值的方式，决定是否将某个有标签样本加入到该批次训练样本的梯度计算中，从而缓解模型可能出现的过拟合问题。两个真实数据集的实验结果表明，与最新的基线方法相比，该方法不论是在中文数据集还是英文数据集上，都表现出很高的准确性。即使在数据分布有偏的情况下，其性能依然表现出很强的鲁棒性。


第二，本文提出了一种基于图转换器和协同注意力网络的事实验证方法。事实验证是虚假新闻检测的关键技术之一，其主要通过引入外部证据的方式来辅助模型做出判断。本方法首先使用事实验证数据集微调预训练语言模型，以适应当前任务。之后使用微调后的模型编码证据和声明，然后通过构造证据图的方式来建立证据间的关系，并借助使用了图注意力机制的图转换器对证据的表征进行更新。最后通过双层的协同注意力网络对证据和声明之间的关系进行推理，并使用关联矩阵让推理过程具有可解释性。大量的实验结果表明，本方法优于现有的模型，且具有更高的去噪能力和综合推理能力。"
2023,字符串数据流的多参考压缩及模糊搜索研究,计算机学院,黄钟毓,蒲凌君,Network,0.2654,"字符串的压缩和模糊搜索操作在许多场合下发挥着重要作用，随着现代科技的进步和发展，出现了很多字符串数据动态生成和到来的场景，例如无线传感器网络和LBS服务。然而，传统的字符串压缩和模糊搜索技术仅适用于静态字符串数据集合，如何对字符串数据流进行动态的压缩操作以及在被压缩数据上进行快速模糊搜索操作是一个具有实用价值的问题。本文通过相关工作的调研，分析了在字符串数据流环境下进行压缩和模糊搜索操作时遇到的问题和挑战，并开展了针对性的研究。主要工作如下：


（1）本文针对字符串应用场景字符串数据流中对字符串压缩和搜索的要求，提出了一种字符串数据流下的实时多参考压缩方法。该方法受到已有的参考压缩算法的启发，构建参考组并使用其中的参考项对数据进行压缩。本方法实现了真正的多参考压缩机制，即一个待压缩的字符串可以同时使用多个参考项，从而更充分地利用被压缩数据之间的相似性；本文设计了两个轻量的、基于分片思想和倒排索引的索引数据结构，以支持对字符串数据流进行动态的压缩和模糊搜索操作；本文还设计了一个有效的启发式参考组选择算法，在压缩过程中动态选择合适的参考项，以提高压缩表现。相较于同类工作，本文提出方法可以在字符串数据流环境下动态高效地对数据进行压缩。


（2）本文提出了一种高效压缩数据流模糊搜索方法，可以在不完全解压缩数据上直接进行快速模糊搜索操作。该方法受到同类算法的启发，为本文提出的多参考压缩方法设计了两阶段的模糊搜索操作：对参考组的搜索以及对覆盖字符串的搜索。和同类工作相比，本方法利用了压缩阶段生成的两个轻量的索引数据结构来加速模糊搜索操作。本文还设计了三种过滤技术，可以对不必要的中间结果进行剪枝，以提高模糊搜索方法的性能。


（3）本文采用了多个公开的真实数据集，对提出的压缩和模糊搜索方法进行了实验。实验结果表明，与同类方法相比，本文提出的压缩和模糊搜索方法在压缩率和搜索速度等方面都取得了明显的进步。"
2023,面向用户的移动应用账号注销可用性研究,网络空间安全学院,刘一静,刘哲理,Security,0.2549,"随着智能手机的飞速发展，移动应用程序（Application，简称“APP”）成为生活中必不可少的组成部分，在为人们带来便捷生活的同时也将更多的数据暴露于互联网中。用户在注册不同的应用程序账号后，往往不能保证及时进行账号注销，导致大量僵尸账号的存在，进而增加个人信息泄露风险。2021年11月生效的《个人信息保护法》明确规定了个人信息删除权的概念和使用范围。其中账号注销作为删除权的重要行使方式，是保护用户隐私安全的有力手段。然而，现有的移动应用程序在账号注销的实现上缺乏统一的标准，甚至有很多应用程序不支持账号注销。为解决上述问题，本文首次对移动应用程序中账号注销功能的可用性进行探究。设计并实现基于模式匹配的隐私政策查询工具，通过实证测试创新性地提出账号注销流程模型，从用户视角对账号注销功能进行可用性测试。


具体来说，本文首先构建并公开了一个包含24942款应用程序隐私政策URL的数据集并爬取相应的隐私政策文本。通过创建匹配规则，实现对相关内容的抽取。该工具不仅可以自动化获取账号注销相关说明，而且支持对个人信息收集以及共享情况的查询。接下来，本文使用该工具对国内外60款移动应用程序进行实证测试，通过模拟应用程序账号注销操作，归纳总结出账号注销流程模型，对应用程序账号注销流程进行概括。最后，基于账号注销流程模型，本文结合人机交互的研究方法，从用户视角对账号注销功能进行可用性测试，设计并实施了涵盖两个国家的647名参与者的在线调查，和包括20名参与者的现场访谈，探索用户对账号注销功能的认识、实践与期望。


研究结果表明，大部分用户缺乏账号注销意识，账号注销的不友好设计阻碍了部分用户完成注销操作，现有的账号注销实现方式也不满足用户期望。为缩小用户期望和应用程序实现之间的差距，本文提出若干建议，包括提高用户意识、简化账号注销操作、提供可自定义的注销设置等，为移动应用程序中账号注销问题提供了一定的解决思路和方法，有助于应用程序厂商和政策制定者更好的理解与完善账号注销功能，保障个人信息删除权的有效行使。"
2023,基于Transformer的多特征融合代码克隆检测方法研究,网络空间安全学院,王思谦,张金,SE,0.3529,"代码克隆（Code Clone）是一种常见的代码重用和软件原型化方法。通常，过度的代码克隆会影响软件质量、造成高额的维护成本，同时引发软件知识产权纠纷等问题。代码克隆检测（Code Clone Detection）是漏洞检测、代码搜索等诸多软件工程任务的基础。现有的代码克隆检测技术对于1-3型代码克隆可以达到预期的检测效果。但在真实的代码库中，由于编程语言的多样性和灵活性，仍然无法准确地检测到“异文同义”的Type-4型语义代码克隆。如何编码一种良好的代码表征方式是现有代码克隆检测技术的主要瓶颈。


针对上述问题，本文提出了一种基于Transformer的多特征融合代码克隆检测方法——ATU。该方法利用基于Transformer的应用模型，采取预训练和微调的方式，提取源代码的多种特征表示并融合，以支持复杂代码的更好建模，从而提升检测性能。首先，为捕捉程序自身的语法和语义信息，本文设计了一种基于抽象语法树（AST）的预训练语言模型AST-BERT，来改进BERT面向程序的编码任务。将AST序列输入至该模型可生成表示程序内语法和语义信息的特征嵌入。其次，仅依靠程序内的代码表示是不充分的，因为没有学习到程序间的语义信息。因此，本文采用支持语义相似度计算的Sentence-BERT模型，通过句对匹配任务对其微调。微调后的模型以高级语言文本为输入，生成表示程序间语义信息的特征嵌入。同时，由于高级语言表达方式的多样性，仅依靠高级语言域编码的特征很容易误导模型对程序语义信息的理解。因此，本文引入中间代码这类反映程序行为语义的代码表征方式，设计了针对LLVM IR的IR-BERT预训练语言模型和针对Java字节码的BERT-FIX微调模型，用于生成表示代码行为语义的特征嵌入。最后，本文将三种特征嵌入融合以全面表征代码语义信息，并训练一个有监督的深度神经网络分类器来自动学习判别规则以检测代码克隆。


本文使用OJClone和GCJ数据集对ATU方法进行了广泛验证。实验结果表明，ATU方法展现的克隆检测性能均优于之前的最先进方法。具体而言，在OJClone和GCJ数据集上分别达到了99.5%和 99.4%的精确度。除此之外，ATU方法检测用时较少，具有良好的稳定性、可伸缩性和泛化能力。"
2023,美学引导的图像外裁剪,计算机学院,钟磊,辛运帏,CV,0.3326,"摄影的普及以及微博等社交媒体的流行，使人们对图像美学有更高的要求。 图像的场景构图是影响图像美学的重要因素之一。为了满足摄影爱好者对专业 摄影技巧和提高照片美学质量的需求，学界和工业界开始关注如何自动调整图 像构图。图像裁剪是最常见的图像构图调整方法之一，它用于调整输入照片的 场景组成，去除多余内容，从而提高图像的美学质量。


现有的自动图像裁剪方法通常在图像边界内选择一个符合图像美学规则的 窗口作为结果。然而，这些方法在某些情况下，如主要物体过于靠近图像边界、 显著物体占比占比过高等，对图像构图的改善效果有限本文将以往的图像裁剪 算法称为图像内裁剪算法，提出了一种新颖的、基于美学引导的图像外裁剪方法。本文的方法允许跨越图像边界选择合适的窗口，从而能创建使用现有的自 动裁剪方法无法实现的理想构图。本文的方法首先通过一个视野评估模型来评 估输入图像的视野范围是否需要扩展，并确定扩展的程度。然后，本文合成需 要扩展的图像内容，并综合考虑裁剪窗口内的构图美学和局部图像质量，选择 最佳的裁剪窗口。


具体而言，本文的主要贡献如下总结：


1) 提出了一种内容感知的图像外裁剪方法，扩展了传统图像内裁剪方法的应 用范围。这是首个允许裁剪窗口延伸到图像边界之外以找到合适美学视图 的图像裁剪方法。


2) 引入了基于生成对抗方法的技术，解决了生成图像可能包含在裁剪窗口内， 质量不佳影响最终美学质量的问题。在裁剪窗口选择过程中，平衡了构图 美学和局部图像质量。


3) 通过大量的定量分析和定性分析实验，验证了所提出的图像外裁剪方法的 有效性，并与现有的图像裁剪方法及基准方法进行了比较。实验结果表明， 在传统图像内裁剪算法无法找到合适的裁剪窗口的边界约束情况下，该方 法能够生成符合图像美学的结果。同时，在无需外推或者图像外推质量不 佳的情况下，方法能够自动退化为图像内裁剪算法。"
2023,融合品牌信息的广告点击率预估模型研究,计算机学院,高建波,沈玮,CV,0.3271,"点击率（Click-Through Rate, CTR）预估是在线广告、推荐系统等场景中至关重要的一环，该任务旨在预测用户在给定场景下对物品广告的点击概率，其准确性直接影响用户体验与平台收入。随着广告形式的多样化和品牌广告的发展，用户建立起了对物品所属品牌的不同认知，因此品牌逐渐成为影响用户行为的主要因素之一。然而，现有大多数点击率预估模型虽然利用深度神经网络实现了用户和物品特征之间的低阶和高阶交互建模，但忽略了品牌信息在物品多种特征中的特殊性，未考虑用户兴趣和品牌之间的直接关联。

为了解决上述问题，本文结合图神经网络与对比学习方法，提出了融合品牌信息的广告点击率预估模型BGMC，更充分地考虑了品牌信息的特殊性以及用户兴趣和品牌之间的复杂关联。首先，BGMC模型区分了用户对物品和品牌的兴趣，并基于聚类算法生成物品的原型信息，构建物品、品牌和原型三个视角对物品进行表征，进而从多个视角建模用户对物品的兴趣和意图。其次，模型基于图神经网络建立用户与物品之间的联系，构建了品牌信息融合模块，该模块通过消息传递过程中多视角信息的融合与互补，动态捕捉用户对物品和品牌的不同兴趣。为了进一步优化对品牌和物品的表征学习，模型以品牌信息为核心构建了对比增强模块，捕获了不同视角下信息的相似性与差异性。本文在公开数据集和真实业务场景中进行了充分的实验测试模型性能，实验结果表明，相较于基线方法，BGMC模型在广告点击率预估任务上取得了良好效果。"
2023,视频的多维度超分辨率重建,计算机学院,许刚,王亮,CV,0.377,"视频超分辨率技术的目的是通过利用外部数据、先验知识和视频的内在信息，实现从低分辨率视频到高分辨率视频的映射。在色彩、空间和时间这三个维度上对视频应用超分辨率重建技术能够从低质量视频中重建出色彩更逼真、细节更丰富以及运动更流畅的高质量视频。然而当前的方法存在着计算复杂度高、灵活性差和泛化性低等缺点，具体表现为：1）低频信息感知能力较差；2）无法针对高低频信息进行长距离依赖建模；3）传统分解技术在复杂场景下泛化性较低；4）无法兼具灵活性与高效性，无法高效地插值出任意时间分辨率的视频。


为解决上述不足，本文提供了不同改进方案。具体研究内容为四个方面：


1. 针对当前方法对低频信息感知能力较差的问题，本文提出了一种频率感知特征调制技术，以实现频率自适应的色彩维度超分辨率重建。该技术根据不同内容的频率响应来动态地调制特征，从而提高低频特征的提取和重建效果。基于该技术，本文构建了一个频率感知调制网络，并且通过基准数据集上的实验验证了该网络可以显著提升低频区域的超分辨率重建质量。


2. 针对当前方法无法对高低频信息建模长距离依赖的问题，本文提出了双频注意力以实现高低频分量上长距离依赖关系的针对性建模。基于双频注意力，本文构建了一个色彩维度超分辨率模型。基准数据集上的实验结果显示该模型在仅采用之前方法10\%的参数量的情况下，在主要评测指标上取得了最优性能。


3. 针对传统分解技术在复杂场景下泛化性较低的问题，本文提出了一种特征分解技术，以实现数据驱动的自适应特征分解。基于该技术，本文设计了一个轻量化的色彩维度和空间维度联合超分辨率网络。此外，本文还构造了一个大规模数据集，其在图像多样性上与之前数据集相比有较大提升。在基准数据集上的实验结果表明本文所提的网络具备更低的模型复杂度与更优异的性能。


4. 针对之前方法无法兼具灵活性与高效性的问题，本文提出了时域调制技术，以实现任意时刻下的特征插值。基于该技术，本文提出了时域调制网络。该网络能够在保持较低计算开销的同时，以任意帧率生成高空间分辨率的视频。在基准数据集上的实验结果验证了该网络的灵活性和高效性。"
2022,面向生物大分子关键问题的注意力机制深度学习模型与应用研究,计算机学院,金宸,张瀚,CV,0.3302,"以蛋白质和核酸为代表的生物大分子，在细胞内参与生命活动最相关的重要生化反应。使用以深度学习模型为代表的人工智能技术，对生物大分子的关键问题进行预测，能够有效地规避盲目生化实验带来的资源消耗，并且能够为下游生化研究提供良好的方向指导。在生物大分子序列分类预测和关系型预测这两类重要问题的研究中，由于生物信息的复杂性与多样性，数据特征的多尺度性与异构性导致其难以被抽取，数据标签的稀缺性导致其训练易过拟合，模型往往缺乏鲁棒性、泛化性和可解释性等。这些急需解决的共性问题在不同种类具体任务中的表现形式有着很大差异，重点也各不相同，大大增加了研究难度。本文从注意力机制出发研究深度学习模型，针对四个重要任务中的不同难点，系统地提出了以下四种基于注意力机制的深度学习算法模型。


对于序列分类预测中的蛋白质结晶预测任务，传统计算方法需要复杂的特征工程，泛化性不强效果也不佳，大都没有基于蛋白质特殊结构进行特殊设计。对此我们提出了一种基于自注意力机制多尺度卷积网络模型的序列分类预测算法 ATTCry。模型集成多尺度卷积网络和多头自注意力机制，可以同时捕获局部和全局的蛋白质特征，端到端的算法提升预测的鲁棒性和泛化性。我们将其应用在蛋白质结晶分类实验中取得了良好的效果。


对于序列分类预测的蛋白质结晶多阶段预测任务，蛋白质分类标签相对稀缺，训练时容易出现过拟合的问题，对此我们设计了一种基于双重注意力机制迁移学习模型的序列分类预测算法 TLCrys。预测算法模型由两块构成，在预训练过程中设计基于自监督多任务的全局注意力模块可以有效获取蛋白质表征，在微调过程中，使用多头自注意力层可以提取层次不同的蛋白质全局表示进行分类预测。基于迁移学习模型的算法可以将从源域学习到的蛋白质表征应用到具体分类任务上，模型更具可解释性和泛化性，我们将 TLCry 模型应用于蛋白质结晶分类多阶段预测以及抗微生物肽分类预测两个实验中，均取得了很好的分类效果。


对于关系型预测中的 lncRNA 与蛋白质相互作用预测，在常见的标签传播预测算法中，相似性计算和标签传播过程需要依据具体数据集设计才能使其收敛，这意味着这些算法非常复杂，泛化性不强。对此我们提出了一种基于图注意自编码器协同训练模型的关系预测算法 LPIGAT。我们通过 BioVec 方法进行相似度计算，使用图注意自编码器抽取相互关系的特征表示来模拟图上传播标签的过程。两个图注意自编码器协同训练的方式，提高了模型从原图空间获取有效表示的能力。整合的端到端模型更精确也更具有鲁棒性。我们将该模型用于 lncRNA 与蛋白质相互作用预测的实验中，实验效果优于目前最先进的方法。


对于关系型预测中的 miRNA 与疾病的关联预测任务，现有基于矩阵补全的关系型预测模型难以有效地整合矩阵补全和标签传播过程，对此我们设计了一种基于自注意力机制归纳矩阵补全模型的关系预测算法 NIMGSA。算法结合了归纳矩阵补全和图注意自编码器来进行关联预测。标签传播过程可以通过图注意自编码器的重构过程来模拟。同时，神经归纳矩阵补全算法在确保表示学习的低秩属性的时增加了协同注意力训练，有效地提升了表示学习的能力。这种端到端的关联预测框架，可以加强矩阵补全和标签传播的鲁棒性和精确性。我们将 NIMGSA 方法用于 miRNA 与疾病的关联预测中，实验结果验证了模型的有效性。


本文面向生物大分子关键问题提出了四种基于注意力机制的深度学习模型，针对复杂多样的生物信息任务场景，分别从网络层间、模块融合、节点选择、协同训练等四个不同层面进行注意力机制深度学习模型的探索，深化了注意力机制模型的应用研究。在所针对的具体预测任务上，实验测试集和实例测试均得到了很好的预测效果。同时我们的模型泛化性较强，可以被重新训练和设计用于更多具体的生物学问题。基于可解释性设计的深度学习模型结构也为深度学习研究者提供有实际背景与生物意义的实例。"
2022,基于非负矩阵分解与深度学习的零样本学习算法研究,计算机学院,王佳,辛运帏,CV,0.3543,"在这个以数据为基础的大数据时代，模式识别算法的准确率往往取决于训练集样本数量。然而，获取大量有标签的样本需要消耗大量的人力和物力，另一方面，我们很难获取足量的新出现类别的样本，因此，对于零样本学习问题的研究至关重要。零样本学习问题是解决训练样本与测试样本不相交的情况下的一类问题。尽管近些年来涌现出大量对于零样本学习问题的讨论与研究，零样本学习领域依然存在许多难以解决的困难与挑战。其中，由于训练集类别与测试集类别完全不同，在训练集上训练出来的模型在为测试集样本分类时往往会产生领域漂移问题；使用单一的语义空间难以准确描述细粒度数据集中的类别差异，而目前为数不多的使用多种语义空间解决零样本学习问题的方法大多比较简单，并且难以达到理想的多语义融合效果；在更符合现实应用的广义零样本学习问题中，由于测试集中既包含可见类样本又包含不可见类样本，在可见类别上训练的模型很容易出现过拟合问题，从而错误地将所有测试样本都分到可见类中。为了解决这些难题，本文将从以下三个方面进行深入研究。


首先，在训练集上训练的零样本学习模型应用到测试集时，容易产生领域漂移问题，针对这一问题，本文设计了基于可见类别和不可见类别的联合矩阵分解算法；同时，为了保持数据原始的类内和类间结构，使得数据原始信息在非负矩阵分解过程中不被破坏，本文利用可见类别的标签信息和不可见类别的聚类信息为原始数据建图，并通过加入图约束的方法保持数据结构信息在矩阵分解的过程中不丢失，从而显著提高了零样本分类的效果。


其次，为了更有效地融合不同的语义空间，本文设计了基于GCN的多语义融合的零样本学习算法。为了更好的实现多语义空间的融合，本文深入研究了不同语义空间的特质与差异，并根据这些特质为不同的语义空间分配不同的角色：使用包含类别间相互关系的词向量构建类别间的图结构，并将更为准确的属性列表作为节点表示。同时，设计GCN模型用于将构建好的图映射成为具有区分性的零样本分类器，这些分类器最终用于执行零样本分类任务。该方法充分地融合了不同的语义空间，使不同地语义空间产生互补的效果，从而有效地提高了零样本分类准确率。


最后，为了解决广义零样本学习问题中的过拟合问题，本文设计了基于局部邻域领域检测的广义零样本学习算法。首先，通过观察和分析总结出样本的局部邻域中包含决定其领域分类属性的重要信息这一规律。根据这个现象，本文设计了基于加权k近邻的领域检测算法，在测试阶段，首先在测试集中区分可见类别样本和不可见类别样本，从而将广义零样本学习问题分解为经典零样本学习问题和有监督分类问题，从根本上解决过拟合问题。实验表明，该算法极大提高了广义零样本分类的准确率。"
2022,云计算环境下可搜索加密问题研究,计算机学院,Mingyue Li,贾春福,Security,0.3647,"随着个人和企业数据爆炸性增长，云计算市场呈现快速增长态势，但也导

致了众多的数据泄露事件，这给用户的数据资产带来严重损失的同时，还造成了巨大的社会影响。可搜索加密是一种能够使合法用户将加密数据存储到云中，并在密文域执行搜索的技术，可实现用户数据隐私保护，又不妨碍数据可用性的目标，是云计算安全领域的重要研究课题。本文通过研究与分析国内外可搜索加密领域的已有成果，凝练出现有可搜索加密方案中存在的研究问题，从外包数据隐私保护以及方案实用性的角度出发，设计了不同的解决方案，主要的研究进展总结如下:




1. 针对现有动态可搜索加密方案中查询和更新方式不灵活、通信开销大

的问题，利用二部图结构设计了满足前向和强后向安全的可搜索加密方案

(FBSSE-BG)，通过利用每个节点的更新计数器作为标签，构造了一个新的双向索引结构来实现二部图上的连接查询。由于无需客户端指定服务器就可以删除与给定文件标识符相关的所有边，因此该索引结构可支持高效的删除操作。为了不破坏索引结构的双向查询特性，本文基于茫然数据结构提出了新的存储方法，该方法可以通过调整 ORAM(Oblivious Random Access Machine) 树的高度来降低通信成本和访问延迟，并且当用户执行查询和更新操作时，服务器只能观察到一系列 ORAM 位置和加密路径，从而保护数据访问模式。最后在真实数据集上进行了多次实验来衡量 FBSSE-BG 的性能，实验结果表明该方案能够提高检索效率，同时理论分析也验证了 FBSSE-BG 能满足前向安全和 Type-I 后向安全。




2. 为了防止云服务器恶意篡改数据以及返回错误查询结果，利用可编辑的区块链提出了一种新的外包数据隐私保护框架 (DSE-RB)，允许用户通过交易的方式存储数据，并使用智能合约 (如查询合约和更新合约) 与加密文件交互，而无需担心恶意服务器。尤其为了解决可搜索加密系统由于区块链数据规模的不断增长而造成的存储和性能问题，使用可控交易级编辑技术在无需添加额外交易的情况下可实现对外包加密数据更灵活的更新操作。为了进一步避免 gasLimit限制交易操作的大小和设计功能的复杂性的问题，本文还将传统的二叉树索引分解为多个子索引，并引入多项式的概念来创建交易数据的访问控制机制来实现链上的可控查询，减少数据所有者计算负担的同时使数据用户能够及时查询数据。最后利用 Solidity 智能合约在真实数据集上进行了多次实验。实验结果和理论分析分别表明了该方案在加密数据上的实用性和安全性。




3. 为了实现电子医疗数据的多用户安全共享，提出了支持 SQL 查询的多用户选择性授权可搜索加密方案 (MSE-SQ)，将 Diffie-Hellman 协议应用于陷门生成算法解决增量转换导致的巨大计算开销的问题，进而提高检索性能，同时实现对数据使用者的细粒度搜索控制。为了给予用户数据更多的安全保障，通过双线性映射利用私钥生成加密索引，使得敌手不能通过遍历关键字空间获取陷门关键字并进行关键字猜测攻击，并使用双层加密来加密电子医疗数据的对称解密密钥，使得只有属性与访问控制列表匹配的代理才能获得解密数据的密钥。通过理论安全性分析和实验验证，MSE-SQ 可以在没有安全信道支持的情况下提供安全有效的密文检索。


4. 为了实现加密空间数据的安全高效查询，结合 Z 阶空间填充曲线、保序加密和差分隐私的概念，提出了一个新的面向隐私保护的学习索引结构，并利用该学习索引结构实现了空间数据的连接几何范围查询 (ECGRQ-LI)，以解决现有方案因使用传统的索引树导致的搜索和存储开销大的问题。为了进一步提高查询性能，提出了一种空间分割技术避免在查询过程中访问大量不必要的 Z 编码。通过形式化安全分析表明，该方案能够在查询过程中保证索引数据的安全性和查询的私密性。利用真实数据和合成数据进行了大量的模拟实验，结果表明，ECGRQ-LI 方案比最有效的支持连接范围查询的 PBRQ-Q 方案和 PBRQ-L方案的查询效率分别快 5 倍和 40 倍。此外，ECGRQ-LI 的数据存储开销也显著降低，比 PBRQ-Q 节省了约 64% 的内存。"
2023,基于异构实体图的多模态阅读理解学习研究,计算机学院,张慧斌,杨征路,CV,0.3082,"在人工智能的新浪潮中，理解和推理是模型实现认知智能的核心能力。已有的研究工作主要侧重于研究单一模态数据，例如针对文本数据的阅读理解任务和针对图像数据的图像内容理解任务。因此，如何在多模态场景下赋予模型理解与推理的能力成为近年来的研究热点。多模态阅读理解任务旨在使模型具备理解和推理多模态文档中的图像和长文本，并回答相关问题。本文重点关注针对程序性多模态文档的阅读理解任务。程序性多模态文档中通常包含以连续步骤的图文对进行表示的指导性文本，例如教程、工具手册等。此类型文档具备丰富的内容以及内容间复杂的关联关系，使得模型的理解和推理变得更具有挑战性。

       已有的研究工作大多采用针对单一模态的文本阅读理解技术或视觉问答技术，然后设计模态融合策略来提升模型的理解和推理能力。然而，现有工作对程序性多模态文档的信息并未充分利用。一方面，现有工作以文本或图像为直接处理对象，难以对于程序性多模态文档中的关键信息进行更细粒度的处理。因此，本文关注程序性多模态文档的实体层级，即文本中包含的名词和图像中的显著目标。另一方面，现有工作难以建模程序性多模态文档中多个实体之间的交互与演变过程。因此，本文关注程序性多模态文档在实体层级上的步骤间时序性关系与模态间匹配关系。

       基于以上分析，本文构建了一种用于表示程序性多模态文档的新颖的图结构，即异构实体图。在此图中，节点表示文档中的实体，边表示时序性关系和多模态匹配关系。接着，本文基于异构实体图提出了多模态异构实体图模型MHEG，该模型包括编码模块，融合模块与推理模块。其中，编码模块与融合模块负责对实体与关联关系等信息进行编码与融合，融合后的结果将由推理模块应用于具体任务之中。在实验阶段，本文收集整理了工具指导手册的相关文档，构建CraftQA 数据集。本文基于CraftQA 数据集和公开数据集RecipeQA 开展了多种多模态阅读理解任务的测试，包括视觉填空、视觉识异和视觉排序。实验结果表明，引入异构实体图后，MHEG 在两个数据集上的各类任务的准确率平均提高了3.82 个百分点和4.68 个百分点。此外，在RecipeQA 数据集的图像排序任务中，MHEG 的表现首次超越了人类基准1.56 个百分点。同时，本文开展的参数实验与消融实验进一步验证了MHEG 的有效性。"
2024,两种遗传图类中的染色临界图,计算机学院,李佳伟,黄申为,Security,0.2308,"图染色是图论领域的重要分支，其研究核心是要用尽量少的颜色为图染色。图染色起源于著名的四色猜想，即所有的平面图都是4-可染的。在四色猜想提出一百多年后，才通过计算机辅助的方式得以证明。在图染色中，研究一种特殊的图类——临界图——是非常有帮助的。近年来，许多学者对临界图的有限性问题展开了广泛研究。研究这个问题很有价值，因为临界图的有限性具有基本的算法意义。


给定两个图$H_1$和$H_2$，如果一个图中不包含同构于$H_1$或$H_2$的导出子图，则称这个图是($H_1$,$H_2$)-free的。如果图$G$的色数为$k$，并且从$G$中去除任意顶点都能得到一个($k-1$)-可染的图，则称$G$为$k$-点临界图。$P_t$表示有$t$个顶点的路，$C_t$表示有$t$个顶点的圈。如果$k$-点临界的($H_1$,$H_2$)-free图只有有限个，那么存在多项式时间算法来确定一个($H_1$,$H_2$)-free图是否是$(k-1)$-可染的。


对于5顶点图$H$，当$H$为$P_4+P_1$，$C_4+P_1$，$\overline{P_3+2P_1}$或$W_4$时，$k$-点临界的($P_5$,$H$)-free图的有限性问题还没有相关结论。bull表示由一个三角形加上两条不相交的悬挂边得到的图，$\overline{\text{diamond}+P_1}$是diamond+$P_1$的补图，其中diamond是从4个顶点的完全图中删掉一条边得到的图。本文分别研究了5-点临界的($P_5$,bull)-free图和5-点临界的($P_5$,$\overline{\text{diamond}+P_1}$)-free图的有限性。


通过Ho{\`a}ng等人证明的5-点临界的($P_5$,$C_5$)-free图的有限性结论，本文证明了5-点临界的($P_5$,bull)-free图和($P_5$,$\overline{\text{diamond}+P_1}$)-free图中都必存在$C_5$这个基本结构。然后基于$C_5$结构，将其他顶点按照与$C_5$上顶点的相邻情况进行划分，再分别对这些集合进行界定，从而证明了这两种图类中的顶点数都是有限的，即这两种图都只有有限个。


对于5-点临界的($P_5$,bull)-free图，本文基于$C_5$将其他顶点划分成了$S_0$，$S_5$，$S_2$，$S_3$和$S_4$并依次界定。对于5-点临界的($P_5$,$\overline{\text{diamond}+P_1}$)-free图，本文基于$C_5$将其他顶点划分成了$S_2$，$S_5$，$S_3^2$，$S_3^1$和$S_4$并依次界定。"
2023,基于深度学习的早产儿视网膜病变早期筛查和分期的方法研究,计算机学院,陶思凯,邵秀丽,CV,0.3863,"随着计算机技术和医学影像技术的发展，深度学习方法被广泛应用在临床诊断过程中，旨在实现对于病症的自动化分析和识别，从而帮助医师判断病情并辅助后续治疗手术的实施。本文基于早产儿视网膜医学图像，旨在通过深度学习方法实现对视网膜图像中的血管和嵴的语义分割，进一步以血管特征和嵴特征的差异为基础，使用深度模型实现对于早期病变的分类（正常、1期、2期和3期），进而为后续的人工干预治疗提供依据，挽救早产儿的视功能。然而，这一流程的实现会面临一些挑战：一方面在语义分割阶段，视网膜图像中的血管和嵴具有特殊且复杂的结构，例如边缘模糊、形态和走势多样、末端细小难以分辨等，其全局特征在不同样本间的差异较为明显，且视网膜图像包含较多噪声，由此影响了模型的分割性能。另外，像素级人工标注的高成本导致难以获取大量的标注样本；另一方面对于病变的识别和分期来说，针对早产儿视网膜病变有无的定性筛查依赖于视网膜图像全局视野下的特征差异，而对于病变分期的定量分析则依赖于局部区域中血管和嵴的形态差异，所依赖特征的类型和尺度的不同将导致深度模型在分类时面临困难。在病变分期过程中，不同分期图像的血管差异度沿着血管区域所在图像位置变化的分布是不均匀的。另外主流病变分类模型缺乏较强的可解释性。针对上述两方面问题，本文相应地开展了如下两个工作。


为解决视网膜图像血管和嵴的形态走势复杂、边缘对比度低、样本间全局差异显著、标注成本高等挑战给语义分割带来的问题，本文以U-Net为基础构建了一个“Y”字形的融合语义分割模型，该模型的编码阶段包含两条并行通路，一条通路实现对于血管或嵴的静态特征的提取，另一条则实现对于种子图像生长成原血管或嵴的动态生长映射的提取，两类特征在融合后进行单通路的解码过程。这样的设计旨在挖掘血管或嵴的局部生长趋势信息作为额外特征，这种连续性特征具有更精细的边缘信息和线条走势信息，对静态特征进行补充和精修，提高模型的分割精度和抗干扰能力。生长映射在样本的不同局部区域内或在不同样本间具有较高的共性，使得训练所需的标注样本数量减少。在血管和嵴的语义分割实验中，本文提出的融合模型相较于其他基准方法在准确度、灵敏度、特异度等评价指标上均取得更优越的结果，且在训练样本规模受限时，依然可以表现出优秀的分割性能。


为解决分类过程所依赖特征的类型和尺度不同的问题，本文设计了一个包括筛查和分期的两阶段分类流程，其中筛查阶段的鉴别网络根据早产儿视网膜图像在全局视野下的特征来筛查出正常图像并输出结果，而病变图像则进入分期阶段。在分期阶段中先对图像中的血管和嵴进行提取，而后使用分类网络分析其特征来输出图像的病变分期。对于血管特征，为解决其差异度分布不均匀的问题，本文设置了感兴趣区域同时用掩膜过滤其他无关信息。在分类对比实验中，本文提出的分类流程相较于其他基准模型具有更优秀的分类性能。同时，本文通过消融实验验证了流程中所设计的各个部分的有效性。另外，本文通过特征统计和回归分析证明了分期过程具有较强的可解释性。"
2022,基于深度学习的糖尿病视网膜病变分类方法研究,网络空间安全学院,胡春雨,李涛,CV,0.4327,"糖尿病视网膜病变（Diabetic retinopathy，DR）是一种不可逆的眼底疾病，也是导致人类致盲的主要原因。临床上，眼科医生根据彩色眼底图像对DR严重程度进行诊断。随着深度学习的不断发展，利用计算机技术辅助眼底筛查逐渐成为一种十分普遍的方式。但是仍然存在以下不足：（1）眼底图像中DR相关病变点微小、特征不明显，不同类别DR难以区分。（2）在单个数据集上训练的模型往往泛化能力较差，无法应用于真实临床场景中。本文采用深度学习方法，对DR分类问题进行深入研究。主要研究内容如下：


      首先，针对眼底图像病变区域较小，不同类别DR难以区分的问题，本文提出了一种基于注意力机制的DR分类方法CSCANet。通过加入混合注意力模块和类别注意力模块，将骨干网中浅层特征与深层特征相融合，学习更有区分性的特征。在三个公开数据集上的结果表明CSCANet可以有效提升DR分类问题的性能。其中，在DDR数据集上，以VGG16、ResNet18、ResNet50为骨干网的CSCANet较基准模型分别提升2.71%、4.73%、2.47%。与其他前沿方法对比时，CSCANet均取得较好的性能，充分说明CSCANet的有效性。


      其次，针对模型泛化能力较差的问题，本文提出了一种基于无监督域自适应学习的DR分类方法UDC-Net。通过加入自注意力模块对图像全局特征进行提取，加入最大均值偏差MMD将源域（Source Domain）和目标域（Target Domain）间标准分布距离度量不断缩小，学习源域和目标域的域不变性特征。在三个公开数据集上，UDC-Net较基准网络均有性能提升。

其中，在APTOS→DDR的实验结果中，以VGG16、ResNet18、ResNet50为骨干网的UDC-Net较基准模型分别提升2.23%、1.70%、1.94%。与其他两种方法进行对比时，UDC-Net均取得较好的性能。"
2022,海面和海岸场景下的小行人检测方法研究,计算机学院,盛晓颖,王恺,CV,0.4007,"2012 年 AlexNet 被提出，突破了传统机器学习的性能瓶颈，是深度学习和 时代算力发展结合的里程碑。近十年来，采用深度学习的方法解决社会生活中 的视觉问题成为了学界和工业界的热点研究方向，人脸检测是最为成功的一个 案例，被广泛用于安保系统和支付系统等。本文重点研究深度学习在水上遇险 人员搜救任务中的应用，体现为根据可见光图像，检测行人所在的矩形区域位 置。无人机拍摄的海面和海岸场景图像，具有分辨率高、行人区域面积小、受 海水遮挡的特点，在神经网络模型设计上面临着细节信息难以捕捉、正负样本 比例不均衡等挑战。


本文从特征提取和检测任务定义两个角度，探究优化海面和海岸场景下小 行人检测的方法。针对特征提取这一角度，本文引入医学图像分割领域常用的 编解码结构，使用短连接复用特征提取子网络浅层输出的细粒度特征，同时， 新增分割分支，生成像素级分类伪标签，促使特征提取子网络对细粒度特征的 学习；针对检测任务定义这一角度，本文首先设计了双回归分支协同监督策略， 以削弱 CenterNet 无锚点检测模型训练过程中，中心点位置波动对行人区域边界 学习的不利影响，而后，尝试修改无锚点检测模型的关键点选择策略，抛弃预 设标注框中心点为关键点的思路，在关键点选择上同时考虑分类属性，避免了 以标注框中心点为关键点定义模糊且学习困难的问题。


为了证明优化方法的有效性，本文在海面和海岸小行人检测公开数据集 TinyPerson 和私有数据集 SeaSOS 上进行了实验，采用本文提出的细粒度特征 优化方法，基于 ResNet18 骨干网，在 TinyPerson 数据集上相较基准检测模型 AP@tiny@25 提升了 15.63 个百分点，在 SeaSOS 数据集上提升了 8.21 个百分点。采用 本文使用的结合分类和空间属性的关键点选择策略，基于 ResNet18 骨干网，在 TinyPerson 数据集上 AP@tiny@25 提升了 3.14 个百分点，在 SeaSOS 数据集上提升了 8.63 个百分点。与此同时，本文实验结果证实了无锚点检测模型在小行人检测 任务中的应用价值，模型在 TinyPerson 上最优指标 AP@tiny@25 达到了 74.34%。"
2023,基于计算图的神经网络推理适配框架与优化研究,计算机学院,刘义情,卢冶,CV,0.322,"神经网络模型部署在深度学习硬件上时，往往面临推理框架对 AI 加速器兼容性受限、框架硬件适配周期长、调度不合理等诸多挑战，不能实现深度学习 模型的快速应用部署。计算图作为深度学习模型的中间表达，具有强大的分析、 优化和转换能力，在各种深度学习框架和图推理引擎都得到广泛应用。本文基 于计算图对神经网络的推理框架硬件适配和硬件与框架联合优化进行深入研究。


具体而言，本文对具有图级别运行时类库和异构平台两类硬件的框架硬件 适配和优化进行如下探索：在硬件适配方面，设计一种兼容 CPU、GPU、NPU 和FPGA 等硬件的通用计算图推理方案。对于具有图级别运行时类库的硬件平台（X86 CPU ），通过本文计算图推理方案在开源框架 PaddleLite 适配 OpenVINO推理库。对于 FPGA+ARM 异构平台（Xilinx ZU3EG），设计 FPGA 硬件抽象层将稀疏 FPGA 加速器集成至 PaddleLite 进行异构加速; 在框架优化方面，提出模型计算图重构方法解决框架算子调度不合理问题和设计框架层模型缓存方法解 决框架运行时内存占用过高问题，并从结构化剪枝、硬件子图优化等多个角度 进行 FPGA 硬件与框架联合优化提升模型异构推理性能。


实验表明通用计算图推理方案在典型 DNN 模型上可以获得与原生 Open- VINO 推理库相近的推理性能，框架增加的内存占用约为 PaddleLite 动态链接库加上模型输入输出的内存大小；提出的计算图推理方案在目标检测模型上的端 对端推理性能与 ARM 多核 CPU 相比速比为 2.8x 至 7.9x。对 SSD-MobileNetV1 和 YOLOv3-DarkNet53 模型分析模型计算图重构性能收益，模型计算图重构方法降低推理时延分别为 50% 和 12%。综上，本文提出的通用计算图推理方案对加速神经网络模型的推理与推动人工智能应用的落地具有实践意义。"
2024,基于多频信号的图节点分类技术研究,计算机学院,李玉奇,袁晓洁,CV,0.2517,"图数据是建模现实世界的重要数据结构，节点分类作为图数据研究的核心任务之一，主要依赖于图神经网络捕获邻居节点信息的强大能力来预测节点标签，广泛应用于情感分析、异常检测、实体识别和推荐系统等多个领域。


在现有图节点分类研究中，主要存在三个核心问题：第一，传统的图神经网络（GNN）在处理静态属性图时，往往类似于低通滤波器，即低频约束，这导致相邻节点的表示趋于相似，从而忽略重要的异常信息；第二，异常节点能够通过模仿正常节点的特征和结构来伪装自己，使得检测变得更加困难；第三，在处理静态异质属性图的节点分类任务时，现有异质图神经网络（HGNN）模型基于传统的空域图神经网络框架，在捕获高频信号和高阶邻居信息方面存在不足，常常导致过平滑问题。


本文主要聚焦于静态图数据上的节点分类研究，旨在合理利用高频和低频信号提升图节点分类的准确性和泛化能力。包含特征和标签的静态图数据主要分为两种类型：静态属性图和静态异质属性图。本文围绕如何高效利用高低频信号的问题，分别对这两类图的异常检测（极端类别不平衡数据下的节点二分类）和节点多分类进行了深入研究。对于静态属性图，本研究中引入了自适应带通滤波器，动态调整其频率以优化节点信号的捕获，解决了低频约束问题，并结合k最近邻（kNN）和k最远邻（kFN）图的方法，有效应对了节点伪装问题。在静态异质属性图的节点分类任务中，本研究提出了一种自适应多频率异质图神经网络方法。该方法通过设计新型的高低频信号滤波器，能够自适应地整合高频和低频信号，并利用受约束的马尔科夫过程对图拓扑结构进行优化，从而有效地捕获高阶邻居信息，显著提高了模型在节点分类任务中的准确性和泛化能力。


实验结果显示，本文针对两种图数据集提出的节点分类方法，在真实环境下均展现出卓越的性能。这些方法在准确率、通用性和可扩展性等多方面均超越了现有技术，证明了其在实际应用中的重要价值和深远意义。"
2023,基于双流双头卷积神经网络的图像复杂度评估方法及应用研究,计算机学院,冯停磊,杨巨峰,CV,0.4273,"图像复杂度 (Image Complexity, IC) 是人类观察图像时的一种重要视觉感知。精确地评估复杂度具有挑战性，一方面其因为依赖于人类的感知而相对主观；另一方面复杂度依赖于高层语义信息，所以需要模型拥有良好的泛化能力来处理多样的现实世界图像。


      为了能够精确地评估图像复杂度，本文提出了一种双流双头卷积神经网络，该网络使用细节和全局双流分支提取两种重要的复杂度特征。随后，这两种特征沿着通道融合，并送入后续的两个神经网络预测头来分别生成 IC 分数和 IC热度图两种复杂度模态。由于深度神经网络的数据驱动的特性，本文构建了已知的第一个大规模图像复杂度评估数据集 IC9600。该数据集包含 9600 张良好标注的图像，涉及抽象、绘画、真实世界场景等广泛的图像主题。为了减少标注者的个体主观性，数据集中的每一张图像都由 17 位标注者精心标注。在这个高质量的数据集以及先进的模型支持下，本文提出的方法在复杂度评估中表现出良好的性能，预测结果与人类感知的相关性（皮尔逊相关系数）达到了 0.955。


      进一步，本文探索了复杂度与多种图像属性之间的关联，大量的实验证明了复杂度影响着图像质量、美学等基本视觉属性。利用上述关联信息，本文将复杂度作为一种先验知识，提升了诸如图像分类、语义分割、显著性检测等多种计算机视觉任务的性能。最后，本文探索了将图像复杂度应用于计算机视觉外的更多研究领域的可能。"
2022,基于深度学习的跨外观行人重识别方法研究,计算机学院,马志,王刚,CV,0.3792,"行人重识别(Re-ID)是对目标行人进行跨时空检索的技术，在监控网络日益庞大的智能社会中具备极高的应用价值。现实生活中，工作人员往往需要凭借人眼在海量的视频数据中寻找相关线索，耗费了大量的人力物力。随着深度学习算法的进步，基于深度网络的行人重识别技术逐渐引入了研究者的关注。但是，现有的行人重识别研究工作大多基于行人不会变换外观的假设，基于外观变化的行人重识别数据集和模型严重缺乏。因此，本文从跨外观行人重识别数据集的建立到跨外观行人重识别模型及其损失函数的设计展开研究，主要内容如下：


针对现有跨外观行人重识别数据集严重不足的问题，本文提出了一个包含外观变化的行人重识别数据集NKUP，其中包含了来自362个行人的40217张监控图像。该数据集中不仅具备常规行人重识别数据集中的姿态、光照、背景、分辨率等多样性，还包含跨季节导致剧烈外观变化的行人图像。与此同时，本文基于9种常见的深度网络和4种效果最优的行人重识别模型在NKUP数据集上进行测试，为剧烈跨外观的行人重识别研究提供了可参考的实验基准。实验结果显示现有的深度网络应对跨外观行人重识别的能力存在严重不足，需要引入额外的先验知识来提升行人重识别性能。


针对基准模型在识别过程中仅考虑了RGB图像信息的问题，本文提出了一个基于多模态信息的M2Net网络，通过引入经预训练网络得到的行人边缘信息和语义信息，可以降低RGB图像中外观敏感型信息在特征中的权重，进而使网络学习对于外观鲁棒的行人特征。针对现有跨外观模型中外观属性先验知识缺乏的问题，本文设计了一个基于多外观的随机采样算法(RAS算法)，以保证跨外观模型训练的稳定性，以及基于此采样算法的多外观损失函数(MA损失)，以拉近同一人不同外观图像之间的距离。最后，再结合本文设计的基于外观的交叉熵损失函数(AP损失)，三者共同促进M2Net学习对外观鲁棒的行人特征。


实验证明，将Densenet121作为骨干网络，相比于基准网络，M2Net在NKUP和PRCC数据集的跨外观行人重识别数据子集上分别提升了1.6%、7.5%的Rank 1值和0.7%、6.1%的mAP。而在网络中加入RAS采样算法、MA损失和AP损失的辅助之后，M2Net在两个数据子集上Rank 1值的提升分别达到了8.3%和30.7%，而mAP指标的提升则分别达到了3.1%和26.6%。最终指标同时也显著超出了同期所有的行人重识别模型，充分证明了本文所提算法的有效性。"
2024,基于视频和图形混合流的云游戏系统,计算机学院,成钟书,李雨森,OS,0.2133,"云游戏市场随着游戏行业的发展日益扩大，降低云游戏服务的成本成为云游戏平台不可忽视的问题。现有商用云游戏平台采用的基于视频流的系统架构，游戏程序的负载全部由服务器承担，这导致了服务端高昂的硬件成本；视频流的带宽开销随着画面分辨率增长也使得高品质云游戏服务的带宽成本较高。基于图形流的云游戏系统通过将渲染命令发送到客户端执行的方式使客户端承担了游戏程序的渲染负载，降低了服务器的硬件成本却提高了客户端的硬件要求而难以商用。若云游戏系统允许将渲染负载同时划分给服务端和客户端，便能够结合视频流和图形流两种方案的优势。


设计一个能够划分渲染负载的云游戏系统面临两个主要挑战：（1）渲染负载需要能被细粒度的划分。在已有的分布式渲染云游戏系统研究中，渲染负载的划分基于渲染管线拆分或帧与帧的划分，划分的粒度较粗使得系统难以通过负载调度算法达到最佳性能。（2）渲染负载的划分比例需要调度算法进行动态地调整。在游戏运行过程中由于场景变化和玩家输入等原因常常导致渲染负载发生变化，此时需要重新调整负载划分的比例以达到理想的系统性能。渲染负载划分粒度较粗的系统便难以应对真实情况下经常变化的渲染负载。


结合上述分析，本文设计了一个基于混合流的云游戏系统，通过分割帧的方式将渲染负载划分给服务端和客户端，一部分画面在服务端渲染后发送到客户端，客户端则接收服务端发送的渲染命令并执行得到另一部分画面，最终合并二者得到最终画面；并设计了相应的负载调度算法使系统在运行时维持满足QoS且将渲染负载尽可能划分给客户端的状态。实验表明，相比基于视频流的架构，本文提出的云游戏系统能够在将帧率从45FPS提高到满足60FPS的同时减少服务端约50%的GPU使用率以及约25%的带宽占用，或将帧率提高到最大70FPS。"
2022,基于深度强化学习的商用服务器资源划分算法研究,计算机学院,吴进平,李雨森,Network,0.3397,"将多个应用运行在同一台服务器上是商用服务器提高资源利用率的常用方

法。然而，由于应用之间的资源竞争，应用之间会发生性能干扰，从而导致应用的性能显著下降。这使得设计高效算法为应用进行资源划分以提高应用性能成为一个重要的问题。由于资源竞争的复杂关系和当前资源隔离技术的局限性，在应用之间实现精确和细粒度的资源隔离仍然是一个具有挑战的任务，尤其是当考虑到多维资源协同划分的时候，问题变得更加困难。


  本文建立了一种基于图神经网络的性能预测模型来预测任意的资源分配方

案下应用的性能。因为资源隔离技术本身的局限性，导致资源划分方案具有同构性，即存在表示不同但是实质相同的划分方案。本文将资源划分方案抽象为二部图，以使得同构的资源划分方案具有相同的表示从而降低预测难度，然后基于图神经网络设计预测模型。进一步，利用预测模型提供训练所需的奖励，本文基于深度强化学习框架提出了一种新的资源划分算法，用于以端到端的方式同时为应用划分最后一级缓存和内存带宽。为此，本文将资源划分问题抽象为马尔可夫决策过程以使用深度强化学习方法。然后，本文基于编码器-解码器结构设计策略网络，编码器提取应用特征，解码器输出划分方案，并利用注意力机制在编码器和解码器之间更有效的传输特征。最后，使用 Actor-Critic框架训练策略网络。


  本文通过在真实服务器上进行大量的实验来测试本文提出资源划分算法的

性能。结果表明，本文的方法能够做出即时且性能优异的划分方案，以显著的优势优于现有的基准方法。例如，对于6 个应用的组合，与完全共享资源相比，本文的算法可以将系统吞吐量平均提高9.0%（最高32%）。"
2022,基于GPU内存分区的图计算数据超载问题研究,计算机学院,唐瑞琦,杨愚鲁,Network,0.2741,"图计算，因其有效表达现实数据模型的特性，在现今这样一个大数据时 代，越来越受到真实计算应用场景的青睐。而相比传统的 CPU计算，GPU拥有 着更强大的并行计算能力和内存带宽，因此逐渐成为图计算应用的主要计算 加速工具。然而，由于社交网络和大数据爆炸式增长，图数据样本随之逐渐增 大，GPU现有内存可能无法完全加载完整的待测试数据。在此情境下，为了完 成计算必须自 CPU端频繁的搬运数据至 GPU端。由于现行 CPU/GPU一般使用PCI-E总线进行数据交互，传输带宽相比 GPU计算带宽较小，因而这样的频繁 数据传输将极大的拖慢 GPU端计算速度。


    为了解决数据频繁传输导致的计算性能低下，较为理想的方案是通过探索 图计算的数据访问规律，从中寻找计算中数据访问的局部性，进而通过预取的 方式将频繁访问的数据提前存储至 GPU端，从而解决由于 GPU内存有限导致 计算中数据传输时间过长的问题。但是因为图数据集的体量过大，目前的解决 方案都无法充分利用图计算迭代间的数据复用性。因此在本篇论文中，将对现 有常用图计算应用进行分析，发现在 GPU图计算应用尤其是图数据样本大于GPU内存情况的图计算应用中，每次迭代对图数据的访问具有一定的规律性。 


    基于这样的观察，本篇论文构建出一个基于 GPU的图计算加速框架——Ascetic，利用观察到的图计算数据访问局部性的特点，减少 GPU图计算过程中 的冗余数据传输，从而提升计算性能。Ascetic通过将 GPU内存划分为两个区域，一个区域为频繁使用数据的固定保存区域，另一个则为计算需要使用但未保存至GPU内存中的临时数据存储区域。通过有效的内存区域划分和数据调度，该计算框架可以有效的减少数据传输，并将数据传输与 GPU计算并行进行。 


    本工作使用 CUDA实现了基于 GPU内存区域分块的图计算加速框架Ascetic，并在该框架基础上进行了一系列性能验证实验。实验结果显示，使用Ascetic，可在现有工作基础上达到2.0倍以上的加速效果。"
2023,分布式生成对抗网络的模型研究,计算机学院,吴子文,王玮,CV,0.3328,"生成对抗网络（GAN）是一种强大的生成模型，能对真实数据分布进行建模并生成高质量数据。在大数据时代，由于隐私保护等原因，许多数据被分散存储在不同的节点上，无法直接共享。因此，在不进行数据共享的条件下，设计出高效集成各节点数据分布信息的分布式GAN模型是本文的主要研究目标。


    Fréchet Inception Distance（FID）是评价GAN性能的常用指标，它评估了数据的真实性和整体多样性，但不能单独度量数据的类内多样性，很难准确评价模型生成同类不同风格数据的能力。本文提出一种风格多样性加权距离SDWD，它通过预训练的分类器和孪生神经网络对生成数据的各类内部的风格差异进行度量，专注于评价类内多样性。实验表明SDWD能独立反映数据类内多样性的变化，并能稳定地评价分布式GAN的生成能力，可作为FID的有效的补充指标。


    通过分析现有的中心化架构的分布式GAN，本文发现它们直接平均各个节点模型的参数或梯度，忽略了数据分布的差异，导致生成的数据缺乏多样性，出现模式坍塌的问题。为了解决这个问题，本文在中心化架构的基础上提出了一种潜空间指导的分布式GAN模型LDGAN，它利用潜空间相似度来度量节点之间的分布差异，并用潜空间集成模块来加权集成各节点的梯度，从而指导生成器生成更多样化的数据。在三种数据集上的实验结果表明，LDGAN在FID分数上分别比最先进的分布式GAN降低了15.0%、22.8%和15.3%，SDWD分数也均低于其他模型。此外，本文还提出了变体模型LDGAN-RO，通过加入过拟合抑制模块引入随机性，进一步提高模型的泛化性能。


    通过进一步对两种分布式GAN架构的分析，本文发现基于单独一种架构的模型难以同时兼顾数据的全局和局部信息。为此，本文提出一种半中心化的分布式GAN模型SCGAN，结合两种架构的优势，利用特征提取器动态加权集成去中心化和中心化两种生成器，实现数据全局和局部分布信息的有效融合。实验结果表明，SCGAN在四种数据集上的生成能力优于其他模型，FID和SDWD分数均为最低。在分布式隐私场景下，通过引入差分隐私技术提出了SCGAN-DP模型，在保证隐私安全的同时，获得了比其他隐私保护模型更强的生成性能。"
2019,融合多视角RGB-D图像帧的物体检测和场景理解,计算机学院,李祥攀,孙凤池,ML,0.2437,"随着人工智能技术的发展，机器人逐渐成为人类社会生活中不可或缺的助力因素。在过去的一段时间中，工业机器人已经显示出了强大的生命力。随着全球人口老龄化问题愈演愈烈，家用服务机器人的重要性也愈发突出。相比于工业机器人，家用服务机器人需要具备更强的移动性和更高的智能性。


服务机器人在工作时需要尽可能全面地掌握其所在场景中的信息，但是由于视野有限，机器人难以直接采集到整个场景的图像数据进而进行信息提取。于是，机器人需要在不断游走的过程中在不同视角采集多帧数据，并且将多帧数据中的信息进行融合，以完成对其所在场景的综合性的信息提取。一些传统的方法是通过图像拼接技术将多帧图像拼接成为一幅全景图像，进而进行物体检测等步骤。但是图像拼接技术对视角变化较为敏感，拼接效果并不稳定。另一些通过三维重建算法进行信息融合则需要进行大量计算，对机器人平台所携带的计算能力提出很大挑战。


基于以上背景，本文提出了室内场景基于多帧图像的物体检测与关系提取算法，该算法通过基于颜色直方图特征的物体实例识别算法，将多帧图像中的物体检测结果进行融合，同时通过基于深度图像的关系提取算法构建整个场景的物体关系图。该算法在控制计算复杂度的同时，在一定程度上克服了视角变化对物体实例识别带来的影响。而相比于语义地图和场景三维模型等数据信息，物体关系图的表达更加灵活，不受到物体的位姿变化等因素的影响。整个场景内的综合性物体关系图对多帧数据融合的结果进行总结和展示，同时也能够为机器人的其他任务提供指导性帮助。"
2019,基于深度学习的眼底黄斑水肿OCT图像上的智能诊疗算法研究,计算机学院,李紫薇,殷爱茹,AI,0.3115,"在多种医疗健康场景下，将人工智能卓越的计算能力和现代医学诊疗相结合，开展智能医学影像识别进行辅助诊疗势在必行。深度学习作为人工智能中扎根数据来解决问题的手段，已经在多种研究场景下发挥了不错的辅助诊疗效果。


视网膜黄斑水肿是造成人眼中心视力损害的最主要原因，为大量患者造成了很大的生活困扰。光相干断层扫描成像（Optical Coherence Tomography，OCT）在眼底黄斑病变诊疗中是极为重要的医疗影像资料。利用深度学习构建针对性的算法，应用在眼底黄斑水肿病变OCT图像的自动诊断上，在辅助诊断方面能产生积极作用。本文针对实际的应用场景，在眼底水肿病变OCT图像数据集上提出了三个任务，并利用深度学习的技术加以解决。


首先，为了将积累的海量眼底OCT图像医学资料进行分门别类的整理，以便医生日后的查阅和学习，本文提出了基于卷积神经网络的眼底病变OCT图像分类模型。结合对数据集中样本的统计分析和实验探究，本文认为在效果和效率上都表现优异的以ResNet作为骨干网络、以Multi-label作为标签生成风格，是比较适用于该任务的解决方案。


本文接下来提出了在该数据集上开展病灶区域检测，以辅助医生快速识别和定位图像中的病变区域。通过分析数据集中病灶区在形状和尺寸上的统计结果，本文在经典Faster R-CNN的基础上，对初始候选框的生成作了调整。由于医学问题中需要辨别的生理构造在特征上缺乏一定的规律性，本文提出通过修改标签生成方式来调整模型的处理策略。实验证明，本文的改造在该任务上有显著的效果。


最后，为了方便医生对具体眼底水肿严重程度的量化分析，本文提出了病变的图像分割任务。结合OCT图像中病变有效面积较小且位置不定的特点，本文在U-Net的基础上设计了三种模型来提升分割效率。实验证明，本文构建的在U-Net的解码阶段融合多注意力机制的模型取得了不错的效果。为了增加模型的可解释性，本文利用可视化对实验效果进行了展示和分析。"
2019,基于深度学习的博物馆照片超分辨率重建系统,计算机学院,魏鑫,张金,Security,0.2483,"随着技术的发展，照片的分辨率越来越高。但是，无论是博物馆还是普通居民的家中，都有上个世纪留下来的老照片。这些照片由于当时拍摄与胶片冲洗技术的限制，往往尺寸较小。修复放大这些照片，不仅可以帮助历史学者更好的还原历史，还可以让普通居民缅怀过去，具有很高的科研价值与社会价值。


超分辨率重建技术是指利用多幅图像的互补信息，将一幅或多幅低分辨率图像重建出相应的高分辨率图像。当前超分辨率模型的数据集多是高分辨率图像双三次插值生成低分辨率图像，其低分辨率图像与真实应用环境有一定差距，训练的模型往往鲁棒性不高。而基于特定领域的超分辨率模型在特定领域上比通用模型效果好，比如人脸、风景等，但基于特定领域的模型实用性不强。


针对上述数据集与真实应用环境存在差距的问题，本文建立了老照片数据集，共包含高分辨率与低分辨率图像82组，其低分辨率图像为黑白胶片相机拍摄的照片扫描获取。在建立过程中，设计优化了图像对齐的流程，提出了基于匹配位置约束的特征匹配优化方法，使得胶片相机拍摄的图像与CCD相机拍摄的图像可以完全对齐。最终完成贴合实际应用环境的老照片数据集。


针对上述基于特定领域的超分辨率重建模型效果好但不实用的问题，本文设计了基于分类的超分辨率重建模型（Classification Based Super-Resolution，CBSR）。首先，设计一个通用粗粒度重建模型以提高后续分类的准确性。然后设计分类算法，将图像切块，并将图像块分为包含较多高频信息的纹理类与高频信息较少的平滑类。之后，设计不同的细粒度超分辨率重建模型对不同类别的图像块进行重建。最后，使用图像融合得到完整的高分辨率图像。


综上，本文建立了老照片数据集，并设计了基于分类的超分辨率重建模型。实验证明，基于老照片训练的模型较VDSR模型使用的数据集PSNR提升了约0.7（2倍至4倍），SSIM提升了约0.06。最后，基于老照片数据集训练的CBSR模型，本文设计了博物馆照片的超分辨率重建系统，可以同时重建单幅或多幅指定倍数的老照片，并选择单幅下载或批量下载。"
2019,基于代码语义分析的应用研究,计算机学院,樊亚青,许静,Security,0.3017,"近些年来，随着各类软件使用量的增长，维护和重新设计现有软件系统的需求不断增加，该工作需要花费大量精力来理解源代码。为了理解不熟悉软件源码的基本意图，软件工程师会在代码和文档中寻找程序的结构信息，如编程模式和习语，程序依赖性等。但却忽略了程序代码内部潜在的语义，反映软件行为，功能，组织和体系结构的信息，对于理解源代码和软件的维护更新工作具有非常重要的意义。


代码语义分析是通过应用于大型文本的统计计算，来提取和表示单词上下文使用意义的理论和方法。现有的代码语义分析方法仅仅通过分析软件系统的结构来理解程序，而忽视了隐藏于代码内部的程序语义。为了更精确地表征程序语义和功能信息，使得研究人员更好地理解源代码，本文提出了基于代码抽象语法树的语义分析方法，通过生成抽象语法树，分析统计树结构和树结点信息，来提取到源代码内部更多语义和功能特征以表征源代码。


我们将基于代码抽象语法树的语义分析方法应用于两个实际问题的解决中。第一，早期的缺陷检测可以帮助开发人员在测试和维护阶段正确分配有限的资源。但现有的软件缺陷预测方法无法捕获程序中隐含的语义差异，预测准确率偏低。本文通过对Android应用软件的源码进行语义分析，结合深度信念网络从代码的抽象语法树中提取语义特征用于缺陷预测。第二，随着互联网技术的发展，用户评论扮演着重要的角色，缺乏足够的用户反馈使得开发人员难以通过真实的用户体验来改进产品质量。经调查发现，对于相似度极高的代码对，也称克隆对，有些有数百条评论，但有些却很少。为解决这一问题，本文通过使用代码语义分析的方法检测出克隆对，实现相似代码段之间的用户评论共享推荐，为软件的更新改进工作提供了指导。


实验结果表明，本文提出的基于语义分析的Android缺陷预测模型具有一定的可靠性，可以准确预测缺陷代码所在文件，提高了缺陷预测的准确率。同时，本文构建的用户评论推荐系统在真实情况下有着良好的共享表现，能够检测到更多真实的克隆对，实现评论共享。"
2019,前端java代码保护方案的研究,计算机学院,逯海涛,贾春福,Security,0.3607,"在互联网规模的快速增长过程中，浏览器已成为当今互联网的一大流量入口，驱使大量web 应用出现并给用户提供便捷服务的同时，也涌现出大量恶意用户通过不正当手段来谋取利益。java 作为一种直译式的脚本语言，被广泛地嵌入到html 中为web 应用提供丰富的交互功能，但往往也是恶意用户最先突破的地方。由于web 端的代码是加载到用户浏览器上在本地执行，所以恶意分析者可以很轻易找出程序的核心算法和业务逻辑。有效保护前端代码是当前业务流程当中非常重要且急需解决的问题。




目前的保护技术主要是代码混淆，但当前已有的针对java 代码的大多数混淆工具保护强度较弱，并不能有效阻止恶意分析者。针对上述情况，本文基于collatz 猜想提出一种新的混淆策略collatzjs。对于每一个大于一的整数，经过collatz 猜想的计算，会生成对应的一组序列，collatzjs 的基本思路便是通过if 判断语句对顺序输出的序列进行条件判断来对代码进行流程控制，实现混淆的目的。在此基础上，根据collatz 猜想生成序列的特性，并设计出多种适应不同程序结构的混淆规则。以collatzjs 为核心，本文又结合当前已有的保护方法：反调试、防篡改、异常处理和代码压缩，形成一个针对java 代码的完整保护方案。




在整个保护方案中，混淆作为核心部分，为此对提出的混淆规collatzjs对比当前已有的混淆工具和混淆策略，从被保护后文件体积的增长率，程序执行效率和抗逆向性三个方面证明其具有实用性。并将collatzjs 实现为一个可在线使用的混淆器，提供对java 代码的实时混淆。"
2019,基于FFmpeg的分布式视频处理管理平台,计算机学院,李旭,赵宏,Security,0.2563,"随着智能手机的普及，以及5G网络时代即将到来，用户的注意力慢慢从文本转向音视频。音视频应用如雨后春笋般兴起，在视频应用行业，每天需要进行大量的视频转码工作，以适应不同网络传输环境、不同播放终端设备、不同应用场景的需要。


当前，分布式转码、高质量低码率转码、多倍速转码、实时极速转码、并行转码、管道任务正在成为视频处理系统的发展趋势。服务器资源低空闲、转码任务集中控制调度、基于转码中间产物的识别，也逐渐崭露头角。众多转码系统横空问世为国内移动音视频行业、新媒体发展、自媒体推广提供坚实的基础服务。


本文首先介绍了视频处理平台的发展现状以及文中涉及的关键技术点；其次针对广电领域的业务需求，综合运用软件工程的方法，设计、实现了一个面向广电领域客户的视频处理管理平台。本系统采用基于容器化的分布式微服务架构，能够实现高效地处理转码任务，与现有内容管理平台做到无缝对接。此外，系统还具图像识别等功能，提高视频编辑日常工作效率。系统遵守通用WEB浏览器范例实现，符合通用浏览器的标准，使用浏览器便可完成提交转码工作、实时查看转码进度、存储配置管理等一系列与广电系统生产相关的业务。


该系统在天津网路广播电视台的实际运行表明，系统能够达到提升转码效率的目的。"
2019,复杂室内环境中智能手机用户的定位方法研究,计算机学院,于文平,张建忠,Security,0.2465,"在过去的十几年中，伴随着移动互联网的飞速发展，人们对各类位置服务的需求日益凸显。研究表明，人们生活中$80\%$到$90\%$的时间是在室内环境中度过的，因此开发实用有效的室内行人定位技术具有现实的迫切需求和巨大的应用前景。由于智能手机不仅支持多种无线通信技术，而且内置的传感器可以提供室内行人定位所需的各类基础数据，基于智能手机的室内行人定位方法逐渐成为室内定位研究中的热点。但是，迄今为止还没有任何一种基于智能手机的室内行人定位方法能够满足面向广域覆盖、长期运行、定位准确的实用定位系统的技术要求。这其中一个主要原因是相比于室外环境，复杂且多变的室内定位环境给室内行人定位方法的研究带来诸多挑战。本文从室内定位环境的时空统一抽象研究出发，将室内定位环境的复杂性和多变性带来的挑战归纳至三类定位场景下：应急定位场景、长期运行场景和无线新增场景。应急定位场景下，可定位信息少，行人定位实现难；长期运行场景下，环境动态变化，无线数据更新弱；无线新增场景下，定位精度与设备数目正相关，方案部署成本高。针对上述三大问题，本论文围绕着提高室内行人定位方法的实用性这一目标，以智能手机对室内环境中的物理地标、无线指纹和蓝牙地标的感知为主线，展开深入的研究和讨论。本文的主要研究内容如下：


（1）室内定位环境的复杂性和多变性的抽象方法。从空间视角下分析了室内物理空间的平面结构的分类，并探讨了定位方法、空间结构和行人之间的约束关系；从时间视角下分析了室内可定位信息对不同时序变化程度的容忍性。结合时空双视角下的研究结果，提出了一种室内定位环境的时空统一抽象模型。该抽象模型不仅在空间维度上实现了对于多种多样的室内物理空间结构的统一分类，而且在时间序列上实现了对于环境多变性的归纳总结。从定位理论角度来看，时空统一抽象模型无论对现有室内行人定位方法的环境适应性评估，还是对室内行人定位研究过程中的可行性分析，都具有指导意义。


（2）应急定位场景下结构化空间行人定位方法。针对应急定位场景下行人定位实现困难的问题，充分挖掘智能手机对于行人的运动模型和室内物理地标的感知能力，提出了基于行走轨迹序列匹配的室内行人应急定位方法。首先，本文利用运动传感器识别行人的位置相关动作，以动作发生时间为基准将行人的行走轨迹划分为子轨迹序列；其次，本文在物理空间的平面抽象模型基础上将室内平面地图映射为一个有向图；最后，利用隐马尔科夫模型将行人的行走子轨迹序列匹配至有向图中的结点序列，进而实现地图匹配和行人航位推算（Pedestrian Dead-Reckon, PDR）方法的融合定位。实验结果表明，本文提出的行人定位方法不仅能够在结构化空间中提供准确定位，而且对于行人的步长估计误差和前进方向测量误差具有一定程度的鲁棒性。


（3）长期运行场景下无线指纹数据库的运维和改进方法。结合群智感知机制，设计出面向行人的指纹数据库的长期运维方法。特别是针对指纹数据库对环境动态变化的适应问题，提出了一种基于置信度计算的指纹数据库自适应更新算法。该方法利用普通智能手机用户在室内行走的时机自动上传位置指纹，从而实现指纹数据库的自动构建。为了保证位置指纹的正确性和时效性，引入了位置指纹置信度属性。首先，考虑到PDR方法累积误差和行走步数的正相关关系，本文用行走步数的倒数作为权重表达绑定误差对于指纹正确性的影响；其次，用位置指纹上传后的时间间隔结合一个时间更新因子作为权重表达环境变化对指纹时效性的影响；最后，上述两个相互独立的权重的乘积即为位置指纹的置信度。实验结果表明了基于置信度计算的指纹数据库自适应更新方法的有效性。另外，本文还利用层次聚类的算法对指纹数据库的扁平化存储方式进行了改进，提升了指纹数据库使用效率。


（4）无线新增场景下低功耗蓝牙辅助的行人定位方法。针对无线新增场景下方案部署成本高的问题，本文通过对PDR方法的误差分析推导出定位精度需求到行人行走距离阈值的映射关系，进而提出了一种面向定位精度和部署成本优化的低功耗蓝牙（Bluetooth Low Energy, BLE）信标部署方案以及对应行人定位方法。室内物理空间的平面抽象模型将复杂多样的室内平面结构划分为结构化空间、开放空间和毗邻区。BLE信标部署方案分别针对上述三类独立空间的结构特点，设计出了不同的BLE信标部署方式。以此部署方案为基础，本文提出了一种BLE信标和PDR融合的行人定位方法。我们在三处不同结构的室内环境中对BLE信标部署方案和行人定位方法进行了实验验证，实验结果表明本文的行人定位解决方案能够在保证定位精度的前提下从整体上降低BLE信标部署成本。"
2019,面向开放域的对话生成关键技术研究,计算机学院,赵雪,袁晓洁,AI,0.2791,"对话系统是人工智能面临的一项重要挑战之一，要求机器在理解人类自然语言后能够产生相应的回复，以达到能与人交流的目的。近年来，对话系统已经成为自然语言处理领域最活跃的研究问题之一。尤其是面向开放域的对话系统，其上下文复杂性、回复空间的开放性使得生成语句通顺、语义合理且有意义的回复十分具有挑战，而这也是本文的主要研究内容。


在透彻分析面向开放域的对话系统存在的主要问题后，结合深度学习技术，本文对面向开放域的单轮对话、多轮对话以及多轮长对话进行深入研究，分别提出能够提高生成回复质量的解决方案。主要的贡献和创新性包括：


第一，提出了面向开放域的单轮对话回复生成方案。在单轮对话情景中，现有模型大多通过神经网络学习输入和回复语句之间的映射关系，对输入的上下文对词典进行概率分布预测，再通过集束搜索方式得到概率最大的k个生成回复。由于上下文短且信息量少，回复空间很大，模型更倾向于学习预测高频、无信息量的回复，比如“呵呵”，“我不知道”等；且k个回复中往往存在大量低质量或重复的生成样本。因此，本文通过构建基于 Transformer 解码器的语言模型，利用其多头注意力操作对输入语句更有效地进行特征表示学习；通过抽取表情符并作为条件加入语言模型中进行训练，使模型学习有情感的对话生成；通过在对话数据上微调预训练语言模型BERT使其具备判断两句话是否具有语义关联的能力，创新性地利用BERT作为排序模块，为采样得到多个的候选回复进行排序，实验证明，提出的方法能够得到流畅、上下文语义一致且有意义的高质量回复；


第二，提出了面向开放域的多轮对话回复生成方案。多轮对话问题中上下文信息抽取十分重要，而问题的关键在于从语义层面构建上下文表示能力。现有模型多依赖神经网络构建词之间或句子之间依赖关系，并未从语义角度进行建模，生成效果不够自然合理，且模型往往预测无意义的高频语句。本文通过引入预训练语言模型BERT来增强模型对语义知识的学习。BERT在多项自然语言理解任务上达到了先进水平，本文通过利用BERT作为特征抽取模块，将BERT的语义知识迁移到对话理解和生产任务中；再结合变分思想，引入隐变量来对高层潜在语义如话题、情感等进行建模，增加了采样过程，以获得语义合理的回复同时保证生产回复的多样性。


第三，提出了面向多轮长对话的回复生成方案。因开放域对话话题随意且多变，导致对话过程边界难以确定，因此多轮长对话在开放域对话系统中十分常见。现有的相关研究工作并未对长对话进行专门探究。尽管可以通过设置最多轮次或最多字词数来处理历史聊天信息，但是都不可避免地导致历史信息丢失。另一方面，历史信息的参考价值比较有限。因此，我们对历史信息进行句子级别粗粒度表示、对最近的对话内容进行词级别细粒度表示。通过构建记忆模块来保留对话中的句子表示；提出混合粒度注意力机制的语言模型，对记忆模块中的句子表示和最近轮次的词向量表示同时进行注意力操作，以实现多轮长对话的生成。并且，历史词序列则可以完全清除以节省存储和计算资源，因此可以处理更长的对话过程。实验效果证明本方法与其他模型相比生成效果更优且更高效。"
2021,农业精准扶贫信息系统的设计与实现,计算机学院,姚秉泉,李忠伟,SE,0.2465,"在社会主义新时代，在贫困区域实施精准扶贫，进行脱贫攻坚，落实农村贫困群众全部摆脱贫困，已成为全面建成小康社会过程中中国人民必须要完成的任务。研究与开发农业精准扶贫系统，具有重要的意义和现实价值，有利于实现对农业精准扶贫工作的信息化管理，以促进精准扶贫领域信息共享、提升扶贫资源利用效率。


针对农业精准扶贫领域的现状和主要业务流程，本文首先分析了系统的功能需求。然后，文章阐述了系统的设计方案，主要包括功能模块设计和数据库设计，功能模块设计包含系统管理、农业扶贫对象管理、农业扶贫方案管理、农业扶贫项目管理、扶贫干部考核管理、数据分析管理等功能模块。最后，在系统需求分析、系统功能模块设计和数据库设计的基础上，使用Java语言和MySQL数据库开发了基于B/S三层体系架构的系统，并对农业精准扶贫系统中主要功能界面的实现情况进行介绍。


本系统的应用能够有效提升农业精准扶贫领域的信息化水平，促进农业资源配置效率提升，推进农业生产要素和扶贫资源合理有序流动，以及调动扶贫干部积极性，因此，开发本系统具有重要的现实意义和应用价值。


关键词：扶贫系统；农业精准扶贫；功能需求；信息化水平"
2021,加密去重系统中的密钥更新问题研究,计算机学院,哈冠雄,贾春福,SE,0.2514,"外包数据的隐私保护和高效存储是目前云计算中存在的两大关键问题。加密去重结合了密码算法和去重技术实现了外包数据的隐私保护和存储空间的有效节省，得到了学术界和工业界的广泛关注。现有加密去重系统大多基于消息锁加密（Message-Locked Encryption, MLE）或服务器辅助MLE实现，这导致了系统在密钥更新时通常存在以下两个挑战：1）在MLE中，拥有相同数据的多个用户共享同一加密密钥，某一用户更新密钥时需要全体数据所有者同步该更新，这将引起较大的计算和通信开销。2）在服务器辅助MLE中，一旦密钥服务器管理的系统主密钥泄露，外包数据的机密性将无法得到保证。而系统主密钥泄露后的密钥更新问题目前还未得到足够的关注和有效的解决。针对上述加密去重系统中存在的密钥更新问题，本文的主要工作如下：


（1）提出了一个基于全有或全无转换（All-or-nothing transform, AONT）和NTRU的密钥更新方案，其设计了一个AONT的变体以解决多用户数据去重时密钥更新的同步问题，引入了一个基于NTRU的代理重加密算法以降低密钥更新过程中的系统通信开销和客户端计算开销。方案利用AONT将用户数据拆分为用于数据去重的修剪包和用于密钥更新的存根。基于AONT的全有或全无特性，密钥更新时客户端仅需重加密存根部分，而用于数据去重的修剪包可维持不变，这一特性避免了多用户密钥更新时的同步问题。方案还引入了基于NTRU的代理重加密算法，密钥更新时客户端仅需计算并上传一个重加密密钥至云服务器，大大提升了密钥更新的执行效率。


（2）提出了在MLE中引入可更新加密的思想，设计了两个基于可更新加密的密钥更新方案以解决服务器辅助MLE中系统主密钥泄露的这一问题。所提方案中的系统主密钥泄露后，密钥服务器发送一个更新令牌至云服务器，简洁高效地更新外包数据的加密密钥，大大节省了这一过程中所需的计算和通信开销。方案中的密钥更新具有用户透明的特性，即整个密钥更新过程仅由密钥服务器和云服务器完成，无需客户端参与，这避免了数据所有者离线时难以进行密钥更新的问题，使方案更适用于真实场景。"
2020,基于区块链的数据交易系统的研究与实现,计算机学院,陈孟骐,贾春福,SE,0.26,"在大数据时代的背景下，人们期望着能从形式复杂的海量数据中获取到能够对预测未来发展提供帮助的关键性信息，这使得如今各行业对数据的需求达到了前所未有的程度。由数据提供者精心收集并加工处理的数据一般需要付费购买，而在线上数据交易的过程中，数据购买者的权益很难得到保障：数据集信息的多元化使得数据购买者难以在数据交易过程中对自己所购买数据的可用性做出判断；线上数据交易通常采用的“先付费后下载”交付模式也难以保证数据购买者获取到应得的数据。


现有的第三方数据交易平台一般采取保留交易凭证并在事后进行监管的运作方式，在一些方面存在着局限性。一方面，缺乏数据可用性检验方法使得平台难以涉及争议的交易做出正确的仲裁结果。另一方面，事后处理的监管方法无论从时效性或实用性角度都无法保证数据购买者利益。最后，中心化的监管处理以及交易凭证存储带来了单点信任的安全问题。


针对上述问题，本文提出了基于机器学习模型的数据可用性检验方法，确保数据购买者在进行数据交易前有能力对数据的可用性做出独立的判断；设计了基于以太坊智能合约的数据交易交付方法，利用区块链去中心化且不可篡改的存储特性以及智能合约自动执行的特点，保证数据交易中金钱与数据的顺利交付。


基于上述工作，本文编写了智能合约与本地运行的客户端软件，实现了基于以太坊的数据交易平台原型系统，并对系统进行功能测试以及性能分析，对所提出数据交易关键方法的可行性与安全性进行了验证，为数据交易中存在的问题提供了现实可行的解决方案。"
2020,面向物联网的智能视频监控系统设计与实现,计算机学院,赵泽鲲,卢冶,OS,0.2404,"视频监控技术在各个方面都得到非常快速的发展，交通、社会安防等许多领域都应用到了视频监控系统，利用视频监控可以掌握监控场景下的信息。但是普通视频监控需要安保人员集中精力地守在监控屏幕前，被动式的监控不仅耗费人力且容易漏报突发事件。结合了计算机视觉技术的智能视频监控系统嵌入了智能分析算法，不但可以减轻安保人员的负担，并且提高视频监控系统本身的性能。以下为本文研究的主要内容：


      本文介绍了面向物联网智能视频监控系统的研究背景，对现存的国内外智能视频监控系统作了阐述，列出了该智能监控系统中应用到的相关理论知识。对系统功能模块总体非功能性需求与功能性需求研究，设计系统总体架构与流程。对系统进行模块化设计，将系统详细地分为了五个模块包括视频采集、视频传输、视频处理、视频存储、Web展示，由数据形式和结构出发绘制E-R图并建立数据库。详细介绍系统开发环境与系统的具体实现，本系统应用FFmpeg、Nginx、OpenCV、深度学习多种技术对系统模块进行实现。使用模块化方式进行系统开发，使得开发效率大大提升。最后对系统主要功能做系列测试，根据预期结果给出测试报告。


      本系统智能表现在应用了深度学习算法进行行人检测，原有的YOLO v3行人检测模型存在检测精度相对较低、对视频中小人物检测效果不够好的问题，本文基于YOLO v3进行优化，增加检测层数，并且在训练时应用融合数据集，解决了原有YOLO v3存在的问题，提高模型精度提升系统性能。"
2020,面向中药材质量溯源的区块链系统设计与实现,计算机学院,王子纯,李涛,OS,0.2197,"中药材是中华民族上下五千年孕育的无价瑰宝之一，对于我国医学界和药学界发展不可或缺，在养生保健和防病治病中发挥着重要作用。由于中药材普遍成分复杂，药品的生产、运输、贮存等环境要求比较严苛，药品的疗效容易受到外界因素的影响。为了减少因为药品质量导致的安全事故，营造安全可靠的中药材市场氛围，建立完善的中药材质量监控系统势在必行。传统的中药材质监系统的架构存在诸多问题，如数据真实性、完整性难以得到保障，中心化的数据存储不利于隐私的保护，交易层级过多导致系统效率低下等，而区块链技术的出现为解决上述问题提供了可能。随着计算机技术的不断发展，具有去中心化、多方共识、不可篡改等特点的区块链技术发展迅速，相关的应用场景也不断涌现。区块链以密码学和分布式架构为基础，以“比特币底层技术”的身份问世，在融合了智能合约、物联网等技术后，区块链被广泛地应用在政务、交通、医疗、供应链等领域。区块链技术凭借分布式、防篡改、可追溯、高可信等特点，为传统中药材质量溯源系统中存在的问题提出了改进的方案。


基于以上考虑，本文以区块链为基础，针对目前中药材质量溯源系统中存在的问题和缺陷，设计并实现了一个中药材信息溯源系统，由各溯源信息发布者共同维护唯一账本，形成不可篡改、透明可信的数据链条。本文提出了一个中药材质量溯源模型，通过建立多通道业务架构实现对溯源数据隐私的保护，实现系统中溯源数据交互的安全可控。在系统的具体实现方面，本方案以Hyperledger Fabric为底层区块链平台，使用Golang语言开发智能合约，业务端使用Fabric Java SDK与智能合约交互，并集成了Spring Boot和Vue.js等新兴开发框架，完成系统前后端的分离开发。经实验测试，本系统具有较高的实效性和鲁棒性，在保障数据安全的同时实现了药品数据的共享与管理，为区块链技术在医药溯源领域的应用提供了探索实践。"
2020,基于图神经网络的比特币网络用户匿名化研究,计算机学院,钟梦之,温延龙,OS,0.2396,"作为公认的虚拟货币领头羊，比特币的设计初衷在于追求一个匿名的数字货币，用于保护买家和卖家的隐私。对于使用比特币的用户而言，不通过中央实体就能完成交易。随着比特币网络的快速发展，越来越多的人在比特币网络上进行交易，然而利用比特币交易分析等方式可以在一定程度上破坏比特币的匿名性。研究使比特币重归其匿名性的方法，使得使用比特币的用户个人信息不被泄露，在为用户提供隐私服务的同时，保障交易的安全性，这一研究具有重要理论意义与实际应用价值。


      本文是在比特币网络层通信上对其进行匿名化保护。通过对比特币传播网络进行建模，进而采用图神经网络的方法对图中节点进行向量化表达，达到计算传播时延的目的，加入传播时延后再进行消息发送对比特币网络层上的匿名有积极作用。


      本文先大量调查总结分析国内外比特币匿名保护相关方向的研究现状，明确研究背景和意义；然后简单介绍加密数字货币、网络通信协议等相关基础知识，奠定本文的研究基础；随后给出基于图神经网络的比特币网络用户匿名化研究的总体工作路线，该方法将比特币图数据进行建模构建交易数据图，通过给消息传输过程加入时间延迟进行消息传播；接下来详细介绍了本文的基于图神经网络的比特币网络用户匿名化研究方面的主要工作，利用GraphSAGE方法对比特币交易数据图中的节点进行向量化表示，再以两种数据降维的方法为手段，计算消息传播的延迟时间。结果表明本文提出的基于图神经网络的比特币网络匿名化方法能够为用户提供更强的匿名性。"
2020,基于SDN的地理空间云数据中心QoS传输保障机制研究,计算机学院,黄飞龙,张建忠,OS,0.231,"随着移动互联网时代的到来，大数据、物联网和智慧城市等领域呈现出如火如荼的发展态势，极大推动了人类科技和社会生活的不断进步。同时，作为大数据的典型代表，地理空间大数据也保持着极快的增长态势，为地理空间大数据的存储、管理与服务应用带来巨大压力和挑战。不断增大的地理空间数据体量，不断拓宽的地理空间数据服务范围，不断增长的用户量，导致地理空间数据服务负载过高，出现了服务性能低下，响应时间变长，服务请求丢失甚至拒绝服务的现象，严重限制了地理空间数据服务的发展。如何提高地理空间数据服务质量，满足多样化和精细化的服务需求，支撑物联网和智慧城市的发展建设成为研究人员的重要关注点。


为更好地提供地理空间数据服务，本文利用SDN逻辑集中、数控分离的优势，重点研究了地理空间云数据中心的QoS服务保障问题。首先，本文介绍了地理空间云数据中心的研究背景，在分析国内外的研究现状基础上，介绍利用SDN解决地理空间云数据中心环境下不同优先级数据流QoS传输保障机制的相关技术基础。随后，本文通过引入模糊论中的模糊层次分析法，提出了一种地理空间数据流的综合优先权计算模型，利用定性与定量相结合的方法，确定了地理空间数据流综合优先权的影响因素和数据流类型的影响权重，解决地理空间数据流综合优先权的计算问题。在此基础上，本文通过调查与计算，确定了综合优先权的影响因素和常见地理空间数据流的权重因子，实现了地理空间数据流综合优先权计算，并提出一种综合优先权与QoS优先级的映射策略。最后，本文利用SDN集中控制思想和网络分片功能，提出了分片网络环境下地理空间数据流的动态路由机制和带宽调整算法，实现了不同优先级的地理空间数据流享受不同QoS传输服务，并针对所提QoS服务保障机制及其性能影响进行了实验验证。


实验结果表明，所提QoS服务保障机制可以缓解网络拥塞，提高网络负载均衡能力，实现地理空间数据流的精细化、差异化的传输服务，保证高优先级数据流的服务质量。"
2021,基于NVMe SSD的搜索引擎系统架构设计,计算机学院,刘欣瑀,王刚,SE,0.2525,"搜索引擎作为用户检索信息、获取知识的重要媒介，一直以来受到广泛关注。为了快速响应大批量用户查询，商用搜索引擎通常维护数十万台支持全内存型索引的检索服务器。然而由于网页信息规模不断膨胀，加之内存价格昂贵，全内存型索引会导致较高的硬件成本。近几年，一种基于非易失内存（Non-Volatile Memory，NVM）的新型存储设备NVMe SSD出现，如英特尔推出的Optane SSD，由于其相对于传统块设备的显著速度优势以及相对于内存的价格优势，受到学术界和工业界的广泛关注，被视为很有前途的替代传统HDD硬盘和慢速SSD固态盘的存储设备。由于其性能远高于传统块设备，采用NVMe SSD优化搜索引擎系统性能、甚至替代内存成为可能。然而和内存相比，NVMe SSD的带宽和延迟依然存在数十倍的差距，将NVMe SSD应用于吞吐和延迟敏感的搜索引擎系统仍面临巨大挑战。


       本文提出了一种基于NVMe SSD的索引服务器架构，旨在解决NVMe SSD应用于搜索引擎系统带来的延迟增大和带宽瓶颈问题。首先我们提出了查询请求处理流水线机制，通过细粒度数据读取和异步I/O，重叠CPU计算和I/O数据读取，一定程度掩盖由于NVMe SSD数据读取造成的延迟损失。第二，针对多线程查询可能造成的NVMe SSD带宽瓶颈，提出了缓存感知的贪心查询重排算法，聚集相似查询，增加线程间复用数据量、提升缓存命中率，从而缓解带宽瓶颈问题。第三，对于线程间数据共享可能导致的线程阻塞现象，提出了一种索引数据预取机制，调节复用数据读取速度，减少线程空闲等待。第四，提出了针对长尾请求的并行处理算法，其中设计了top K堆的并发访问、任务调度顺序调整、并行度预测及自适应流水线等机制来提高请求内并行处理效率。


       实验表明，我们提出的基于NVMe SSD的流水线查询架构在不同分布的数据集合、不同侧重的检索应用场景上均可降低约90%的内存占用，同时保证查询效率达到全内存索引的95%。本文工作对于商用搜索引擎系统优化有较为重要的实际意义。"
2021,基于SAP系统风险管控平台的设计与实现,计算机学院,刘林,王志,SE,0.2637,"2001年以来，由于美国爆发的以安然、世通、施乐等公司财务舞弊案为代表的会计丑闻，重创了美国资本市场和现存的公司管理体系，同时也集中暴露了美国公司在内部控制上存在的问题，公司管理者CEO及财务主管CFO对内部控制负直接责任，并将承担经济与刑事后果，不仅大幅提高了会计舞弊的处罚力度，同时强化内部审计、外部审计及审计监管。


针对贸易企业，采购及供应商管理是业务经营的起点，对前端业务环节的风险管理是杜绝后期财务舞弊的基础。


本论文以TW集团解决方案为例，通过加强TW集团SAP系统中贸易供应商及客户的管理，使具备优秀服务能力和潜力的采购供应商进入集团采购体系。满足集团业务发展的战略要求,确保集团贸易经营高效平稳运行。本系统应用于集团及所属企业贸易采购供应商管理，主要适用的业务环节包括:供应商客户的分级管理、信用额度及审批管理、合同履约管理。本文中TW集团供应商及客户风险管控的常见问题入手，并以其作为研究背景，展开全面的可行性分析。从SAP供应商及客户风险管控平台的需求分析、主要功能设计、系统实现、系统测试过程进行论证。"
2019,面向生物医学文本的药物-药物关系抽取研究,计算机学院,安颖,师文轩,Database,0.2848,"药物-药物关系抽取是生物医学领域的一项重要内容。据相关医学报告统计：每二十五人中就有一人可能通过同时服用药物而引起药物不良反应，严重的药物不良反应甚至会导致患者失去生命，通过构建完善的药物-药物相互作用关系库可以有效地防止药物不良反应等情况。 DrugBank医药数据库作为全球最大的生物医药数据库，其2018年的统计数据显示，该数据库收录的药物-药物关系对已达到了365984个，高于上一个版本2.6倍。生物医药行业的发展涌现出海量的文章、病例、摘要等文本，凭借传统的人工记录手段从文本语句中提取药物间相互作用关系不仅费时费力，且无法保证全面覆盖，如何有效快速的识别药物间相互关系急不可待。


本文以SemEval-2013中药物-药物关系抽取测评任务为基础进行实验，测评数据集中的文本语料主要来源于DrugBank数据库和美国国立图书馆生物医学文献数据库中的部分文献摘要。训练集共包含714个文档，27794对药物关系对，测试集共包含191篇文档，5761对药物关系对。


本文首先提出了一个以双向门控循环单元联合卷积神经网络的学习模型。实验结果表明，双向门控循环单元可以更好地学习到文本中的序列化信息，而卷积神经网络则在此基础上能够进一步提取空间信息。该模型在SemEval-2013药物-药物关系抽取测评任务上获得了73.8%的F1值。而后，针对样本集负样本数目过多的问题，本文又提出了两阶段的关系抽取方式，通过集成学习和欠采样的方式过滤掉部分测试集样本和训练集样本，进行药物间关系抽取，基于两阶段的学习方式在SemEval-2013药物-药物关系抽取测评任务上获得了75.3%的F1值。针对两阶段的关系抽取在探测时过滤掉了部分正例测试样本以及分类任务中对于训练集样本数据利用率不完全等问题，本文提出了基于多任务学习进行关系抽取的方式，以BiGRU-CNN模型为基础，同时通过参数共享的方式，让粗粒度的二分类任务辅助训练细粒度的多分类任务，该模型在SemEval-2013药物-药物关系抽取测评任务上获得了76.3%的F1值。"
2019,基于LoRaWAN协议的无线网络系统设计与性能评估,计算机学院,李洋,张建忠,Security,0.2743,"信息通信技术发展到今日，人和人之间的通信需求正向人与物以及物与物通信的方向发展。随着物联网应用的兴起，基于物联网的无线通信技术越来越受到人们的关注。


通过研究传统的物联网通信技术，我们发现在面对千万级的接入需求时，采用蓝牙、ZigBee、Wi-Fi等短距离通信技术都存在明显的局限性。这些通信 技术移动性强、终端承载量大、成本过高等特性，对于一些移动性低、通信不频繁、数据量小的物联网应用存在着一定的问题。为了解决这些问题，本文基于 LoRaWAN协议实现了一种覆盖广、成本低、部署简单、支持大连接的低功耗无线网络通信系统。


本文介绍的无线网络系统包括LoRa终端模块、网关模块、网络服务器模块和 应用模块。本文研究的具体内容如下：


分析了LoRa的物理层、MAC层和协议栈的结构，并对LoRa终端的3种工作模式进行了详细的对比。同时，对LoRaWAN协议中ADR自适应速率、安全加密、重传机制进行了详细的分析。


基于LoRaWAN协议设计和实现了一种无线网络通信系统。整个系统包括4部分：基于Arduino开发板的LoRa终端模块、基于树莓派的网关模块、网络服务器模块和Web应用模块。


在不同的通信模式下，对设计的无线通信网络进行了性能实验和分析。在通信距离方面，该系统在复杂环境下能达到200米，在空旷环境下可达1.5公里；在功耗方面，每个LoRa终端结点可运行两年左右。"
2019,客服系统中呼叫任务分配与处理方法及应用研究,计算机学院,马婷婷,程仁洪,Security,0.2822,"随着网络通信技术的发展，以及人们对高质量、个性化服务需求的提升，互联网各种增值业务得到进一步发展。其中，以互联网为载体的网络电话备受关注，这对呼叫中心行业的全新升级有深刻意义。早期呼叫中心客服系统是通过PSTN的电路交换方式实现语音传送，系统构成复杂、前期投资巨大，对区域性建设与运营管理有较高要求，一般企业往往无法承担，因此本文提出了一种基于SIP协议的VOIP呼叫中心客服系统，并重点研究了系统中呼叫任务分配和处理方法及应用，旨在降低呼叫中心建设成本的同时最大限度地提高其客服系统的服务质量和服务效率，进而给企业带来更大收益。


        本文以传统呼叫中心客服系统架构及应用作为研究基础，针对成本高、线路资源利用率低及地域局限性等问题，分析并设计了基于SIP协议的VOIP通信领域的呼叫中心分布式架构，同时对其所涉及的VOIP技术和软交换技术进行了深入研究，着重探讨了SIP协议相关技术模型及其如何实现语音模拟信号在IP网络上进行实时传输的问题。另外，本文重点对能直接影响整个客服系统服务性能和工作效率的呼叫任务分配和处理模块进行了详细探讨。针对常规VOIP客服系统呼叫任务分配和处理能力弱，导致人力资源利用率低、呼叫折损率高、坐席操作失误率高等问题，在ACD模块中，综合考虑各方面因素设计了多种应对不同场景的自动呼叫调度策略，能很大程度上提高呼叫分配的效率和精准度；在坐席终端模块，设计了能够取代硬话机和软话机客户端的采用纯HTML和一组JS控件实现的软电话条，能大大提升坐席话务处理效率。


论文通过对客服系统中负责呼叫任务分配与处理核心功能的ACD与软电话模块进行改进，提出并实现了区别于传统插件式软电话客户端的网页式软电话条，对传统ACD调度策略的改造和优化，研究成果有较好的应用价值，有效提高系统性能和服务能力。


关键词：呼叫中心；PSTN；SIP；VOIP；语音模拟信号；软电话；ACD"
2022,导线熔痕金相图像智能处理与识别方法研究,网络空间安全学院,张浩然,赵宏,CV,0.348,"鉴定不同类别导线熔痕的特性，对于推测、认定电气火灾事故的原因和责任具有重要意义。金相分析国家标准中将显微孔洞和晶粒的形状结构作为区分导线熔痕的重要依据，但只进行了定性描述，缺少量化指标，使得相关工作依赖于具有丰富经验的专业人员，不具有普适性和客观性。因此，本文着眼于这一问题，对导线熔痕金相图像的处理和识别方法进行了研究。


本文研究的导线熔痕金相图像，与以往相关研究中的图像不同，不仅包含了带有孔洞和晶粒的熔化区，还包含了熔化过渡区和导线本体等。因此，需要先对图像进行分割，得到熔化区图像。空洞卷积网络DeepLabV3+在图像分割领域取得了巨大成就，本文将其应用到熔化区分割任务中来，通过分析捕获、融合图像特征和生成像素级分类图像的不同方式，设计提出了注意力模块和亚像素卷积上采样法，对DeepLabV3+进行了改进，得到了导线熔痕金相图像分割模型Att-SPCU-DeepLabV3+。Att-SPCU-DeepLabV3+改善了多尺寸熔化区及其边缘的分割效果，使类别平均像素准确率（MPA）提高了5.22%，平均交并比（MIoU）提高了8.05%。


金相定量分析研究近年来取得了一定的成果，但多借助于Image-Pro Plus图像处理软件，需要工作人员一张张地导入图像、手动测量几何特征，存在效率低、耗时长的问题，而且不同研究中比较和分析特征的角度不同，难以形成统一、普遍的规律。本文基于滤波、二值化、形态学膨胀以及分形几何等数字图像处理方法，设计提出了BMMC孔洞检测法和CBCG晶粒检测法，从图像熔化区中定位出孔洞和晶粒，并从尺寸、形状和复杂度三个方面，详细地量化提取孔洞和晶粒的几何特征。整个过程智能化、自动化、批量化，大大减少了对人力的依赖，提高了计算效率，相较传统的手动特征提取方法，平均用时减少了12秒。本文还分别统计了各类熔痕孔洞和晶粒的特征参数分布，总结了不同类别熔痕的特征差异，为相关金相定量分析研究提供了一个参考和借鉴模板。


导线熔痕金相图像分辨率较高、纹理较复杂，且不同类别熔痕图像的数量相差很大、分布很不均衡，采用传统的单一机器学习分类模型，最高仅能取得83.36%的F1-score。金相图像纹理特征与孔洞和晶粒的几何特征相辅相成、互为补充，且集成学习策略可以综合多种特征或分类器的优点，提高分类精度。因此，本文利用过采样少数类熔痕图像的方法，来平衡数据分布，并对图像纹理特征和金相几何特征进行集成，对多个分割效果较好的模型进行集成，最终将F1-score提高到了90.76%，得到了导线熔痕金相图像的最优分类模型LBP-RF + Gabor-RF + Geometry-XGB。


本文中导线熔痕金相图像的智能化分割、定量分析和分类方法是在对数字图像处理方法和机器学习算法等进行了广泛、深入的研究和分析后设计提出的，并通过充分的实验，验证了方法的有效性和优越性，对于现实导线熔痕鉴别工作以及相关金相图像处理研究工作，具有重要的指导和借鉴作用。"
2022,加密DASH视频流识别技术研究,网络空间安全学院,马晓威,张玉,Network,0.3222,"当前，在线流媒体视频分享与社交平台蓬勃发展，面对庞大的用户群体和动态多变的网络环境，动态码率视频流传输技术DASH已经成为各网站的不二之选。大量加密DASH视频流在互联网空间涌动。加密DASH视频流识别对于做好网络监管工作，打造安全网络空间具有重要意义。


加密DASH视频流识别研究的基本目标为通过分析视频流确定其传输的视频的内容标签。其基本方法是提取加密DASH视频流的模式特征，与视频指纹特征相匹配。该研究面临的主要挑战在于：（1）加密传输协议和网络环境波动为视频流模式特征提取带来影响；（2）当前研究没有关注网络波动状态下即传输的视频分片质量等级频繁切换的场景下视频流识别方案的有效性。


本文针对加密DASH视频流识别主要做了以下研究：（1）首先考虑到网络波动、加密协议和视频流特性，利用机器学习回归算法构建加密DASH视频流负载长度推断模型，提升视频流模式特征提取的准确性。（2）其次，针对网络波动或网络状况较差情况下视频质量级别频繁切换场景下的视频流进行识别，提出一种基于多分辨率视频分片大小的视频指纹和多层模糊匹配方案。除了网络波动环境下的视频流，本文提出的方案可以识别混入其他视频内容的视频流以及视频流进度条拖拽场景下跳跃播放的视频流。本文提出的方法无需访问远端数据或重放视频流，能够有效应对多种场景下的加密DASH视频流识别。实验显示，对于YouTube视频只需侦听时长为6秒的视频流即可达到98%的准确率，所需视频流侦听时间远小于同类研究所需时间。（3）最后本文综合视频流负载长度推断模型和基于多分辨率视频分片数据量大小的多层模糊匹配方案构建加密DASH视频识别原型系统。实验结果显示，原型系统能够识别超过98%的来自不同时长视频的含有3个视频分片的视频流，且系统识别效果具有稳定性。实验结果进一步验证了视频流负载长度推断模型和多层模糊匹配算法的有效性。"
2021,全同态加密构造问题研究,计算机学院,李瑞琪,贾春福,SE,0.2594,"全同态加密（Fully Homomorphic Encryption，FHE）是一种新兴的密码学原语，其支持在密文上进行任意计算。全同态加密为云计算安全提供了一种理论上的解决方案，对云计算的发展有着强大的推动作用。然而当前全同态加密技术仍无法在实际中大规模部署，最主要的问题就是当前全同态加密的实用化程度仍未达到工业界的要求。影响全同态加密实用性的因素多种多样，而根源问题是全同态加密的构造方法是否能满足实用化的需要。因此研究应用导向型的全同态加密构造技术是具有理论研究价值和实际意义的。本文以全同态加密为研究对象，以提高全同态加密的实用性为目标，分别从基于格的全同态加密（Lattice-based FHE）和整数型全同态加密（FHE over the integers，FHE-OI）两个角度，开展研究工作。本文对基于格的多密钥全同态加密（Multi-Key Fully Homomorphic Encryption，MKFHE）的实用化构造，基于格的全同态加密的自举（Bootstrapping）程序设计，以及基于整数的全同态加密（Fully Homomorphic Encryption over the Integers，FHE-OI）的构造进行了研究，得到了更适合于云计算场景的多密钥全同态加密方案和多密钥同态代理重加密方案（Multi-Key Homomorphic Proxy Re-Encryption，MKH-PRE），改进的格基全同态加密的自举程序，以及实用性更强的整数型全同态加密方案。具体工作如下：


      1. 为了改善当前多密钥全同态加密构造方法较为复杂以及在云计算环境中用户端计算开销较大的问题，以Number-Theory-Research-Unit（NTRU）加密方案为基础构造了一个多密钥全同态加密方案。利用工具向量和比特分解技术构造了一个多密钥层级全同态加密（Leveled MKFHE）方案，并分析了噪声变化与方案安全性。构造的MKFHE方案可以将此前方案中的重线性化和密文扩展等过程移除，进而使得用户端无需生成计算密钥和密文扩展所需的参数。同时，证明了可以通过Gentry的Bootstrapping定理将构造的leveled MKFHE转化为真正的MKFHE。此外，利用中国剩余定理将此方案转化为支持批处理的MKFHE方案，并给出了同态置换运算的实现方式。与一些经典的MKFHE方案相比，本方案构造方法更为简洁，在云计算环境下用户端的开销相对较低。


      2. 为了提高同态加密在多用户云计算场景中的实用性，构造了一个基于NTRU的多密钥同态代理重加密方案。首先借助于密文扩展的思想设计了一种新的NTRU型多密钥同态密文形式，在此多密钥同态密文形式的基础上给出相应同态运算的构造方法，得到一个支持分布式解密的NTRU型MKHE方案；再利用密钥交换的思想设计重加密密钥和代理重加密过程，并将新的NTRU型MKHE方案与PRE功能整合起来。本方案既具有MKHE的同态计算特性和分布式解密特性，又保留了多跳PRE的功能，用户可以根据实际需求选择合适的功能来使用。与此前的MKH-PRE方案相比，本方案实用性更强；同时实验结果表明，方案的各个算法的计算开销也可接受。


      3. 提出了一种新型同态取模运算并在此基础上提出一个针对Brakerski-Gentry-Vaikuntanathan（BGV）同态加密方案的新的自举程序构造方法。利用盲轮换的思想辅以其他数学技巧构造了一个新型同态取模运算算法。此算法不需要使用基本同态运算，因此噪声积累较低，并且适合于代码实现。以此新型同态取模运算为基础，借助于TFHE（Fully Homomorphic Encryption over the Torus）方案中的技术，构造了一个针对BGV方案的新型Bootstrapping程序，并分析了此Bootstrapping程序的噪声积累情况和计算开销。与此前的Chen-Zhang方案（ProvSec2017）相比，本文的新Bootstrapping程序噪声积累更低，从而方案安全性能够规约到近似因子更小的近似最短向量问题（GapSVP）问题，故而具有更好的安全性；与此同时，本文的Bootstrapping程序的计算开销与Chen-Zhang方案相当，但更适合于代码实现。


      4. 为了解决当前整数型全同态加密在实际计算场景中实用性较差的问题，提出了支持近似数计算的整数型层级全同态加密方案（Leveled FHE-OI for Arithmetic of Approximate Numbers）。将加密时添加的噪声看作近似计算过程中产生的误差，从而扩大了FHE-OI方案的明文空间；为了使明文和误差的大小在同态运算过程中一直保持在特定的范围内，设计了一个适用于FHE-OI方案的重放缩（Rescaling）算法以及相应的重放缩密钥（Rescaling Key），并以此为基础构造了支持近似数计算的整数型层级全同态加密方案，同时也构造了一个支持在实数或复数向量上进行同态并行计算的版本。与现有FHE-OI方案相比，本方案能够处理更多类型的数据，应用场景更多；同时实验结果表明本方案的实际运行速度较快，且计算误差很小。"
2021,LocalFS：基于SPDK的用户态文件系统,计算机学院,胡泽瑞,王刚,OS,0.2796,"随着硬件技术的不断发展，硬件自身性能不再是制约计算机系统整体I/O性能的主要因素。从上层应用软件层次角度看，高性能NVMe SSD上的一次I/O所用时间，软件栈开销占据了大部分，尤其是内核部分。SPDK的出现使得将I/O软件栈搬到用户态成为了可能，并且以其轻量级的特性，可以在性能上明显优于传统的内核I/O软件栈。


目前学术界以及工业界陆续出现了一些基于SPDK的用户态文件系统工作，例如BlueFS、DashFS等等。但是这些文件系统大多缺乏对目录树的支持，将侧重点更多地放到了实际数据路径上，因此它们的应用存在很大的局限性。


针对此问题，本文面向侧重于数据写的日志类应用场景，基于SPDK设计实现了高性能的用户态文件系统LocalFS。LocalFS使用的存储后端是BlobFS，出于极致高性能的追求，从消除内存拷贝以及减少I/O次数角度充分优化了BlobFS的性能，使之更加能胜任高性能场景。与此同时，相比于目前大多数基于SPDK的用户态文件系统对目录树的不重视，LocalFS充分借鉴了TableFS以及IndexFS基于KV系统构建目录树模块的思想，采用RocksDB来存储文件系统目录树。核心是通过设计Meta Table、Dentry Table以及Inode Table结构实现了存储所有元数据并且能够高效访问，并在此基础上完善了加锁算法，以及设计了若干优化策略来提升性能。


为了全面评测LocalFS的性能，本文设计了多个维度的性能实验，包括文件系统基本操作性能测试、实际工作负载测试（包含LevelDB以及某公司日志系统）。通过测试结果，可以发现LocalFS相较于ext4在吞吐、延迟方面实现了显著的性能提升，无论是小粒度写还是大粒度的写，都表现出更加性能，一些情况下能实现3~4倍的提升。总之，高性能、支持目录树这两个特性使得LocalFS能够充分提高日志类应用的工作效率。"
2021,混合型可解释医疗决策支持系统关键技术研究,计算机学院,宋珂慧,袁晓洁,NLP,0.3045,"医疗决策支持系统指的是一类可用于医疗诊断和辅助决策的计算机应用，能够为医疗工作者提供数据管理、知识参考和决策辅助等服务。随着医疗大数据时代的来临，医疗电子数据呈爆炸式增长，设计并实现一个特定疾病领域的医疗决策支持系统有助于挖掘医疗数据中的重要特征，能够将有价值的知识和疾病诊断结果以合理的形式呈现并辅助医生诊断，从而能够在提高诊断效率的同时提升诊断的准确率。因此，对医疗决策支持系统关键技术的研究具有重要的现实意义和广泛的应用价值。


医疗决策支持系统有知识库系统和非知识库系统这两大类，其中知识库系统由医疗数据、用户接口和IF-THEN规则共同组成，而非知识库系统在知识库系统的基础上，使用基于深度学习或机器学习的挖掘算法代替IF-THEN规则，将基于规则的疾病推理过程转变为一个简单的疾病分类问题。早期的医疗决策支持系统大多为知识库系统，通过对理论知识和临床经验的总结，得到一组可用于因果推理的规则。这些基于规则的系统虽然具有较强的可解释性，但在规则管理、诊断效率和准确率方面仍有缺陷。近些年来，随着对挖掘算法研究的不断深入，越来越多的医疗决策支持系统使用基于神经网络的挖掘算法用于疾病诊断，这样的系统虽然在诊断准确率方面有较好的性能表现，但不具有较强的可解释性，其系统性能过分依赖医疗数据的质量，且并未正确引入专家知识进行诊断。因此，本文在非知识库系统的基础上，从提升医疗数据质量、增加挖掘算法可解释性和引入专家知识这三个方面展开具体研究。本文的主要研究内容包括：


第一，针对医疗数据样本类别不平衡的问题，本文首次提出针对医疗列表数据的生成模型，按类别生成样本并补全少样本类别，以达到优化下游疾病诊断模型训练过程的目的。本文发现，医疗列表数据中连续型属性的分布不与任何一种高斯分布相近，因此直接使用生成式对抗网络生成数据会带来梯度消失和模式崩溃等问题，从而导致生成的低质量样本无法减轻不平衡医疗数据对下游诊断模型的负面影响。因此，本文针对医疗列表数据，将离散型属性和连续型属性区别对待，设计并实现一个高斯化模块将非高斯分布的连续型变量分解为多个高斯分布的线性组合，并对离散型变量进行独热编码处理，随后输入判别器网络进行判别。与此同时，使用条件生成器按类别合成数据，其中激活函数一共有两种，分别针对离散型属性和连续型属性进行数据生成。最后，将补全少样本类别后的医疗数据对三个不同的诊断分类模型进行训练，用他们在真实测试集上的预测结果来评价数据生成的质量，结果表明本文模型相比于其他生成模型有更好的表现。


第二，为提升医疗决策支持系统的可解释性，提出一个基于批量学习的特征权重计算模型，通过批量学习医疗数据来得到所有输入特征的权重大小，为医生进行准确的疾病诊断提供了可用于参考的有效信息。研究发现，现有的方法大多设计一个通用策略来计算权重，但该策略往往与下游的诊断模型毫无联系，这会导致得到的权重无法提高诊断模型的性能，甚至会起到负面作用。因此，本文提出的特征权重计算模型将特征权重作为下游诊断模型的一个训练参数，利用批量学习和反向求导等技术不断优化，最终得到的特征权重不仅能够提高疾病诊断的性能，还可以保证诊断结果的可解释性。通过在一个心脏衰竭数据集上以准确率、F1分数和AUC值作为评价指标进行实验，结果表明本文模型能有效提升系统疾病诊断的性能，且相比于其他的权重计算模型具有一定的优越性。


第三，为引入专家知识，提出一个基于专家知识的特征权重计算模型，通过对专家知识进行建模和计算，将语义信息转换为可被诊断模型计算的权重信息。现有方法的知识获取过程较为复杂，专家需要提供较为完整的知识信息，即通过一个判断矩阵对特征的重要性进行成对比较，这是一项费时且费力的工作。并且，现有方法未考虑由知识的模糊性和不确定性以及专家背景知识不同等现实问题带来的巨大挑战。因此，为解决上述问题，本文提出使用模糊判断矩阵和多粒度语言术语集模型进行知识获取，并基于这两种方式获取的知识应用不同的特征权重计算策略，为专家提供了两种提供知识的渠道，以达到根据实际场景科学获取知识并计算特征权重的目的。本文通过获取合作方医院三名医生的专业知识，将得到的知识转换为特征权重后，在一个真实的场景下将特征权重应用在两种不同的诊断模型上进行疾病诊断，结果表明，基于专家知识的特征权重能进一步提高系统疾病诊断的性能。"
2022,数据中心负载再平衡关键技术研究,计算机学院,段剀越,王刚,Network,0.2855,"在负载再平衡问题中任务已经被部署在服务器上且分配了资源，由于任务在运行过程中破坏了数据中心的负载均衡状态，因此需要数据中心调度器为任务制定资源分配方案以实现负载的再次平衡。负载再平衡问题面临许多新的挑战，例如服务级协议（SLA）规定在执行任务调度时任务提供的服务不可被中断。因此任务在迁移过程中会同时在初始机器和目标机器上占用一些资源（如CPU、内存）以持续响应用户请求，这些资源被称为“过渡资源”。由于过渡资源在任务调度过程中不会被释放，导致已知的负载再平衡算法在一些资源紧张的场景中无法达成较好的负载均衡状态。


本文研究的负载再平衡问题考虑了过渡资源，在保证数据中心服务可用性以及服务质量的前提下（约束条件），最小化服务器资源利用率不均衡指数和服务器资源使用量（目标函数）。为解决过渡资源不足等多个挑战，本文受商品经济的等价交换原则启发，提出了三种负载再平衡调度算法。


第一，本文提出基于“中转机”策略的负载再平衡算法。中转机是指数据中心的空机器，这些中转机初始时没有部署任何任务，因此中转机有充足的过渡资源，使得一些原本因资源需求过大而无法被迁移的任务会被迁移到中转机上，这有助于数据中心实现负载再平衡。作为使用中转机的代价，一些机器需要被清空（迁走这些机器上的所有任务）并返还给数据中心。本文基于中转机策略设计了大邻域搜索算法以优化负载再平衡。实验结果表明在模拟数据集上中转机策略的平均性能超过已知最优算法达58%，在真实数据集上则为25%。


第二，本文提出基于“资源等价类”策略的负载再平衡算法。前人提出的资源等价类是一组资源配置，当任务被分配了其中任一资源配置后将具有相似的性能。该策略采用资源隔离技术为混部的任务划分资源以缓解性能干扰，并调整任务的资源配置使得在不迁移任务的前提下改变服务器的负载，或者调整任务的配置以降低任务迁移的难度，这促使数据中心实现负载再平衡。然而使用资源等价类会增加任务对某些资源的需求，因此本文基于资源等价类策略设计了局部搜索算法以联合优化负载再平衡和资源使用量这两个目标。实验结果表明该算法的性能超过已知最优算法达26%。


第三，由于尚不存在考虑过渡资源的负载再平衡与负载再合并联合优化算法，本文提出基于帕累托最优性和“流量调整”策略的负载再平衡与负载再合并联合优化算法（即一个局部搜索算法）。由于资源等价类会随着任务收到的请求量而改变，流量调整策略将改变任务收到的请求量从而使用新的资源等价类中的资源配置，该算法使用帕累托最优性保证优化其中一个目标时另一个目标不会变差。此外，本文针对其中单个目标的优化设计了基于流量调整策略的模拟退火算法。在优化负载再合并问题中，模拟退火的性能超过已知最优算法达10.5%，在优化负载再平衡问题中，超过最优的对照组算法达4%，在联合优化两个目标时本文提出的局部搜索算法实现了最佳的负载均衡状态。


综上，本文针对负载再平衡问题采用不同的等价交换思路设计优化算法，中转机策略可视为对机器的“交换”，规定使用的中转机的资源容量与清空的机器的资源容量相等，资源等价类策略可视为对任务资源配置的“交换”，规定调整任务配置前后其性能相似，流量调整策略“交换”同类任务的请求量，规定同一类任务的总请求量在调整前后守恒。本文的提出的三个算法适用于具有大量长时间运行的任务的数据中心，且资源等价类策略适用于具有特定配置的服务器。"
2023,基于持久性内存的在线重复数据删除优化,计算机学院,李奕琛,刘晓光,Network,0.2356,"随着大数据时代的到来，全世界的数字信息飞速增长。存储海量信息，提高存储设备效率，降低存储成本成为了计算机存储的重要问题。重复数据删除是主要的数据去重手段，通过数据块级别粒度的重复数据识别、删除进行全局的数据去重，被广泛应用于各种存储场景中。

  持久性内存（Persistent Memory，PM）作为新型的存储设备，有着更低的访问延迟和更高的读写带宽，成为了近年来的研究热点。持久性内存在重复数据删除领域有着广泛的应用场景：既可以把持久性内存作为内存的拓展，用于存储数据索引和元数据表，加速重复数据删除中指纹查找的性能；又可以把持久性内存作为目标存储设备，通过在线重复数据删除消除重复数据写入，提高持久性内存的使用寿命，降低存储成本。持久性内存应用于重复数据删除时同样面临着诸多挑战：重复数据删除的主要性能瓶颈由I/O瓶颈转变为了计算瓶颈；基于持久性内存的在线重复数据删除对各步骤的延迟更加敏感；需要在持久性内存上重新设计指纹索引和元数据集，结合硬件特性，发挥持久性内存的优势。

  基于上述的挑战，本文从数据分块、指纹计算和指纹查找三个方面，对基于持久性内存的在线重复数据删除进行了优化。针对数据分块，本文提出了采样预测的方法消除重复数据块的分块过程，同时利用相似数据块加速部分独特数据块的分块过程；针对指纹计算，本文利用SHA-1优化算法消除采样预测方法引入的额外计算开销，同时基于采样预测方法提出了异步指纹计算的方法，消除I/O路径上独特数据块的指纹计算开销；针对指纹查找，本文在持久性内存上构建了数据块头指纹索引和元数据表用于采样预测，在基于空间局部性的指纹查找算法的基础上利用快速读buffer替换传统慢速LRU cache，并通过调整布隆过滤器和读buffer的查找顺序进一步提高指纹查找效率。本文在四个真实数据集上进行了实验验证，实验结果表明本文提出优化工作可以显著提高基于持久性内存的在线重复数据删除性能，适用于不同工作负载和数据分块算法，对比传统重复数据删除最高可以得到2倍的总性能加速比。"
2023,加密去重中的抗侧信道攻击问题研究,网络空间安全学院,陈杭,贾春福,Security,0.3255,"随着数据量的快速增长和人们对安全意识的提高，如何在提升云存储效率的同时保护用户外包数据的安全已成为焦点问题。加密去重技术有效地实现了密文的重复删除，在保证数据安全性的同时提高了存储效率。然而，服务端去重中的确定性加密会泄露明密文频率特征引发频率分析攻击，客户端去重中的存在性检测结果也易引发存在性攻击。这两类攻击均通过非侵入式手段破坏数据隐私，属于侧信道攻击的范畴。针对这两类侧信道攻击，本文的主要工作如下：


（1）提出了一个抗频率分析攻击的方案SDAF（Secure Deduplication Defend Against Frequency Analysis Attacks）。它使用密钥认证协议确保了密钥的合法性，并采用伪随机密钥生成算法混淆了密文频率特征。此外，SDAF还引入了安全指标Kurtosis以更全面地评估方案的安全性。具体而言，SDAF基于变色龙哈希实现了密钥认证协议以防止密钥服务器返回伪造值，并使用分布式密钥服务器分散了数据频率信息以防某一密钥服务器获取全部数据的频率信息。SDAF还采用了伪随机密钥生成算法生成符合均匀分布的密钥以扰乱密文频率特征，在提高抗频率分析攻击能力的同时实现了较高的存储利用率。此外，SDAF引入了安全指标Kurtosis以更全面地评估方案防御频率分析攻击的效果。与现有方案相比，SDAF在提升密钥生成性能的同时降低了安全指标KLD和Kurtosis的值，大大提升了方案抗频率分析攻击的能力。


（2）提出了一个抗存在性攻击的方案SDAE（Secure Deduplication Defend Against Existence Attacks）。它使用数量混淆算法扰乱了存在性检测结果，并引入了Intel SGX以防敌手通过观察数据的加密上传情况得到目标数据的存在性。具体来说，SDAE通过数量混淆算法将一定数量的已存在数据视作未存在，有效地防止了敌手借助存在性检测结果或传输流量差异等信息得到目标数据的存在性。SDAE还在客户端部署了SGX并在其内部进行数据加密。这种安全可执行环境保护了其内部程序的运行情况，而且有效地防止了敌手通过观察数据的加密上传情况获得目标数据的存在状态。与现有方案相比，SDAE能够更有效地防御存在性攻击，并且具有较低的计算和通信开销。"
2023,基于GNSS-R的多模态特征融合海面风速反演模型的研究与应用,计算机学院,屈芳瑜,赵宏,CV,0.3333,"随着空间信息技术的发展，基于GNSS-R监测海面风速成为近年来国内外海洋遥感领域的研究热点之一。当前GNSS-R海面风速反演模型的研究正处于初步探索阶段，该研究面临的主要问题在于：（1）如何有效提取卫星数据图像中对海面风速敏感的特征；（2）卫星数据中多模态特征融合方式对海面风速反演准确度的影响。


       为了解决上述问题，本文基于美国旋风全球导航卫星系统的卫星数据DDM、 BRCS图像和 11 个多模态信息的数据集，开展了如下研究工作：（1）对多个卷积 神经网络模型的实验验证，初步发现GoogLeNet在本问题上的适用性的基础上， 通过优化Inception模块、引入注意力机制，提出了面向图像的GNSS-R海面风速 反演模型CyIECANet。（2）在CyIECANet的基础上，引入多模态特征融合技术， 提出了基于动态多层感知机的GNSS-R海面风速反演模型CyIECANet_DMLPB。


       实验表明，CyIECANet_DMLPB的均方根误差为 1.3714m/s，相较于现有的基于深度学习的GNSS-R海面风速反演模型平均降低了 6%左右，实现了海面风速反演准确度有效提升。为了方便遥感专业研究人员使用CyIECANet_DMLPB， 本文开发了海面风速反演应用系统。"
2022,基于AUC的多分类器评价方法及其集成学习应用研究,计算机学院,杜科宇,卫金茂,CV,0.3394,"随着计算机科学技术的蓬勃发展，涌现了大量的机器学习和模式识别算法，有越来越多利用分类算法进行大规模复杂数据分析的研究。在分类器设计流程中，如何评估分类器至关重要，一个好的评估指标更能够帮助模型优化，但是现有的分类器评估指标却无法同时评价多个分类器的联合预测能力。受试者工作特征（ROC）曲线作为一个通用的分类特性评估标准，在处理目标分类中不平衡和成本敏感性的数据时有着明显的优越性。因此，本文基于ROC曲线分析，利用可能误分类样本转换为正确分类样本的过程，设计并实现了一种有效的针对多个二元分类器进行性能评估的方法。基于分类互补性的思想，本文提出的多个分类器性能评价方法不仅可以用于评估单个分类器，而且可以应用于评价多个分类器的集合，能够实现在多个互补分类器的组合空间上对其全局互补能力和联合分类能力进行评估。

       目前，集成学习的经典算法已被广泛应用于癌症诊断和疾病分类等任务中。作为机器学习领域的一项深入研究技术，集成学习在高维、小样本等复杂数据结构的分类任务上具有独特优势。构建集成分类模型时，理想的情况是训练准确多样的基学习器。然而，在分类器集成模型中可能会产生一些冗余或坏的基分类器，这将严重影响整体预测性能。因此，本文提出了一种基于多分类器性能评价AUC并以分类互补性为核心的集成学习算法，集中于处理集成分类器中的共同误分类样本，以找到能够互补地减少共同误分类样本的候选基分类器。该方法利用不同的数据子空间实现基分类器的多样性，并且在此基础上考虑了分类互补性，衡量候选基分类器独立的分类信息和已有分类器集合保有的共同分类信息，生成最具分类互补性的集成模型，最终由多个基分类器的独立AUC值进行加权投票得到预测结果，避免弱分类器可能造成的负面影响。实验在多个公共数据集上进行不同的分类任务，通过多种分类评价指标与其他集成学习方法的对比验证了该方法的有效性。"
2022,区块链共识算法研究,网络空间安全学院,赵逸文,苏明,Security,0.298,"近年来，区块链去中心化、公开透明、不可篡改的特性，使其赢得了广泛的关注。共识算法是区块链系统研究的重要课题。目前许多公链项目都采用了PoW共识算法，并使用了SHA-256哈希函数，但其安全性和随机性并没有经过严格的证明，仍存在安全性隐患。此外， DPoS共识算法虽然共识性能较高，但有中心化的趋势，且缺乏拜占庭容错能力。基于此，本文针对这两类共识场景，设计一个能满足随机性、安全性要求的PoW共识协议及一个满足去中心化需求、具备拜占庭容错能力的改进DPoS共识协议，主要研究工作有以下几点：


Legendre序列是一种线性复杂度、自相关性、伪随机性能优越的二元伪随机序列，其性质经过了许多学者的深入研究，在区块链共识算法中有一定的应用价值。为了解决目前PoW中哈希函数可能存在的安全隐患，提出一种基于Legendre序列的挖矿模型，作为一种可替代的区块链挖矿共识算法，并在全网部署分布式节点，通过实验验证了本文挖矿方案的可行性。相比传统的哈希函数，Legendre序列可以提供严格的随机性证明。


为了降低了PoW共识协议中轻节点的带宽压力和存储压力，使得手机、IoT设备等轻型终端能够更好地参与到区块链系统中，本文提出了一种由数据链和验证链构成的双层链模型以及一种SPV协议，仅需验证链末尾区块即可快速完成对最长链的确认和对交易的验证。


针对联盟链场景，为了在DPoS共识算法中，满足区块链去中心化及拜占庭容错的需求，本文基于VRF可验证随机函数提出了一种密码抽签算法，能够随机选举共识中的各个角色。通过快速一致性算法，在不存在拜占庭节点情况下，提高共识性能；在存在拜占庭节点时，采用PBFT共识实现容错能力。"
2022,基于全局重构和对比学习的少样本原型网络及鲁棒性研究,网络空间安全学院,李子薇,白刚,CV,0.3897,"近年来，深度神经网络在计算机视觉、自然语言处理、语音识别等众多领域都取得了很大进展。然而深度神经网络仍不能广泛实际应用的主要原因在两个方面：一方面，深度神经网络需要大量的标记数据来学习新的任务，但在实际应用中很难获取大量的标记数据且获取成本高昂，若将有限标记数据直接利用传统深度学习方法进行学习往往会产生过拟合等泛化性问题；另一方面，深度神经网络极易受对抗扰动的攻击，人类视觉系统不可查觉的微小扰动就能使深度神经网络模型做出高置信度的错误决策，深度神经网络的安全性是当前亟待解决的问题。


       针对上述问题，本文从少样本学习与深度神经网络模型鲁棒性两个角度出发，针对少样本学习原型网络方法提取的嵌入空间特征代表性不强，样本数目过少导致原型表示存在偏差等问题，提出了基于全局图像重构和对比学习的少样本原型网络模型——GR-CPN，并对该网络模型的鲁棒性进行了研究。本文主要工作包括：（1）采用原型网络嵌入空间方法，使用全局图像重构损失与对比学习损失优化嵌入空间选择，以提升嵌入空间对物体本身代表性特征的关注程度，而不只是关注不同类别样本的区分性特征。（2）在模型推理阶段，通过给查询样本赋予伪标签来扩充支持集，实现原型修正，以此减小支持集样本特征在嵌入空间中的类内偏差。（3）提出适合GR-CPN模型的鲁棒决策策略，通过动态选择输入样本或重构样本作为决策依据，增强模型的安全性（对抗防御能力）和鲁棒性。


       GR-CPN模型在miniImageNet数据集5-way 1-shot和5-way 5-shot两种情景任务中的分类准确率分别是58.66%和71.70%，FC100数据集两种情景任务中的分类准确率分别为41.67%和54.75%。同时，提出的鲁棒决策策略也使得miniImageNet数据集5-way 1-shot情景任务中 FGSM对抗扰动愚弄率从76.78%降低为59.60%，5-way 5-shot情景任务中愚弄率从82.77%降低为63.84%。"
2022,PE恶意代码API序列特征的智能变异方法研究,网络空间安全学院,刘元诏,王志,Security,0.3692,"安全供应商和学者们提出了很多恶意代码查杀技术，基于特征码的恶意代 码查杀技术因其效率高脱颖而出。但是特征码技术很容易被混淆和加壳等改造手段躲避，所以当今很多恶意代码检测软件都使用沙箱动态执行以检测恶意行为。使用沙箱技术动态执行的恶意代码检测软件，可以很大程度上避免混淆和加壳等行为的干扰，使得混淆的恶意代码无法免杀。二进制代码静态特征重构混淆生成技术可以躲避静态特征引擎的识别，但是代码执行时还是会暴露其行为，最终被动态特征引擎所识别。针对这种问题，本文提出一种基于API序列特征的智能变异方法，通过实验验证躲避动态特征引擎的效果。


  本文实现的原型系统主要分为机器学习模块和代码智能变异模块。机器学习模块基于恶意代码的API序列特征训练多个机器学习模型，对机器学习模型使用SHAP和LIME解释模型进行解释，得到对机器学习模型分类结果影响最大的黑、白API序列特征。代码智能变异模块在二进制重写方法的基础上，实现等价指令修改和噪音API插入等功能，在不影响程序功能的前提下改变API执行顺序。


  测试表明经过智能变异生成的恶意代码对抗样本不会改变原有功能，可以躲避沙箱和杀毒软件的检测。并且本文实现的原型系统可以自动化生成大量的免杀样本。"
2022,基于可信度置信区间的多模型入侵检测系统,网络空间安全学院,邵乐石,王志,Network,0.3869,"入侵检测是网络安全的一个重要研究领域。入侵行为可能会影响系统的安 全性。目前已经提出了许多异构入侵检测方法，在异常检测的方法当中，对偏离 正常行为的恶意行为进行识别，将其视为攻击，它可以检测到未知的入侵。可 以选择不同的机器学习算法去做异常检测，但这些算法之间缺乏协调，面对不 同环境模式下的恶意行为，再加上新的恶意行为层出不穷，有时候很难选择一 种适合不同环境模式的算法。除此之外，在传统的机器学习两类预测中，通常 只对结果进行正常或异常预测，因此很难知道算法对需要预测的样本有多大的 判定信心。


      为了解决上述问题，本文提出了一种基于统计学习的多模型协同预测的入 侵检测框架。这种方法采用了一种设置可信度置信区间的办法。在预测过程中， 每个算法模型只对自己的可信概率区间进行预测判断，拒绝对置信区间外的样 本数据进行预测。最终通过多个算法模型的投票生成最终判定结果。


     实验在 4 个不同环境的数据集中使用了 11 种检测算法进行实验分析，经过 实验发现相同算法在不同数据集当中检测效果差异明显。本文采用的多模型投 票预测方法与多个不同的单个算法检测方法相比，基本达到了最高的检测精度， F1 值基本处于最高的水平。在设置置信区间预测后，检测效果再次显著提升， 在 4 个数据集当中均达到了最高或者次高的检测精度，证明了本文的方法具备 很强的检测适应能力。"
2022,动态异质网络表示学习关键技术研究,网络空间安全学院,郭佳雯,袁晓洁,Network,0.3441,"动态异质网络是一种具有多类型节点且随着时间推移不断演变的网络数据，其包含丰富且复杂的信息，在各类真实场景中应用广泛。然而由于维度灾难，将大规模网络数据直接输入到机器学习方法中进行数据分析是很困难的。因此，将高维网络映射到低维向量空间的网络表示学习问题，是一项重要的研究工作。


目前，在静态网络和动态同质网络上的表示学习研究已较为充分，但对于现实生活中更常见且复杂的动态异质网络的表示学习研究仍然较少。现有静态网络或动态同质网络表示学习方法由于忽略网络的动态或异质信息而存在严重的信息丢失。因此，本文关注具有细粒度时间信息的动态异质网络表示学习问题，分别设计了两种基于不同技术路线的网络表示学习模型，以期良好保留网络的动态和异质信息。


本文提出了一种基于非递减时序随机游走的动态异质网络表示学习模型。该方法引入类别约束，能够解决动态异质网络中由于异质特性带来的语义信息保留问题。并且该方法采用非递减的时间约束进行增量式随机游走，能够解决网络同时具备动态和异质特性而引入的强语义局部结构上时间戳一致的挑战，避免游走时出现时间戳陷入的问题。通过对实时变化进行增量游走和表示学习，提供了一种高效的在线表示学习算法。三个真实数据集上的实验证明，该方法能够显著提升表示学习质量，并在保证良好表示质量的前提下，大大缩短算法运行时间。


为进一步提升动态异质网络表示学习质量，本文提出了基于节点类别签名的动态异质网络表示学习模型。该方法构建了两种不同的节点类别签名模块，分别通过类别质数和类别度对节点邻域类别进行编码表示，保留网络异质信息；并采用时间编码模块，将细粒度时间信息编码为无关网络拓扑结构的向量表示，保留网络动态信息。该方法在多头图自注意力层中引入节点类别签名和时间编码，自适应调整不同邻居节点对中心节点的贡献权重，平衡异质信息和动态信息对表示学习的影响。实验证明，该方法能够显著提升动态异质网络表示学习质量，其提出的两种类别签名模块均能够良好保留网络异质信息。"
2022,显著性目标检测中的特征高效融合研究,计算机学院,刘姜江,程明明,CV,0.4057,"显著性目标检测任务的目标在于：通过分析图像的内容来准确定位并分割出其中最吸引人类视觉注意的对象或者区域。作为一种通用的图像预处理方法，显著性目标检测不依赖于待检测对象的语义类别，已被广泛应用于多个计算机视觉任务中，以帮助它们高效地捕获图像中最重要的部分。目前，深度全卷积神经网络因其强大的多尺度特征提取能力，已在显著性目标检测领域占据了主流地位。然而现有的显著性目标检测算法在多尺度特征融合方面通常存在着设计复杂、计算复杂度高、泛化性差等缺点，当被部署到实际应用场景时，常常面临效率的瓶颈，并且它们也无法满足实际场景中常见的多目标任务同时预测需求。


具体而言，当前的显著性多尺度特征融合算法主要存在以下不足：1）特征融合没有充分考虑显著性目标检测的特性，导致结构设计冗余；2）在同等计算复杂度下，依靠基础U型结构融合得到的特征表征力有限；3）多任务协同的方法对所有任务采取预设、固定的特征融合策略，导致模型泛化能力差。通常而言，一个更加先进的多尺度特征融合方式往往意味着更高的效率、更优的性能和更广的适应性。因此，设计更加先进的特征融合方式成为了一个亟待解决的问题。


为此，本文提出通过对提取自骨干网络的金字塔状的多尺度特征设计更加高效的融合方式，来同时提升显著性目标检测算法的性能和效率。针对上述具体的不足，本文在现有研究的基础上从神经网络中基础的池化算子和注意力机制出发，分别在三个不同的方面提出了对应的改进方案。本文的具体贡献如下：


1. 提出通过利用高效、无参数的池化操作来解决基于U型结构的显著性目标检测模型中实际感受野不足，以及高层语义信息会逐渐被浅层细节所稀释从而导致的主体缺失问题。实验结果表明所提出的方法可以更准确地定位显著性目标，并具有更清晰的细节，较现有领先方法有着明显的性能提升。所提出的方法同时也在RGB-D显著性目标检测、边缘检测和伪装对象检测任务上获得了良好的跨任务泛化效果。


2. 提出通过跨尺度共享可学习的滤波器来促进基于U型结构的显著性目标检测方法中多尺度特征之间的信息交互，以极少的参数和计算量代价换来更具表征力的特征和模型性能提升。在五个广泛使用的评测数据集上的实验结果表明，所提出的方法相比于现有的领先方法有着更优的表现，并且计算复杂度更低。


3. 提出利用注意力机制来动态选择多尺度特征并平衡不同任务分支，以实现显著性目标检测、边缘检测和骨架提取三个任务在一个模型下高效地协同学习。在多个具有代表性的测评数据集上的实验结果表明，所提出的方法可以同时且高效地完成上述三个不同任务，并取得了比当前领先的单一目标方法更好的性能。"
2022,基于深度模型的批次效应处理算法研究,计算机学院,王枭,黄申为,ML,0.285,"基因表达数据是生物信息学中的一类重要数据，对于分析很多典型的生物问题具有重要作用。近年来，随着测序技术的发展，多源大规模基因表达数据的整合受到广泛关注。在进行多源数据整合时，由于各种测序条件的差异而导致的批次效应是一个必须解决的问题，如果不能对批次效应做出合理的处理，极有可能对整合后数据的进一步分析造成严重的负面影响。本文主要针对批次效应中的两类问题，即批次信息未知时的批次检测问题和批次信息已知的批次效应消除问题，在层次化批次因子建模、数据分布的充分匹配以及过校正问题的处理等三个方面展开研究，运用深度建模和深度学习的思想，为批次效应的处理提出了完备的解决方案。


       首先，对于批次信息未知的情况，本文提出了基于深度矩阵分解的层次化批次因子检测算法。在借助数据自适应收缩获得批次效应矩阵的基础之上，充分考虑了多种批次因子之间的层次关系和相互影响，提出基于深度矩阵分解的方法对这种层次关系进行建模，得到与真实情况更加贴近的特征向量，从而得到更加准确的批次因子检测结果。


       其次，对于批次信息已知时的批次效应消除问题，本文提出了基于层次性分布匹配的深度学习算法，以深度自编码器为基础，设计了基于对抗训练的全局匹配结构和基于迁移损失的局部匹配结构以及集成两个步骤的层次性结构，通过全局分布的匹配来为局部分布匹配提供进一步准确处理的基础，从而让两种分布信息更好的结合。


       最后，针对批次效应消除时的过校正问题，本文提出了基于深度对比学习的过校正处理算法。在处理数据的局部结构时，本文的方法既考虑了同类样本的匹配问题，又使得不同类样本间相互远离，从而获得更加准确的聚类结构，有效地避免了过校正问题的产生。本文设计的上述三类算法均在典型数据集上进行了实验并与主流方法对比，在各类评价指标上都超过了现有方法，验证了本文算法的有效性。"
2022,基于对比学习和分组卷积注意力机制 的细粒度图像识别模型,网络空间安全学院,陈彪,白刚,CV,0.3978,"图像识别任务可以根据所识别图像类别的划分粒度分为传统图像识别任务和细粒度图像识别任务两种。细粒度图像识别任务是指对某个粗粒度类别的样本进行更细粒度地划分。例如针对鸟类品种的分类任务来说，可以将类别标签划分为家麻雀、丛林麻雀或山麻雀等细粒度类别。由于不同细粒度类别图像之间往往具有相似的主体结构或颜色构成等信息，而由于拍摄环境的不同，相同类别图像之间却存在着较大的差异。因此细粒度图像识别数据集往往具有相对较大类内差异性和相对较小类间差异性的特点。与传统图像识别任务相比，细粒度图像识别任务一般具有更大的难度。


本文从对比学习思想和分组卷积操作中受到启发，提出了一种基于对比学习和分组卷积注意力机制的细粒度图像识别模型——CGCANet。本文的主要贡献为：


1.利用分组卷积操作对主干网络提取的特征进行处理，并借助通道-空间混合注意力机制对上述特征进行注意力加权。实验结果显示分组卷积注意力机制能够在不同分组分支内提取不同语义表达的细节特征。


2.模型输入过程分别使用图像块屏蔽、结构打乱重拼和添加噪声等方法生成原始图像的正样本图像，并以两幅图像组成样本对的形式作为模型的训练数据。实验结果显示上述方法提高了模型的分类性能和泛化能力。


3.在模型训练阶段添加辅助对抗训练过程，通过将特征的分组分支标签和输入域来源作为辅助预测目标，增加了模型中不同分支网络参数之间的泛化性，并减弱了生成正样本对模型所带来的噪声影响。


4.采用多损失函数组合方式训练模型。其中MACD对比损失从多分组分支、多类别和多输入域三个角度优化特征空间；困难负样本惩罚损失通过惩罚锚点样本的困难负样本预测概率来减弱不同类样本的影响；对抗训练损失为对抗训练过程的交叉熵损失，作为提升模型稳定性的正则化项。


5.实验结果表明，CGCANet模型有良好的表现，在CUB-200-2011数据集测试准确率达到90.3%，在FGVC-Aircraft数据集测试准确率达到94.2%，在Stanford Dogs数据集测试结果达到88.6%。"
2022,基于参考的图像超分辨率重建研究,计算机学院,匡增晟,邵秀丽,CV,0.3932,"图像超分辨率重建旨在从低分辨率图像利用算法生成准确且视觉上令人满意的高分辨率图像，是计算机视觉和图像处理中一类重要的技术。它在诸如医学图像、卫星遥感影像、视频监控、军事等领域中有着广泛应用。在无参考的图像超分辨率重建算法中，随着超分辨率放大因子的增大，单通过算法设计进一步提升图像超分辨率质量已经成为一项极其困难的工作。因此，使用高分辨率参考图像引导低分辨率图像恢复成为近年来研究的热点。此研究根据恢复数据模态类型可分为基于参考的彩色图像超分辨率重建和基于参考的深度图像超分辨率重建两个方向。

       本文针对以上两种不同模态数据的超分辨率重建问题展开研究，主要做了以下工作：


     （1）针对基于参考的彩色图像超分辨率重建任务中如何更准确地进行特征匹配和更有效地转移参考图像信息这两个问题，本文提出了一种基于小波的图像超分辨率纹理重构网络，该网络可以从参考图像中传输理想和有意义的纹理。另外为了生成更加真实以及视觉效果更好的图像，本文还提出了一种基于小波的纹理对抗损失函数，以在复原图像上生成更真实的纹理信息。上述所提方法在四个标准数据集上进行实验，实验表明了本文提出的方法在定量和定性的实验上都优于其他先进的基于参考的彩色图像超分辨率方法。

    （2）针对基于参考的深度图像超分辨率重建任务中模态对齐以及彩色和深度模态充分融合这两个问题，本文提出了一种基于对齐的双向融合深度超分辨率重建网络。该网络首先设计了一个动态对齐模块，将参考图像特征和深度图像特征对齐。为了更充分地利用和融合这两个特征，该网络还设计了一个双向交叉的像素注意力融合模块，以帮助深度图像超分辨率网络重建出更加清晰尖锐的边界信息。最后，上述所提方法在三个标准数据集上进行实验，实验表明了本文提出的方法在定量和定性的实验上都优于其他先进的基于参考的深度图像超分辨率方法。"
2022,基于异构数据的多模态融合模型在股票趋势预测中的应用研究,网络空间安全学院,林广莲,邵秀丽,CV,0.3282,"金融时间序列常被用于股票趋势预测，但由于该序列具有非平稳和信噪比低等特点，导致预测效果不理想。为此，本文首先探索其他形式的股票数据。考虑到图像具有较高的信噪比和额外的空间信息，而新闻具有反应股票走势的关键词信息，本文分别基于两类数据提出相应的网络模型以进行股票趋势预测。然后本文对异构数据进行融合以从多角度提取股票趋势的相关特征。具体的：


(1) 基于金融数据可视化使用CS-ACNN(Channel and Spatial Attention Convolutional Neural Network)进行股票趋势预测。本文通过三种方法将金融数据可视化，得到的三类图像都显著区别于普通图像，即很难用肉眼辨识不同类别的图像之间的差异。为提取图像特征，本文提出了CS-ACNN模型，该模型在卷积层间引入通道和空间注意力模块，使模型学会关注与股票趋势相关的图像特征。实验结果证实了本文提出的CS-ACNN较其他模型具有更好的预测效果，并找到了三种图像中最具价值的图像。通过对模型的中间输出进行可视化，发现CS-ACNN相比于普通2D-CNN能更有效的激活图像中有关股票趋势变动的区域。


(2) 基于新闻标题使用BAG(BERT-Attention and Gated Fusion Module)进行股票趋势预测。为基于少量训练样本提高预测效果，本文提出了BAG模型。该模型基于BERT提取新闻标题的语义特征，然后通过注意力机制融合单位交易日内所有的语义信息，并提出门控融合模块以提取时间窗口内多个交易日的语义特征之间的时间依赖关系。最后通过微调训练来实现基于新闻标题的趋势预测。


(3) 基于三类异构数据源的多模态融合模型进行股票趋势预测。为多角度综合不同因素对股票走势的影响，本文基于图像、新闻以及交易数据这三类异构数据创建了自适应多模态融合模型来预测股票趋势。首先通过跨模态自注意力机制将CS-ACNN提取的多尺度视觉特征分别与BERT提取的新闻语义向量进行融合，并将得到的多尺度融合特征分别输入门控融合模块以获得相应的图文融合特征。然后通过跨模态注意力机制将多尺度的图文融合特征、CS-ACNN提取的图像深度特征以及Bi-LSTM提取的历史股价波动特征进行融合以获得最终的融合特征。最后，通过模拟交易场景，证实了模型在金融交易中的可行性。"
2022,融合知识图谱的虚假新闻检测研究,计算机学院,顿雅倩,陈晨,Network,0.3182,"近年来，社交媒体上的虚假新闻数量呈爆炸式增长，由于虚假新闻恶意地歪曲和捏造事实，其广泛传播会对个人和社会产生极大的负面影响。检测和管 理社交网络上的虚假新闻，使用户收到真实的信息，对于保证社会和谐稳定十 分重要。因此，进行社交媒体虚假新闻检测十分迫切且对社会非常有益。 本文整理调研了虚假新闻检测方法的国内外研究现状，发现目前已有的虚 假新闻检测方法大多侧重于新闻内容本身和社交上下文，忽略了人们在判断新 闻真假时会用到的外部知识。由于新闻文本高度稠密化，包含大量的实体，如 人名、地名、机构名等，仅通过新闻文本内容难以理解它们。因此，本文提出 融合知识图谱信息的虚假新闻检测方法。


      本文的主要工作内容包括： 第一，为引入知识图谱中的知识，并将其与新闻文本有效融合，本文提出 基于注意力网络的知识感知虚假新闻检测方法。首先，将新闻文本和引入的外 部知识同时建模，获得新闻表示和知识表示。其次，考虑到不同知识对于新闻 理解的重要性不同，本方法中设计了两个知识感知注意力机制，衡量外部知识 的重要性。最后，将带权的知识表示与新闻表示相融合，获得融合语义水平和 知识水平的新闻表示，进而完成虚假新闻检测。本方法在三个真实数据集上进 行了大量的实验，结果表明，融合外部知识能够提升虚假新闻检测效果，且设 计的两个知识感知注意力机制能够更有效的融合知识。


       第二，考虑到同类别的新闻之间存在相似性，不同类新闻之间存在差异性，且每条新闻对应的知识在一定程度上反应新闻的真假性，因此，本文提出一个 两阶段的基于孪生网络的知识感知虚假新闻检测方法。第一阶段通过构建的新 闻孪生网络判断新闻文本对是否相似。然后，通过构建的知识孪生网络判断新 闻对应的知识对是否相似。第二阶段对新闻文本进行编码，并使用阶段一训练 好的新闻孪生网络和知识孪生网络中的分支分别对新闻、知识进行编码，获得 新闻孪生表示和知识孪生表示，并将它们与新闻表示相融合，进行虚假新闻检 测任务。本方法在两个基准数据集上进行了大量的实验，结果表明，基于孪生 网络的知识感知虚假新闻检测方法能够进一步提升检测效果。"
2023,融合知识的语法错误校正研究,计算机学院,何博,陈晨,NLP,0.3017,"近年来，互联网上的海量数据资源呈爆炸式增长，

但网络数据的激增导致了互联网信息质量的显著降低。

仅依靠人工进行审查和校正如此庞大数据量是不现实的，

但排除语法错误在商务电子邮件、学术论文等严肃场合造成的沟通障碍却十分必要。

同时，人们对该技术存在广泛需求，

语法错误校正技术具有极大的商业应用价值。

因此，进行语法错误校正研究十分迫切且对社会非常有益。


本文调研并整理了语法错误校正方法的国内外研究现状，

发现已有的语法错误校正方法大多使用基于编码器解码器框架的序列到序列模型，

通过自然语言生成的方式进行语法错误校正。

现有方法鲜有融合额外的知识对语法错误校正进行辅助，而额外知识对语法错误校正有正面效果；

同时现有方法仍受困于低资源的问题。

因此，本文提出融合知识的语法错误校正方法，主要工作包括：


第一，为引入语句固有的语法结构信息作为额外知识，

本文提出融合成分语法信息的语法错误校正方法。

该方法使用多任务训练，将融合成分句法信息作为辅助任务，

将序列化的成分句法树输入进对应解码器模块，

使整体模型能学习额外的成分句法知识，

捕捉成分句法结构信息并提供正确的成分语法，

从而提高语法错误校正的性能。

另外，该方法使用参数效率调优方法减少调优阶段模型参数量，

使用大规模预训练模型对未标注语料库进行筛选构建人工合成数据，

进一步缓解低资源问题。

充足的对比实验与消融实验展示了本方法已优于目前先进技术，

证明本方法通过融合成分句法知识显著提高语法错误校正效果。




第二，为引入更加广泛的语法错误相关知识，

本文提出基于检索增强的语法错误校正方法。

此方法结合参数化的内部隐式知识存储和非参数化的外部针对性知识存储，

使用检索模块、生成模块将上述两种知识进行融合。

检索模块对错误语句进行和根据编码信息在知识库中进行检索相应知识。

生成模块根据错误语句与检索结果，进行语法错误校正。

检索外部知识可以为模型提供正确的语法信息，从而提高语法错误校正效果。

充足的对比实验与消融实验展示了本方法与目前先进技术相齐平，

证明本方法通过融合参数化和非参数化知识显著提高错误校正效果。"
2021,空管大数据平台及相关服务设计,计算机学院,施文杰,李雨森,SE,0.2508,"空中交通管制（简称空管）是民航业的重要基础，相应的，空管自动化系统在民航信息系统中占据重要地位。近年来，随着我国民航业的迅速发展，各管制部门的空管自动化系统的数据量快速增长，空管数据已经符合业界对大数据的定义。然而，现有的空管基础设施无法满足民航业对大数据的使用需求，主要问题集中在以下几点：（1）各管制机构的空管自动化系统均为独立建设，无法实现对数据的集中管理，不利于各机构之间对数据的互操作；（2）业界对数据格式没有制定统一的标准，各空管自动化系统存在数据异构问题，无法有效地实现数据共享；（3）空管自动化系统的传统技术架构无法有效应对海量空管数据的存储和计算。


针对以上问题，本文提出了一种空管大数据平台架构，该架构针对空管自动化系统的特点进行技术选型，采用以Hadoop为核心的技术体系，借鉴Lambda架构的思想设计。该架构实现了空管数据即服务（Data-as-a-Service），以一种透明、标准的方式为用户提供空管数据服务。为达到这一目标，标准化是首要问题，即，大数据平台应提供与空管自动化系统之间、与用户之间的标准化接口，便于用户使用、便于系统扩展。为此，论文提出了一种空管数据服务标准化方案，包含了基于数据标记语言XEDML的数据解析算法和标准化数据共享方案。该方案解决了空管自动化系统的数据异构问题，能够降低数据源的接入成本，为数据的统一管理提供了基础。考虑到空中交通管制工作的特殊性，数据安全是空管数据服务的另一个重要问题。为此，论文提出了一种面向空管数据的访问控制方案，该方案是一种基于属性的访问控制方案，能够在系统运行时修改权限规则和属性，灵活调整系统权限，有效保证DaaS服务的数据安全性。


我们与华北空管局共同合作实现了大数据平台的系统原型，并对其进行了软硬件测试，包括大数据平台的部署和DaaS服务的各项功能。测试结果显示，系统完成并达到了设计目标，能够有效实现空管数据的接入、采集、预处理、存储和共享等功能，并能对用户访问做细粒度的访问控制，验证了设计的有效性。


 

关键词：空管；大数据；DaaS；访问控制"
2021,集成用户内容的旅游知识图谱和多任务推荐算法研究,计算机学院,吴美学,赵宏,NLP,0.2819,"随着互联网的发展和普及，网络承载的数据量飞速增长，与此同时，信息过载和数据无法有效利用等问题显现出来。知识图谱的出现为数据的整合利用提供了新思路，知识图谱不仅存储了大量丰富的知识内容，而且包含了知识之间的相互关系。基于知识图谱的推荐系统能够从大数据中挖掘用户兴趣，满足不同用户的个性化需求，将知识图谱融入到推荐任务之中能够有效提高推荐系统模型的效果。


近年来人们旅游的频率也逐渐增加，用户在互联网上生成了大量旅游相关的数据。本文首先构建了包含用户和旅游相关实体的旅游知识图谱，在知识图谱的构建过程中针对旅游领域数据特点和已有模型的不足之处提出了适用于旅游领域的实体对齐模型和命名实体识别模型；基于已建立的旅游知识图谱，提出了双知识图谱学习模块的多任务景点推荐模型。本文主要研究内容如下：


（1）采用自顶向下的方法构建了旅游知识图谱。首先基于斯坦福七步法构建了旅游本体；然后对来源于不同网站的旅游数据进行数据处理，针对旅游实体各个属性的特点，使用不同的编码方式对属性进行编码，提出了基于孪生网络的实体对齐模型。在本文的旅游实体对齐任务中F1值达到了95.2%；最后将旅游数据形成的三元组存储到Neo4j图数据库中，完成了旅游知识图谱的构建。


（2）采用自底向上的方法将用户游记中的旅游实体与已有旅游知识图谱进行实体链接。在实体链接过程中提出了TF-BERT-BiLSTM-CRF模型进行命名实体识别，有效筛除掉游记中用户提及但并未实际到达的旅游地点，此模型的F1值可达到90.8%；通过扩展指称、候选实体集排序等步骤进行实体消歧，将游记中的旅游实体与已有旅游知识图谱中的实体进行链接；通过游记中提取出的内容扩展旅游本体，将包含用户内容的新三元组融合到已有旅游知识图谱中。


（3）基于多任务学习框架提出融合知识图谱的个性化景点推荐模型。模型主要包含两种模块：推荐任务模块和知识图谱学习模块。模块之间通过交叉压缩单元实现特征的相互学习，获取对方的信息。其中，推荐任务模块数量为1，而知识图谱学习模块数量为2。其中一个知识图谱学习模块与推荐任务中的用户向量进行交叉学习，另一个与推荐任务中的景点向量进行交叉学习。同时在模型中加入因子分解机来学习高阶特征和低阶特征的组合特征。整体模型的损失函数为三个模块的损失函数之和，有效避免了单任务学习模型反向传播可能陷入局部极小值的问题。景点推荐模型的CTR实验准确率达到0.886。"
2024,面向分布式计算的高性能区块链系统,计算机学院,姜奕兵,李涛,Security,0.2534,"现有的分布式计算框架存在单点故障问题，即框架中集中式的调度节点一旦其发生故障，整个系统就会崩溃。此外，现有的分布式计算框架直接信任任务执行结果，没有验证措施，缺乏了可信性。区块链是一个具有去中心化、公开透明、数据防篡改等特性的系统，现有的研究工作证明了区块链具有解决上述问题的潜力。然而，区块链的冗余计算和共识过程延迟造成了链上任务执行性能下降。目前针对区块链与分布式计算的融合技术均致力于解决诸如单点故障等问题，因为缺乏在区块链上支持高性能计算的架构设计，所以将任务调度和执行等多个关键操作放在链下来保证执行效率，导致这些操作失去了可信性。


        针对上述问题，本文设计并实现了面向分布式计算的高性能区块链系统。该系统融合了区块链与分布式计算两种技术的优势，其核心思想是为链上任务调度和执行提供体系架构上的支持，以实现一个高性能、可信、公开透明的系统。在系统中，本文提出了分组调度的方法，通过调度块来组织区块链节点以减少搜索空间，加快任务处理速度，同时减轻了存储压力。此外，本文还提出了一种新的贡献度证明共识算法，其能够有效地加快区块生成速度，同时解决了传统共识算法资源利用率高、缺乏公平性等问题。


        本文基于主流区块链客户端Geth实现了面向分布式计算的高性能区块链系统原型，分别从时延、账本大小、资源利用率等方面将系统实现通过六个测试任务来与 Geth 进行对比。实验结果显示，本文设计的系统与 Geth 相比减少了97.5%的账本大小，提升了约 6 倍的任务处理速度。与 PoW 共识算法相比，贡献度证明共识算法将区块生成时间由164.9ms 降低到6.37ns，CPU利用率由96.7%降低到4.3%。此外，本文将系统的实现应用到基于区块链的物资流转系统软件中，通过多节点提供分布式的合约漏洞监测等计算服务，证明了系统设计的稳定性和实用性。本文设计的系统为快速和健壮的分布式计算带来了新的发展方向。"
2020,基于深度时序模型的硬盘故障预测方法研究,计算机学院,刘冬实,王刚,NLP,0.2911,"随着大数据时代的到来，数据中心数据存储规模和服务器集群规模日益增大。硬盘作为数据中心数据的主要存储设备，发生故障的频率越来越高。相比被动容错，主动容错可以提前预警故障，及时迁移数据，减少对硬盘资源和网络带宽的浪费。基于硬盘SMART属性，使用统计和机器学习方法建立硬盘故障预测模型，可以实现对硬盘故障的主动预测。


然而，现有硬盘故障预测研究多针对二分类问题，只能简单地区分健康硬盘和故障硬盘，缺乏对硬盘故障紧急程度的划分，无法实现对硬盘故障的分级处理。现有研究中用于验证模型效果的数据集不尽相同，部分数据集存在采集时间早、数据量小、采集时有偏等问题，无法实现模型效果的直接对比。实际数据中心存在硬盘厂商不同、型号不同、应用场景多样等诸多挑战，单一型号硬盘上的良好模型效果无法证明模型通用性。同时，现有研究逐渐使用集成模型、神经网络等复杂模型来提高故障预测效果，而缺乏模型可解释性分析，可能会掩盖模型偏差或过拟合等问题，影响模型在实际数据中心的应用效果。


本文根据硬盘剩余寿命区间对硬盘健康度进行划分，使用多通道循环神经网络建立多分类硬盘故障预测模型。在统一数据集、统一评价指标情况下，多通道循环神经网络模型相比现有多分类硬盘故障预测模型研究中效果最好的长短时记忆网络模型在多分类预测准确率、故障硬盘预测准确率、健康硬盘误报率等指标上均取得了性能提升。本文通过模拟其他型号硬盘数据集、中小规模数据集、交叉数据集和混合数据集等实际应用场景，对不同场景下的模型性能进行测试，给出模型使用建议。本文同时使用DFPE方法，通过一系列替换测试，对模型全局和故障预测结果进行解释，解释信息可以对运维操作提供指导，更有利于模型在实际数据中心的应用。"
2024,基于内核融合的PIM多任务调度系统,计算机学院,杨帆,李雨森,OS,0.2559,"PIM（存内计算）技术是解决内存墙问题的核心手段，常用来加速具有内 存瓶颈的应用。然而，本文发现，在诸如云计算等涉及多个 PIM 任务执行的真实场景中，单个 PIM 任务因为内核中的计算指令和访存指令数目很难恰好匹配， 在运行时往往不能充分利用全部 PIM 资源。同时，本文观察到，PIM 内核融合可以提升资源利用率和系统性能。对此，本文设计了 PIMFuse 高效 PIM 内核融 合算法。PIMFuse 通过解析内核代码、建立运行时性能模型，进而精确估计各内核运行时性能，并运用决策模型确定内核融合中的资源分配策略，最终采用水平融合技术实施内核融合。实验证明，PIMFuse 在内核融合实验中可实现最高 36.79%，平均 18.16% 的性能提升。


另外，在真实 PIM 多任务场景中，通常有很多内核混部执行，如何对这些内核进行调度会对系统整体性能产生重要影响。本文通过实验发现系统在处理多个 PIM 内核时，不同的融合内核顺序、内核数据切分方案对系统性能的影响不同。这需要高效的调度系统确定合理的融合内核顺序和内核数据切分方案来提高系统性能。对此，本文提出了基于内核融合的 PIM 多任务调度系统—— PIMKoordinator。PIMKoordinator 首先解析内核对内核进行分类，建模内核融合中内核的最优融合比例和每个内核的融合优先级，并且使用堆数据结构维护 PIM 内核，然后使用调度器调度执行内核堆中的内核。实验数据显示，在真实数据场景中，PIMKoordinator 可实现最高 25.25%，平均 18.58% 的性能提升。"
2019,基于Conformal Prediction的威胁情报繁殖方法,计算机学院,武艺杰,李朝晖,Security,0.392,"随着网络科学技术的迅猛发展，人们在享受网络带来的便捷生活的同时，网络安全威胁事件也频繁发生。习近平总书记曾说过，没有网络安全就没有国家安全。在日益复杂的网络环境下，对抗网络安全威胁的有效方法之一，就是增强网络空间安全的核心竞争力，即威胁情报的获取能力、分析能力和利用能力。


       传统的网络安全与智能技术，已经无法应对爆炸式增长的网络攻击与威胁。面对威胁情报的时效性短、变化速度快、每日新增数量大等特点，还没有一个成熟的威胁情报智能处理体系和方法。如何更加全面、有效地利用威胁情报，增强对网络空间安全态势的感知与预测能力，已经成为目前网络安全领域研究的一个重要问题。


       针对目前威胁情报的研究现状，本文提出了一种基于Conformal Prediction的威胁情报繁殖方法。该方法改进了传统“0-1”检测的静态阈值方式，对已知的威胁情报进行深度分析、利用。引入统计学习中的可信度，提高了模型对未知威胁的识别能力。并通过可信度的度量，实现对未知情报的有效繁殖。实验结果证明，此方法是有效的，并且对DGA恶意域名进行检测的准确率平均可达到94%以上，经过繁殖的DGA恶意域名的错误率也仅在2.4%左右。"
2020,K12纸笔物联手势符号有限集的无损实时识别研究与实现,计算机学院,赵晓琳,李庆诚,SE,0.2671,"信息技术在各个方面的应用为我们的生活带来了很大的改变，而教育信息化是信息化过程中的重要部分，但由于教育场景的特殊性，当前微课和智慧校园等教育信息化的应用存在一些广泛存在的误区弊端。，所以针对教育信息化的主流常态误区弊端设计了K12纸笔物联教育系统。K12纸笔物联教育系统是以点阵智能笔为系统核心设备，页群为系统基础，教学指令集为系统常态操作的适用于小学和初中基础教育阶段的教育信息化系统。

本文的核心问题是教学手势指令集的识别问题。

教学指令集是将K12纸笔物联教育系统与一般的基于点阵智能笔的智慧课堂系统区分开来的关键点。K12纸笔物联系统系统不仅可以通过点阵智能笔将手写笔迹数据采集，教学手势指令可以实现教学生成性数据筛分标记，信息实时同步到屏幕进行展示和师生之间交互，还可以利用教学指令集将普通加点阵码的纸张完全的变成类似屏幕的可交互介质，利用多种教学指令向系统下达命令并得到相应响应。K12纸笔物联教育系统的教学手势指令符号的识别是实时的联机手写识别，但不同于常见西文识别、数字识别和汉字识别，教学手势指令符号集中不同的符号有不同的手写特征，所以本文的教学手势符号的识别采用多种算法进行识别根据各手势符号的手写特征分为点击类手势符号、单笔手势符号、对错判别手势符号和减分手势符号，根据场景信息对手势符号识别算法进行调度，在提高算法识别率的基础上利用场景限制和策略排除增加识别正确率，再加上识别错误时人工修正、和回滚重新、优化识别算法参数和训练个性化识别模型等从而达到广义上的无损识别。在实验场景中验证了手势指令符号的识别，达到无损实时识别的初步目标。


本文的主要内容包括：

首先，本文介绍了国务院和教育部教育信息化需要达到的目标和要求和当前教育信息化过程中存在的主流常态误区；以及在手写识别领域，脱机手写识别、联机手写识别和基于深度学习的手写识别发展史和现状。描述了有限集手势符号的无损识别框架，定义了教学指令集的基础概念和无损识别，介绍了手势符号实时识别系统的六个模块和K12纸笔物联系统的系统逻辑。

然后，根据教学手势符号的应用场景和手写特征对教学手势符号进行分类，根据分类后结果选用适合的识别算法，分别各识别算法的原理、识别流程等进行介绍。将教学手势指令的识别在实验环境中的K12纸笔物联系统的识别效果进行验证，分别分析了手势指令的整体识别效果和各种类别的手势符号利用不同的识别算法的识别指标。利用不同适用识别算法对手势符号进行识别，再利用使用人、使用时间、作用位置等对手写符号进行约束得到唯一的教学手势指令，再进行对应响应。

最后，介绍各识别算法的具体实现和识别算法中可优化的参数介绍以及手势符号识别后得到的具有标签的生成性教学数据在页群计算中的应用。将教学手势指令的识别在实验环境中的K12纸笔物联系统的识别效果进行验证，分别分析了手势指令的整体识别效果和各种类别的手势符号利用不同的识别算法的识别效果，将本文的多算法手写识别与原有的手写识别算法进行比较，并对常态化教育场景中教学手势指令符号识别的应用进行设想。"
2024,微课辅助线上汉语教学的行动研究,计算机学院,胡小涛,程明明,CV,0.3517,"底层视觉任务包括图像超分辨率、图像去噪和视频未来帧预测等基本图像和视频处理任务，这些任务旨在于探索图像以及视频帧的高效语义特征表达，为其他计算机视觉任务奠定了基础。底层视觉任务的技术被广泛应用于监控安防、医疗图像、自动驾驶以及天气预测等领域。近年来，深度神经网络在底层视觉任务上取得了显著的性能提升，但这种进步往往以更大的深度神经网络和更高的计算成本为代价。这导致相应的网络难以部署到边缘设备上，例如摄像头和手机，从而严重限制了算法的实际应用前景。事实上，不同的输入通常需要不同复杂度的网络和不同的处理强度。动态深度神经网络根据输入的复杂性动态调整其深度，从而可以更灵活、更高效地处理视觉信息。基于此，本文探索用不同处理强度处理不同输入的动态网络的应用研究，帮助优化现有网络以及提出新的网络结构来以更小的计算代价获得更好的结果。本文围绕两个经典的底层视觉任务：图像超分辨率和视频未来帧预测，研究如何加速现有的神经网络方法，以及专为低级视觉任务定制的动态深度神经网络的设计和实现。本文的主要贡献如下:


• 提出一种图像超分辨率的掩膜引导加速神经网络方法，旨在减少现有图像超分辨率网络的浮点运算量和运行时间，同时保留其图像超分辨率能力。该方法将现有的网络划分为基础网络和精细网络。精细网络进一步处理没有被超分辨率好的特征块。该方法使用提出的轻量级掩膜预测模块，来自适应地选择没有被超分辨率好的特征块。在七个基准数据集上的实验表明，该方法可以加速五种不同尺度的超分辨率网络，同时保留超分辨率性能。


• 提出了一种基于动态体速流网络的视频预测算法。该方法通过动态光流估计来明确建模相邻视频帧之间不同尺度的复杂运动线索。该方法的神经网络由多个多尺度体素流块组成。每个多尺度体速流块中有一个轻量级的路由模块，根据输入帧自适应地生成路由向量，并动态地选择一个子网络以实现有效的未来帧预测。在四个基准数据集上的实验表明，该方法相比于领域内其他方法，在视觉质量、参数量和浮点运算量上取得了全面优势。"
2020,基于机器学习方法的生物序列分类预测问题研究,计算机学院,许婧,张瀚,NLP,0.2969,"随着测序技术的迅速发展，研究生物序列、发掘其潜在意义是当前研究的重点。科研人员利用数学、计算机等知识，开发算法和工具来探索生物序列的奥秘，目前相关工作已经取得了相当多的成果。


生物序列的研究涉及很多内容，其中如何准确的识别lncRNA、预测lncRNA与miRNA是否存在相互作用关系以及如何精准的给酶进行分类和注释是当前研究的热门内容。本文针对这三个问题，相应的设计了三个方法来解决。


首先，本文设计了一个基于xgboost的lncRNA识别工具。该工具从序列中提取出一些特征，然后通过xgboost进行预测。该工具在人类和小鼠数据集上同多个方法进行了比较，实验效果有一定的优势。


其次，本文提出了一个基于神经网络的lncRNA与miRNA相互作用预测算法。该算法分别将lncRNA和miRNA的one-hot编码作为输入，通过卷积和双向LSTM来提取序列特征，通过计算特征之间的距离来判断两者之间是否存在相互作用。该算法在多个数据集上测试，并与常见的分类算法以及最新的预测方法进行比较，在预测准确度和预处理上具有很大的优势。


最后，本文开发了一个基于二部图结构的酶分类和注释方法。该方法利用短序列作为蛋白质序列特征，对酶进行分类并提取了每一类的短序列特征信息作为注释信息，对未知的蛋白质序列根据短序列和类中的短序列信息的匹配情况将其划分到合适的类中，赋予该蛋白质相应的标签信息。本文将该算法应用到碳水化合物活性酶家族数据和酶的EC数据中，分别采用了五折交叉验证来评估分类和注释效果，并与最新的方法进行比较，比较结果表明无论是在子家族的分类和注释方面还是在EC的分类和注释方面，该方法普遍都具有更好的分类注释效果。另外，本文还将该方法应用到全部的碳水化合物活性酶家族数据中来提取各个家族的短序列信息，对12个基因组的蛋白质序列进行家族注释，同样与其他方法作对比，对比结果表明该方法在注释准确度的表现更好。"
2019,中文事件图谱构建方法研究与实践,计算机学院,章华龙,师文轩,Security,0.2738,事件信息是人类活动中一类重要的知识载体，事件图谱旨在以图（Graph） 的形式组织现实世界中的事件知识。构建事件图谱有助于厘清事件的要素以及事 件之间的关系，以结构良好的事件知识服务众多应用，如时序问答、因果推理和 事件脉络生成等。目前事件图谱相关的技术环节已经被广泛地研究，如事件抽取、 事件关系抽取以及面向实体的知识图谱等。而对于完整构建以事件为中心的知识 图谱的研究与实践尚显不足，在中文事件图谱方面的探索更显不足。 本文以中文事件图谱的构建方法与实践为题，主要研究内容如下： 首先，本文综合国内外已有的研究成果，梳理出了事件图谱构建的一般方法。 并从事件图谱建模、事件抽取、事件关系抽取、事件图谱评价这几个方面对事件 图谱的构建理论进行了分析和讨论。介绍了构建事件图谱过程中各个环节的一般 可行方案，以及整体的设计原理。 其次，本文基于梳理出的事件图谱的构建理论，在中文环境下进行了几个方 面的探索与实践。主要指时序事件库 CN-StartEnd 的搭建。时序事件图谱以事件 之间的时序关系和共指关系为重点，在有限的人工标注语料集上对事件图谱的构 建理论进行了实践与验证。CN-StartEnd 则是借助大规模外部知识库，针对事件 起始与终止这两个特殊的概念进行了事件图谱的尝试与创新。 在时序相关的事件图谱实践之后，本文进一步对复杂的中文时间表达式的识 别进行了研究。时间表达式识别不准会严重影响事件图谱中事件与关系的质量。 对此本文提出了利用大规模中文知识库以远程监督学习的方式优化中文时间表 达式标注的方法，实验证明该方法在开放领域的中文文本中提升时间表达式的识 别率是有效的。 上述研究均围绕事件图谱的构建展开，实践工作均面向中文语言环境。本文 研究成果对于构建限定领域和开放领域的事件图谱都具有参考价值，为中文文本 的事件信息抽取与结构化提供了经验，有助于更好地服务事件相关的信息类应用。
2019,带有惩罚机制的拜占庭容错算法的区块链性能改进研究,计算机学院,程书芝,师文轩,Security,0.2877,"随着比特币不断发展，它的底层技术区块链也逐步走进公众视野。由于区块链的去中心化和不可篡改性，它已经从单一的数字货币应用延伸到经济社会的各个领域，包括金融、供应链、公益和教育等。但是目前区块链的交易吞吐量、交易确认时延等性能还无法满足实际应用场景的需求。


从性能指标来看，以工作量证明机制、权益证明机制等为共识的公有链的系统吞吐量只有每秒7笔交易，交易平均确认时间是10分钟，而交易无法篡改的时间则达到了1小时。这类共识不仅交易效率低，而且资源耗费严重，无法满足实际应用的性能需求。以Paxos、Raft等一致性算法为共识的联盟链系统没有考虑拜占庭容错，对诚实节点和恶意节点共存的区块链系统并不适用。而考虑了拜占庭容错的实用拜占庭容错算法是面向分布式系统消息设计的，一方面对恶意节点没有采取一定的惩罚措施，另一方面三阶段的广播协议浪费了通信资源，降低了通信效率。


本文在对以上共识算法进行详细研究之后，提出了一种对实用拜占庭容错算法的改进方案（Performance-Improvement Practical Byzantine Fault Tolerance, PI-PBFT）。PI-PBFT一方面引入了对系统中拜占庭节点的惩罚机制，根据节点在之前的共识过程中的表现进行排序，排序靠前的节点参与下一轮共识过程的概率更大，从而降低拜占庭节点参与共识过程的概率。另一方面对无拜占庭场景进行优化，当所有节点的状态和响应都一致时，无需进行节点间的n2通信，从而降低节点通信频次，减少消息确认时延，提高共识效率。


最后，本文在不同系统状态下，对PI-PBFT进行性能测试。实验结果表明：随着最大同时处理消息数量增加，PI-PBFT的系统消息平均确认时间震荡增加；随着节点数量增加，PI-PBFT的系统消息平均确认时间逐渐增加；随着拜占庭节点比例增加，PI-PBFT的系统消息平均确认时间基本不变；随着节点误差概率增加，PI-PBFT的系统消息平均确认时间逐渐增加。而无论是在哪一种场景下，PI-PBFT的系统消息平均确认时间都远低于PBFT的消息平均确认时间，PI-PBFT的系统吞吐量都远高于PBFT的系统吞吐量。"
2019,分布式消息中间件的设计与实现,计算机学院,孙大鹏,程仁洪,Security,0.2556,"随着互联网的飞速发展，21世纪以来传统的单一架构已经难以很好的支持日益复杂的业务，互联网公司基本在不断地进行将传统的单一架构向分布式架构的转变，而RPC成为了分布式架构中通信的解决方案，但是分布式架构中RPC同步处理技术使得系统在性能、健壮性以及可扩展性上存在诸多缺点，因此需要一个中间层来将各个服务系统连接在一起，消息中间件应运而生。消息中间件作为一个中间层将各个子系统有效地粘合在一起，通过其高效可靠的消息传递机制实现平台无关的数据交流，使得各个子系统之间实现低耦合，且易于维护和扩展。


消息中间件是一个基于发布-订阅模式（即生产者-消费者模型）的消息队列系统，整个中间件由服务器和客户端组成，客户端由消费者和生产者组成，提供发送消息和接收消息的接口，而消息服务器负责消息的可靠传递。


本文设计并实现了一个高可靠、高可用的分布式消息中间件，介绍了网络通信I/O的模型以及基于事件驱动的Netty高性能网络编程框架以及其零拷贝技术，并基于Netty实现通信服务端和通信客户端，为了改善消息通信的性能，本设计采使用了高性能的二进制序列化框架Kryo进行消息的序列化，为实现消息的高可靠性与高性能，消息中间件的服务端采用KV集群存储和消息的确认机制，在消息的发送端，通过消息的发送失败重试保证消息可靠投递,通过负载均衡保证消息的高可用性，以及通过多线程任务的分派处理。此外，针对分布式架构中的事务事件，本文基于两阶段提交协议对分布式事务作出基于消息中间件的实现，以此支持业务系统中的事务。


最后对系统进行了功能性和性能的测试。"
2019,晚明山水画写实语言研究（1540-1689）,计算机学院,高歌,赵宏,Security,0.3068,"我国火灾中电气线路短路故障是引发电气火灾的主要原因。为了对电气火灾事故原因进行认定，消防部门的工作人员首先需要在火灾案发现场对所有疑似短路导线进行采样，然后将采得的样品送达鉴定中心进一步分析鉴定，最后生成鉴定报告从而确定火灾原因。在此过程中主要存在两点问题：其一，普通的消防人员由于经验不足难免遗漏或者错误采样导线样本，因而降低了火灾原因鉴别的准确度；其二，我国从事短路导线熔痕鉴定的专家数量极少，而需要鉴别的导线样本却很多，因而降低了火灾原因鉴别工作的效率。


本文针对上述电气火灾原因鉴定采样过程中存在的问题，主要做了两方面的工作：第一，根据某市消防研究所提供的导线熔痕（含短路和火烧）的样本，分别使用Lenet-5以及Inception-v3进行建模实验，对算法模型进行相关优化，经过大量实验结果的对比分析得出识别两类熔痕的最优模型，最终将导线熔痕识别准确率提升到了88.2%；第二，针对消防研究人员的工作需求，开发了相应的应用程序，并将熔痕识别模型集成到系统中，使消防人员在火灾现场可以依据AI识别结果对导线样本进行采样，并且方便消防研究人员对导线熔痕样本图片进行管理和研究。


本研究为消防工作人员以及鉴别专家提供了有效和便捷的辅助鉴定工具，能够大大地减少人力、物力以及时间的耗费，解决了火灾现场采样存在的样品识别问题。"
2019,要素市场扭曲、环境规制与环境污染——基于2003-2016年中国省级面板数据的分析,计算机学院,侯博然,李忠伟,Security,0.2446,"倒排索引是当今搜索引擎中最核心的技术之一。随着互联网上信息的爆炸

式增长，搜索引擎在构造和访问倒排索引信息时也需要更多的时间和空间成本。通过对倒排索引进行压缩，可以有效地节省存储空间并且减少访问磁盘和内存的次数。在查询过程中应用提前停止技术则可以有效地降低时间成本，因为其可以在不遍历完所有倒排列表信息的情况下，返回与用户查询最相关的查询结果。

本文结合了以上两种思想，提出了一种基于提前停止技术和混合编码的倒

排索引压缩算法并利用贪心算法对其进行了证明。算法的基本思想是利用提前停止技术得到数据集文档的估值分数 GS，然后将每个定长的倒排列表分块的GS 分数作为它的热度，对于热度高的分块采用解压缩速度快的编码方案，对于热度低的分块采用压缩率好的编码方法。通过这样，可以让整个索引在占用较小的空间下，保持极高的查询速度。实验表明，无论编码集合中是否含有 SIMD 优化算法，本文所提出的算法都可以获得比单一编码算法更好的“时间-空间”权衡。通过与学术界公认的“时间-空间”权衡特性最好的 optfor 算法进行对比，我们可以发现：当含有 SIMD优化算法时，混合编码算法相比 SIMD 优化的 optpfor 算法，编码后体积减少了4.65%，查询时间减少了 7%；当不含有 SIMD 优化算法时，混合编码算法相比无SIMD 优化的 optpfor 算法，编码后体积减少了 3%，查询时间减少了 9.2%。除此之外，我们还发现含 SIMD 优化的混合编码算法的效果更好，可以获得更低的压缩率和更快速的查询时间。"
2019,社会支持对空巢老人生活质量的影响研究,计算机学院,周均明,程仁洪,Security,0.2722,"社会经济的高速发展促进了各行业科研水平的不断进步，各个领域的学术文档数量也是与日俱增。我们清楚的知道，阅读他人的优质论文，恰当地引用适量论文，是学术研究者进行交流学习的重要部分，而如何帮助他人在大量文献库中高效地找到符合个人需求的学术文档，如何找到高可用价值的论文，再者如何挖掘用户兴趣特征并将论文精确地推荐给用户,这无疑都是极具意义的事情。针对上述问题的解决，搜索和推荐是两个行之有效的手段，本文采用Lucene全文检索引擎工具包，深入分析论文文本信息特征，提出了基于结构和内容信息的文档智能搜索与推荐方法。


      本文重点研究了两种Lucene的相关度排序算法中的各种影响因子，并对原排序算法的不足，也即只重视查询词在检索文档中出现的频率，而查询词出现的位置关系并不被重视的情况，引入了词语次序相似度计算方法和词语的邻近距离相似度计算方法，以反映查询语句和检索的文档内容在句子上的相似性。针对学术论文这种特殊的数据源，其结构层次相对固定的特征，将查询词出现在论文中标题、摘要、正文不同位置部分赋予了不同域权重，以期更加满足用户检索的需求。本文引入PageRank算法来统计论文之间的引用关系，并总结了一些论文相关指标的评价计算方法，融合到当前的Lucene排序算法中使检索结果有了论文影响力和适用价值的评价指标。本文采用基于Java语言的DL4J神经网络训练工具，进行Word2Vec的词向量训练，构建用户兴趣模型，进行了个性化的学术文档搜索排序算法的优化。同时研究并设计了基于用户协同过滤的论文推荐方法，依据用户—论文之间的关系生成的用户相似度表，对用户尚未关注的论文进行预测推荐；依据论文的正文、标题、摘要、作者等内容信息的相似性，实现基于Lucene的相关文档推荐。


       实验结果表明，本文有效地弥补了论文检索中查询词在学术论文中位置并不重视的情况，提高了论文查询的相关程度，使搜索结果更加符合用户的查询需求，提高了检索论文的质量，关注了用户兴趣，设计和实现了基于用户协调过滤方法和基于内容相似性方法的文档混合推荐。"
2019,多重刺激响应性聚合物的合成及自组装的研究,计算机学院,罗看看,苏明,Security,0.2853,"随着互联网技术的发展，数据呈现爆炸式地增加，数据的安全问题因此埋

下隐患，这对数据安全的保障提出了更高的要求。密码学作为保障数据安全的一门学科得到快速发展。多项式乘法是密码学底层的重要组成部分，对其运行效率进行优化符合密码学快速发展的趋势。在现今的互联网环境中数据安全依然面临巨大的挑战，有限域内的多项式乘法作为密码学底层的一个基础计算问题，对其快速算法的实现与优化具有重要的理论和实践意义。

多项式乘法的应用领域十分广泛，如区块链、同态加密等当今热门的技术

领域中都有应用。平凡的多项式乘法算法时间复杂度为 o(n 2 )，目前可以用于进行快速多项式乘法的算法有 FFT、Toom-Cook 算法等。本文利用了 CRT、FFT等算法提出了可以快速计算有限域上的多元多项式乘法的算法框架，利用 CRT将多变元多项式规约至单变元多项式，并且可以将单变元多项式还原至多变元多项式。FFT 算法将单变元多项式乘法的时间复杂度从 o(n 2 )减少至 o(nlogn)。针对计算过程涉及到的大整数运算，本文利用 GMP 大整数库解决了算法实现过程中遇到的整数溢出问题。

在实现算法框架的基础上,本文针对系数为 GF(2)的多项式乘法进行了优化，并对 GF(2)上的 Toom-Cook 算法进行了实现，对 FFT 与 Toom Cook 算法的优劣性进行比较分析。针对一些特殊域，对多项式乘法中的模运算进行优化。本文提出了一种可以利用 GPU 并行技术计算的多项式乘法分解方案，可以利用多项式基对系数为 GF(2 n )的多项式进行分解，使一个多项式相乘分解为多个互不相关的子多项式相乘从而达到并行优化的目的。"
2019,基于博弈论的比特币定价模型构建及模拟分析,计算机学院,孟祥娟,王斌辉,Security,0.2809,"如今，供应链过程变得越来越复杂，涉及的参与实体也越来越多，影响了商品的成本和对消费者的可用性。当前的供应链信息系统也往往缺乏货物运输过程中向所有参与方（尤其是供应商和消费者）准确传递实时信息的能力。因此，可追溯性和透明度在供应链物流中变得越来越重要。


本文设计并实现了一个基于Hyperledger Sawtooth区块链的冷链系统，主要包括以下组件：用于处理业务逻辑的Transaction Processor、创建交易并实现系统前端页面显示的Client、为Client提交交易操作提供HTTP/JSON API的Server、将区块链数据同步到本地数据库的Event Subscriber、用于运行系统命令和脚本的Shell。系统涉及供应商、物流公司和消费者三类角色，其中供应商添加货物订单信息，授权相应的传感器，选择物流公司进行货物托管权的转移；物流公司进行托管权的接受或拒绝操作，如果选择接受，则继续添加运输车辆的相关信息；运输过程中，传感器将定期上传货物的信息进行数据更新；当货物到达目的地后，物流公司和供应商分别将托管权和所有权转移给消费者，同时供应商还将撤销对传感器的授权；消费者拿到货物后，对收到的权限转移进行接受/拒绝操作；最后，由物流公司终止整个运输过程。


本系统利用物联网传感器获取货物在运输过程中的位置、温度和湿度等数据，并采用Hyperledger Sawtooth保证记录的不可变。供应链中的参与者能够通过这个透明可靠的统一信息平台，获取货物的各种信息，实现对货物的溯源防伪。"
2019,交联型聚乙二醇功能化金纳米粒子的制备及其抗蛋白冠研究,计算机学院,卢乾坤,王超,Security,0.2349,"随着信息时代的发展，人们已经不再满足于仅仅用文字来传达自己想要展示的信息。图片作为一种更加形象、信息量更大的载体，在日常生活中被广泛应用着，对其需求的不断变化也带动着图像处理技术的发展。在想要获取清晰度更大、视角更广的图片的需求下，图像拼接作为一个重要的研究领域，越来越被全世界的研究学者们所重视。在图像拼接中，由于原图像拍摄位置视角的变化而产生了视差，由于位置的变化引起的光照强度变化而产生了色差。视差与色差是不可避免且频繁出现的两个问题，往往会给图像拼接结果带来很大的影响。因此，如何有效地处理这两大缺陷，是提高图像拼接质量的研究重点。


本文主要研究的是图像拼接中的颜色校正算法，通过对近年来颜色校正算法的研究，在现有算法的知识基础之上，结合图像增强方法，参数估计方法，误匹配去除算法和最优化理论，提出了一种适用于不同视差条件下进行图像拼接的颜色校正算法。首先通过重叠区域的点匹配计算映射关系，然后进行颜色映射模型的估计，最后通过最优化方法得到最终的颜色映射函数，颜色校正后进行图像拼接。图像拼接时我们使用了As-Projective-As-Possible（APAP）算法和缝合线拼接算法，以适应不同的具体情形。


同时，本文还与经典的颜色校正算法以及最新的颜色校正算法进行了对比实验。结果证明，本文提出的方法具有较好的实用性和鲁棒性，而且针对不同的实际情景，进行了相应的优化处理，不仅能够解决在各种复杂场景下的颜色校正问题，同时又能保证较快的处理速度，得到很好的图像拼接效果。总体而言，本文实现的方法是一种高效的图像拼接颜色校正算法。"
2019,"含季碳α-氨基酸结构单元的3,4-二氢香豆素的立体选择合成",计算机学院,刘中睿,张波,Security,0.2916,"随着人工智能时代的到来，越来越多的行业领域使用机器来替代人工，促进了计算机、模式识别、图像及语音处理等技术快速发展。同时，新技术的出现对计算机的运算能力提出了更高的要求，使用虚拟化技术分配调用GPU计算资源，使计算能力充分发挥，极大的提升了计算资源的使用效率，具有广阔的应用前景。


本文以CUDA并行计算架构的虚拟化为研究对象，基于CUDA远程调用技术开发了GPU云计算管理平台。本文工作主要包括以下几方面内容：


CUDA函数的远程调用研究，通过修改CUDA的API，将其内部函数进行拦截并映射转发。实现CUDA库的虚拟化以及远程调用。分别开发位于虚拟机侧的客户端以及物理机侧的服务器端，处理函数远程调用。


云平台管理系统的实现，本文通过使用Yii2框架开发出面向用户的云平台管理系统，实现云计算平台的通用管理功能，支持跨平台多用户多终端同时访问，方便用户使用。平台基于分布式架构，后期随着业务量不断增大，支持平滑扩展升级。


评估云计算平台的性能，编写测试程序，测试使用远程调用技术修改后的CUDA库的效率及稳定性。执行官方和未修改过的原生CUDA程序，测试CUDA远程调用技术的透明性。同时采用多种机制，提高平台整体性能。


通过软件测试以及试用，对计算平台的可靠性、安全性、处理能力进行验证并测量了系统误差，符合最初的设计，获得了较好的效果。"
2019,短视频APP用户持续使用意愿研究——以“抖音”为例,计算机学院,徐希浩,高铁杠,Security,0.2897,"在信息爆炸的现今时代，网络已经逐渐成为人们获取信息的主要渠道。数字图像是一种重要且直观的网络信息传输媒介，其真实性和可读性非常重要。但是图像的篡改会使得图像的原有信息发生了改变，从而给司法取证、新闻传媒、科研诚信、军事领域等带来负面影响。图像的中值滤波，作为一种非线性处理方式，往往用于掩盖图像的篡改痕迹，给图像的取证工作带来很大挑战。因此，对中值滤波的检测算法的研究具有重要的研究意义。

      目前多数的中值滤波取证算法面临图像分辨率低和JPEG 压缩失真两个主要挑战。本文从这两点出发，研究了对低分辨率JPEG 压缩图像进行中值滤波取证的有效算法。通过分析，发现滤波图像和未滤波图像的傅里叶频谱的高频分量有较为明显的区别。本文在已有的算法基础上，结合傅里叶变换和二维自回归模型提出一个新的中值滤波取证算法2D-ARF。经过大量实验，验证了2D-ARF算法在综合性能上优于该领域的基于空域、频域图像进行中值滤波取证的算法。尤其是针对低分辨率以及压缩因子QF 较低的图像，本文算法有更为突出的优势，但从整体上还是存在很大的提升空间。

      为了进一步提升算法性能，本文提出了一种将2D-ARF 和相邻元素直方图特征融合的DAP 算法。该算法利用一阶差分对图像进行预处理，然后分别计算2D-ARF 和相邻元素直方图特征，最后得到融合特征。本文最后将DAP 算法用于检测图像的局部篡改，取得了很好的效果。本文最后对今后的中值滤波取证算法研究工作指出了进一步的研究方向。"
2019,美国对巴勒斯坦地区政策(1945-1948)：白宫与国务院的分歧与合作,计算机学院,钱恒,任明明,Security,0.2666,"在搜索引擎系统中，查询时延是十分重要的，而最有效降低查询时延的方

法就是使用缓存策略。尽管如此，搜索领域中对于搜索引擎中的缓存策略仍未有最优的解决方案。对于一个传统简单的搜索引擎架构，DRAM 是其唯一的数据缓存设备。在该架构中，我们往往会使用启发式贪心的策略来解决缓存的填充和替换问题。而如果系统中存在多种缓存类型，为了最大化缓存的效果，我们还需要考虑多类型缓存之间的依赖问题。这种单一缓存架构存在的一个问题是 DRAM 的单位空间成本过高，这同时也限制了服务器中被用作缓存的空间大小。最近一系列新型的非易失内存（NVM）设备，例如 Intel 的 3D Xpoint，正在被高度关注研究并且一阶段产品已进入市场。这种非易失内存设备拥有更加低的价格但性能接近 DRAM，并且还有断电以后不丢失数据的新特性。非易失内存的逐渐普及促使我们研究同时使用 DRAM 和 NVM 的两级缓存搜索引擎架构，并为此设计相应的缓存策略。

基于该非易失内存设备，本文提出了新型的两级缓存搜索引擎架构。在此

架构中，非易失内存设备作为内存缓存的二级扩展，这种扩展性两级缓存能够提高原有缓存系统的空间上限；此外由于混合使用了更高性价比的缓存设备，整个搜索引擎系统具有了降低硬件开销的潜力。在本文工作中，我们在缓存系统中混合使用了七种常见的缓存类型（查询结果缓存、结果文档编号缓存、倒排列表缓存、求交缓存、摘要缓存、精简摘要缓存和文档缓存），并为该两级缓存架构提出了一种新型的静态缓存填充策略。该搜索引擎架构致力于利用非易失内存的扩展空间来进一步优化查询时延，相应的也降低了系统在缓存空间上的成本开销。实验结果表明这种 RAM 和 NVM 的两级缓存架构比传统单一缓存架构在查询延时方面要最多缩短 6.4%。并且在两种架构拥有相近的查询延时的情况下，与单一缓存架构相比，两级缓存架构可以在缓存空间开销上节省最多75%，而在系统总开销上可以节省将近 25.2%。"
2019,Schubert 多项式理论中的几个问题,计算机学院,白建伟,刘明铭,Security,0.291,"编程课程教学辅助系统在软件学院的日常教学活动中有着重要的作用，老师会通过教学辅助系统给同学们布置一些编程作业，或者是组织同学们在系统上进行编程课程的考试。然而教学辅助系统存在着一些缺陷，在较高流量的场景下，比如临近考试结束前考生们集中提交代码，教学辅助系统会长时间处于卡顿状态，考生们提交的代码会长时间得不到反馈，令使用者产生困惑。本文梳理现有的教学辅助系统，研究教学辅助系统的总体框架，业务流程，分析关键代码，以期找到系统的性能瓶颈和问题所在。


随着云计算技术渗透到计算机互联网的方方面面，而且云服务可以按需使用，按使用量进行付费，能有效的降低采购服务器的成本，可以很方便扩容，这对于教学辅助系统是一项很适合的技术。Docker技术也有很多不错的特征，比如持续部署、更新和测试，隔离特征，方便的版本控制等。Docker还提供了集群化部署的工具Docker Swarm。使用DockerSwarm可以很方便创建并管理集群。


因此本文采用Docker技术和Swarm集群技术来优化教学辅助系统。将现有的教学辅助系统进行服务端和Judge端分离，降低系统的耦合度。将教学辅助系统由单机部署的方式改成Swarm集群部署的方式，使用Docker-compose技术对有依赖关系的服务进行编排。使系统可以根据业务的增长调整服务器的部署，提升系统服务的稳定性和扩展性。使用cAdvisor、InfluxDB、Grafana建设教学辅助系统的监控平台，方便管理员对系统错误定位和管理维护。进一步研究Docker Swarm技术，并对其调度算法进行优化，使得Swarm集群对于资源的利用率更高。编写脚本，使系统在负载过高时能自动扩容，采用本文优化后的调度方法来进行扩容。


本文最后通过搭建实验环境，对优化后教学辅助系统进行功能测试，测试系统的监各项功能能否正常运行。然后再进行性能测试，通过对比使用优化方案的系统与原系统的性能，证明了本文所提出方案的实际意义与可行性，对于编程语言教学辅助系统的管理维护，发展更新具有实际意义。"
2019,基于WEB的PM2.5在线源解析平台的研究与实现,计算机学院,杨远熙,赵宏,Security,0.2729,随着我国经济的快速发展，人们生活水平的不断提高，我国正逐步进入生态文明社会，因此环境保护在社会发展中起到重要作用。而近年来，大气的重度污染却严重威胁着人们的健康，大规模的雾霾天气严重影响了人们的正常工作和生活，因此对大气的治理逐渐被环保部门重视起来。 PM2.5在线源解析是指基于高时间分辨率（一小时或以下）大气颗粒物在线分析仪器提供的数据，通过CMB、PMF等源解析技术定性识别污染物来源，定量解析出移动源、固定燃烧源、农牧源等污染源对颗粒物的贡献。近几年，我国信息化技术的迅猛发展，许多先进的科技产品应用到环境保护当中，尤其是大量的高精尖环境监测设备研发和应用，使我国逐步完善了环境监测网络。通过完善的环境监测网络，许多城市和地区的大气监测部门不断获得实时的空气质量监测数据，使得实时在线污染源解析工作可以逐渐展开起来。 本研究通过监测大数据实时采集、结合大气污染源成分谱管理系统，使用化学质量平衡法(Chemical Mass Balance，CMB)和正定矩阵因子分析法(Positive Matrix Factoranalysis，PMF)源解析模型实现大气污染源实时解析。 本系统主要有三部分内容： （一）数据采集和预处理。通过FTP、HTTP协议将监测数据汇总至数据中心服务器，数据中心服务器通过异常值剔除原则对数据进行预处理，将处理合格的数据上传至分布式文件系统(HDFS)。然后再利用分布式运算(MapReduce)技术，将样本数据按照小时、天等时间分辨率通过求平均值的方式融合成一个样本（以下简称样本融合），并将样本融合后的结果存入MySQL数据库。 （二）利用JAVA编程实现CMB与PMF源解析模型，将获得的实时样本数据输入模型进行源解析计算，并将污染源种类、污染源贡献率、拟合优度诊断等源解析结果存入MySQL数据库。 （三）利用Spring、SpringMVC、Mybatis框架搭建Web服务平台，主要实现在线源解析计算参数设定、在线源解析实时计算、离线源解析计算、源解析结果可视化展示、结果打印等功能。 本研究基于国际通用的CMB、PMF算法，实现了高效、高时间分辨的颗粒物源解析计算，计算结果与EPA（美国环境保护署）提供的EPA-CMB与EPA-PMF软件计算结果接近，具有很高的参考价值。本研究还实现了基于WEB源解析结果的可视化展示、结果打印等功能，方便了环保工作人员进一步研究。
2019,财税激励对企业全要素生产率的影响研究 ——基于重点产业与非重点产业的对比分析,计算机学院,张一凡,李旭东,Database,0.2556,"大数据时代，每天在互联网上产生着海量的结构化和半结构化数据。传统的关系型数据库难以实时存储查询海量的非结构化数据，新型非关系型数据库NoSQL具有丰富的特性，能够很好地作为关系型数据库的补充解决此类问题。针对不同的业务场景，企业根据各类业务场景选择将数据分别存储在不同数据库中。然而异构数据源的多样性和异构性，造成了企业内部信息交换的闭塞，难以将多种数据整合起来进行复杂的数据分析和统计。即使是同属于关系型数据库的两种数据库，它们的数据类型仍存在很多不同。由此可见对于多种异构数据源之间的数据同步研究是很有必要的。通常在多种数据源之间进行数据同步时，需要有针对性的编写两种数据源之间的交互代码，通用性不强，不具有可扩展性。


       本论文针对这种现状，设计并实现了针对多种异构数据源的数据同步系统。该系统基于中间数据进行数据交换，避免了数据源相互之间都要编写代码的复杂性。数据源之间具有相同的工作模式，即通过多线程方式从源端数据源中并发抽取数据并转换为系统定义的中间数据，经过数据处理后在内存中传输，线程并发取出数据后转换为目的数据源所需数据类型进行写入。该系统实现了MYSQL、MongoDB、HDFS文件系统三种数据源全量数据之间的相互转化。系统同时提供了针对MYSQL数据库Binlog日志的解析功能，获取全量同步后MYSQL产生的增量变化数据，写入用户指定目的数据源中。


       通用的中间数据和工作流程提高了代码的复用性，可扩展性。系统需要增加新的数据源时，针对该数据源编写数据抽取模块和数据写入模块代码，易于实现与系统整合数据源之间的相互数据同步。系统很好解决了企业多数据源之间所需的数据同步和数据共享问题。"
2019,基于差分像素对特征集的图像锐化 取证技术研究,计算机学院,张元,高铁杠,Security,0.3011,"数字图像作为一种传递信息的重要媒介，其样式多样、内容复杂，在各个领域均有广泛的应用。伴随着信息时代的到来，数字图像作为互联网信息发布的重要载体，信息共享的同时也一定程度上影响着受众。对数字图像的相关处理，通常能够使其视觉质量得到提升，易于传播，并反馈最适宜的信息。然而，对图像结构与内容进行恶意的篡改，往往会歪曲事实，造成一定的负面影响。图像取证技术，作为一门专门针对图像篡改进行科学检测的技术，在数字图像领域扮演着一个重要的角色。


本文主要针对数字图像的USM锐化理论及其相关的检测技术进行了阐述。USM锐化技术，作为一种较完美的数字图像锐化技术，从图像细节入手，以此增加图像的视觉质量。与此同时，这种锐化技术也能够有效地隐藏恶意篡改的痕迹，给图像的安全性带来挑战，所以，对其进行研究取证工作有重要的理论意义和实用价值。在相关的研究工作中，利用USM锐化处理所带来的图像边缘区域的过充效应，能够辅助研究人员更好地进行检测，以此为切入点的众多算法都对USM锐化检测具有较高的准确率，本文对这些算法进行了介绍与分析，针对其特征提取方法也提出了改进意见。


同时，与以往针对图像边缘区域进行取证检测不同，本文提出了一种对图像进行全局考虑的差分像素对特征集的统计检测取证算法。经过算法的相关处理，收集并统计了由USM锐化带来的散落在图像各处的细微变化，并利用SVM分类器对其进行分类处理。这种算法，将图像整体作为研究对象，从实验结果可以明显看出该算法在进行USM锐化相关检测时，识别准确率有较大的提高，是一种行之有效的取证检测方法。"
2019,基于深度学习的声学模型设计与实现,计算机学院,牛正青,李忠伟,ML,0.2667,"随着人工智能技术的不断发展，语音识别作为智能人机交互的关键技术已经成为一个研究热点。目前在语音识别系统声学模型的建模方法中，高斯混合模型有着广泛的应用。但是高斯混合模型在对非线性数据建模时，存在大量的全协方差高斯分布计算，致使其捕获语音特性的能力较差，不能满足实际应用需求。因此需要一个模型能更加有效地挖掘隐藏在长窗宽语音帧中的信息。


此外，语音是由调节动态系统中相对少的参数产生的，隐藏在语音特征下的真正结构的复杂度，比通过应用傅里叶窗等方法直接描述特征的模型要低很多。而在模型中应用梅尔频率倒谱系数进行特征提取，在计算时对特征进行降维处理，会造成一部分的特征信息丢失，降低语音识别效果。为降低特征对识别效果的影响，本文搭建了基于Tandem特征提取的语音识别系统。


本文以语音识别系统的特征选择和声学模型为切入点，主要工作内容如下：


(1)分析研究了基于深度神经网络和卷积神经网络的声学模型，应用Kaldi语音识别开发平台搭建基于深度神经网络和卷积神经网络的声学模型，通过实验的方法分析不同网络深度下的DNN、CNN声学模型的表现。实验结果表明，DNN-HMM模型的词错误率相对于GMM-HMM模型有着5.03%的提升，CNN-HMM模型相对于GMM-HMM模型有6.8%的识别效果提升。


(2)应用深度神经网络模型优化声学特征提取部分，搭建了基于Tandem特征的语音识别系统。深度神经网络用来对声学特征提取获得高维Tandem声学特征，设计并融合BLSTM模型。实验结果表明，基于Tandem特征的语音识别系统能够显著改善系统识别性能。


关键词：语音识别系统、Tandem、深度学习"
2019,中国城乡教育不平等研究 ——基于OED理论分析框架,计算机学院,张曌华,刘晓光,Database,0.241,"在互联网和移动应用中，搜索引擎是人们检索信息的基础工具，并始终面临着性能问题的严峻挑战。搜索引擎的性能优化围绕着存储和计算两个关键命题，两者彼此制约且相互影响。倒排索引作为搜索引擎的核心数据结构，其压缩问题不仅涉及海量数据如何存储，同时会对召回计算效率产生直接影响。国内外的研究人员已经在倒排索引的压缩问题上积累了大量的研究成果。目前压缩倒排索引主要采用整数序列编码方案，通过精心设计的紧凑格式，降低数据元素的存储位宽。

由人类语言书写的文档集合中，文档之间存在着天然的相似性。因此已经有研究者另辟蹊径，提出基于归约冗余的索引压缩方案。这种方案简化索引中重复内容的表述形式，降低索引文件的存储开销。尽管国内外的研究者对这种方案进行了诸多探索，但这些探索主要针对高重复度的特殊文档集，还缺少针对普通网页文档集的研究工作。更重要的是，很多工作没有考虑搜索引擎对召回计算效率的需求，缺少将归约冗余的压缩方案与召回计算有机地结合。

除此之外，学术界的研究人员提出了多种基于Bitmap形式的索引结构。特别是近些年来，工业界已经实践了完全采用Bitmap形式实现的签名文件作为核心数据结构的文档召回模型，从而显著提升搜索引擎的文档召回效率。不过签名文件存在着严重浪费存储空间的突出问题，并且对可能采用的压缩方案提出了苛刻的设计需求。为了解决上述的这些难题，本文提出了基于上下文无关文法的压缩方案。针对倒排索引和签名文件分别设计了相应的压缩算法和文档召回算法，从而为搜索引擎优化问题中关于存储和计算的关键命题提出统一的解决方案。本文的具体工作包括如下三个方面：

第一，提出基于上下文无关文法的倒排索引压缩算法和文档召回算法。本文首先分析网页文档集的倒排索引中普遍存在的数据冗余现象。针对这些冗余，提出了基于上下文无关文法的压缩算法——搜索原始索引中的重复序列，并通过归约替换消除重复序列的存储开销。采用新编号替换重复序列，能够在降低索引存储开销的同时，为加速召回计算提供可行的优化方向。为了进一步提升压缩效果，本文提出了结合整数序列编码的文法压缩方案，并提出了完整的基于文法压缩和整数序列编码的分级索引结构。在Gov2索引和ClueWeb09索引中，编码文法方案比OptPFD编码方案分别减少至多17%和12%的文档编号存储位宽，同时会降低至多20%和8.4%的求交平均响应时间；它比Elias编码方案减少至多8.7%和6.4%的文档编号存储位宽，但会比后者增加至多15%和9.9%的求交平均响应时间。

第二，提出优化文法索引时空性能的具体策略。首先针对网页文档集中文档相似度较低的问题，提出了一种自顶向下的文档重排算法，优化倒排列表中重复序列的分布情况。然后针对搜索引擎关于索引存储开销和召回计算效率的性能需求，提出了两种字典精简方案。两种方案分别基于归约序列对压缩效果和召回计算效率的实际贡献，有针对性地裁剪文法字典内容，提升索引的时空性能。最后通过分析文法压缩加速召回计算的具体原因，提出了适于文法压缩的选择性压缩方案。相比基于分段结构的PackedBinary编码方案，采用相同编码结构的选择性文法压缩方案在Gov2索引和ClueWeb09索引中分别减少至多11%和9.4%的文档编号存储位宽，并降低至多8.1%和14%的求交平均响应时间。

第三，提出有损的基于上下文无关文法的签名文件压缩方案。首先通过具体分析和统计数据，阐述签名文件压缩问题的特殊性，提出解决相关难题的关键命题。然后针对关键命题，提出并验证了采用文法压缩方案的合理性与局限性。针对无损文法压缩方案的缺陷，提出了针对签名文件的有损文法压缩算法和序列求交算法。随后重点阐述有损压缩带来的压缩损失问题，提出并分析采用局部压缩的合理性与可行性。针对不同的局部选择策略，分别提出了选择性归约的文法压缩方案，以及基于文法压缩的混合索引方案。由Gov2文档集构建的签名文件中，选择性归约方案能够在召回错误率增幅不超过6.7%的条件下，降低原始签名矩阵约19%的存储位宽。在取得最优时空权衡结果的条件下，由Gov2文档集构建的混合索引会比归约签名文件降低约62%的存储位宽，但同时会增加约14%的求交平均响应时间。

基于上下文无关文法的压缩方法为搜索引擎核心数据结构的存储和计算问题提供了完整的解决方案，同时也为提升索引结构在时空性能上的表现开拓了不同的优化思路。尽管本文是以搜索引擎的性能优化作为研究工作的应用背景，但所提方案在涉及大规模文档召回的相关场景中也是具有参考价值的。"
2019,汽车领域知识图谱的设计与实现,计算机学院,王英杰,王斌辉,Security,0.2501,"互联网的不断发展，使得数据量也呈现爆炸性的增长。由于这些数据存在组织松散、规模巨大以及结构不一等问题，使得人们在获取有效信息时的困难程度不断增加。在这个大背景下，谷歌为了优化它的搜索引擎，使得人们能够更加方便的获取到所需信息，提出了知识图谱这个概念。并且伴随着人工智能的发展，知识图谱也广泛应用于智能问答、推荐系统、智能家居等领域。


   汽车领域知识图谱主要分为5个功能模块，分别是：数据获取模块，它的作用是从汽车之家、CN-DBpedia以及互动百科来获取数据；数据处理模块，它的作用是利用自然语言处理技术，将获取到的非结构化的文本数据处理出结构化数据的三元组数据；数据存储模块，将处理好的数据保存到知识库中；系统应用模块，依赖构建好的知识库提供上层应用；业务处理模块，用来接收从网页发送过来的请求，并进行相应的处理，返回给用户结果。论文从需求分析，到详细设计再到具体实现都给出了详细的说明，并且也针对本系统提出了未来的展望。


  本系统以构建汽车领域知识图谱为核心，结合三部分数据源：汽车之家、CN-DBpedia以及互动百科，实现了实体识别及实体分类功能、汽车实体查询功能、实体关系查询等功能，达到了预期的目标，具有一定的应用价值。"
2019,结构指导的实时鲁棒高效三维重建系统,计算机学院,李仕杰,程明明,ML,0.2375,"近些年来，随着深度传感器的发展，采用深度传感器的三维重建算法取得

了极大的进步。因为深度传感器能够直接获得稠密的深度图像，因而能够重构

出更为精准的三维结构。所以三维重建算法被广泛的应用于各个领域。

尽管三维重建算法与过去相比有了显著的提升。仍然有许多问题制约了其

在实际生活当中的应用：

1. 位姿估计模块是三维重建算法的重要组成模块。然而由于噪声的存在,

位姿估计的结果存在很大的误差。更为严重的是，在一些极端场景下

（如少纹理场景），位姿估计算法经常会失败，进而导致整个三维重建系

统的失效。

2. 重建的尺度严重制约了三维重建算法在实际场景中的应用，然而现有的

数据管理结构并不是十分的高效。

3. 现实世界中，通常存在许多几何结构，如线结构、平面结构，但是现有

工作并未很好的利用这些几何结构对算法进行提升。

针对这些问题，我们在本文中提出了以下的改进方法，构成了我们结构指

导的实时鲁棒高效三维重建系统：

1. 因为直线结构广泛存在于自然场景当中，且在少纹理场景中具有足够的

辨识度。对于位姿估计模块，我们引入了直线结构，提升了位姿估计的

精度与鲁棒性。

2. 对于存储效率，我们利用数据分布特性设计了一种半顺序结构来提升系

统的存储效率。这种结构既能够只存储有效的数据，又能够减少所需的

索引数据。

3. 由于平面结构不仅仅存在于大多数场景中，而且采用体素对于平面结构

进行存储往往会消耗很多空间。为了提升系统的存储效率，我们将平面

结构引入到我们的模型表达中来。

我们在许多数据集上测试了我们的方法，发现我们的方法均优于传统方法。

这证明了我们设计的有效性。"
2019,分布森林：一种新的异常检测方法的研究与应用,计算机学院,姚程飞,白刚,Security,0.3461,"在实际生产活动中，人们可以采集到大量的数据。某些情况下，人们可能只关注其中某些较少的异常的数据。异常数据可以是异常点、行为、事件或记录等，这些异常数据对于生产活动往往具有特殊的意义。异常检测所研究的内容就是如何挖掘这些数据。异常检测与聚类分析、模型预测以及关联分析一样，是数据挖掘领域中的重要分支。异常检测从数据中找出那些不符合预期或正常行为的模式，被广泛地应用于实际生活中的各个领域，例如机器故障检测、银行信用卡欺诈检测以及网络入侵检测等。


本文在总结归纳异常检测技术发展的基础之上，提出了一种新的异常检测技术——分布森林，并将该方法应用于神经网络安全领域的对抗攻击检测问题上。本文的主要贡献是：


1.提出了一种新的异常检测方法——分布森林。分布森林充分利用了多维属性信息，利用不同属性的组合组成不同的子空间，在该子空间上对正常数据和异常数据进行区分。另外，分布森林采用了局部分布的概念对结点进行划分。


2.在归纳总结对抗攻击和对抗攻击检测技术发展的基础上，指出了使用异常检测技术进行检测的必要性。传统的使用分类器进行对抗攻击检测的方法往往都是使用特定类型的对抗样本构建分类器，只在检测特定类型的对抗样本时比较有效。因而将异常检测技术应用到该领域具有非常重大的意义。


3.提出了一种基于分布森林的对抗攻击检测方法。通过探测可能的对抗样本并拒绝进一步处理来达到保护神经网络模型的目的。实验证明本文所提出的方法可以有效的防御多种不同类型的对抗样本。"
2019,基于深度学习的高光谱图像分类模型框架及应用研究,计算机学院,雒雅楠,白刚,ML,0.2766,"由于数据本身的特性，高光谱图像在遥感、军事和农业等诸多领域存在着广泛地应用。如何将高光谱图像进行分类一直是相关领域的研究热点之一，具有非常重要的理论意义和应用价值。以深度学习方法的兴起作为分水岭，过去需要在领域知识的指导下，使用模式识别和传统机器学习方法来解决，但由于信息利用的局限性，只能做到最高80%的分类准确率。随着深度学习方法的兴起，高光谱图像分类研究也有了新的突破，分类准确率超越了90%。


结合数据的特性以及卷积神经网络的特点，本文分别从数据特征的相关性和数据自身不同波段频谱的相关性出发，提出了两种结构：重拼接层(Reshape layer)和金字塔层次感知层(pyramid perception recognition layer)。基于这两种结构，本文设计了两个高光谱图像分类模型框架——HSI-CNN(高光谱图像卷积神经网络分类模型)和HSI-PPR(高光谱图像金字塔层次感知神经网络分类模型)，并构建了相应深度神经网络模型。模型分别于四个公开数据集（KSC、IP、PU和SA）上进行了性能测试，测试的性能指标包括：损失值（loss）、总体准确率（OA）和单类平均准确率（AA），经过实验验证，各指标比目前公开发表的最好分类结果均有至少1%的提高。


高光谱图像样本标注困难以及深度学习模型需要巨量训练数据样本是影响分类模型性能的重要因素，对此，本文提出使用迁移学习（transfer learning）技术，降低分类模型所需训练样本的复杂度，在不影响分类性能的情况下，降低高光谱图像数据的标注需求及成本。相关实验结果表明：迁移学习的方法对于利用小规模数据实现高光谱图像的分类标注是可行且鲁棒的，准确率至少可达90%。"
2019,基于WiFi与地磁融合的位置指纹室内定位技术研究,计算机学院,冯仕佳,吴英,Security,0.2533,"随着移动互联网技术的发展以及移动终端设备硬件性能的提升，基于位置的服务正逐渐成为移动互联网时代的一颗新星。然而，在室内环境中，信号受建筑物遮挡和多径效应的影响，传统的室外定位技术如GPS、移动基站等常常不能满足室内定位的实际需求。本文即对WiFi和地磁室内定位进行研究。


首先，本文对各种常见室内定位技术进行了介绍。传统匹配定位算法计算量较大且精度不高，具有明显的缺点和局限性。而WiFi分布广、成本低，地磁不受环境条件限制，因此采用WiFi与地磁融合的室内定位方案具有较好的可行性与实用性。对此，本文提出了最强AP有序存储指纹与近邻RSS指纹选取的策略，通过筛选掉大量与待测指纹AP信号差异较大的指纹，从而极大减少了在线匹配的指纹数量，降低了计算量。随后，本文通过WiFi距离选取得到地磁匹配集合，并使用WKNN算法进行精确定位。考虑到地磁在不同维度的差异性，本文以标准欧氏距离进行距离度量，降低了某一维度的数据对距离计算影响过大的可能性。一系列实验表明，本文提出的算法具有较高的精度和时间效率，进一步证实了其有效性。


最后，本文开发出一个室内定位系统模型，将定位算法进行应用。该定位系统包括离线指纹采集与在线定位部分。指纹采集部分获得定位区域采样点数据，并将其上传服务器建立完成指纹数据库。在线定位部分将用户当前位置相关数据采集上传，服务器接收到定位请求后使用本文定位算法进行计算，并将预测结果返回至用户端。"
2019,基于FastDFS的云存储文件管理系统的设计与实现,计算机学院,李冰茹,高铁杠,Security,0.3136,"在互联网飞快发展的大背景下，各种各样的企业对处理和保存数据的需求也在飞速增长，而且模式各异。比如电商类的需要大量的图片存储，视频网站需要的是大量视频文件存储，对性能的要求也都不同。电商类的需求是数量巨大的小文件存储，而视频网站的需求则是大量的大文件存储。这些需求在传统系统中已经无法得到满足。本文研究一款文件管理系统应用于研究所，极大地方便了研究所的日常工作。可以在好友之间无需介质，实时分享文档。也可以在任何地方的任何一个终端登陆管理系统，打开所需要的文档，摆脱了工作地点的限制，使工作效率大幅提高。


基于云存储技术的文件资源保护系统有很广泛的应用场景。现在，许多服务商正在致力于研发云存储技术产品的开发，并且正在推出多种云技术产品。中国一些服务商也开始研究这一技术，旨在加强建设采用云存储技术的整个安全体系。文件若想基于云存储技术构建合理的文件资源保障体系,不仅要了解云技术产品的市场服务情况，还要关注基于云技术资源保障体系的功能需求。


本文基于FastDFS搭建的云存储文件管理系统，具有FastDFS有的一切优势。比如：可以解决数据海量存储问题，特别适合把中小文件作为载体的在线服务。基于FastDFS的现有功能，本文又探索了扩展用户需求的新功能。系统新增了用户注册，用户登录，用户存储文件的功能，使用户可以直观的使用分布式文件系统，并且增添了许多的个性化设置。界面设计简单大方，相关的功能一目了然，具有很好的交互性。综上所述，本系统实现了所有预设的功能，也为用户提供了很好的交互性，达到了预期目的。


本文从整体上按照软件开发的过程。从开发本系统需要使用关键技术到需求分析再到系统设计，最后到系统的实现。详细阐述了该系统的要求，以便于后续设计。系统设计主要描述了系统的详细设计和实现，包括系统结构，系统环境部署和公共接口部署。系统测试主要描述了该系统的测试过程。根据本系统提出确切的测试方法进行测试。最后，根据本系统设计初期的预期，对本系统进行功能和性能的检测并分析相应的测试结果。上述测试验证了系统详细设计的功能和性能。"
2019,汽车口碑方面词的评分分析,计算机学院,曾志贤,王斌辉,ML,0.2531,"随着因特网的快速普及，越来越多的消费者热衷于通过在线口碑的形式，来表达产品的看法，但消费者很难从成百上千万的文本数据中，快速直观的获取到产品有价值的信息。那么利用自然语言处理等相关技术，来对汽车口碑进行分析，为用户提供直观汽车产品信息，就显得非常有用了。


本文的研究目标是，基于汽车行业，利用自然语言处理技术，自动地从消费者发表的在线口碑文本中，分析和得出消费者对汽车各方面的具体反馈。首先通过爬虫技术获取“汽车之家”网站上的口碑、评论等文本数据；然后采用方面词匹配规则，对获取的文本数据进行预处理，为解决数据不平衡问题，将上采样与分层分类的思想结合进行处理；再提取文本的词向量均值特征、人工构建的文本特征以及非文本特征进行融合；最后应用文本分类模型进行训练，构建出方面词评分分析模型，得出汽车预定义方面词的评分。通过对该评分结果的分析，不仅可以作为消费者购买时的参考和依据；还能为汽车制造商提供车辆性能改良方面的建设性意见；同时能为汽车网站的管理者，在为用户制定推荐策略方面提供帮助。


本文把方面词评分分析任务，应用到了汽车领域的中文文本分析中。实验证明，利用本文获取到的数据集，三种特征融合的方式以及处理非平衡数据的方法，能提高文本分类模型的准确率。"
2019,结构保持的图像风格迁移,计算机学院,刘笑畅,程明明,ML,0.2568,"近来，利用神经网络进行风格迁移展现出惊人的视觉效果，从而受到了广泛的关注。现有的基于神经网络的风格迁移方法按照策略大致可以分为两类：训练前馈卷积神经网络和使用迭代优化。尽管策略不同，但相同的是，这些方法中所用的图像表征都由两部分构成：风格表征和内容表征，而且这些图像表征都是从预训练的分类网络中提取得到。在这种图像生成类问题中，合适的图像表征是产生良好视觉效果的关键。然而，由于分类网络原本是用来做目标检测的，所以从中提取的图像特征通常会集中在中心对象上而忽略其他细节。这样以来，风格迁移的结果会呈现某些共性特点，比如：图像原有的结构遭到破坏，局部细节保留不完整，风格纹理倾向于散布在整幅结果图上，等等。


 本文针对现有风格迁移方法不能很好地保留图像结构这一现象，提出用一种通用的策略解决上述问题：设计新的图像表征对原有的风格迁移系统进行修正和完善。并通过两个应用实例对提出的方案加以印证。内容要点包括：


1. 提出了一种基于图像结构表征的风格迁移算法。通过借助从深度预测网络提取得到的全局结构表征和从边缘检测网络提取得到的局部结构表征，该方法可以在进行风格迁移的同时保留图像原有的结构信息和局部细节。在一些对结构变化敏感的图片上（如人脸、建筑），该方法可以避免过度形变和过多杂乱纹理对外观造成的负面影响。用户调研表明，与其他方法相比，本方法的结果普遍更受欢迎。定量分析和比较表明，本方法可以更好地保持图像的结构一致性。


2. 提出了一种基于图像形状轮廓表征的风格迁移算法。通过借助从图像距离场计算得到的形状轮廓特征，该方法可以实现2D/3D表面特定区域的风格迁移。在剪影图片和三维曲面上的实验结果表明，本方法可以消除指定区域外的纹理元素，将风格迁移的作用域控制在指定轮廓内。


3. 本文工作显示了图像表征对风格迁移结果的影响，凸显了其在风格迁移中的重要性，同时也为风格迁移提供了一种新的思路：通过靶向性地设计合适的图像表征使结果呈现特定特点。"
2019,单目鱼眼相机SLAM系统研究与实现,计算机学院,王亚慧,李涛,Security,0.2444,"鱼眼相机是一种折射相机，该相机的镜头由多种折射镜头组合而成。通过该折射镜头，鱼眼相机能够观察到比针孔相机更大范围的视场(Field-of-View)。在SLAM 系统中，以鱼眼相机作为传感器能够增加相邻图像帧之间的重合范围，同时也能够增加周围环境中落在静态物体上的特征点数，因此将鱼眼相机作为传感器相对于传统的窄视角针孔相机能够给SLAM 系统带来更大的优势。然而现在基于特征点的SLAM 系统，比如PTAM 以及ORB-SLAM，大多数只能够支持传统的窄视角针孔相机而不能直接将鱼眼相机等宽视角相机作为传感器。


       在本文中，我们通过实验证明了分块去畸变模型(Cubemap) 在特征匹配中的优秀特性，并建立了一种实时的基于特征点的鱼眼相机实时定位与建图系统(SLAM)。由于传统的SLAM 流程与算法通常只适用于针孔相机模型，因此为了使SLAM 系统能够处理来自宽视场角相机的输入需要解决包括初始化与三角化、适用于宽视场角相机的内点检测标准、以及优化算法等一系列问题。在本文中我们以所提出的Cubemap 模型为基础，为宽视场角SLAM 的各关键算法环节提出了解决方案，并在一个公开数据集以及自行采集真实世界数据集上对所提出系统做出了全面的测试，测试的结果表明我们所提出的系统相对于其他的鱼眼SLAM 方法具有更高的准确性和稳定性。同时我们发现，通过调整相机在车辆上安放的位置以及相机的朝向，鱼眼相机本身低角分辨率的缺陷可以在很大程度上被弥补。具体地说本文有如下创新点：


1. 我们实现了一个基于Cubemap 模型的特征点法鱼眼SLAM 系统，所采

用的分块透视的Cubemap 模型能够去除鱼眼镜头所带来的畸变，因而对

基于特征点的SLAM 系统更加有利。


2. 在初始化和三角化中，我们采用了一种均一化的基于向量的表示方法从

而可以高效地解决Cubemap 中跨面匹配的问题。并提出了一种新的内点

判据，该判据可以用于RANSAC 过程中对外点的剔除。


3. 在优化过程中，我们提出了一种基于Cubemap 模型的损失函数，并通过实验证明了所提出的损失函数优于之前的损失函数。"
2019,基于BP神经网络的空气质量预测平台的设计与实现,计算机学院,贾梦飞,赵宏,ML,0.3138,"改革开放以来，我国经济快速发展，工业化水平不断提高，伴随而来的大气污染问题也愈发严峻。全国范围内的雾霾问题已经持续数年之久但仍然无法得到有效解决，对人们的身体健康造成了极大的影响。作为全国大气污染最严重的京津冀地区，北京、石家庄、保定、唐山经常位于 PM2.5 指数前列。解决大气污染的一个重要前提是对未来的大气污染物浓度进行准确预测，这样便可以为环保部门制定治理计划、确定治理方向提供帮助，从而做到精准治理，提前预防。

       本文的目标是通过实现大气污染物预测模型，为大气污染物浓度预测提供一种科学技术手段和方法。同时，还设计和实现了一个空气质量预测平台，该平台能够借助训练好的预测模型对研究地区的主要大气污染物浓度进行预测，为大气污染精确治理提供帮助。

       首先，本文介绍了我国大气污染的严峻现实，重点针对京津冀地区的情况进行分析，同时对国内外大气污染物预测相关技术的发展历史以及研究现状做一定的介绍。

       其次，本文深入研究了各种神经网络的适用场景，选取 BP 神经网络和 LSTM神经网络作为重点研究对象，利用机器学习常用数据处理方式，结合特征工程，实现了一个可用于实际预测的大气污染物预测模型。该模型可以根据不同地区的实际数据集进行训练，并且可以随时基于新补充的数据集对预测模型进行更新。

       最后，本文对空气质量预测平台的实现过程进行了详细论述，包括平台实现依赖的开发框架 SSM，整个软件系统的设计实现过程，以及平台需求分析、概要设计、系统架构设计、数据库设计和实现等内容。

       本研究通过科学方法，实现对大气污染物浓度的精准预测，使大气治理工作工作可以有目的、有针对性的展开，为大气污染的治理提供了新的思路。"
2019,基于加密技术的医疗大数据管理系统的设计与实现,计算机学院,张悦,高铁杠,Security,0.2636,"随着时代以及科学技术的发展，大数据医疗越来越和我们的生活密切相关，但是，网络科技带来便利的同时，也更加将患者的医疗健康数据置于风险之中，每个都人会产生多条医疗数据，传统的医疗系统已经无法满足大量的数据存储需要以及用户快速查询处理数据的请求。


  该论文针对这一现状，设计并实现了一个可供患者以及医生使用的医疗大数据管理系统。在本系统中，在HBase中建立相应的数据库表，实现数据之间的关联，HBase是基于列的，不同于传统的关系型数据库，因此可以实现更快的查询速度。同时，存储的底层是通过HDFS来实现的，通过动态的添加集群节点的个数，来实现集群的扩容，为海量数据的存储提供了底层保障，又因为本系统是面向医生和病人的，所以需要有很快的响应速度，因此，在本系统的内存和HBase数据库之间存在Redis，通过Redis实现内存和底层存储之间的连接，通过将热点数据存储在Redis中，能够实现更快的查询速度。本系统通过RSA加密算法对数据进行加密，系统的核心功能大体分为以下功能模块，分别是：患者以及医生相关的个人数据查询模块，医疗数据分享功能(患者)，医疗数据下载模块（患者），医疗数据添加功能(医生)，医疗数据访问功能等，当然，还有系统管理功能。


  本系统将加密技术与大数据技术相结合，实现了医疗数据管理系统的智能化、信息化、安全化与可靠化。综上所述，本系统实现了所有预设的功能，为用户提供了良好的交互性，达到了预期目的。"
2019,用于边缘检测的混合卷积特征学习方法,计算机学院,胡晓伟,王恺,Security,0.2501,"随着互联网的广泛应用与发展，多媒体技术与应用同人们的生活联系越来越紧密，涉及到日常的影音娱乐、图像设计与编辑、工业异常检测等众多域。因此，在技术上对于图像处理与理解的需求也变得越来越强烈。本文的主要研究内容是图像处理中边缘检测的发展历程和方法改进。边缘检测是指从自然图像中检测边缘属性，并得到二值的边缘图的过程。它是一种像素级图像分类任务。常见的像素级图像分类任务包括：图像语义分割、边缘检测、显著性物体检测等。本文回顾了边缘检测的发展历程，并通过将边缘检测与其他像素级图像分类任务比较，例如图像语义分割和显著性物体检测，发现了一个任务的全卷积神经网络方法往往可以直接应用到其他任务上的现象，并总结了边缘检测与其他像素级图像分类任务的三个显著区别。作为边缘检测任务的经典方法，整体嵌套网络在融合多层次图像信息时使用了一种以边缘概率图的方式，然而这种融合方式有明显的不足，即这种方法不能提取不同层次图像信息的交互信息，产生的边缘概率图偏模糊。本文针对这种不足提出了新型的混合卷积神经网络方法，提出的方法具有如下两点创新：

1. 我们的方法采用一种以卷积特征图融合的方式融合多层次图像信息，弥补了整体嵌套网络以边缘概率图融合方式的不足，并在几个广泛使用的数据集上达到了最先进的效果，其中包括BSDS500、NYUD和MultiCue等数据集；

2. 我们结合VGGNet和ResNet的优势，提出了一种称作HybridNet的基础卷积神经网络，使用HybridNet增强的混合卷积特征方法可以进一步提升边缘检测的性能。"
2019,点云分割算法在转子表面缺陷检测中的应用研究,计算机学院,李慧超,邵秀丽,ML,0.269,"叶轮转子表面质量缺陷会加大风机工作噪声，直接影响风机的使用寿命，为工业生产带来安全隐患。为此，本文以在线标记转子表面凹凸型缺陷为目标，开展了基于机器视觉的叶轮转子质量检测工作，通过对转子表面凹凸型缺陷进行检测与可视化显示工作，为车间质检人员提供辅助支持。

针对二维机器视觉检测方法无法有效捕获凹凸型缺陷三维深度信息的问题和基于配准的传统三维机器视觉检测方法计算复杂度大、时效性差的问题，提出了基于点云分割的转子表面缺陷检测方法，论文开展了如下三个部分的工作：

（1）转子表面三维缺陷数据获取与预处理。为了能够稳定地获取转子三维缺陷数据并降低生产成本，本文基于三角测量原理搭建了转子表面三维扫描平台，通过结合激光器和光学相机来获取转子表面调制后的激光线图像。然后，使用Steger中心线提取算法提取激光线，对采集到的激光数据进行插值处理得到稠密的具有纹理信息的转子表面点云。最后，采用孤立森林方法对转子点云进行噪声过滤，旨在解决数据采集过程中出现的边缘低密度噪声问题。

（2）基于无监督学习方法的缺陷区域的粗划分。在无监督点云分割阶段，本文以降噪后的转子点云作为输入数据，应用三维形态学滤波算法实现缺陷检测。针对经典形态学滤波方法无法检测出转子表面凹型缺陷的问题，本文通过交替应用开闭运算的检测流程，使得改进后的形态学滤波算法能够同时检测出转子表面的凸形与凹形缺陷。另外，本文提出了基于表面曲率的滤波窗口递增策略，弥补固定大小的滤波窗口缺陷划分准确度较低的不足。

（3）基于深度点云分割网络的缺陷细划分。由于计算过程中的信息损失，经无监督学习方法给出的缺陷标签并不总是真值。因此，本文提出了弱监督框架下的缺陷检测模型，旨在利用深度学习模型的特征表示能力来提高模型泛化能力与检测精度。该模型整体基于动态图卷积神经网络中边缘卷积模块（EdgeConv）无法捕获相邻点间向量方向的不足，提出了融合余弦距离的近邻图构建方法。并在特征编码阶段引入注意力机制来提取点云特征，实验结果表明改进后网络模型的平均预测准确率相对于无监督学习方法有所提升。"
2019,联合动力下的双馈异步风力发电系统控制策略研究,计算机学院,张彪,苑晶,Security,0.2181,"能源是人类生存发展必不可少的资源，传统的化石能源对于环境造成严重的污染，而且具有不可再生的特性，越来越成为我国可持续发展战略需要亟待解决的难题，如何开发和利用新能源成为我国未来能源开发的一个重要的方向。我国国土面积广阔，地势西高东低为风力资源的存储和开发提供了便利条件，而且风力资源清洁可再生等特性成为我国能源开发发展的一个新的方向。


      本文首先对当前国内外对于风力发电系统控制策略研究进行了较为全面的综述，结合风力发电控制策略研究背景阐述了通过运用仿真软件搭建风力发电系统仿真模型对风力发电控制优化具有十分重要的意义，在此基础上选取了双馈异步发电机组的代表国电UP86-1500风电机组，重点介绍了机组的整体结构、基本原理及运行特点，并在此基础上推导并构建该机型的数学模型，并对相关函数的约束条件进行探讨。结合基于定子电压定向的矢量控制策略，重点对该系统中的转子侧变流器和网侧变流器对应的控制结构进行设计，结合低电压穿越变流器动作原理，对网侧变流器原有的PI控制结构进行设计优化，采用基于自抗扰控制的网侧变流器内模控制策略，然后以此为原理通过MATLAB构建了基于风力发电机模型及整个系统仿真模型，分别对电网电压跌落、最大功率、超速保护验证等对采用该控制策略的有效性进行了仿真分析验证，达到预期效果。"
2019,基于两端模式的密态数据库系统研究与实现,计算机学院,张悦,刘哲理,Security,0.3167,"伴随信息技术的快速发展和互联网时代的到来，每日产生的数据量呈爆发式增长，而数据库系统作为大数据和云计算时代柣基础的数据支撑，其安全性也成为未来云计算时代至关重要并且亟待解决的热门研究问题。要彻底保护数据的安全，就要对数据完全加密。现有的密态数据库研究面临许多挑战，比如分组密码通常会扩充数据，使数据发生很大的改变，这要数据库在底层物理存储和数据模型方面进行相应支持，代价很高；此外，由于加密破坏了明文的操作特性，使得原来可以在明文上进行的查询、聚集、统计操作等无法进行。本文结合现有的研究成果和方案，在客户端-服务器端的基础上引入安全加密设备端，提出仺套基于两端模式的密态数据库系统。主要研究成果如下：


（1）密态数据库系统架构。本文在不改变数据库客户端和服务器结构和能的基础上，引入安全加密设备端，提出基于两端模式的数据库系统架构。安全加密设备作为整个系统的核心部分，承担明文数据加密和密文结果解密的工作；客户端无韠更改，用户可以继续像使用明文数据库亓样使用密态数据库系统；对于服务器而 ，执行的操作和逻辑也与明文数据库相同，只是操作的对象均为加密过的数据。


（2）密文查询方案优化。本文针对关系型数据库，融合月新的密码学技术，提出了针对相等查询优化的方案 RE-DET和针对范围查询优化的方案ONE-OPE。 RE-DET方案解决了重复字段的暴露明文信息的安全问题，达到了选择明文攻击下的不可区分性的安全级别； ONE-OPE方案将客户端与服务器的交互次数从 log(n) 降到丰次，降低了通信量和时间忴间。


（3）系统原型实现。从系统层面实现了基于两端模式的密态数据库系统原型，主要内容包括：综合考虑结构化查询语句的特点，实现密文操作转换解释器；通过新增密码运算 API和用户自定义函数来提供对密态数据检索和密态计算的操作支持；对于各类 SQL语句的支持以及复杂查询语句实现方法等。"
2019,基于 C-RAN 的边缘缓存在线资源调度研究,计算机学院,谢沁益,徐敬东,Security,0.2726,"随着移动设备的普及和内容提供商数量的增多，移动多媒体服务数量急剧增加。为了促进移动多媒体数据流量的持续增长，提高移动应用的服务质量，移动网络运营商需要推动诸如新颖的网络架构和先进的内容交付技术的研发，以此完成未来蜂窝网络运营的创新。在这一背景下， C-RAN（Cloud Radio Access Network）和边缘缓存受到工业界和学术界的高度重视。 C-RAN 旨在将无线接入网和云计算结合以减小无线接入网络的压力，边缘缓存旨在对多媒体数据流量进行缓存，从而大幅度提高内容传递效率，并减轻回程传输网络的压力。虽然 C-RAN 和边缘缓存能支持大量的移动多媒体服务，但是现有 C-RAN 和边缘缓存无法满足人们对服务质量的要求，促进移动多媒体数据流量的持续增长。为了最大限度地发挥 C-RAN和边缘缓存的潜力，满足持续增长的移动多媒体服务对资源的需求，保证运营商和用户对于服务质量的需求，本文开展了基于C-RAN 的边缘缓存网络架构的服务设计及关键问题研究，主要内容如下：

第一，形式化基于 C-RAN 的边缘缓存框架的内容下载服务为资源分配、内容放置和请求路由联合问题。该问题是最小化整个时间段中系统整体代价的优化问题，并且该问题的解能满足用户随时间变化的请求和多种系统约束。

第二，为了提供在线的、最佳的、轻量级的、具有可扩展性的、能适应系统动态性的时间相关调度，本文通过将正则化方法、舍入算法和分解法相结合的方式，提出在线最优调度策略解决资源配置、内容放置和请求路由联合优化问题，使得从长远角度上看调度策略的总体代价最低。在在线最优调度策略的基础上，本文提出基于预测的改进调度策略，该调度策略利用未来信息优化当前的调度策略得到更趋近于最优调度的调度。

第三，由于现有的单个数据集不能很好地描述本文的场景，因此我们基于场景特征将两个数据集的数据进行合并以形成仿真场景。然后我们使用大量基于数据的仿真实验来评估本文提出的在线算法和基于预测的改进算法的性能。实验结果表明本文提出的调度策略具有优越的性能，并且计算开销是轻量级的。"
2019,面向智慧建筑的命名数据传输协议研究,计算机学院,姚思研,徐敬东,Security,0.2649,"随着物联网技术的快速发展，智慧建筑中各种新兴应用层出不穷。这些应用一方面给用户带来方便舒适的建筑环境，另一方面却也给智慧建筑的管理带来更大的挑战，尤其是随着应用的增多，应用传输的数据量也随之变多，如何在智慧建筑中实现高效、可靠的数据传输成了一大难题。


目前，智慧建筑的管理系统有很多，传统方案包括BCAnet/LonWorks和基于TCP/IP的建筑管理系统。前者主要用在建筑内部，不适合与物联网、云计算等技术结合的智慧建筑；后者虽然能够与因特网联通，但是由于智慧建筑中智能设备等终端繁多，很难实现有效的IP地址配置和管理。将命名数据网络(Named Data Networking,NDN)与智慧建筑结合则能够有效避免上述问题，与TCP/IP不同的是，NDN以内容为中心，根据内容的名字转发数据，它还具有网内缓存、多源传输等特点，很适合应用和设备数目庞大的智慧建筑场景。然而，已有的面向智慧建筑的命名数据传输协议多采用单一的层次化名字路由，亦或者广播的方式路由，这两种方案都不能充分满足所有应用的传输需求。此外，在传输数据的同时，如何采用合理的缓存方案提高传输效率也是难点之一。因此，本文主要针对智慧建筑中命名数据传输问题展开研究，主要内容如下：


第一，本文系统地归纳了传统的智慧建筑中数据传输方法，详细地分析了这些方法的优缺点和适用范围。还总结了常见的面向智慧建筑的命名数据传输方法，讨论这些方法的局限性，为本文提出的解决方法奠定了基础。


第二，本文分析了智慧建筑中不同应用的传输类型，并提出了一种能够兼容智慧建筑中不同应用的命名数据传输协议。该协议充分考虑了不同应用的传输需求，设计了物理空间和应用空间两个名字空间，并在此基础上实现两阶段的转发策略，本文还设计了一个适合智慧建筑中数据传输的缓存方案，该方案能够有效地增大缓存空间利用率，提高传输效率。


第三，基于以上方案，本文使用NDN模拟器NDNSIM模拟智慧建筑中数据传输场景，实验表明，本文的传输协议能够保证高度的成功率的同时保证应用的可扩展性，降低网络资源的消耗。"
2019,一种集群环境下的多作业调度算法,计算机学院,李德天,杨愚鲁,Security,0.2508,"计算机集群是一种典型的分布式系统且已被广泛应用。由若干独立任务组 成的作业通常会被随机地提交到集群，当作业内任务数量不同且任务计算量各 异、作业以不同时间间隔到达集群时，多作业调度方案的优化对提高集群性能 有较大意义。通过对现有独立任务调度算法的研究发现，现有算法在调度作业 时主要考虑了当前已到达作业，而未考虑后续可能到达的作业。由于不同规模 的作业可能对节点有不同的需求，若不考虑后续到达的作业，则可能出现已到 达作业过早占用计算能力强的节点，后续到达作业中计算量更大的任务无法获 得计算能力更强节点的情况，导致节点负载不均衡、全部作业的执行时间延长 等问题。因此，本文针对集群环境下连续到达的多作业调度问题进行了研究，提 出一个基于任务计算量分布的多作业调度算法。该算法假设已知可能到达集群 的全部作业内任务的计算量分布规律，通过计算当前待调度作业内各任务的计 算量水平，为各任务分配与其计算量相匹配的节点，并为后续到达作业预留可 能被需要的节点，达到优化系统运行时间和提高系统使用率的目的。 


为验证本文所提算法的性能，设计了模拟实验。在相同条件下，除了与现有 的 OLB 独立任务调度算法和 MCT 独立任务调度算法进行对比外，还与 Hadoop 平台上的 FIFO 先入先出调度算法、Fair 公平调度算法和中值调度算法进行了对 比。实验结果显示，本文所提算法在系统运行时间上不如 MCT 算法，但在特定 条件下，拥有更好的系统使用率。相比 OLB 算法、FIFO 算法、Fair 算法和中值 调度算法，在特定条件下，本文所提算法无论是在系统运行时间还是在系统使 用率上均取得了一定的性能提升，在其它条件下，与这些算法性能相当。"
2019,基于聚类的数据匿名化算法研究,计算机学院,郑万通,贾春福,Security,0.3163,"在互联网技术日益发展的今天，数据的规模呈现指数级增长的态势，而数据中的个人隐私等敏感信息正面临着前所未有的安全威胁。很多企业和组织机构定期地将生产环境中敏感的或者有访问权限限制的数据复制到非生产环境中时，无意间就会造成数据泄露。因此非生产环境中的数据逐渐成为网络犯罪分子攻击的目标，进而造成数据丢失或被窃。然而非生产环境中的数据泄露如同生产环境中的数据泄露一样，可能导致巨大的经济损失，并且对企业的声誉和品牌造成不可挽回的损害。


       在数据共享的热潮下，越来越多的数据集产生和发布，机构甚至普通个人都可以访问大量的数据资源，而海量的数据中包含很多个人隐私数据，用户不希望自己的隐私被泄露出去。匿名化可以最大化数据的可挖掘价值，并可以实现隐私保护，但目前学界提出的算法在将数据匿名化之后数据可用性较差。本文在研究了数据发布场景下已有的的匿名算法的基础上，提出了两个数据可用性更高的匿名算法。




       本文分析并总结了现有的数据脱敏隐私保护技术和匿名化技术，并在此基础上，提出了一种改进的基于聚类的K-匿名算法，把匿名化问题转化为机器学习中的聚类问题，设计了适用于发布数据的聚类算法，算法能够较好地对数值属性和离散属性进行处理，并且改进的聚类算法在质心选择过程考虑了数据的全局分布特征，使得聚类效果更好。在Adult标准数据集上进行了准标识符属性组匿名化和数据脱敏实验验证，实验数据表明，本文所提算法的匿名化效果与传统算法相比具有较低的信息损失，数据质量更高，并且本文所提算法在保护隐私的安全性与保留数据可用性上达到了较好的平衡。


       其次，因为K-匿名方案对用户隐私数据保护的安全程度较低，分布在同一等价类中的敏感属性可能较为相似或者相同，攻击者破解K-匿名之后的发布数据集然后获取用户敏感属性取值的概率较高。因此本文还提出了一种满足L-多样性安全要求的匿名算法，在通过聚类的方式实现K-匿名时，同时考虑簇中记录中敏感是否达到L-多样性，选择以增加最小的信息损失作为标准来就近归并记录点，直到簇满足L-多样性，实现簇的归位。本算法在K-匿名安全性的基础上进一步实现了敏感属性L-多样性的隐私保护，并且改进的聚类算法将K-匿名和L-多样性的实现过程进行了同步，降低了时间复杂度，通过聚类过程的优化，使得生成的等价类更加紧凑，数据损失进一步降低，与已有的L-多样性算法进行实验对比分析,验证了算法的有效性。"
2019,基于机器学习的网络流量分类和新应用流量探测方法研究,计算机学院,付宁佳,张建忠,ML,0.3112,"随着互联网的高速发展，各种网络应用层出不穷，网民数也在不断增长。巨 大的流量给网络管理、网络空间安全、网络服务质量等带来了很大的压力。网 络流量分类是解决这些问题的重要技术之一。目前的流量分类研究大多是将未 知流量分类到几个预先定义的应用类别之中，而没有考虑不断涌现的新型应用。 事实上，在现阶段的互联网流量中有 60% 来于自新型应用。这些新型应用流量 会被分到已知类别中，从而导致较高的分类错误率。因此，对新型应用流量的 探测也是需要解决的关键问题。         


       目前，在流量分类领域存在着诸多研究难点。首先，互联网应用种类繁多， 除了大量已知应用外，还有层出不穷的新应用，这对分类模型是一个极大的挑 战。其次，基于机器学习的分类技术受特征影响较大，而特征种类繁多，导致特 征选择困难。最后，训练模型需要庞大的数据作支撑，而当前仍缺乏公开的数据 集，导致数据获取困难。针对这些问题，本文采用机器学习技术，从优化特征选 择和改进分类算法两个角度来提高分类模型的性能。本文的主要研究内容如下：


       第一，本文归纳了常用的流量分类方法，分析了各个方法的适用场景和优缺点。本文还介绍了近年来“新应用流量探测”领域的相关研究成果，对其基 本思想和限制条件进行了详细论述。这些都为本文提出的“N+1”分类模型奠定 了理论基础。


       第二，为了减少时间消耗和提高分类准确度，本文提出了一种特征选择算法。该算法以改进的 K_means 算法为基础，将特征候选集从低维向高维迭代， 最终为每个应用选取一个最优特征子集。


       第三，考虑到不断涌现的新型应用对分类结果的影响，本文提出了基于集成框架的“N+1”分类模型。该模型由多个基分类器和一个决策器集成得到。这 种集成的框架可以将未知流量分类到 N 个预先定义的类别和一个“未知”类别， 既能准确识别 N 个已知应用流量，还可以探测新应用流量。此外，本文还进行 了大量对比实验来验证“N+1”分类模型的性能。"
2019,基于 RFGRU 的移动应用流量分类方法研究,计算机学院,金宇菲,张玉,Security,0.3204,"目前，数十亿用户通过移动设备连接到互联网以获取服务，移动网络流量

占据了网络流量的很大比重，因此移动网络的安全性成为了亟待解决的问题。流量分类作为网络安全和网络管理方面的基石，越来越多的学者对其进行研究，已然成为了时下热点。

移动网络流量大部分由移动应用产生，而大多移动应用在 HTTP 协议的基础上进行开发。由于 HTTP 协议自身的特点，在目前多数研究中，针对移动应用流量的分类方法的核心思想是提取负载中可以用于分类的特征字符串，以此为依据进行分类。这种方法的难点在于特征的提取往往十分繁琐而复杂，有时还需要人工参与。因此，本文希望提出一种简化特征提取过程的移动应用流量分类方法，其主要研究内容如下：

第一，本文对中国互联网发展状况进行了概述，对网络流量分类的四种方

法进行了详细的介绍，以此引出移动应用分类的问题。对当前已有的移动应用分类方法进行了调查和研究，阐述了其主要思想，并说明了其中的不足之处，为提出本文的方法打下了基础。

第二，通过剖析移动应用流量的结构特点，分析 HTTP 协议以及 HTTP 请求响应数据包的结构，本文提出了基于 RFGRU 的分类算法。该算法将随机森林与深度学习模型 GRU 相结合，摒弃利用特征关键词进行分类的传统方法，将序列预测模型 GRU 用于流量分类中。 GRU 作为端到端的分类器，可以自动提取输入样本的结构特征，发现负载中单词之间的隐含关系。同时利用随机森林分类器分类迅速的特点，将特征明显、易于分类的样本过滤，仅仅将难以分类的样本交予 GRU 模型，在大大加快分类速度的同时保证了良好的移动应用识别率。

第三，本文对提出的算法进行了实现，通过进行多组实验，对算法中涉及

到的几项重要参数的数值进行了设定。另外，为了验证本文方法的有效性，与其他三种已有的移动应用流量分类算法进行了对比。实验结果表明，本文的方法具有优良的识别效果。"
2019,基于改进 FCM 算法的 VoIP 垃圾电话制造者识别研究,计算机学院,武欢,张玉,Security,0.3399,"随着 VoIP（网络电话）系统的普及， VoIP 垃圾电话呈现了大幅度增长，已成为网络安全领域不可忽视的重要问题。

  近年来多种垃圾电话防治方法被提出，这些方法大多基于呼叫行为分析。然而，现有的方法存在以下不足：首先，多数方法在模拟数据上进行实验分析和评价，但模拟数据和真实世界的数据可能不同；其次，多数方法未考虑到不同特征的贡献度不同，所以在识别过程中未涉及特征权重分配问题；最后，通过对真实数据分析，本文发现垃圾电话用户所占比例较小，而多数方法并未考虑类别不平衡问题。因此，本文针对 VoIP 垃圾电话制造者识别中存在的上述问题展开研究，主要内容如下：

  第一，本文系统地归纳了传统的垃圾电话识别方法，详细分析了这些方法的适用场景和存在问题。本文还对近年来的 VoIP 垃圾电话识别方法进行了研究，对这些方法的基本思想和面临的限制进行了详细论述，为本文提出的解决方法奠定了基础。

  第二，本文分析了一个真实的 VoIP 呼叫数据集，并提出了一种基于网络爬取的垃圾电话制造者标签获取方法，为进行相关实验分析和评价奠定了基础。本文详细分析了数据集中的 SIP 信令信息，提取了四个呼叫行为特征。通过分析特征空间的分布，提出了一种新的特征预处理方法，该方法可有效地识别垃圾电话制造者。

  第三，针对近年来相关工作中存在的问题，本文提出了一种基于权重的FCM（Fuzzy-C-Means）聚类算法（Weighted-FCM， W-FCM），该算法可在聚类过程中为不同的特征分配不同的权重。此外，为解决垃圾电话识别中的类别不平衡问题，本文提出了基于决策阈值调整的 FCM 聚类算法（Threshold-FCM，T-FCM）。实验结果表明，本文提出的改进算法能有效的提升垃圾电话制造者的识别效果"
2019,融合子图特征的动态社交网络链路预测研究,计算机学院,李艺,温延龙,Security,0.2759,"随着互联网行业不断发展，社交网络逐步渗透到人们工作、生活、社交等各个方面。社交网络通常以图数据形式表述，对其分析、挖掘可以获得巨大经济价值和社会价值。链路预测是社交网络研究的重要任务之一，通过分析、挖掘网络特征信息预测网络未来发展演化，可广泛应用于学术合作者推荐、通讯状态跟踪、在线商品推荐等领域。因此，社交网络链路预测研究具有重要理论意义与实际应用价值。


社交网络随时间推移结构不断发生变化，每个时刻可以表示为一个静态社交网络图。现有链路预测研究分为静态社交网络链路预测与动态社交网络链路预测两类。静态社交网络预测方法基于某一时刻的网络状态预测网络未来状态，难以捕获网络时态特征，预测效果有待提升。动态网络链路预测方法借助随时间变化的社交网络拓扑特征获取更好预测效果。现有动态社交网络链路预测研究大多专注于局部拓扑特征的近似提取，忽略整个网络全局特征以及时态转换因素，所提取的近似拓扑特征难以准确代表网络真实状态。


本文充分利用动态社交网络历史演进规律，研究融合子图特征的动态社交网络链路预测方法。具体工作包括：第一，提取时态网络中节点对的完整局部拓扑特征，同时融合全局集群信息及时间转换因素，以捕获随时间变化的全局结构和时态转换对每个子图特征的影响；第二，使用无监督学习策略对按时序拼接的融合子图特征进行特征重构，以获得更好的时态特征表示；第三，将链路预测问题转化为有监督分类问题，采用分类算法进行动态社交网络链路预测；最后，在多个动态社交网络数据集上开展链接预测实验，结果表明本文提出的方法效果上优于使用其他特征的现有方法，并具有较高效率。"
2019,一种基于快速剪枝的高效频繁项集挖掘算法,计算机学院,杜贺,杨愚鲁,ML,0.2707,随着信息技术的飞速发展，由人类活动产生的数据爆炸式增长。传统的数 据库技术难以在大规模数据中快速地发现数据背后的深刻的知识和规律，数据 挖掘技术应运而生。频繁模式挖掘就是数据挖掘领域的一个重要分支。由用户 指定参数，挖掘算法就可以根据用户需求计算出数据集中的频繁模式，指导用 户进行生产和经营。频繁模式挖掘的一种典型应用是挖掘超市交易记录中的频 繁模式，进而将频繁模式中的商品进行捆绑销售或推荐给消费者，以方便消费 者购物并提高超市营业额，创造更高商业利益。频繁项集挖掘是频繁模式挖掘 中最关键的步骤之一，也是计算量最大的步骤，因此大量的研究工作在频繁项 集挖掘问题上展开。 传统的频繁项集挖掘算法往往面临多次扫描数据库、无法快速剪枝导致的 维护大量候选项集、维护庞大的频繁模式树和大集合求交集效率低等问题，这 些缺点导致传统的频繁项集挖掘算法在大规模数据集和稠密数据集上计算效率 低下。 本文提出了一种基于快速剪枝的高效频繁项集挖掘算法。算法首先对数据 集进行预处理得到垂直格式数据集，然后在垂直格式数据集上进行广度优先的 频繁项集挖掘。通过大规模快速剪枝，大大减少了候选项集的数量，缩小了算 法的搜索空间，从而减少了生成频繁项集的计算量。实验结果表明，本文算法 的计算量整体低于典型算法，尤其是稠密数据集上的搜索空间远远小于对比算 法。实验中本文算法的最好结果是在 mushroom 数据集上，当最小支持度阈值为 0.15 时，本文算法的计算时间仅为传统垂直数据挖掘算法的计算时间的 0.2%。 综上，本文算法是一种垂直数据格式上的基于快速剪枝的高效频繁项集挖 掘算法。算法能在保证计算结果正确性的基础上，大大提高计算效率。
2019,基于多元信息的行人重识别方法,计算机学院,刘腾,王恺,Security,0.2633,"随着人们对社会治安的渴望，我国大部分的公共区域为了防范犯罪者以及为公安机关侦破相关案件提供线索都安装了监控系统。虽然相关部门可以利用监控视频来搜索犯罪分子，但是随着视频数量的增多人工筛选的成本就越来越高。尤其在一些紧急情况时，人工筛选视频需要耗费大量的劳动力以及时间，从而会耽误抓捕犯罪分子的时机，因此利用一定的手段来减少筛选成本尤其是对行人的筛选就尤为重要了。


在不同摄像机拍摄行人的情况下，通过一定的算法来分析拍摄的行人是否为同一个身份，这种图像检索问题叫做行人重识别。由于需要在不同摄像头拍摄的视频中寻找相同的人，这样就造成了几方面挑战，一是摄像头布置位置的不同导致同一个人的图像之间由于人的姿势、角度、亮度、距离、服饰的不同相似性很低；二是在不同的人穿着相同类型和颜色的衣服、拥有着相同的装饰和形体相似的情况下都很有可能被识别成同一人。 行人重识由于具有上述几方面的挑战，所以它是一个极具有难度的课题。


行人重识别的实现一般分为两个步骤，第一步主要是利用常规的图像物体检测方法从图像中检测出行人；第二步通过度量目标行人和检测出来的行人之间的距离来完成行人重识别。常规的行人检索方法仅从单一的行人外观信息特征出发而没有综合行人的各种信息，因而不具有很高的泛化能力和检索的稳定性。例如基于整体行人外观信息的行人匹配方法是基于现有的数据集鲜有不同衣装情景下来进行的行人匹配，因此该匹配方法在不同衣装情景下的匹配效果比较差。


针对上述问题，本论文拟从模型融合的角度来提高行人检索稳定性与精确度。利用深度学习的模型完成人脸检测、行人检测、人脸特征提取和行人特征提取，同时将提取的人脸特征和行人特征进行整合以及匹配，综合匹配结果来提高行人检索的稳定性和泛化能力。为此本论文研究的重点主要表现在以下几个方面：


1、不同相机存在着色差问题，这种问题意味着相同条件下不同的相机可以将相同的颜色转换成不同的RGB值。所以本论文利用GAN网络对目前存在的行人重识别数据集进行了扩充来解决相机色差问题。


2、为了能够获得人脸的准确特征信息，本论文使用了（高）低分辨率的人脸图像来进行模型的训练。


3、为了进一步提高行人重识别的性能，本论文利用了最为稳定的人脸信息来作为辅助。"
2019,一种用于幂律图数据的改进AYZ算法,计算机学院,苑云鹏,杨愚鲁,Security,0.2674,"随着互联网的飞速发展，图数据的分析和处理在许多领域得到了广泛的应用。在这些现实的网络问题中，如：发现密集子图、垃圾邮件检测、计算图的聚集系数、揭示网络中隐藏的主题关系等，均需要使用图数据处理方法，而这些图数据处理的基础环节之一均包括三角形计数。三角形计数问题是大规模图数据分析研究中一个经典而又基础的问题，近年来受到了众多学者的关注。由于大数据时代来临，图数据规模在不断增加，因此三角形计数的计算规模也越来越大，这对三角形计数算法的效率提出了更高的要求，因此三角形计数算法的效率问题开始变得备受关注。


      本文主要研究精确三角形计数问题。针对现实中普遍存在的一类图数据——幂律分布 图，本文提出了一种基于向量乘的改进AYZ算法。首先，本文对于AYZ算法中基于矩阵乘的高维节点处理方法进行了改进，针对邻接矩阵为对称阵的特点进行压缩存储，再用矩阵中有效值的向量乘法代替矩阵乘三角形计数算法，从而减少了矩阵乘算法中的无效值的计算，提高算法效率；其次，本文引入了设定任意序列思想，在Edge-iterator算法的基础上，对边迭代三角形计数算法进行了改进，使用向量乘的方式替代了求交集运算，进一步改善了算法效率；此外，本文利用任意序列思想解决了基于向量乘的三角形计数算法中重复计数的问题。由于AYZ算法的划分阈值与高维节点的处理时间复杂度有关，故而调整改进后AYZ算法的阈值。利用幂律图数据的度分布规律，加以推导，得到理论最优的划分阈值，实验证明，该阈值可以使本文改进的三角形计数算法划分效率提升，调整阈值后的算法的效率大大增加。


      最后，通过实验验证了本文改进算法的正确性并分析评价了其效率：1）验证了高维节点基于向量乘的算法改进效果；2）对阈值 ，验证了调整后的阈值，可以使改进后AYZ算法的性能优势充分发挥，提高算法效率；3）对AYZ算法、改进AYZ算法以及Edge-iterator算法的性能进行了实验与分析，验证了本文改进算法有效提升了三角形计数算法的实际效率。"
2019,中型团餐企业项目管理系统的设计与实现,计算机学院,王国浩,李岳,Security,0.2805,"近年来，互联网行业的高速发展对传统行业的影响逐渐加深。团餐是传统餐饮行业中的新兴领域。中型团餐企业有切实的标准化和规范化的需要，但缺乏足够的信息化建设能力与资金投入。


本文根据这一现状，设计并实现了一个针对中型团餐企业的项目管理系统，该系统基于java语言，使用基于node.js的express框架设计管理网站，通过mongodb数据库进行系统数据库的管理。系统的主要模块有：人员管理模块、订单管理模块、日志管理模块、审批管理模块。本文按照需求分析、系统设计、系统实现和测试的步骤对功能模块进行了详细说明，并在最后对开发工作进行总结并对未来的开发目标进行展望。


本系统为中型团餐企业提供了项目管理的平台，以较低的开发成本实现了项目管理的标准化、规范化、可视化，完成预期目的。"
2019,基于.Net的小型超市管理系统的设计与实现,计算机学院,薛峰,王靖,Security,0.292,"随着社会经济及人民生活水平的不断提高，小型超市也得到了快速的发展，超市数量越来越多，商品的数量越来越多，商品的相关信息也在不断增多。这就对小型超市的管理提出了相应的要求，在如今的小型超市管理工作中，采用传统的人工手段来对数据进行处理的方法已经逐渐显现出了其不足，采用原有的管理方法对大量快消品的超市进行管理，已经很难满足现有小型超市的发展需要，但是对于小型超市来说，如果采用大型超市的管理信息系统进行管理，则会显得太繁琐，而且复杂的系统也为会操作人员的工作带来相应的压力，在降低超市工作效率的同时，也会在一定程度上增大超市的运营成本。


小型超市管理系统是目前最流行的市场，超市等行业常用的系统软件，它主要包括以下模块：购买和退货管理功能模块，销售管理功能模块，库存管理功能模块，人事管理功能模块，系统管理功能模块。本研究设计的超市管理信息系统除了可以对超市的采购、销售及库存等进行管理，还可以对超市的会员进行实时动态地管理。本问所设计的小型超市管理系统是使用基于PC端的，使用SQL server 2012数据进行设计的，与同类产品相比较，更易于维护。系统采用了结构化的方法，根据调查系统，系统分析，系统设计，系统实施，实现结构化程序设计方法。在设计中，信息系统针对特定的业务系统设计的总体方案，采用“自上而下”的方法，并采用“自下而上”的设计方法。尤其是对各类数据操作简单，从而针对小型超市科学建立完善的管理系统，实现对小型超市日常运经营活动全面、动态、及时的管理。"
2019,某科研机构信息管理系统的设计与实现,计算机学院,付缺,张建忠,Security,0.2959,"随着国家对科研的重视程度与日俱增，科研机构不断发展壮大，科研人员不断增加。然而目前大部分科研机构信息化程度相对较低，缺乏一个对外宣传和对内管理的平台。


本文以某一科研机构的实际需求出发，旨在构建一个在线信息管理系统对某科研机构的各项信息进行动态管理，增加高校间以及校企间的互通与合作。针对目前该科研机构的现状，通过对该科研机构的详细分析，明确了功能需求分析，然后结合目前的主流开发框架包括前端bootstrap框架、后端SSM框架以及Mysql数据库，设计并实现了某科研机构信息管理系统，同时为确保系统的稳定性，对系统的功能和性能进行了相应的测试。本系统实现的功能包括以下几个模块：（1）教师管理模块和学生管理模块，实现了在系统前台对师生信息进行了详细的展示，同时系统管理员可以通过后台系统对师生的信息进行增删改查；（2）新闻管理模块，实现了该科研机构新闻的实时更新，保证了新闻的实效性；（3）项目、论文、专利、书籍管理模块，为该机构的科研状况提供了一个展示的平台，同时方便师生对科研成果进行统计和汇总；（4）照片管理模块，将机构的照片划分为科研照片、毕业照片和日常照片，在一定程度上展示了该科研机构的师生的科研风貌和精神面貌；（5）固定资产管理模块，在后台系统上实现了对该机构的各种固定资产使用情况的展示和库存的汇总，在一定程度上降低了管理员的工作量同时提高了工作效率。


某科研机构信息管理系统在完成开发后，进行了多次功能测试和性能测试，没有出现任何问题，保证了后期系统正常使用的稳定性，同时也便于后期机构成员对于系统的维护。"
2019,基于流量的P2P僵尸网络检测,计算机学院,毕乐斌,贾春福,Security,0.356,"僵尸网络严重影响了网络的安全运行，侵犯了用户的数据隐私。大部分僵尸网络为了避免中心服务器被轻易发现和破坏通常采用P2P协议，P2P协议舍去了中心点使僵尸网络更加隐蔽和健壮。对安全人员来说，提高僵尸网络检测的效率变得极为重要。

       基于流量的检测系统广泛地被安全人员研究和应用。这种检测系统通常采用统计学习方法，对感染了恶意软件的僵尸主机产生的流量和网络中的正常流量进行对比分析，建立流量的分类模型。检测系统对分类模型的结果进行统计，如果异常流量比率超出设定阈值，则判断该主机为僵尸主机。

       公开流量数据集的主要供研究数据为双向NetFlow数据，在此数据集上训练出的分类模型存在检测召回率较低的问题。为提高模型检测效果，本文设计出流量特征提取的流程，从公开数据中得到能够体现P2P僵尸网络流量交互行为特点的双向流特征，通过此特征工程提高了模型包括召回率在内的主要检测指标，基于新的特征向量训练出XGBoost、SVM和神经网络三种分类模型，对比其评价指标和训练时间后认为XGBoost为最佳训练模型，其召回率和精确率分别可以达到99%和97%。通过分析模型特征重要度，得到最能体现P2P僵尸网络流量和正常流量区别的特征分别为连接端口、流持续时间、流交互包层面信息和连接状态等。结合这些特征的数据分布情况直观地展示了两类流量在不同行为层面上的区别，并且从P2P僵尸网络和正常网络的通信原理层面对这些行为区别做出了合理的解释，这些工作不仅表明了模型检测效果提高的原因，也为分类模型的可解释性提供了理论上的支持。"
2019,基于WEB的培训学校管理系统 的设计与实现,计算机学院,兰天翔,宋春瑶,Security,0.2853,"目前，中国对于人才的需求量日益增加，各类学校开始注重对学生综合素质的培养，教师的教学方式也更加先进化和科学化。但是学生已经不满足于在学校中获取知识，因此培训学校得到了充分的发展。由于培训学校发展基础比较薄弱、发展势头较为迅猛，培训学校在其迅速发展的过程中很难做到对教学质量的严格把控，因此，开发一款基于WEB的培训学校管理系统非常有必要。


基于WEB的培训学校管理系统采用Java语言进行编写，使用的系统框架为J2EE、MVC架构和SSH框架，数据库版本为SQL Server2014。SSH架构中硬性要求应用系统的输入、数据处理和结果输出三者分别运行，保证每一模块的独立性和每一模块处理各自的任务，使得使用了SSH架构的基于WEB的培训学校管理系统保持了较高的效率性。SQL SERVER 2014是当前软件开发和系统开发过程中比较流行的后台数据库，具有内存数据库、可更新列存储聚集索引和原生备份加密等优点。


基于WEB的培训学校管理系统共有六个模块，分别是系统管理、人事安排、招生咨询、教学组织和其他功能。系统管理模块包括注册及登录、设置子账户、内部发文、留言反馈和设置学校信息功能。人事安排模块包括教师管理和职工管理。课程安排模块包括课程设置、套餐设置、课程计划管理和查看课程。招生咨询模块包括学生管理、咨询管理、试听管理和报名管理。教学组织管理模块包括教学过程、考勤管理、考勤记录、现场考勤、学生综合成长档案和学生课程成长档案功能。其他功能模块包括短信管理、财务管理和资源管理。


基于WEB的培训学校管理系统的使用可以提高培训学校的教学质量，使得学生接受到更先进的教育，同时，系统的使用可以让培训学校更好地对学生的学习情况进行监控，从而为每一个学生制定更符合其发展的学习计划，有利于学生的成长。"
2019,基于LABVIEW的电子产品性能测试平台开发与应用————车载 移动设备充电器性能测试,计算机学院,刘子成,陈万义,Security,0.2406,"汽车已经进入寻常百姓家，自然车载移动设备充电器成为人们经常选配的汽车用品。车载移动设备充电器一般是指通过车载电源给移动设备（例如：手机、行车记录仪，GPS导航等）充电的充电器，常规汽车车载电源一般由电瓶供电。移动设备主要以锂电池为主，所以，车载充电器既要考虑锂电池充电的实际需求(恒压CV，恒流CC，过压保护OVP)，又要兼顾车载电瓶的恶劣环境(瞬态尖峰电压，系统开关噪声干扰，EMI等)，因此车载充电方案选取的电源管理IC必须同时满足：耐高压，高效率，高可靠性，低频率(有利于EMI的设计)的开关电源芯片。

本文以设计一种车载充电器测试系统为例，研究基于LABVIEW 的电子产品性能测试平台，从系统硬件设计、软件设计及系统界面等方面进行了较为全面的分析设计。通过分析国家有关标准讨论通过虚拟仪器软件来实现测试车载移动设备充电器的性能指标。文中分析了车载移动设备充电器的工作原理及主要性能指标，研究了如何采集被测电器的输入输出电参量方法、利用Modbus工业通信协议实现与模块TCP-508N及数控电源通信的问题，并实现控制数控电源参量输出来对车载移动设备充电器的测试。设计基于LABVIEW技术，从系统硬件设计、软件设计及系统界面等方面进行了较为全面的分析设计。"
2019,大中型企业车辆管理系统的设计与实现,计算机学院,闫雪杰,许昱玮,Security,0.2739,"车辆在现代生产和生活中处于重要地位。它提升了社会发展的进程，提高了人民的生活水平。在竞争日益激烈的市场环境中，大中型企业的车辆管理水平对自身的生产效率以及竞争力提升起到了很重要的作用。目前人工管理车辆效率低下，出错率高，迫切需要对车辆进行系统化的管理。本文基于目前企业用车的需求，设计并实现了一套基于Web的车辆管理系统。


本文首先对企业的用车需求进行分析，提出了功能需求和非功能需求。功能需求包括：车辆信息管理、驾驶员信息管理、用车申请管理、车辆维保管理、车辆违章管理、系统权限管理等，并且针对系统不同角色用户绘制了车辆管理系统的用例图；非功能需求包括稳定性、响应时间、可维护性、安全性等。然后根据需求分析进行系统概要设计和详细设计，包括系统的功能模块设计和数据库设计。在功能模块设计中，通过功能结构图和流程图对系统的六大模块进行了详细介绍。在数据库设计中，选择MySQL作为后台数据库，绘制了系统的E-R图，并设计了数据库关系表。最后，根据设计采用Spring架构与Java语言实现了系统。在该部分，详细阐述了车辆管理系统各主要功能的实现过程，展示了系统执行界面与关键代码。


系统开发完毕后，利用黑盒原理进行了集成测试，并对测试中所发现了问题进行了修改与调整。目前，该系统已正式部署上线，运行状况良好，满足了应用的所有需求，降低了企业管理车辆的工作量，提高了企业的工作效率。"
2019,基于区块链技术的电力需求侧管理系统设计,计算机学院,王大召,李涛,Security,0.307,能源、公共管理和金融等领域的中心化特征往往意味着繁琐和重复的操作流程，这种单一、僵化的模式容易造成工作效率低下等问题。电力系统一直试图寻找高效可行的需求侧管理方案，电力需求侧管理是优化电力配置和保证电力供求平衡的重要手段，然而现有电力需求侧管理大多依赖人为规定的方案和用户自觉响应，存在着施行难度高、推广慢、用户不配合和方案合理性较低等诸多问题。随着计算机技术的飞速发展，自2009年区块链的诞生以来，区块链技术得到了迅猛发展，尤其以比特币为代表的区块链技术应用，在世界上产生了巨大的轰动效应。现在，区块链技术作为一种分布式共享数据库技术应用到交易领域，与传统的交易类型相比，共识机制是它的核心特性，节点之间平权；同时，区块链的智能合约不需要第三方参与便可自动执行交易，当交易成功便记录在区块链上，不可撤销。区块链技术在应用上打破了中心化的运营管理模式，为帮助解决电力需求侧管理的问题提供了新的思路。                                                       基于以上考虑，在面对电力的需求侧管理所追求管理高效可行的问题上，本文采用以区块链为底层架构，利用共识机制，部署智能合约，来解决需求侧管理方案中用户信息难搜集、交易不够灵活方便、奖励公平发布的问题。因此，本文以IBM开发的Hyperledger平台为基础，设计出一套基于区块链技术的电力需求侧管理系统。为验证该系统的合理性和有效性，本文用App客户端和Web端给出数据、功能展示；最后再对实验做出测试和分析。实验结果表明，本文的系统设计解决了电力需求侧管理的人为响应低、交易不经第三方审核、奖励发放易受人为因素影响等问题，同时允许用户间电量的灵活交易，为电力需求侧管理提升服务效率，降低管理成本，保证公平提供了重要的技术支撑。
2019,基于Web的酒店管理系统的设计与实现,计算机学院,胡雪蕊,许静,Security,0.2771,"伴随日常生活和商旅出行等需求的大批量增加，以提供餐饮、住宿等多样服务的酒店行业，面临着前所未有的挑战。通过不断地提升酒店行业的工业化和信息化水平，以计算机硬件融合互联网技术，搭建酒店管理信息交互平台，打造规范化、流程化、个性化的服务模式，将是未来酒店业发展的新趋势。这种全新的模式，在降低酒店运营成本和出错概率的同时，可以提升酒店的运行效率和客户的实际体验。


本文的酒店管理系统除了包含软件长时间稳定运行、交互体验快捷流畅、数据库信息安全可靠等基本要求，还需要支持房间状态管理、客户线上预定、订单实时处理等功能需求。以css3和html5技术分别描述网页的样式和内容，以基于java的vue.js和node.js技术进行系统开发，以基于nosql文档存储的mongodb技术搭建系统的数据库。该酒店管理系统的功能主要面向设置业务和预订业务，本文首先对系统的各个功能模块进行用例建模，并完成系统的体系结构、设计模型、时序图及数据库的设计。然后对系统的各个模块流程及界面设计，并完成了系统的用例测试。最后，从系统前端的代码层和页面层出发，优化了系统的加载速度。


本文的酒店管理系统以模块化的方案实现酒店业务的管理和预定，优化酒店的管理和服务架构，减少酒店运营各环节的支出，优化酒店的业务模式，提高了酒店的运作效率和效益，增强了酒店的综合竞争力。"
2019,云存储系统中硬盘故障预测的研究,计算机学院,王博,王刚,ML,0.3433,"随着信息时代的迅猛发展，现代社会每天都会产生大量的新数据需要存储，存储形式也从原来单一的文件系统发展成为云存储模式，云存储由于它良好的拓展性、可快速部署、成本低等优点而备受关注。而为了维持较高的服务质量，通常需要配置大量的机械硬盘来做分布式存储工作。系统的可靠性、稳定性则取决于这些硬盘的工作情况。企业厂商一般为了保证数据的可靠性，都会采用被动容错技术，如多副本和冗余存储，而本文中则使用了新的主动容错技术，力图在故障发生前预测出故障硬盘，从而保证系统的可靠性。


      对于硬盘主动容错技术，前人已经提出了许多统计和机器学习的方法进行硬盘故障预测，这些方法都是基于SMART属性，也取得了一些成果。本文在总结前人经验的基础上，将数据集预处理和特征选取这两个研究较少的领域作为切入点，同时结合新的CatBoost模型重新建模。这种新方法加新模型的组合在故障预测取得了不错的成果。


      传统为了解决数据集中正负样本不均衡采取的问题，通常采用下采样方式，我们改进了这种方法，使用无监督聚类的方法抽取样本，这样可以保证数据的分布不被破坏。同时分析了SMART属性的原始意义以及计算方式，这相当于引入了SMART的专家经验为特征工程做铺垫。最后根据上面的工作我们使用统计和机器学习的方法重新进行特征选取。


      在完成数据处理工作后我们使用新的模型在两种数据集上进行了试验，我们使用的CatBoost模型实现了很高的迁移率（87.54％和91.52%）和0.0％的误迁移率，优于最先进的梯度提升回归树模型，同时达到极高的故障召回率（95.83％和90.90%）和0.0％的误报率，在预测精度方面优于前人提出的递归神经网络模型或是梯度提升树模型。最后我们还在两个数据集上测试了五个模型作为参照，以保证实验的稳定性、可比较性和可解释性。"
2019,基于深度学习的滑动验证码恶意轨迹识别,计算机学院,王欣鸿,贾春福,Security,0.339,"随着微信和QQ用户增多的同时，恶意登录和注册的行为变的越来越多，黑客使用恶意脚本刷号和盗号现象与日俱增，因此腾讯推出了滑动验证码。策略人员通过观察滑动验证码滑动轨迹，标记黑白名单和书写规则来判别正常和恶意用户，本文中解决的主要问题是通过训练滑动验证码的滑动轨迹，达到识别滑动者是人或者机器脚本的目的，实现自动化判别轨迹，缓解策略人员工作压力。


本文的主要工作有以下几个方面：一、提取滑动验证码轨迹数据特征，根据用户滑动轨迹和脚本模拟轨迹的行为提取特征；二、使用SVM、Xgboost的算法训练滑动验证码轨迹特征；三、清洗滑动验证码轨迹并使用CNN和LSTM的算法训练滑动验证码轨迹数据；四、根据训练结果较好的模型设计滑动验证码轨迹识别系统。


本文使用SVM算法和Xgboost算法的模型准确率分别为65%和70%，使用的深度学习算法包括长短期记忆（LSTM）和卷积神经网络（CNN），用于预测本文中的恶意轨迹，其准确率为93.11%和94%，经过实验表明，CNN文本识别算法是识别滑动验证码恶意轨迹的有效方法，并且根据CNN模型设计滑动验证码轨迹识别系统。"
2019,碳水化合物活性酶深度注释生物信息工具及家族预测算法研究,计算机学院,黄乐,张瀚,Database,0.3159,"碳水化合物活性酶(Carbonhydrate Active Enzymes, CAZymes)与我们的生活息息相关，它既可以降解不同的食物和宿主碳水化合物，也可以降解植物的细胞壁来进行新陈代谢。在生物能源领域，随着测序的微生物基因组与宏基因组数量越来越庞大，进行CAZyme相关基因组数据挖掘的重要自动化工具研发与资源建设显得尤为迫切。


  目前与CAZyme注释信息相关的研究存在着三点不足。第一，现有的CAZyme数据库信息结构单一，没有提供详细的蛋白质深度注释信息，缺乏对重要信息的挖掘；第二，传统的生物学家通过实验方法对CAZyme的注释速度较慢，难以满足大规模基因组的功能预测需要，自动化注释尤为必要。第三，对于CAZyme的家族信息的预测，通常是以Cazy数据库的实验标注样本为基准，应用序列比对工具（例如HMMER）进行扩展的半自动方法。其速度缓慢，不适用于大规模酶家族分类。

  针对以上问题，本文完成了如下三方面的工作。


  首先，本文设计并实现了提供全面深度注释信息的在线数据库dbCAN-seq（a database of CAZyme sequence and annotations, dbCAN-seq）。dbCAN-seq为5349个细菌基因组提供计算出的CAZyme序列和功能注释数据；提供全面功能注释数据的CAZyme注释页；根据物种元数据如疾病、生活环境、氧需求、温度、新陈代谢等方面组织了细菌基因组的元数据页；设计并嵌入了CAZyme基因簇的搜索算法（CAZyme Gene Clusters，CGCs），以提供蛋白酶基因簇信息的可视化在线服务，这是一个可用于识别微生物基因组或宏基因组中多糖利用位点(PULs)的非常有用的工具。除此之外，它还提供快速高效数据查询的强大搜索功能与批量下载功能。


  第二，本文设计并实现了一种对CAZyme进行在线注释的服务平台（dbCAN-meta server），提供准确的自动化的CAZome(基因组的所有CAZymes)注释。它不仅可以接受真核生物的蛋白质输入，也可以接受原核生物的草图基因的DNA序列输入。服务平台集成了可搜索dbCAN-HMM(隐马尔可夫模型)数据库的HMMER、可搜索CAZy预注释CAZyme序列数据库的DIAMOND、可搜索CAZyme短序列数据库的HOTPEP的三种工具，输出结果的融合可以显著提高CAZome注释的准确性。此外，dbCAN2还可处理用户提交的核苷酸序列，提供预测物理连接的CAZyme基因聚类的服务。


  第三，本文最后提出了一种基于深度卷积神经网络与双向长短期记忆网络的CAZyme家族分类算法DeepCAZy。该算法采用了K-mer 分词法构造词典，GloVe 进行词嵌入，将现有的CAZy数据库中的10多万条序列进行训练，与传统的机器学习分类算法进行比较，其准确率更高，达到91%以上，其训练时间较短，预测速度远高于HMMER和Blast工具，达到自动化获得家族信息的目的。


  本文了开发了在线数据库dbCAN-seq，为CAZyme研究者提供具有重要价值的Web资源；设计并开发了新一代dbCAN服务器dbCAN meta server，为研究者提交的新测序CAZyme基因组提供全自动CAZyme功能注释的免费服务；提出了基于深度学习的自动化获得家族信息的分类算法。"
2019,基于深度学习的sgRNA中靶率预测与Anti-CRISPR数据库设计,计算机学院,益海冬,杨征路,Security,0.3046,"作为一种天然的获得性免疫系统，CRISPR/Cas系统广泛地存在于细菌和古细菌中，并用来对病毒进行免疫。由于sgRNA在对目标DNA识别时，即使出现一定程度的误匹配仍然可以对目标基因进行切割，这使得基于序列匹配和模型假设的预测方法，难以实现对sgRNA中靶率进行准确预测。基于特征的学习算法的提出，为实现sgRNA中靶率的准确预测提供了新的思路。这种方法能够利用sgRNA的相关特征，通过训练样本来学习出对应的预测模型，从而实现对sgRNA中靶率的准确预测。同时，Anti-CRISPR蛋白能够特异性地沉默CRISPR/Cas系统，并用于提高基因编辑工具的安全性。但是，目前还没有有效的生物信息工具能够在基因组中搜索Anti-CRISPR蛋白。


本文主要分为两部分内容：（1）设计并实现了基于卷积神经网络和长短时记忆网络的sgRNA中靶率预测模型；（2）设计并实现了Anti-CRISPR蛋白的搜索算法，并制作了一个数据库和一个在线生物信息工具。sgRNA中靶率预测模型充分利用了sgRNA的序列信息和表观信息，在来自于4种不同类型细胞系的sgRNA的中靶率预测中都取得了较高的分类准确度。相比基于序列比对和模型假设的方法，该模型无需手动选取特征，而是可以直接从序列中提取相关特征并组合。对于Anti-CRISPR蛋白，本文系统归纳了现有文献中Anti-CRISPR蛋白的特征，并设计了一个在细菌等原核生物中提取这种蛋白的搜索算法。在此基础上，使用该算法对大量细菌和古细菌参考基因组进行了Anti-CRISPR蛋白提取，并将相关蛋白注释信息制作成数据库并提供在线服务。Anti-CRISPR蛋白数据库的网址为：http://ciil.nankai.edu.cn/Acr-Aca_Website/。"
2019,领域知识驱动的多路深度学习模型设计,计算机学院,许畅,刘铁岩,ML,0.2989,"深度学习是机器学习领域的一部分，它对于学习数据的表达、以及学习一些数据的模式有着显著效果。深度学习相对于传统的机器学习算法，在很多应用领域都取得了很好的性能，比如语音识别，计算机视觉，自然语言处理等等。深度学习模型的性能甚至在一些领域超过了人类专家水平。%深度学习可以基于很多种基本模块，比如卷积神经网络，循环神经网络等等，针对具体问题构建出大型的复杂的网络结构。深度学习模型往往具有很强大的建模和表达能力。此外，深度学习和人类认知学，以及人类的大脑密切相关。所以深度学习在很多亟待解决的困难问题上，比如自动驾驶汽车、自动巡航、机器人设计、精准医疗等方面都有很大的潜力。


尽管深度学习在大规模数据学习中具有很多优势，但是在做具体任务时，深度学习与传统的机器学习算法相比，模型的选择和设计要复杂很多。具体来讲，对于广泛的应用场景而言，并不存在一个通用的万能模型可以适用于所有学习任务；相反，人们往往需要根据具体的任务来精心地设计针对特定任务的深度学习模型结构。这种情况下，需要模型设计者根据对特定领域的了解，充分利用其掌握的领域相关的知识，设计与任务密切相关的深度学习网络结构。如何充分利用领域知识，将其融入到对深度学习网络结构的设计，是利用深度学习解决各种特定任务的重点和难点。


为了更好地解决上述问题，我们往往需要通过引入额外的网络结构或者额外的网络损失函数甚至是额外的网络框架来将领域知识结合进模型的训练过程中。而这类改进方法都是通过多路模型的设计思路来实现。本文探究了在不同的应用领域下，利用不同形式的领域知识，结合不同层次上的多路模型设计来帮助改善深度神经网络模型的性能。主要工作有：


第一，对于神经网络层级别的多路设计，我们研究了真假表情识别任务，即区分由于自发表情与伪装摆拍表情。我们利用对此任务已有的先验知识：平静脸和峰值脸的差别可以帮助人脸真假表情的判断多路模型。提出了一种卷积神经网络（Convolutional Neural Network, CNN）的“比较层（Comparison Layer）”来测量高阶图像的起始图像和峰值图像之间的差异（而不是像素级差异）。通过将比较层添加到CNN，并将从CNN中学习到的特征表示结合，形成一个分类器的输入，用于区分伪装假表情和自发真表情。通过这种在神经网络的层级结构上引入多路设计的方式，有效的将“平静脸和峰值脸差异”这一领域知识融入到模型训练。实验结果表明，我们的方法明显优于现有的方法。


第二，对于神经网络损失函数级别的多路设计，我们研究了如何把知识图谱（Knowledge Graph）中的先验知识引入到词汇表示（Word Embedding）当中。本文提出了一个新名为RC-NET的框架，可以充分利用关系和分类知识来产生更高质量的词汇表示。具体地说，我们将关系知识和分类知识构建成两个独立的正则化函数，并将它们与原始目标函数相结合。利用反向传播神经网络解决这一联合优化问题，从而得到知识图谱增强的词表示。通过这种在损失函数上的多路设计方式，有效的引入了更多先验知识到模型训练。通过在常用的文本挖掘和自然语言处理任务（包括类比推理、词汇相似度和主题预测）上进行实验之后，结果表明我们的模型可以显著提高词汇表示的质量。


第三，对于神经网络整体结构级别的多路设计，我们研究了如何对传统的循环神经网络（Recurrent Neural Network, RNN）进行改进，使其能利用句子的语法语义先验知识。本文提出多通道RNN（Multi-Channel Recurrent Neural Network, MC-RNN），用于动态捕获和利用局部语义结构信息。具体来说，MC-RNN包含多个通道，每个通道一次代表一个局部依赖模式。根据语义信息，我们引入注意机制，将这些模式在每个步骤中进行组合。然后通过自适应的办法选择通道间最合适的连接结构参数化结构信息。从而使得 MC-RNN能够很好地捕捉到句子中不同的局部结构和依赖模式。这种在模型结构层面上的多路设计方式可以有效的处理依赖关系复杂的输入信号。我们对包括神经机器翻译、文本摘要和语言模型在内的典型自然语言处理任务进行了广泛的实验，结果表明MC-RNN相比现有的顶级系统有着显著性能提升。


第四，对于神经网络任务级别的多路设计，我们研究了多任务多语言机器翻译任务。在更为广泛的实际应用场景中，往往存在多个不同任务之间相互依存、相互制约的情况。以机器翻译任务为例，多种不同之间的语言之间存在两两翻译的任务，而来自不同语言之间的翻译，往往可以带来比单一语言对之间翻译更多的有用的弱监督信息。我们将不同的语言看成是不同的顶点，而语言对之间的翻译任务看成是连接顶点的边，在这样一个多任务拓扑结构图中，我们提出了一种名为Polygon-Net的模型，用来综合利用整个多任务拓扑结构图上的不同任务的监督信息来优化不同的子任务。通过这种学习框架级别的多路设计方法，有效的将不同的监督信号进行了整合。在无监督机器翻译任务上，Polygon-Net取得了相对于单任务单路模型更加优异的性能。


值得强调的是，上述工作既探究了不同领域，又在本质上相互联系。第一个工作采用了最朴素的思路，即在网络结构上进行多路的设计，来充分利用领域知识。第二个工作不仅在网络结构上采取多路，而且把多种领域知识通过损失函数的方式引入系统。第三个工作又将多路网络设计的思路进一步提升，通过引入注意力机制来自动地自适应地学习最恰当的多路结构，而不是像前述工作基于预设定好的规则设定多路结构。最后我们将多通路模型设计的思路拓展到多任务学习上，并且在更加复杂的任务场景下证明了我们提出多通路模型结构的有效性。本文对在利用领域知识设计多路深度学习模型在多领域上提供了多种层次的思路，并且提出的模型在具体任务上取得了优异的性能。"
2019,三级式交流发电系统控制器的研究与设计,计算机学院,刘子玉,苑晶,Security,0.2388,"本文基于某直升机10kVA交流电源系统，设计三级式交流发电机控制器，具体为控制器的控制电路以及发电机的电压调节器。

控制器的核心控制部件选用TI公司的TMS320F28335控制芯片，首先详细说明了其外部电路连接方式以及详细的软硬件设计方法，与此同时，总结之前产品在控制电路测试及试验时采用的方法，进行模块化软硬件设计，从而实现系统的模块化及通用化。

在调压器设计方面，首先电压检测方面采用有效值采集电路，利用DSP芯片进行有效值计算，在保证精度的前提下，极大的降低了延迟；另外，采用变论域模糊PID调压策略，在提高了系统的稳定性的同时提高了响应速度。

为了更好的验证所设计控制器的正确性，本文以调压系统的每个单独的环节为研究对象，通过理论分析、仿真实验等方法，验证了所设计的调压控制方法能够保证10kVA发电系统的稳定运行。为实现发电机控制器的通用化提供了思路。"
2019,基于带噪词级标签序列训练语音音素序列识别模型,计算机学院,李晨,张波,ML,0.2826,"随着深度学习在语音音素识别任务中的应用，基于 CTC(Connectionist

Temporal Classification)的端到端语音音素识别模型已经成为一个研究的热点。但是，目前的研究更多地将关注点放在如何搭建序列标注模型以提升识别表现上，对于使用带噪标签序列来训练序列标注模型的研究却很少。而实际中获取到的语音数据往往是由音频数据和对应的带噪词级标注组成的，很难直接获得真实无噪的音素级别标注。因此，本文将着重解决如何利用这样的带噪词级标签序列来训练语音音素识别模型。

现有的关于带噪标签的研究更多的是面向于非序列问题以及序列问题中的

序列分类和段分类问题中，对于输入序列和输出序列对应关系未知的时序分类问题的研究相对来讲很少。本文将着重在解决这样的时序分类问题上，探索利用带噪标签序列来训练序列标注模型以提高模型抗噪性的方法。

本文在已有的序列标注模型中引入了一个序列不匹配模型来对真实标签序

列 到 带 噪 标 签 序 列 的 加 噪 过 程 进 行 了 概 率 建 模 。 基 于EM(ExpectationMaximization)框架提出了一种训练算法，该算法能够同时学习到序列标注模型的参数和不匹配模型的参数。将本文提出的方法和标准的序列标注模型训练方法进行了对比，通过在 TIMIT 语料数据库上进行了多种实验验证了该方法的有效性。

本文中，在只有带噪标注序列的数据且噪声分布参数未知的情况下提出了

训练序列标注模型的新方法。考虑到实际应用中干净的序列数据在很多情形下都是难以获得的，可以方便地将该算法嵌入到基于 CTC 的现有的序列标注模型中，以减轻噪声数据的影响。本文的结果激励了以更低廉的代价去收集更多的数据的做法，因为误标注的序列数据在本文提出的算法下对于模型表现的危害变得更小了。"
2019,基于神经网络和注意力机制的谣言检测系统开发与研究,计算机学院,禹秋成,杨征路,Security,0.3051,"当前全球的社交媒体拥有几十亿的用户，信息流通的速度日趋上升，人们日常接收到的信息数据量越来越庞大。因此，如何及时有效地识别谣言信息成为社交网络的重点研究问题。为了解决谣言检测的问题，常见社交平台都采取了多种处理方式，例如推出辟谣官方账号，实时处理用户举报等。同时，科研人员也相应的设计了谣言检测系统模型。目前主流的谣言检测算法主要基于谣言的不同特征对其进行准确度检测，例如谣言的文本特征、情感特征，谣言发布者的用户特征等。


 

本文以微博谣言为研究对象，以谣言识别为研究目标，主要实现了一个基于注意力机制（Attention Mechanism）的双向长短期记忆神经网络（Bidirection Long Short-Term Memory ，Bi-LSTM）的谣言检测系统。此系统结合现有深度学习框架实现对用户不确定真实性的微博内容进行准确度检测，具有较强的时效性，可以有效减少人工鉴别的成本。文章简要介绍了谣言检测问题的发展历程和研究现状，并重点对系统开发所需使用的技术背景和基础进行了总结，详细描述了谣言检测系统核心模型。文中所设计的系统模型采用双向LSTM进行特征分析以保证同层级元素间信息不会丢失，利用注意力机制引入微博外部特征信息，通过实验对比基线模型验证了模型效果。


 

本文以对用户提交的微博信息进行准确度自动化检测作为核心功能，设计并开发了基于NodeJS的Web Service软件系统，采用时序图、用例图表和E-R图等图表结构对此系统的实现过程和细节进行了详细阐述，对多类型的用户需求以及对应功能模块的具体实现进行了分析展示。本文设计并实现的谣言检测系统为针对微博信息的谣言检测任务提供了自动化解决方案，给用户带来了良好的体验，满足用户需求，能够在合理时效内完成对微博信息的鉴定，减少谣言鉴别的人工成本，使谣言检测工作能够更加便捷、高效的完成，可以给社交网络平台供应商提供研究参考和应用借鉴。"
2019,基于用户行为的多样化推荐系统的设计与实现,计算机学院,原博,卫金茂,Security,0.2929,"微博作为以“随时随地发现新鲜事”为宣传标语的信息传播系统，其推荐功能设定为只对时事、明星领域物料做推荐，而这种推荐方式却存在一定的局限性。一方面，微博存在各领域的物料，只对部分领域的物料进行推荐，一定程度影响其它领域博主的创造积极性，不利于微博创作生态的发展。另一方面，人类对信息的需求是动态且多方面的，单一物料会使用户感到单调，不利于维持用户的粘性。因此，除已上线的垂类推荐引擎外，微博仍需多样化的推荐引擎提供多样化的推荐功能，以对微博系统进行补充和完善。但是任何新功能的上线都是有风险和代价的，比如开发成本、用户接受度的不确定性、推荐引擎初期行为数据积累少导致推荐效果极差等。多样化推荐引擎不可冒然上线，需包装为推荐系统供公司内小范围人群使用，从而可以用最小的成本验证多样化需求的价值和积累数据保证其并入微博时的推荐效果。


      本文基于以上背景，在微博现有物料库的基础上，设计并实现了基于用户行为的多样化推荐系统。首先对用户的 需求进行了详细分析，包括用户对微博物料的内容多样化、载体多样化、推荐个性化和时效性的需求；其次在明确需求的基础上对系统进行了设计，包括系统的技术架构、功能架构、推荐流程、数据库和函数设计，其中推荐流程包括基础模块、召回模块、排序模块和多样化模块四个子模块，具体而言使用了多路召回、DeepFM和多样化的重排算法。设计完成后采用了B/S架构、前后端分离式的MVC设计模式对系统进行了开发。最后本文以图文的形式展现了系统内的主要功能，包括推荐功能、个人信息管理功能和推荐管理功能等。系统在公司内部上线后。通过段时间数据积累，以用户对物料的点击比例（准确率）为指标，对推荐模块中的各算法做了测评，系统整体的准确率为0.178 （微博垂类热门推荐准确率为0.2），推荐效果基本符合预期。"
2019,保险行业财产险理赔系统的设计与实现,计算机学院,杨文彤,张波,Security,0.2996,"近年来，中国保险业正在进行着飞跃式地发展，保险业务迅速拓展的同时，行业内竞争不断加剧。传统意义上的人工加物理卷宗的理赔模式已经不能满足行业发展的要求，需要一种更快捷、方便、高效的方式来取代。因此，为了使保险公司适应于当今信息化快速发展的社会，不断提升市场竞争力，扩大受益，非常有必要开发一套财产险理赔系统。

本文从保险公司的实际需求出发，通过驻场参与保险公司的日常业务，深入调研，针对保险公司财产险理赔业务的现状、特点及需求，设计了基于B/S应用架构下以Struts+Ibatis+JSP为开发框架的财产险理赔系统。论文使用了UML用例图对系统的功能需求进行详细的描述，阐述了对系统体系结构、框架结构和功能模块的设计，使用E-R图完成了系统数据库的设计，最后应用J2EE开发技术与Oracle数据库开发了系统功能。系统包括9个功能模块，实现了从客户报案、立案调度、查勘估损、预付理算、核损核赔等财产险理赔的主要业务环节的信息化和流程化。

系统目前已经在保险公司中部署实施，实际运行结果表明，系统实现了财产险理赔业务全流程信息化作业，有效地提高了理赔人员的工作效率。"
2019,企业人力资源信息管理系统的设计与实现,计算机学院,高宇轩,杨巨峰,Security,0.2857,"随着我国市场经济体制的逐步完善与健全，企业赖以生存和发展市场环境变得越来越规范、越来越公平。在企业的发展壮大过程中人力资源管理发挥着基础性的作用，而伴随着企业的成长，员工数量会持续地增加，这也会给企业人力资源管理工作带来更大的难度和复杂性。


文章从企业人力资源管理的实际出发，分析了企业人力资源信息系统的研究背景、研究目的、意义及研究现状；重点阐述了系统构建的基本原则、开发必要性和现实需要。提出系统设计以B/S网络结构为设计基础，并基于JavaEE平台设计了系统的平台结构和关键技术；详尽阐述人力资源信息管理系统的基础框架结构和子系统模块，子系统模块设计部分由员工基础信息管理模块、员工合同管理模块、薪酬管理模块和绩效管理模块组成；重点介绍了人力资源信息管理系统数据库部分及数据库中各种表单之间的内在联系；最后给出了企业人力资源信息管理系统模块的实现流程，并对系统模块的功能、性能与兼容性进行了验证。


结果表明本文设计的企业人力资源信息管理系统的功能较强，通过了多种详细的功能性测试，能够高效满足企业人力资源信息管理及员工数据分析的功能；在系统的性能表现方面，系统设计的响应时间更短、可以满足多用户同时登录的要求；在系统CPU占用、内存占用以及数据库的性能方面相对传统基于C/S网络结构的系统也具有显著的优势，总体测试结果显示本文设计的企业人力资源信息管理系统符合初始的设计要求和需求，能够给企业的人力资源管理和企业战略决策带来更大的便捷性和人力资源数据支撑。"
2019,面向深度神经网络去噪的损失函数对比与分析,计算机学院,彭欣亮,王靖,ML,0.2627,图像去噪一直是计算机视觉中一个很重要的领域。近年来，基于深度神经网络的去噪方法在图像去噪领域里有着优异的表现。在基于深度神经网络的图像去噪中，现有的大多数论文着眼点在于不同神经网络结构的设计，譬如DnCNN，FFDNet，U-Net等，这些关于神经网络结构的研究对提高网络去噪的性能做出了非常大的贡献。然而神经网络训练中，损失函数在计算梯度大小和方向这一关键步骤起决定性作用，而对于损失函数的相关研究却很少，通常是基于简单的预测值与目标值之间的L2范数。目前为止，仅有Hang Zhao和Orazio Gallo等的Loss Functions for Image Restoration with Neural Networks一文专门针对不同的损失函数进行讨论，比较了L1范数，L2范数，SSIM，MS-SSIM等不同损失函数，并且给出了在相同网络结构下， 范数与MS-SSIM组合的损失函数能取得最好效果的结论。在本文中，我们将对此进行更加深入的研究，训练由不同网络结构和不同损失函数组合而成的去噪神经网络，分析训练所得的这些网络的去噪性能以此研究损失函数与神经网络结构之间的关系。结果表明，采用不同损失函数的网络性能与网络结构相关，而Loss Functions for Image Restoration with Neural Networks中的结论在我们所给出的网络中并不成立。同时，我们给出了基于VGG-16网络损失函数，再次印证采用不同损失函数的网络性能与网络结构相关的结论。
2019,基于SSH的物资配送管理系统的设计与实现,计算机学院,王旭,刘晓光,Security,0.269,"为了改善某集团资源架构以及资产配置效率，进而提升企业整体经济效益，建立架构高效、统一的物流配送管理体系，深化集团发展方式改革，某集团下属的物流管理分公司决定采用高效的信息化物流配送技术体系。因此，应用成熟的信息技术架构一套完善的物资配送过程管理系统是很有必要的，一旦系统实现，该企业物流业务将实现配送管理业务的流程化、物流配送过程的透明化、信息监控手段的智能化、精准化，进而实现逐步提高企业发展质量的目标。


本论文以某公司物资配送过程管理系统的设计思路、架构过程和实现过程为主要研究对象，从客户业务需求角度入手，主要分析某公司物资配送业务过程以及配送业务特点，并基于现实配送环境结合成熟的互联网信息化技术手段，设计企业级物资配送全过程管理系统方案。整个系统分为几个子功能模块，包括配送计划管理、配送监控过程管理、物资配送过程监控、配送相关信息查询索引、系统基础业务维护等模块。此外，在满足项目组基本业务需求的基础上，完成系统架构设计以后，对系统架构、数据库、用户使用、系统功能、系统上线等过程进行具体概述。最后，论文对系统实施的效果进行客观的分析。


企业的物资配送过程管理系统的成功设计与正式上线运行以后，在某种程度上为某公司物资配送业务的开展提供了强有力的信息化支持，这种系统化、信息化不仅降低企业的业务管理成本，而且提高了企业整体运作效率，全面提高了物资配送各个环节的管理效率，从而稳步推进某集团物资系统化、信息化管理工作。"
2019,基于PHP的工程物资管理信息系统的设计与实现,计算机学院,刘易豪,王玮,Security,0.278,"在传统的工程物资管理中，一般采用数据报表和数据管理的模式来实现，但对工程物资的库存状态、折损、盘点等数据缺乏认识，论文分析了某公司工程物资管理流程，在采用PHP架构，建立了一个集物资查询统计、入库管理、出库管理、物资处理管理等于一体的应用系统，为公司工程物资管理工作提供综合性的管理平台。


论文立足于目前工程物资管理方面存在的问题，通过PHP架构，结合工程物资管理的需求，提出基于工程物资的库存管理需求，以提高工程物资管理效率为目标，完成了系统需求分析。通过需求分析了解了工程物资管理的业务流程，进而得到系统所实现的功能，系统功能包括集物资查询统计、入库管理、出库管理、物资处理管理等，通过系统功能分析，掌握了系统需要具备的功能，结合系统运行环境，完成了系统非功能性需求分析。在需求分析基础上，完成了系统总体设计，包括：数据支持层、业务关系层、系统功能层和表示层等多层结构设计。系统概要设计完成后，通过类图、时序图完成了系统各功能模块，这些功能模块构建了工程物资管理系统。通过数据库概念模型和物理模型完成了系统数据库设计。


论文在实现功能模块后，分别选取了典型的功能，完成了系统的功能、性能的测试。测试结果表明，系统构建了综合一体的工程物资管理模式，实现了科学、全面的工程物资管理控制；系统性能能够满足公司多用户并发访问，系统运行稳定。"
2019,基于移动平台的课堂教学辅助系统的设计与实现,计算机学院,杨丽,宫晓利,Security,0.2938,本文的研究目的是为了能够使传统课堂和APP进行结合，这是一种新的教学模式，它是网络教育、远程教育的补充，弥补了网络教学的缺点。在高职教育的过程中如何将信息化技术融入到教学管理中，通过移动平台教学创设和谐的教学氛围，带动学生的积极性，从而提高课堂教学效率，设计开发一个良好的、具有职业教育特色的现代化移动平台是具有一定的现实意义的。本文对现有教学辅助系统深入研究的同时，充分利用移动平台的便利性，实现移动系统，使得教学服务进行线上化。 本文对论文的研究背景与意义进行了分析，再根据调研对国内外研究现状进行了分析，然后对本文研究的主要内容进行了分析，最后对本文研究的组织结构进行了说明；完成了课堂教学辅助APP的功能需求分析、可行性研究、非功能性分析等内容。在功能分析中通过用例图来描述功能与用户之间的联系，并且通过非功能性需求对性能、安全性等内容进行了分析；完成了课堂教学辅助APP的总体设计目标，并且对设计的总体框架进行了分析。完成了课堂教学APP的功能模块和子功能模块设计，根据功能模块设计后对业务流程进行了设计，对每个功能的标准化流程设计，最后对数据库进行了设计；通过需求分析、设计对APP进行了实现，并且完成了测试工作，保证系统的稳定性。系统的开发关键技术采用了Android平台以及SQL Server2016。 本文开发的基于移动端的课堂教学辅助系统，是目前最新教学方式的具体体现，学生的学习积极性得到了提高，同时学校的教学管理也得到了提高。最后，通过本文的研究，总结出系统在实际运行中存在的问题，以及下一步的解决方案。
2019,基于表示学习的动态社交网络中用户建模研究,计算机学院,吴沛之,杨征路,ML,0.3039,"随着社交网络服务应用的快速发展，人们更多地倾向于在社交网络服务中对消费的物品反馈评分和与他人建立社交链接关系。所以，对物品反馈评分和与他人建立链接是社交网络中两个关键的用户行为。另外，对社交网络中的用户反馈和社交链接行为建模具有重大的意义，因为它是很多实际应用的基础，包括推荐系统和用户朋友圈分析，并且它可以为社交网络服务优化用户体验。


同时，社交网络并不是静止的，而是随时间演化的，这表示用户的链接行为是跟时间相关的。另外，用户偏好和物品属性也会随时间变化，这让准确地预测用户评分反馈越来越具有挑战性。另外一个问题是，社会科学家证明过这两种用户行为并不是相互独立的。这两种相互影响的关系被称为社会影响和同质性影响。


大多用户建模领域的前人方法都对两种用户行为分开独立建模，并且只考虑上述三种动态观点的某些特定方面，例如动态的用户偏好，动态的物品属性，演化的社交网络，和他们的部分组合。所以，这会导致他们的方法不能利用全部的有用信息来准确地用户建模或者获得最佳的预测结果。


本文提出了一个综合的、泛化的基于神经网络的框架，该框架使用了一些

优化的策略来对动态社交网络中用户反馈和用户社交链接的演化联合建模。本文提出的框架考虑了动态社交网络中动态的用户偏好，动态的物品属性，和时间相关的社交链接。除此之外，本文提出的模型能够量化社会影响和同质性影响，进而更好地理解用户行为。


最后，在两个真实数据集上的实验结果证明了本文提出的模型相比于其它最先进的方法具有更好的表现。"
2019,云存储系统主动容错编码及其优化研究,计算机学院,李鹏,王刚,Security,0.2678,"近年来，出于成本的考虑，大规模云存储系统越来越多地采用纠删码技术保护数据。纠删码以较少的存储开销获得等于甚至高于多副本的可靠性，然而它存在降级读延迟高和故障修复时间长的固有问题。针对云编码系统的存储代价和故障修复这对矛盾，有研究者研究磁盘故障预测技术，通过预测磁盘潜在故障并提前进行数据迁移以降低故障时的重构代价，然而这些研究未能将故障预测和纠删码系统冗余方案的构造结合起来，未能从系统角度考虑故障预测各个指标对系统运行时的影响。此外，磁盘故障预测不是“万能”的，总有部分故障无法被及时准确地预测出来，这些故障依然需要纠删码被动容错技术。对传统的纠删码技术的优化研究一直是存储领域的一个重要方向，现有研究分别从某个方面提升了纠删码存储的性能，然而仍然有两点未能覆盖：（1）中心化降级读带来的高降级读延迟问题：现有的降级读普遍采用的是一种集中式的串行重构读方法，导致很高的读延迟；（2）阵列码及其缩减技术在分布式系统的应用问题：前人针对阵列码缩减技术的研究大多只用于扩展编码参数，以适配系统的各种冗余需求，而少有研究考虑将缩减技术应用到降低重构代价上。


为了解决上述难题，本文在前人研究的基础上，从主动容错和被动容错两个角度，分别设计了相应的主动冗余方案和高效的降级读以及阵列码缩减重构算法，从而为纠删码在云存储系统中的应用关键点提出了统一的解决方案。具体来说，本文的工作包括如下三个方面：


第一，针对纠删码云存储系统中存储代价和故障修复开销这对矛盾，引入主动容错思想加以解决，提出了一种纠删码结合动态弹性副本的主动冗余方案。系统构造以低存储代价的纠删码为基础，辅以低重构开销的副本技术，对预测出的潜在故障磁盘，主动调整其上危险数据的副本数，从而在保证系统高可靠性和故障重构能力的前提下大幅度降低存储成本。本文设计实现了主动冗余云存储系统原型，分别从理论分析和实验论证角度度量了主动冗余方案对系统可用性的提升效果。本文还采用蒙特卡罗仿真结合组合数学分析的方法，验证了主动冗余方案能显著提高纠删码云存储系统的可靠性。实现验结果表明，相比于纯副本或纯纠删码系统，主动冗余方案提升了超过2个数据量级的可靠性，并降低了63.4%的降级读延迟和77.6%的故障修复时间。


第二，针对纠删码云存储系统故障状态下的高降级读延迟，提出了基于组通信的并行重构读算法。针对传统的中心化降级读方法导致的高读延迟问题，本文提出了基于组通信的并行重构读算法，将数据读取和解码运算分散到多个参与的数据节点上，显著降低了降级读延迟。传统的组通信归约算法契合于条纹中发生单故障情形的修复，然而对于多故障场景却无能为力。基于此，本论文进一步提出了吻合多故障修复的全新的组通信操作 双路/多路归约，并为其设计了高效的算法 亦即多故障修复的高效并行算法，进一步降低了多故障场景下的降级读延迟。另外，本文通过参数化建模的方法评估了并行方案对降级读性能的显著提升作用。本文实现了具有并行降级读功能的云存储系统原型，评估了并行重构读算法的实际性能表现。实验结果表明，相比于串行降级读，在不同的故障场景中，并行重构读算法在不引入任何额外存储和 I/O开销的前期下提升21.2%∼75.6%的降级读速度。


第三，针对阵列码的高重构代价问题，提出了行列缩减阵列码的构造及其重构算法。本文首先揭示了阵列码中不同行列之间所承担重构负载的不平衡性问题，进而提出了基于行列缩减的重构代价优化策略。本文从阵列码三等分构造修复的思路出发，分析了带约束的最优行缩减的等价问题，并在此基础上对列缩减优化问题进行建模，进而设计了高效的启发式构造型缩减重构算法，在给出缩减方案构造的同时，描述了故障情形下的重构流程。本文通过模拟实验，验证了其在不同参数组合下高效性以及降低重构所需数据量的优越性。本文实现了基于缩减构造的阵列码存储系统原型，并将其同多种典型阵列码的优化进行对比，验证了其在提升重构吞吐上的显著优势。实验结果表明，行列缩减阵列码仅引入少量存储成本，但是其能减少43.7%∼56.3%的重构所需数据量，相比于多种经典阵列码重构优化策略，本方案可提升35.9%∼54.0%的重构吞吐。


面对纠删码在云存储系统中应用的关键难点问题，本论文分别从主动容错和被动容错两方面的协同优化为纠删码存储系统故障场景下的高降级读延迟和高重构代价问题提供了完整的解决方案。这些研究成果形成了完善的主动冗余和被动重构优化的机制，为纠删码在分布式存储系统中的应用提供了方法基础。"
2019,基于实时流式计算的网约车智能干预系统设计与实现,计算机学院,王明明,王刚,Security,0.2481,"随着现代互联网技术的快速进步，网约车平台越来越多，随之专门接送网上客户的司机也越来越多。但是伴随着近几年网约车事故多发的事实，网约车司机的素质被各个公司重视起来。各个公司通过创新运用信息能力、大数据分析能力和管理能力，在平台发展的同时开发和集成了驾驶员服务质量和信用评价、导航、拼车等一系列综合服务的管理平台。有些公司甚至对城市交通机械化调度、交通拥堵治理做出了贡献。各种研究方案中基于流式计算的智能管理平台脱颖而出，基于流式计算的实时数据分析平台能够实时地对平台进行智能管理，本文研究的正是基于实时计算的网约车管理平台。


本文设计和实现的网约车智能干预系统主要收集业务实时数据到kafka消息队列，然后使用flink计算引擎进行订阅，然后进行分布式流式计算进行特征计算和数据流关联等操作，选用xgboost算法对大数据框架计算出来的数据进行模型训练。训练好的模型形成策略，被策略命中的消息源会接收到一个干预信号，即客户端收到干预信号，然后在客户端进行语音播报。此外，本文采用hive数据库对数据进行存储。总体上应用大数据技术实现了一个能够对网约车司机进行智能管理的干预系统。本文数据来源于某网约车公司真实数据，各实验均是基于真实数据进行分析。


关键词:网约车，实时流计算，xgboost，flink,kafka"
2019,NKNet：一种防御对抗攻击的深度网络框架,计算机学院,邹杰,白刚,Security,0.3319,"深度神经网络已经成为人工智能领域里最前沿的技术，被广泛运用于计算机视觉、视频图像跟踪、语音识别等领域。然而最近的研究发现，尽管神经网络在大量的任务中取得越来越好的结果，神经网络系统本身却引起质疑。通过计算，在样本上添加精心设计的微小改变可以使神经网络在该改变后的样本上以极高的置信度得出不一致的决策。这类精心设计的改变称为对抗扰动，使用对抗扰动误导模型的行为称为对抗攻击。由于对抗扰动的普遍存在，有人认为这可能是神经网络本身的原因，模型的安全性引起质疑。由于在研究和实践中的大量使用基于神经网络的系统，因此，增强神经网络的鲁棒性，有效防范安全风险，成为一种迫切的需要。


针对神经网络的脆弱性问题，在归纳了常见对抗扰动生成和防御方法的基础上，本文提出了一种有效地防御对抗攻击的深度神经网络框架。该框架使用集成方法，聚合多个深度网络模型共同决策，同时动态地选择决策结果。该架构具有异构性、动态性、随机性、主动性和通用性的特点。


为验证该框架的有效性，特别是以防御对抗攻击为代表的模型鲁棒性问题，本文构造了该框架的一种实现。在该实现中，通过在数据处理方式和网络结构两个方面的不同选择构造了多个深度网络模型。根据集成方法的原理，在使用基于该框架的模型作为机器学习模型时，为使模型具有较强的泛化性和鲁棒性，模型中各个神经网络模型之间需要存在足够多的差异性。本文通过测试对抗攻击在构造的多个神经网络模型之间跨模型攻击的能力，证实了在数据处理方式和网络结构两个方面的不同可以使深度网络之间形成差异。最后测试模型，证实该模型具有较强的鲁棒性，可以有效地防御对抗攻击。


本文在理论上提出了使用集成方法的神经网络模型框架，分析了该框架的特点，同时阐明了为防御对抗攻击该框架在实现时应满足的要求。同时通过实验验证了该框架的有效性，可以将其作为一种防御对抗攻击的有效手段。"
2019,基于AUC和近邻互补性的特征选择算法研究,计算机学院,姜雪萌,沈玮,ML,0.3419,"目前正处于大数据时代，各个应用领域中均存在着大量的高维数据。这种高维数据加大了训练机器学习模型的难度。此外，原始高维数据中大量冗余和噪声特征也将降低模型的泛化能力。特征选择作为一种有效的降维方法，在近年来也变得更加重要。

  受试者工作特性曲线（ROC）下的面积，AUC，是一种评价特征分类性能的指标，尤其是在类别不平衡和代价敏感问题上有着显著的优势。因此在特征选择以及生物信息学领域，经常应用各种基于AUC 的评价标准。然而，已有的这些方法通常不能很好地衡量特征间联合识别能力，即特征的互补性。

  目前最新基于AUC 的方法，通过衡量不同特征维度中每个实例与最近一个邻居之间的差异来评估特征的互补性。这种基于局部学习的方法引入了一种独特的方式去衡量特征之间的互补信息。然而，近邻的信息很容易受到噪声的影响。此外，在特征互补性计算时仅评价特征的单方面识别信息很明显会忽略了来自同类样例的近邻信息。因此，文本提出了一种基于AUC 并且融合了特征全面近邻信息的特征选择方法，称为ANNC。该方法通过k 个同类近邻以及k 个异类近邻去分析其公共错误分类信息来评估成对特征的互补性。然而，这种引入k 个近邻的策略同时也带来了选择k 值的问题，尤其是出现数据分布不均和类别分布不均的情况时。因此，本文提出了一种基于局部密度和纯度自适应k值选择算法，划分公共误分类样例所属密度区域，然后再根据其类别纯度选择不同的k 值。这样既可以避免选取固定k 值的缺点，对每个错分类样本选取不同的k 值，将自适应的方法引入到互补性计算中，也能选择更加可信的k 值去评价特征间的互补性，减少了近邻数目给互补性计算带来的影响。在已有公开数据集上的实验结果表明，本文提出的算法具有较强的抗噪能力。并且，使用本文提出方法选出的特征子集能够使分类器的正确率达到最高，且选出的特征子集包含的特征数目较少。"
2019,基于正交元空间的多模态图像增强技术研究,计算机学院,梁杰,杨巨峰,ML,0.2557,"图像增强技术自适应地为普通图像（主要体现为光照、对比度等的不和谐）赋予适当的美学特征，同时保留图像的内容细节。给定普通图像，多数现有工作仅输出唯一的具有统一美学样式的增强图像，该样式来自于给定的参考图像集。然而，不同用户的审美偏好、具体应用场景均可能存在较大差异，这需要图像增强技术具有单一模型多模态处理的能力，给出尽量多样化的输出供用户选择。与此同时，图像内容和样式特征需要充分解耦，并有多模态融合的能力。


为了实现图像增强的多模态输出，本文提出了一种基于正交元空间的多模态图像增强框架，提取出参考图像中具有视觉吸引力的美学特征，并将其显式地编码到一个正交的元空间中。具体地，本文首先使用编码-解码器及对抗训练策略提取出高美学质量图像的样式和内容特征。接下来，本文将参考图像的样式编码映射到一个由一组正交基张成的样式元空间中，其中正交基通过引入正交正则化损失来实现。在测试阶段，给定任意一张普通图像，本文由编码器提取其内容特征；同时在样式元空间中随机采样多个样式特征；最后将原图内容编码与样式特征分别融合后，送入解码器得到最终的多模态增强图像。


另外，为提高多模态场景下样式、内容特征解耦及融合的稳定性，本文针对图像增强任务提出一种改进的自适应实例标准化模块。该模块在训练过程中使用图像样式编码的通道维统计信息及其在元空间中的位置信息对内容编码进行规范化，从而使解码器同时得到高频样式特征和普通图像的内容特征。同时，本文提出最大化元空间位置编码和判别器输出向量之间的互信息，提高元空间的建模能力。改进的自适应实例标准化模块及互信息最大化策略提升了同一图像样式、内容特征解耦及跨图像样式内容特征融合的稳定性。


在实验部分，本文方法在基于美学评分、真实性评分和多样性评分等的多种量化度量指标上均取得了优于其他相关工作的性能表现。另外，增强图像的可视化及用户调查结果也说明了本文方法可以生成有美感且多样的图像。"
2021,基于强化学习的软件漏洞挖掘技术研究,计算机学院,武辰璐,贾春福,SE,0.3438,"漏洞挖掘是软件安全领域的重要研究课题，模糊测试（Fuzzing）是漏洞挖掘领域最有效的方法之一。其中，覆盖率引导的模糊测试工具AFL（American Fuzzy Lop）以其效率高、易配置、效果好著称，是工业界和学术界常用的挖掘漏洞的工具。AFLFast研究发现AFL为每个种子分配相同的能量，使测试朝着高频路径进行，低频路径未能得到更好的测试，因此提出了能量调度算法。


    强化学习是新近兴起的深度学习算法，能有效依据环境状态做出最优的动作决策，在模糊测试领域具有一定的发展空间。论文通过分析发现强化学习和模糊测试具有一定的相似性，因此创新性地将强化学习应用于模糊测试，提高传统模糊测试工具的智能性。论文主要工作总结如下。


    论文首先将AFL的模糊测试流程建模为马尔科夫决策模型，探讨强化学习应用于模糊测试的可行性；然后，建模基于强化学习的AFL测试框架，令强化学习依据当前待测试种子的状态学习种子的能量；接下来，设计n步跳帧的实时异步架构，改进AFL和强化学习执行速度不一致的问题；同时，改进AFL的优先种子确定算法，增加种子的执行次数、生成次数两个评判标准，提高AFL探索度；最后，提出新的种子选择算法，和实时异步架构进行结合，改进原生AFL种子选择算法的缺陷的同时提高测试效率；最终实现新的模糊测试工具RL-AFL。


    经过对14个常用工具集24小时的测试，RL-AFL的平均边覆盖率相比AFL和AFLFast分别提升了1.72%和1.37%，比AFL增加的边覆盖率范围为0.07% ~ 7.94%，比AFLFast增加的边覆盖率范围为0.06% ~ 4.35%；在漏洞发现能力上，RL-AFL在LAVA-M测试集上比AFL和AFLFast发现更多的漏洞数；对实时异步架构效率进行测试，一般情况下，RL-AFL测试效率低于AFL，使用实时异步架构后RL-AFL和AFL测试效率相同。上述实验结果说明论文提出的基于强化学习的模糊测试方法可有效增加模糊测试的代码覆盖率，提高漏洞发现能力，实时异步架构也可以提高测试效率。"
2020,基于多视角RGB-D图像帧的场景理解和地图构建,计算机学院,张彪,孙凤池,Robotics,0.3334,"人工智能和机器人是未来社会发展的潮流和趋势。在过去的几十年中，工业机器人的使用和普及促进了生产力的发展，提高了人们的生活水平。如今随着老龄化社会的到来，市场对服务机器人的需求越来越强烈。相比工业机器人，服务机器人需要具备更高的智能，对工作环境进行智能感知，识别其中的物体和场景，建立环境地图，自主决定在场景中需要执行的任务。


      场景信息的智能感知是机器人实现智能化的重要前提。机器人可以通过搭载的RGB-D摄像头采集室内场景信息，但信息采集过程受视角、光照、传感器精度和范围的限制，采集的图像帧有模糊、信息不全等问题，难以采集到完整的场景信息。当前流行的神经网络方法大多识别单帧图像上的语义信息，而不完整的环境信息会影响这些方法的识别准确性。相比于物体类别，场景类别的感知更为复杂，还与人在场景中的活动有关，而人在场景中的活动与物体息息相关。


       基于以上背景，本研究使用同一房间的多帧图像改善单帧图像包含场景信息不全的情况，并提出了基于物体重要性的场景类别融合方法。首先，本研究从单帧图像的智能感知出发，初步识别环境中的物体类别和场景类别。其次，基于物体重要性对多帧图像中识别的物体进行排序，保留重要的物体，使用场景类别融合算法判定多帧图像场景类别；然后基于从多视角图像帧构建的完整场景信息建立了物体关系图，方便服务机器人和人的语义交互。最后，对采集的稠密图像帧信息，本文提取关键帧，使用迭代式场景类别融合算法识别场景。实验结果表明所提算法的有效性。"
2020,基于情感结构嵌入的零样本视觉情感识别,计算机学院,展翅,杨巨峰,NLP,0.3236,"在过去的几年里，视觉情感识别取得了巨大的进步。大多数现有方法都遵 循心理学的普遍观点，即某种特定的情感可以被看作是固定数量的基本情感之 一。随着心理学领域理论的发展，基本的情感范畴越来越细化。传统的监督学 习方法所训练的分类器只能识别固定类别的情感。当根据不同的心理学理论探 索新的类别时，这种以预先定义的类别为训练对象的识别模型就不能动态地识 别新的情感类别。此外，对于罕见的情感类别，采集训练样本也是一项费时费 力的工作。因此，本文主要研究零样本学习场景下的视觉情感识别。


    零样本学习的目的是识别训练集中不存在的新概念，并且已经被广泛应用 于各种计算机视觉任务。传统的零样本学习方法通常是根据已知类别的图像和 其类别语义表示之间的对应关系来构建一个公共嵌入空间。该空间依赖于共享 知识。这些共享知识是关于已知类别和未知类别之间的语义关系，因此公共空 间也共享于已知类和未知类别。然后零样本学习就可以简化为一个最近邻搜索 任务，将测试图像的类别定为公共嵌入空间中最近的未知类别。这种零样本学 习范式依赖于视觉特征和类别语义表示之间的交叉模态相似性。低层次图像特 征和高层情感语义之间存在着情感上的差异，直接计算相似度很难准确描述它 们之间的相似关系。因此，零样本情感更加具有挑战性。


     为了解决上述问题，本文提出了一种利用中层语义特征，即形容词-名词对 (ANP) 特征来构造中间嵌入空间的情感结构嵌入框架。视觉特征和类别语义特 征都被嵌入到所学习的语义空间中，并与 ANP 特征的情感结构保持一致。因此， 我们的方法可以有效的填补情感鸿沟，解决图像情感识别中的零样本问题和广 义零样本学习问题。这里需要注意的是，在传统的零样本学习中，训练集和测 试集合是不相交的，而在更具现实意义的广义零样本学习中，训练集也将作为 测试的一部分出现。此外，在训练过程中，视觉特征嵌入和情感语义嵌入都是 动态变化的，很难直接有效地结合。在此基础上，我们进一步提出了一种情感 学习对抗约束，迫使视觉嵌入在学习过程中选择一个保留情感结构信息的嵌入 空间。最后本文进行了全面的实验验证并对实验结果进行分析。"
2020,基于深度学习模型的社交媒体虚假新闻检测技术研究,计算机学院,侯赵伟,陈晨,NLP,0.331,"以微信、微博、Twitter、论坛等网络平台为代表的社交媒体具有使用方便、传播速度快、成本极低等优势，已经逐渐发展成为人们获取信息咨询及表达诉求的主要平台和渠道。然而由于发表新闻消息较为容易、缺乏有效监管等因素使社交媒体新闻的质量低于传统媒体，导致虚假新闻频繁出现，不仅会造成巨大的经济损失，还会破坏社会稳定和谐。为了避免这些负面影响，研究社交媒体虚假新闻自动检测技术变得十分必要。


本文从社交媒体虚假新闻检测技术的研究现状出发，充分利用社交媒体新闻的属性特点，综合门控循环神经网络和卷积神经网络的优点，提出了一种虚假新闻检测混合模型BiGRU-CNN。该模型不仅可以捕获到虚假新闻文本的时序特征，也可以提取到其中关键的局部特征。


为了保证训练效果，深度学习模型训练需要大量的标注数据，本文在混合虚假新闻检测模型BiGRU-CNN的基础上，提出了一种弱监督预训练方法，能够自动化地利用大规模无标注社交媒体新闻，节约专家手工标记数据集所需大量的时间和精力。


为了利用社交媒体新闻相互之间的联系信息，本文设计了一种基于图卷积神经网络的虚假新闻检测模型FND-GCN，使用社交媒体新闻内容及其评论建立包含文档节点和单词节点的整体图，将虚假新闻检测任务转换成整体图上的节点分类。


在中英文真实数据集上的相关实验表明，本文提出的模型和方法均有效，并在多个指标上取得了最好表现。此外，本文提出的模型还可以拓展应用到新闻分类、谣言识别等其他领域。"
2020,基于预测机制的英特尔 SGX 运行时优化方法研究,计算机学院,刘希明,张金,OS,0.3417,"随着虚拟化技术的快速发展，云计算逐渐成为当下的热点，云计算中存在的各种安全问题也逐渐暴露出来。在这之中，针对操作系统和虚拟机监视器的攻击尤其重要，它使得使用云计算的用户无法信任操作系统和虚拟机监视器。为了解决这个问题，硬件厂商们纷纷提出更具安全性的处理器架构，英特尔提出的 SGX 便是这之中的典型代表。但是 SGX 为了保证安全性牺牲了大量的性能，大大降低了程序的启动速度和运行速度。


本文首先对 SGX 的结构和它设计上的性能瓶颈进行了详细的分析。在 SGX的进行缺页异常处理时，CPU 需要在 SGX 执行状态和非 SGX 的执行状态直接进行切换。这个操作使得在 SGX 中处理缺页异常的开销是原来的数十倍以上，这是 SGX 中程序性能变差的主要原因。而程序的启动时间变长则是因为运行在 SGX 中的应用程序在启动时首先要花时间去创建一个名为 Enclave 的安全区。


为了解决 SGX 中存在的性能瓶颈，本文提出了三种机制。针对 SGX 中缺页异常处理产生的巨大开销，本文提出了一种基于缺页异常记录的运行时页面预取机制 (DFP) 以及一种基于源代码进行插桩的页面预取机制 (SIP)。其中 DFP 机制可以有效的减少具有大量连续访存行为的应用程序的缺页异常数量，而 SIP 机制可以有效减少具有大量随机访存行为的应用程序的缺页异常数量。此外，本文还提出了一种基于 Enclave 池设计的 Enclave 预启动机制，这个机制可以大幅度的减少使用 Python 编写的应用程序的启动时间。最后，本文通过实验论证了三种机制的可行性和有效性。实验结果显示，对于 SPEC CPU2006 中的测试程序，DFP 和 SIP 两种页面预取机制平均得到了15.9% 和 10.5% 的性能提升，在效果最好的情况下可以达到 18.5% 和 11.5% 的性能提升。对于两个真实应用场景下的图像处理程序，DFP 和 SIP 也分别得到了 11.8% 和 10.8% 的性能提升。而 Enclave 的预启动机制根据 Enclave 的大小可以将启动速度提升 7 到 110 倍。"
2020,基于5G架构的MEC资源分配策略研究,计算机学院,孙梦瑶,徐敬东,OS,0.2679,"近年来，随着智能终端和蜂窝网络的发展，诸如增强现实、混合现实以及AI 识别等多种移动应用营运而生。然而，这些新兴应用通常需要较高的计算量以及较低的延时响应，这给终端处理能力以及蜂窝网络带宽带来极大的挑战。通过扩大网络部署的方式虽然在一定程度上满足了网络带宽的需求，但会给网络运营商带来巨大的部署费用和运维费用。因此，如何在降低服务开销的同时满足移动应用的需求是一个厄待解决的问题。 


论文采用多址边缘计算（Multi-access Edge Computing, MEC）的计算模式，在基于下一代前传接口（NGFI）的云无线接入网络（C-RAN）的场景下，综合考虑移动运营商的设备运行成本、应用提供商的服务运行成本以及因传输和处理数据造成的延时转化成本，解决虚拟机开关切换、服务部署、流量分发问题。论文对最小化总成本的问题建模出一个带约束的混合整数问题，并在离线场景和在线场景下分别提出了不同的算法。在解决离线问题时，为了减少求解混合整数问题的时间复杂度，论文提出一种多项式时间的算法，并证明了算法得到的解与最优解之间的比值。在解决在线问题时，论文引入了流量队列，允许流量在短时间内堆积。为解决带队列的优化问题，论文基于随机近似方法提出了单时间片算法、长短时间片算法和学习加速算法。单时间片算法计算时只针对当前时间片进行优化。长短时间片算法考虑到虚拟机开关切换耗时较长，而服务部署和流量分发耗时较短。因此，该算法在一个长时间片决策虚拟机开关切换，在此基础上，决策每个短时间片的服务部署和流量分发。为解决长短时间片算法减慢队列的收敛速度的问题，论文采用学习加速算法，通过历史数据学习对偶参数，加速算法收敛。


论文通过数学分析和实验两方面对算法的性能进行了验证。对比其他算法，论文提出的算法在不同网络规模下，都有良好的性能。对比单时间片算法，长短时间片算法能够减少虚拟机开关切换频率，但是增加了队列的长度。学习加速算法可以缩短队列长度，令队列快速达到稳定状态。"
2020,基于区块链的关系型数据库防篡改安全中间件研究,计算机学院,谢彦苗,张金,OS,0.236,"互联网技术和信息技术的发展促使各行各业以电子化的方式提供服务，伴随而来的是大量需要数据库进行管理的电子数据。这些数据中存在着重要的敏感数据，一经篡改就会产生严重的危害。虽然目前有诸如防火墙等手段来防止数据库的外部攻击，但是缺少防止拥有权限的用户对数据库恶意篡改的有效方法。区块链具有防篡改的目的，目前有一些用于防篡改数据库功能的区块链产品。但是这些产品要么将数据直接存于区块链，降低了数据的访问速度。要么将数据存于链外数据库，不支持丰富的sql查询，并且损耗了大量容量。


       为了保持关系型数据库数据持久化、大容量存储、访问性能优的特性，并且利用区块链防篡改的特性来保护数据库安全防止内部篡改，本文设计并实现了基于区块链的关系型数据库防篡改安全中间件。论文首先对关系型数据库中每个表中所有记录的所有主键进行哈希摘要计算，并将其存储入区块链，以实现非法插入和非法删除的检测，并将数据库中每行数据的串联值进行摘要计算存入区块链来实现非法修改的检测。接着，由于区块链中的数据具有透明性，区块链中存入的值除数据项的哈希摘要值之外，还有查询键值，以及数据表名称，为了实现敏感数据的隐私保护，本文对存储入区块链中的键值和数据库表名称进行对称加密。最后，由于区块链访问速度限制了中间件的处理速度，为了提高该中间件的处理效率，本文设计了针对于区块链的缓存数据库。


       论文最后对设计的数据库防篡改安全中间件进行实验测试，实验结果表明该中间件具有良好的防篡改特性和处理效率。"
2020,肝移植术后风险预测方法研究,计算机学院,曹睿,刘晓光,NLP,0.2886,"肝移植是目前治疗肝癌和终末期肝病的最有效方法。随着科技的进步，肝移植手术的成功率高达98%以上，但与此同时，术后五年生存率仍有待提高。肝移植长期预后与病人的术前基础疾病等因素有关，再加之供肝短缺，因此需要在肝移植手术前对病人做严格的筛选。医学领域已有许多用来衡量病人是否适合做肝移植手术的标准，但这类标准大都只考虑了部分影响因素，计算方法也很简单。近年来，机器学习因其处理高维数据的能力在医疗领域应用广泛，用来分析和处理复杂问题。基于上述背景，本文采用机器学习模型来预测病人肝移植术后移植物状态、生存周期及肝癌复发情况，同时针对建立模型过程中遇到的类别不平衡问题、可解释性问题、数据隐私保护问题提出了解决方案，下面分别加以介绍。


针对建立肝移植术后移植存活预测模型面临的类别不平衡问题，本文提出了基于样本缺失度的代价敏感学习和重采样方法。本文使用的数据集缺失比较严重，虽然采用了数据填充的方式，但填充的数据并非是原始数据，模型性能也受到影响。通过有倾向性地提高少数类中数据缺失严重的样本权重或增加样本的数量，可以使得模型着重考虑这些样本，从而提升模型性能。


针对模型可解释性问题，本文从全局和局部两个角度着手分析。在医学领域，模型的可解释性非常重要，提高模型的可解释性不仅可以获得决策依据增加模型的可信度，更重要的是辅助医生发现新的影响因素。因此，本文采用MDI（Mean Decrease Impurity）和MDA（Mean Decrease Accuracy）计算的特征重要性来解释模型建立的依据，采用LIME（Local Interpretable Model-Agnostic Explanations）计算测试样本中特征的重要性来解释模型决策的依据。


针对机器学习模型建立过程中涉及的隐私泄露问题，本文采用差分隐私策略设计隐私保护方案。国内外针对数据隐私都有相应的法律法规，因此医疗数据隐私保护是将模型转化为实际效益必不可少的保障。本文基于拉普拉斯机制和指数机制提出了两种隐私保护方案raw_noise和map_noise，通过对原始数据进行扰乱来达到隐私保护的目的，实验证明两种方案都可以在损失一定模型精度的前提下保护数据。"
2020,基于生成对抗网络的图像异常检测算法研究,计算机学院,丁晓珂,袁晓洁,NLP,0.2767,"随着互联网的飞速发展与大数据时代的来临，图像数据量呈爆炸增长趋势，导致图像数据集内图像种类和质量多样化，掺杂部分异常图像。图像异常检测的目标是分析并检测出异常的图像，可以用于数据去噪，检测并剔除出数据集中的异常数据，可以用于医疗领域，帮助医生检测出医学图像数据中的异常并进行疾病诊断，还可以应用于工业、金融等多个领域，具有重大的现实意义。

       由于异常的数量和种类未知，列举出所有异常情况是不可行的，而且获取成本较高，本文仅利用已知的正常类别图像来学习正常类别图像的特征，异常检测时，与正常类别图像特征相差较大的图像被认定为异常。

      本文分析并总结了图像异常检测在国内外的研究现状，结合当前深度学习领域高度关注的生成对抗网络技术，对图像异常检测进行了深入研究，发现目前的图像异常检测方法存在的问题是：现有的图像异常检测模型主要是作为特征提取器学习正常类别图像特征，还需要进一步的异常值计算，端到端的模型较少；其次深度神经网络面对从未见过的图像时预测概率不准确，导致异常检测时预测误检率高。针对以上两个问题，本文对图像异常检测开展了进一步的研究，主要工作包括：

       第一，提出一种端到端的基于生成对抗网络的图像异常检测算法。本文设计了基于生成对抗网络和对抗性自编码器的图像异常检测算法，该算法是一种端到端的模型，可直接输出图像异常检测结果无需额外计算。其次，该算法增加了置信度估计，可以反映出预测结果的可靠度，并创新性的使用置信度估计进行异常检测，克服了深度学习网络面对从未见过的图像预测概率不准确的问题，取得了更加准确的异常检测结果。

       第二，本文在四个常用图像数据集上进行了大量的对比实验，结合异常检测常用指标，对实验结果进行评测。实验表明，相比于已有的图像异常检测算法，本文提出的基于生成对抗网络的图像异常检测算法能够更准确的检测异常图像。此外，本文还进行了模型简化实验、参数实验等来研究各模块的重要作用和参数对实验结果的影响，并在两个数据集上进行了检测效果示例展示。"
2020,基于离线学习历史查询的近似查询技术研究,计算机学院,李云,温延龙,NLP,0.2442,"随着信息社会的发展，数据规模呈爆炸式增长。大数据具有规模巨大但价值密度低的特点，如何快速地从大数据中提取出有效信息成为数据库领域的一项重大挑战。传统的数据库查询方法在处理大数据时具有较高的延迟，而近似查询则可通过高效的计算方式快速得到满足需求的近似结果，能够帮助企业实现智能决策、提高运行效率和风险管理能力，因此具有重要的理论研究意义和广泛的实际应用价值。


       本文首先总结近似查询处理的国内外研究现状，深入分析现有近似查询技术的不足，在此基础上提出了一种基于离线学习历史查询和在线预测未来查询结果的两阶段近似查询处理方法。在离线学习历史查询阶段，首先将历史查询拆分为结果为单值的简单查询，接着利用特征工程从简单查询中抽取特征形成特征向量，将特征向量作为因变量，简单查询的真实结果作为标签，利用回归模型学习特征和查询结果之间的关系从而离线构建数据分布模型。在线预测阶段，首先对到来的查询进行拆分形成一组简单查询，通过底层的近似查询处理器求得简单查询的初始近似结果，将查询本身和初始查询结果作为输入，访问预先建立的模型得出预测的提升后结果，接着合并多个简单查询的预测结果，并将其作为最终的预测结果。本文提出的方案能够通过学习历史查询建立底层数据分布的模型，并利用该模型预测未来查询的结果。


       本文在多个数据集上设计对比实验，用以验证本方案的有效性。实验结果表明，本文方案能够在额外计算时间很小（可以忽略不计）的情况下，减少近似查询结果的误差，且在不同分布的数据上都有良好的表现。因此本文方案具有可行性、有效性和健壮性。"
2020,基于张量的彩色图像零水印技术研究,计算机学院,蒋飞凤,高铁杠,SE,0.2373,"在强大科学技术的推动下，当今社会可以说是瞬息万变，各种信息载体都有了现代化的表现形式和处理方式。图像作为一种较为形象、直观的数据载体，目前主要以数字形式呈现。但在这种情况下，不法分子可以借助便利的网络和一些软件肆意下载、复制、篡改数字图像等，严重威胁版权所有者的合法权益。数字零水印技术可以有效提供对数字图像的版权保护，主要通过提取原始载体图像的特征数据，以此构造唯一零水印的方式实现，但现有的算法主要应用于灰度图像。


出于上述考虑，本文提出了一种新的基于彩色图像的数字零水印算法。由于使用的图像是三维彩图，因此在算法中引入了张量及张量展开，这几乎是张量概念及其相关技术在零水印领域的首次尝试。在算法具体实现时，首先从原始载体图像中取各个维度的数据，构造不同的三维张量，然后再根据特定的方向对获得的张量进行张量展开。这几步操作同时实现了三个功能，包括全面利用彩色图像各方面数据，无损维度压缩和数据置乱。另外，在后续过程中还使用了奇异值分解和离散余弦变换，对张量展开后的不同数据进行分别处理。上述两个操作能够减少零水印数据量，并促进算法鲁棒性的提高。根据大量仿真实验和对比实验的实验结果，可以确定本文提出的算法同时具备较好的唯一性和较强的鲁棒性。并且，当应用于不同类型的原始载体图像时，算法的性能依旧维持在较好的水平。在文章的最后，基于本文算法的一些不足，对后续进一步的研究方向进行了大致的说明。"
2020,基于混合模拟退火算法的云资源调度研究,计算机学院,张东,李旭东,SE,0.2622,"随着当今社会的快速发展，云计算行业由于其新型高效和价格相对低廉的特性也在迅速兴起，是一种商业化计算模式和云资源共享的服务形式，目前服务供应商主要提供的服务为：软件即服务、平台即服务和基础设施即服务。其中，云计算下的资源调度是云计算应用的核心，是提高云计算服务性能的关键，高效的资源调度直接影响云计算中的运营成本、资源利用率以及用户满意度等。


由于云计算中资源调度的性能直接影响云计算系统的性能，所以，本文对云计算中使用的调度算法进行了深入研究，着重研究了一些启发式算法，并对资源调度算法进行优化，在既能保证云计算的运营成本下，保证对云用户的服务质量。本文的主要研究内容如下:


首先通过文献阅读，了解了当前云计算中的相关概念知识以及主要技术，比如虚拟化技术，再对云中资源调度的各个研究方向进行分类总结，如：调度粒度、调度平台、调度目标和调度算法，在调度算法的研究中，本文着重研究了模拟退火算法，对算法的基本原理以及优缺点等进行分析讨论，并在该算法中引入遗传算法中的选择、交叉和变异以及禁忌搜索算法中的禁忌表等，形成一种混合模拟退火算法，通过旅行商问题验证了混合模拟退火算法的优越性，算法也将运用到云资源调度模型中，进行分析讨论。


其次提出一种虚拟机资源调度模型，详细介绍了模型的调度流程、编码方式和适应度函数，并以调度时间，调度费用和负载均衡为调度目标，利用CloudSim仿真平台，将研究的调度算法在此模型上进行运行分析，可对比各种算法的优劣性，同时也更加深刻的了解云中资源调度的细节。


最后提出一种容器级别的云资源调度模型，同样介绍了调度流程，编码方式以及适应度函数，并以相同的调度算法和相关实验参数运行此模型，通过与前一章节的虚拟机调度模型对比分析，可知在调度时间和调度费用上，此模型性能更佳。"
2020,基于微服务的大气污染源信息管理及数据分析平台的设计与实现,计算机学院,董兰天,赵宏,SE,0.2539,"随着工业的发展和城镇化进程的加快，环境污染问题成为了百姓健康生活的痛点。近几年，国家和各级政府对环境问题愈发重视，多措并举开展大气污染防护工作。为了方便对大气污染物进行特征分析并合理制定污染防控方案，从2014年开始，各地陆续开展大气污染源清单的编制工作。然而，国内的大气污染源清单编制过程并不规范，大气污染源清单数据的管理缺乏系统性，因此，排放清单对于环境管理和防控方案制定的支撑作用得不到充分发挥。


      基于上述背景和相关部门实际需求，本文设计并实现了大气污染源信息管理及数据分析平台。首先对相关企业和部门进行了大量调研，进行需求分析，然后对系统各模块功能进行了详细设计。接下来基于微服务的架构方式进行系统开发，采用Spring Cloud框架作为技术支撑，针对不同的功能需求，将系统划分为多个微服务，服务之间通过RESTful方式进行通信。前端使用Layui和Vue进行构建，业务逻辑层使用SpringMVC进行搭建，持久层使用Mybatis搭建，为提高系统的并发效率，使用Redis进行数据缓存。同时，平台还使用了Spark MLlib机器学习库中的随机森林模型进行污染物排放预测。最后，将平台进行部署并全面测试。通过对大气污染源数据进行系统有效的分析与管理，使得相关人员可以更加便捷高效的管理大气污染数据，同时为污染源排放特征分析和防控方案的制定提供数据支撑。


      本平台通过对大气污染源数据系统有效的管理，方便了企业填报、环境管理人员审核和科研工作者使用数据进行相关分析，为环境管理者进行污染防控方案的制定提供了数据支撑。"
2020,基于DRLSE的肝脏分割方法研究,计算机学院,仇晓彤,辛运帏,Robotics,0.2625,"随着现代医疗技术的日新月异，肝癌治疗方法也在不断完善。肝移植手术、精准放射治疗等较为复杂的肝癌治疗手段，都依赖于医学影像技术特别是医学图像分割技术的研究进程。高精度的肝脏分割不仅可以提高手术的安全性以及肝癌的治愈率，而且能够明显降低手术对人体正常组织造成的伤害。因此，研究肝脏分割方法具有重要的临床意义，被越来越多的专家学者所重视。


肝脏分割一直以来都是医学图像分割中的难点课题。为了解决现有肝脏分割技术存在的鲁棒性差、自动化程度低等问题，本文提出了一种基于DRLSE的肝脏分割方法。首先，利用腹腔组织结构关系，提取骨骼坐标信息构建多边形掩模，建立肝脏位置约束，避免心脏等密度相近组织的干扰；第二，使用离群值消除技术平滑肝脏内部噪点，在保留肝脏边缘信息的同时清除肝脏外部的粘连，提高分割方法的鲁棒性；第三，引入自适应系数改进DRLSE演化过程，实现扩张演化与收缩演化之间的自动转换，提出了一种基于改进DRLSE的肝脏分割方法。依据肝脏相邻切片形状位置相似的特性，分别向上、向下对整个肝脏CT序列迭代分割，提高自动化程度。第四，针对下腔静脉、肋间肌肉等造成的过分割，利用肝脏边缘平滑特性优化轮廓，使用A.Carmona-Poyato算法提取候选关键点集，通过计算瓶颈率确定过分割点对，去除过分割区域。优化轮廓不仅可以提高分割精度，而且避免了迭代分割时发生的误差累积。


本文通过实验验证了该改进方法的有效性并评价了其分割精度。在数据库中抽取7组肝脏CT序列进行分割实验，实验一使用Dice相似系数、假阳性率、假阴性率3个评估指标进行分割精度的定量评估；实验二通过对比边缘优化前后评估指标的变化情况，验证本文的边缘优化方法的有效性；实验三通过对比传统医学图像分割方法与本文方法的分割结果，验证本文方法的改进效果。最后，将肝脏分割结果导入重建平台3D Slicer获得三维重建影像，方便医生进行术前评估演练、术中引导治疗。"
2020,云游戏自适应资源分配方法研究,计算机学院,刘浩源,李忠伟,OS,0.2789,"最近，云游戏越来越受欢迎，云游戏交互延迟与玩家对游戏的体验息息相关，如何降低云游戏的交互延迟成为了云游戏行业最大的挑战之一。云游戏服务器端响应时间是交互延迟的主要组成部分，它受进程如何进行资源分配的影响很大。然而，要找到使云游戏服务器端响应延迟最小的最佳资源分配方法是非常困难的。


在本文中，我们提出了一个自适应资源分配方法，该方法能够有效地、自适应地进行在线资源分配，以减少云游戏服务器端响应延迟。我们首先使用机器学习技术建立一个性能预测模型，这个模型能够捕获系统资源划分方法和系统性能之间的复杂关系。利用该模型，我们将系统进程划分为几个不相交的组，在进程组之间进行资源的划分，这种分组划分资源的方法既保证了资源分配的高效性，又大大简化了资源分配的问题。在线阶段，为了处理系统负载的动态变化，我们将系统负载进行量化并聚类，利用强化学习模型来学习不同的系统资源划分操作是如何影响系统性能的，并自适应地选择最佳资源分配操作来实时地最小化响应延迟。


我们在真实的云游戏环境中使用几个真实的游戏来评估我们的方法。实验结果表明，与自然状态下系统进程共享资源的方式相比，自适应资源分配方法可以减少20% - 41%响应延迟，并且显著优于其他资源分配方法。"
2020,复杂网络的跨社团链路推荐技术研究,计算机学院,葛瑶,袁晓洁,SE,0.2467,"现实生活中存在着大量由相互作用的实体组成的系统，复杂网络在这些系统的建模方面得到了广泛的应用，以构建的复杂网络为基础进行分析和挖掘具有重大的现实意义和经济价值。链路预测是图数据挖掘中的重要研究方向，链路推荐作为链路预测技术的主要应用之一，通过分析网络特征和实体属性来预测未来网络链路形成趋势并进行实体推荐，具有学术网络合作学者推荐、在线购物平台商品推荐、社交平台朋友推荐等应用场景。然而由于网络中自然存在的社团特征导致社团内部的节点联系更为紧密，传统的链路推荐更倾向于推荐位于同一社团内彼此之间尚未存在链路的节点，这极大的限制了链路推荐的多样性和扩展性。 


本文关注跨社团的链路推荐问题，考虑到同质信息网络和异质信息网络是当前网络建模的主要模型，提出了分别适用于以上两种网络模型的跨社团链路推荐算法。同质网络上的算法以核心节点和有限跳步遍历为基础，采用有偏随机游走策略来获取在网络中发挥重要作用的节点，结合有限跳步遍历计算重新定义的节点间相似度，从而达到降低计算复杂度的目的。为了有效提取异质网络中的社团信息，本文提出了基于随机游走和skip-gram的网络嵌入方法。该方法在利用特定的随机游走方式生成蕴含社团特征的节点序列的同时，能够很好的平衡序列中异质边和同质边的比例。结合skip-gram模型，可以为后续的链路推荐提供具有社团特征的节点特征表示。对于以上两个方法，本文利用线性函数来增强跨社团节点间的评分，从而得到最终的推荐结果。 


真实数据集和合成数据集上的实验结果表明，本文在两种网络模型上提出的跨社团链路推荐算法，在准确度和时间效率上能够实现较好的平衡，利用本文方法得到的推荐结果具有更高的合理性和实用价值。"
2020,持久性内存统一虚拟化技术研究,计算机学院,张佳辰,刘晓光,OS,0.2964,"在现代计算机系统的层次化存储结构中，靠近处理器的内存设备容量较小，性能较强；连接于外围I/O总线的外存设备速度缓慢，但承担了系统的数据持久化任务。考虑到内、外存设备的不同特性，进行硬件资源虚拟化的系统一般将内存和外存的管理模块分开：内存管理模块实现了多个任务对有限内存资源高效透明的共享；外存管理模块则为上层应用提供了丰富易用的持久化数据管理功能。


    然而，近来出现的持久性内存（persistent memory, PM）设备模糊了内存和外存之间界限。PM基于3D XPoint等高性能非易失存储器，且连接于内存总线，因此兼具外存设备的持久化特性和内存设备的可字节寻址特性。学术界和工业界已经进行了很多关于PM资源虚拟化的相关工作，但大多数工作建立在仅将PM作为内存使用或仅将PM作为外存使用的基础之上。本文工作将围绕如何兼顾PM的内存和外存特性，实现更⾼效透明和功能丰富的PM虚拟化展开，具体分为两个主要部分： 


    在为应用提供虚拟化环境的Linux操作系统中，本文基于虚拟内存系统和文件系统提出了VMFS系统。VMFS支持以单个PM分区同时为应用提供内存分配和文件存储服务。VMFS还利用了内存、存储空间统一管理的优势来减少数据拷贝次数，提升了文件读写的性能。测试结果表明，在文件压缩等一些特定负载下，VMFS的性能优于使用DRAM和PM分别作为内存和存储的方案。


    在为系统提供虚拟化环境的QEMU虚拟机管理器中，本文强化了现有的内存虚拟化机制，在利用EPT页表保持高性能虚拟地址转换的同时，支持了原本实现于I/O虚拟化模块中的瘦分配、快照和基础镜像等三种存储虚拟化特性。本文为这种机制设计了一种新的虚拟机镜像格式pcow，并针对PM的特性进行了优化。测试结果表明，本文的方案不仅相对原生内存虚拟化方案几乎没有性能损失，而且相对基于I/O虚拟化和qcow2镜像格式的方案有高达40倍的性能提升。"
2020,车联网环境下安全认证和位置隐私保护方案研究,计算机学院,张增辉,吴英,OS,0.2442,"随着汽车数量的日益增多，交通事故、位置隐私泄露等问题逐渐加剧。车载 自组织网络(VANET)旨在给用户提供一个安全行车和高效行车的交通环境，具 有广泛的应用前景，对未来交通发展有着重要的意义。 


然而，由于 VANET 中车辆节点规模大、流动性强等特点，车辆与其他车辆 或路边单元之间的通信面临着巨大的挑战。这些挑战主要分为两种:第一，在车 速较快的公路上，低时延的安全认证需求越来越迫切;第二，恶意的网络服务商 可能通过车辆的位置服务请求来追踪到车辆节点，对车辆的位置隐私安全造成威 胁。因此，本文针对车联网环境下安全认证以及位置隐私保护问题，深入研究了 相关解决方案，具体工作如下: 


1、本文首先介绍了车联网的概念以及研究现状，深入研究了车联网环境下的 安全认证方案以及位置隐私保护的方案。分析并概括了不同方案之间的联系及其 优缺点。 


2、针对安全认证问题，本文提出了一种车联网环境下的通信安全认证方案。 该方案使用身份加密的方法来保护车辆的隐私信息，并加入了批认证处理来提高 认证效率。经过对本方案的安全性分析，发现其符合车联网系统下的安全需求。 最后，仿真结果表明本文所提出的安全认证方案在通信开销、时间开销等方面要 优于其他方案。 


3、针对位置隐私保护问题，本文提出了一种新颖的车辆位置隐私保护方案。 当前，在车联网环境下，普遍采用在混合区内所有车辆同时更换假名的方法来保 护车辆的位置隐私。若混合区内愿意更换假名的车辆数达不到 K 匿名的阈值时， 车辆的位置隐私就无法得到有效保护。针对这一问题，本文通过 Stackelberg 博 弈的激励机制来刺激周围车辆，使其产生更多的假名交换消息来达到 K 匿名阈 值。在本方案中，主导车辆设置奖励，跟随车辆进行博弈产生更多的假名更换消 息来获取奖励。仿真结果表明本文所提方案通过激励机制总能实现 K 匿名位置 隐私保护。 


关键字:车联网、安全认证、位置隐私、激励机制"
2020,一种基于有损压缩位置索引和单词临近度的动态索引剪枝算法,计算机学院,刘纪鹏,任明明,NLP,0.2684,"信息时代的到来使得互联网上的网页信息呈爆炸式增长，为了帮助用户面对海量的信息能快速找到自己需要的内容，很多公司开发出了商用搜索引擎，例如Google、百度等。搜索引擎的主要目标是快速准确地返回用户查询对应的结果网页，查询效率和准确度都直接影响用户体验，因此搜索引擎领域的研究者们致力于通过各种办法来提升搜索引擎的查询效率和查询准确度。


本文设计并实现了一种结合单词临近度信息的动态索引剪枝算法，这种剪枝算法在对文档的分数上界估算和实际算分过程中，都使用了查询词对临近度信息，因此比传统的只基于词频信息的剪枝算法能取得更高的准确度，由于引入了词对临近度信息，因此需要使用包含位置信息的倒排索引，这种索引的体积是普通倒排索引的数倍，使用这种索引的查询效率也比较低。为了解决这一问题，本文采用了一种经过ALC算法有损压缩的位置索引，通过实验数据分析，这种索引比普通无损压缩的索引压缩效果更好，而且可以有效消除位置索引中冗余的位置信息，提高文档算分准确度和算分效率。本文实现的剪枝算法与这种有损压缩索引能够很好地结合，通过对倒排项和候选文档的过滤来减少文档算分时间，提高查询效率，而且本文提出的剪枝算法是精准的，不会破坏有损压缩索引带来的准确性提升。


为了测试本文提出的剪枝算法的性能，在实验与分析部分，本文使用了三个查询集分别对查询效率和查询准确度进行了实验，通过实验结果可以看出，本文设计的剪枝算法与有损压缩索引结合使用可以取得比使用原始索引更高的查询准确度和查询效率，因此是一种有效的针对位置索引的剪枝算法。另外，本文在实验中尝试在剪枝算法中引入PageRank分数，通过实验结果可以看出，在PageRank分数权值较小的条件下，剪枝算法的效率虽有下降，但也有较好的性能表现，与此同时，查询准确度的各项指标都有所提升，所以在剪枝算法中引入PageRank是可行且有意义的。"
2020,基于多点语义表征和双向注意力机制的意图分类模型,计算机学院,张晶晗,杨征路,NLP,0.3751,"随着互联网和移动终端的普及和发展,智能语音助手和人工客服机器人被广泛应用在实际生活中,旨在回答用户的闭合域任务型导向对话或开放域闲聊型对话。因此,如何提升机器人对用户话术的理解能力(口语或文本)是目前亟需解决的问题。在任务型导向对话中最重要的就是分析用户的话术并理解用户的意图,也就是意图分类任务。一句话中所包含的信息是十分有限的,因此需要通过语义表征,有效表示用户话术语义信息,映射到用户的意图,提升意图分类任务的效果。本文基于此对意图分类任务进行了研究。

本文的研究方向是基于多点语义表征和双向注意力机制的意图分类模型,旨在通过对用户话术进行文本和结构上的同时拆解,寻找句子和意图间的映射关系,提升意图分类的结果。本文的主要贡献如下:首先,基于多点语义表征方法,将用户话术拆解为四个语义因子。在不增加大量标注成本的基础上,共享意图中的因子信息,增加意图间的区分度,从后文的实验结果中可以看出,本文所提出的多点语义表征方法能够明显地提升下游任务意图分类的效果。

其次,本文为了更好地利用多点语义表征的结果,在意图分类模型中引入预训练模型、双向注意力机制和多任务学习框架,捕捉句子和因子及意图间的相关性,有效结合粗粒度的意图信息和细粒度的因子信息。

实验结果表明本文所提出的框架和模型能够有效提升意图分类的结果,与先进模型相比,在不同的数据集上,本文所提出的模型在精确度指标上实现了1.35%-2.47%的提升。"
2021,主题感知的文档摘要模型,计算机学院,付曦燕,杨征路,NLP,0.3676,"网络的快速发展导致文本信息量爆炸式增长，日常生活中，人们不得不每天面对繁杂的文本内容，但是这些文本中含有大量的信息冗余，因此，迫切需要自动对长文本进行信息摘要。由于人工摘要的成本很高而且生成困难巨大，自动文档摘要技术是解决这个问题的关键方式。


       文档自动摘要技术不仅可以直接应用于长文档内容的概括和提炼，相应的结果也可以用于下游的后续任务，例如文档检索、文档匹配、问答系统等，因此，目前有大量的研究学者对这一领域进行学习研究。考虑到主题是文章的一个重要特色表征，蕴含丰富的语义信息，前人的部分研究工作采用主题信息辅助的方式进行文本摘要，旨在获取语义更准确，内容更精炼的相关摘要，但是这些工作多采用外部预训练好的主题模型进行浅层融合来辅助摘要生成，这样的模式存在以下的问题：1）预训练好的主题模型无法根据文章动态提取主题。摘要任务旨在摒弃大量的无关信息，但是外部完整的主题模型无法根据摘要的特性进行部分主题的抽取，而是获取全部的主题，导致部分噪音主题被提取，影响了整体的文本摘要生成效果；2）低层次的主题融合方式。现有工作大多采取在词语编码阶段进行主题信息的融合，这样的融合方式只考虑了最基础的词语信息，忽略了文章段落的语义等高层次主题信息。


       为了解决现有主题摘要模型存在的问题，本文提出了变分的层次性的主题摘要机制（Variational Hierarchical Topic-Aware Mechanism，VHTM），该方法更好地将主题信息融入文本摘要过程。具体来说，该机制首次采用变分编码解码器将主题推断和文本摘要模型相结合并共同训练，同时，被提取的主题信息将通过词向量和主题注意力两种方式多层次融入文本摘要的生成过程，低层次采用主题词编码，高层次采用段落切割并结合主题注意力的方式。基于CNN/Daily Mail这一公开数据集的实验表明VHTM机制能够在摘要过程中有效提取相关主题信息，并且提升最终生成的文本摘要质量。"
2021,基于SpringBoot的房屋销售租赁系统的设计与实现,计算机学院,刘炜,王超,SE,0.2329,"随着互联网的飞速发展和科技技术的不断进步，信息化和数字化的管理系统逐渐受到了大型公司的青睐。其巨大优势不仅体现在运作成本低，节省巨大的人力、物力、财力，还能够对海量的数据进行精准的分析和归档，相较于人力管理方式更加高效。脱离了纸质化带来的束缚，数字化管理方式有着先天的数据共享的优势，用户可以随时随地的进行数据分享和数据管理。以上优势都满足了目前大型企业日常管理的需求。

    目前，随着房地产行业的蓬勃发展，传统的房屋销售租赁服务已无法满足行业需求，如何能够建立一个高效且低成本的房屋销售租赁系统成为了房地产行业所面临的一个难题。本文针对于此，设计和实现了一款基于B/S模式结构，采用SpringBoot框架，数据库使用MySQL，以Tomcat7为服务器的房屋销售租赁系统。

    该系统采用前后端分离形式，前端可进行新房、二手房和租房的信息浏览。后端可进行顾问管理、经纪人管理、二手房信息管理、新房管理和租房管理等操作。后台管理员分为三个级别：超级管理员、顾问和经纪人。超级管理员具有后台管理的最高权限，可以对房屋信息、顾问信息和经纪人信息进行管理，经纪人仅能够对房屋信息进行管理，而顾问则只拥有在线咨询和查看客户看房信息等功能。该系统风格简约，操作方便，基本上满足了房屋销售和租赁的需求。"
2021,存储系统混合容错编码研究,计算机学院,罗金飞,李忠伟,SE,0.2445,"随着时代的发展，用户产生的数据呈现爆炸式增长，如何可靠存储这些数据是企业设计存储系统时不可回避的问题。通常企业在设计存储系统时会采用一定的冗余机制来保证数据的可靠存储，然而随着存储系统面临的环境越来越复杂，同时用户在不同领域产生的数据越来越多，常规单一的存储冗余机制在这种情况下存在一定的局限性，已不能很好保障存储系统可靠性。


例如，在航天器搭载的存储系统中，传统采用纠一检二功能的海明码来保证数据存储的可靠性，但是随着空间环境越来越复杂，只能实现一位纠错的单一编码并不能保障航天器中数据的可靠性；在互联网云服务器后台存储系统中，企业为了保证内存缓存系统的可靠工作，通常使用多副本机制，但是当数据规模变得越来越大时，会导致巨大的存储开销。


本文针对典型存储系统面临的上述“共性”问题，研究混合冗余机制，通过结合不同编码方案的优点来提高存储系统可靠性。同时针对混合冗余机制在不同系统中面临的“特性”问题，如航天器搭载系统中编码计算瓶颈、分布式系统中网络传输瓶颈，设计针对性的优化方案，尽可能地降低混合容错对于系统性能的影响。


本文工作分为两个部分。第一部分为，针对航天器存储系统面临的恶劣空间环境导致严重单粒子翻转问题，设计一种两级编码方案，通过不同的编码组合来提供高可靠性和存储空间、性能的权衡；并且还针对编码计算和访存瓶颈，设计了相应的优化策略，尽量降低引入混合编码对数据访问性能所造成的影响。实验结果表明，混合容错编码的容错能力明显超过单一编码方案，通过优化，性能已与单一编码相当。


本文第二部分工作为，针对云平台中广泛使用的内存缓存系统，提出一种基于新型存储设备的副本—纠删码混合冗余方案，在保证容错能力的前提下显著降低存储开销。还设计了基于新型持久性内存设备（PM）的存储方式来加速数据恢复，以及编解码流程优化方案来解决网络传输瓶颈。实验表明，优化后的系统性能与副本方案相当，证明了提出的混合容错机制的可行性。"
2021,融合评论文本信息的股票预测模型的研究,计算机学院,周洪雨,邵秀丽,NLP,0.2714,股票是金融市场的重要组成部分，实现股票价格的精准预测从微观上可以 增加中小投资者的投资收益，从宏观上有助于政府提前察觉金融市场的异常动 向及时出台政策规避风险。但股票数据高噪声、非线性、不平稳的特点，使得传 统模型在股票数据上的预测效果不甚理想。基于以上背景本文通过分析传统模 型在股票预测中的局限性，提出了更加贴合股票数据特征的优化模型，以提高股 票预测准确率。为此，本文从以下三个方面展开具体研究工作。 （1）股票数据滤波及评论文本向量化。获取到的原始股票数据中不可避免 地存在缺失值和极异常离群点。对此本文分别基于多元拟合法和卡尔曼滤波算 法在保证信息损失尽可能小的前提下实现了噪声过滤。此外，论文在数据维度上 增加了股吧评论，因此本文首先使用 jieba 分词提取评论关键词并基于 Word2Vec 对其进行了向量化处理，将评论文本同股票基本属性数据和技术指标数据进行 融合，解决了股票文本数据与数值型数据异构的问题。 （2）基于 DE-SVM 算法的股价涨跌预测。论文首先基于股票单日回报率构 建数据标签并使用 SVM 对股价的涨跌进行预测；然后，为了提高股价涨跌的预 测准确率，论文基于差分进化（Difference Evolution，DE）算法对 SVM 中难以 人为获取最佳配置且对预测结果影响较大的两个重要参数：惩罚因子 C 和核参 数  进行了超参数优化，提高了算法的预测准确率。最终实验结果表明：相较于 经典算法，DE-SVM 的预测结果在各个评价指标上的表现均有提升。 （3）基于 CNN 和注意力机制的 LSTM 股价预测优化模型。针对长短期记 忆网络（LSTM）无法提取多维股票数据深度特征的问题，本文提出了 3CNNLSTM 网络模型，使用 3 个卷积神经网络（CNN）分别提取了股票基本属性、技 术指标和评论信息中的深度特征，基于矩阵融合进行特征合并，将融合特征输入 LSTM 增强了原模型的特征提取能力；然后针对 LSTM 隐藏层神经元在时间维 度上共享权重的局限性，论文基于注意力机制对神经元权重计算进行优化，提出 了 LSTM-A 网络模型。最后，将两种优化策略进行了融合。实验结果表明，相 较于 LSTM，论文提出的以上模型在股价预测效果上均有很大的提升。
2021,基于隐私保护的可监管法定数字货币研究,计算机学院,许佳佳,刘晓光,OS,0.2456,"区块链作为一种分布式存储技术，具有去中心化、不可篡改和公开可验证等特性，目前许多国家都在积极探索基于区块链技术的法定数字货币方案。然而，区块链技术在保证数据公开可验证的同时，却无法保护用户的隐私信息。目前已有许多可保护用户隐私的数字货币系统，但普遍无法隐藏交易之间的关联性，且这些数字货币系统在强隐私保护下不能提供有效的监管手段，会滋生如洗钱、恐怖融资等违法行为。此外，区块链技术要求系统中每个用户存储一份账本副本，以验证交易的有效性并保证账本的不可篡改性，但是随着交易记录的增多，系统用户的存储压力会增大，这极大地损害了系统的扩展能力。基于此，本文设计了一个既能保护用户隐私，又能支持监管手段的法定数字货币架构，主要研究工作有以下几点：


为了保护用户的交易隐私，本文提出一种表格式分布式账本隐藏交易参与者身份信息，使用Pedersen承诺隐藏用户的交易金额，并通过构建Bulletproof范围证明等证明使得验证者能够在交易参与者身份隐藏、交易金额为密态的情况下，验证交易的合法性。


为了央行能够对违法交易进行有效的监管，本文基于Bulletproof零知识范围证明提出了一个Bit可追踪Bulletproof零知识范围证明算法BTBuRP，BTBuRP算法在Bulletproof范围证明的基础上添加追踪密钥。当监管者追踪交易的具体金额时，只需要使用监管陷门以及追踪密钥即可追踪出Pedersen承诺隐藏的具体交易金额。


为了减轻交易共识者银行的存储负担，本文将所有共识节点都设计为半轻量级节点，即系统中的银行节点无需存储整个表格式分布式账本的全部内容，仅需存储账本部分列的交易历史内容即可。具体地，本文将表格式账本划分为多个分片，并设计了一个分片分配算法将这些分片分散存储在不同的节点上。当节点需要读取本地未保存的分片内容时，只需向保存该分片副本的节点进行请求即可。"
2021,面向社交问答文本的实体链接研究,计算机学院,王亚东,沈玮,NLP,0.3344,"社交问答平台中包含大量的社交问答文本（即，一个问题及它相应的多个回答），实体名字频繁地出现在其中。为了利用知识图谱所提供的大规模语义知识对社交问答文本进行更好的语义理解，我们需要将出现在社交问答文本中的实体名字映射到其在知识图谱中对应的真实实体，该任务被称为面向社交问答文本的实体链接任务。这个任务对知识图谱领域的研究具有现实意义并且能够服务于众多的下游任务，如知识图谱补全、专家发现、回答排序等。之前实体链接工作的重点是链接新闻文本或微博文本中的实体名字，而不是针对社交问答文本，他们并不能有效利用社交问答文本的语义信息及社交问答平台中的元数据（如主题标签、用户）来帮助实体链接。

       为了解决这些问题，本文提出了一个新颖的基于注意力机制的双层表征学习框架CQAEL，它能够以端到端的形式处理社交问答文本实体链接任务。该框架使用两个两级表示学习模型以及注意力机制来捕获社交问答文本的语义信息和元数据知识，通过整合利用待链接实体名字的上下文语义特征、元数据特征以及实体流行度特征来将实体名字链接到知识图谱中的对应实体。我们在一个手工标注的数据集和一个公开数据集上设计了系统的实验来测试框架的性能。实验的结果表明我们提出的框架在实体链接正确率上相对于九个基准方法都取得了显著的提升。"
2021,考虑可靠类间相关关系的多类数据特征选择方法,计算机学院,王振宇,卫金茂,SE,0.2662,"随着大数据时代的来临，海量高维的数据广泛存在于各个领域，数据的指数级增长使得数据集中含有大量冗余、不相关的特征，增加了机器学习模型的训练难度，降低了模型泛化能力。为避免“维度灾难”，特征选择能够有效地进行降维，成为数据预处理中必不可少的步骤。与简单的二类数据不同，多类数据的类别之间具有相关关系，不同类别在一定程度上相容或者互斥，传统特征选择方法多以“类间可分”作为评价标准选择特征子集，或是利用原始特征空间的不准确的类相关关系指导特征选择，这些做法破坏了数据的内在结构，给分类结果带来不利影响，并且造成对数据模型的理解困难。因此，在特征选择的过程中挖掘并保留可靠的类间相关关系具有重要的意义。


    本文首先回顾了特征选择的概况，接着介绍了三种现有的保留实例间相关性、保留类间相关性和保留类别分组间相关性的特征选择方法，分别介绍了其特点并分析存在的不足。针对目前研究中存在的问题，本文认为特征子集中类相关关系应尽可能地接近真实空间，而非原始空间，最后提出了一种考虑可靠类相关关系的特征选择算法（ERCCFS）。


    本文提出的方法通过两阶段方式交替进行，动态地同时学习类相似度矩阵 P和特征选择矩阵 W，在 O-stage阶段进行保留降维特征空间相关性的特征选择，并依据特征选择矩阵 W形成新的降维空间，在 C-stage阶段计算新空间的类间相关性，用各类特征的几何中心表示类中心，选用 RBF核函数度量相关性，指导下一次迭代中 O-stage的特征选择，重复此过程，直到满足设定的算法停止条件。算法是基于这样的思想：经过特征选择形成的降维空间会剔除部分冗余和不相关特征，由其计算得到的类相关关系较原始空间更为可靠，用降维空间的类相关关系指导特征选择会更为精确，因而，O-stage阶段的特征选择和C-stage阶段的类相关性计算两者之间相得益彰，互相助益。在添加噪声特征的人工数据集上，ERCCFS能够有效识别真实特征，降低特征的冗余度，提升算法的分类性能；在21个公共数据集上，ERCCFS与其他保留相关信息的方法相比，在多个分类器的多个指标中表现更优。"
2021,基于多变量AUC组合特征互补的特征选择方法研究,计算机学院,苏月,卫金茂,SE,0.2697,"随着大数据技术的飞速发展，许多领域的数据在规模和维度上都呈现爆炸式增长。庞大的高维数据中存在着的大量冗余和噪音特征易引发维度灾难，加剧数据处理和分析的难度，降低学习模型的性能。特征选择是一种有效解决上述问题的降维技术。最初的特征选择方法是通过度量特征的类相关信息，来选择类辨别能力最强的特征。但该类方法没有考虑特征间的冗余，可能会导致所选特征之间高度相关。后续的许多研究主要着眼于降低特征冗余，筛选与类相关性高而彼此相关性低的特征。然而，能够为目标类识别提供有用信息的特征不一定是不相关的，更可能是互补的。且这类特征选择方法忽略了被选特征子集整体上的相互合作对全局分类性能的作用。出于实际计算可行性的考虑，现有的特征选择方法均是成对地评估特征间冗余，可能会导致评估不准确。


针对现有特征选择方法存在的上述问题，本文基于ROC分析，设计并实现了一种有效的特征选择模型。本文关注的是特征之间的互补作用，即特征是否能够正确分类其它特征错误分类的实例。在互补特征的组合空间上，原来单特征维度上不可分的实例变的可分。基于特征互补性的思想，本文利用可能误分类实例转变为正确分类实例的过程，首次提出一种新的AUC计算方法以及多变量AUC。多变量AUC不仅可用于单特征评估，而且可以应用于特征子集的评估，将多个特征看作一个整体来评估其全局互补性和联合分类性能。该方法不需要任何的成对处理，成功地避免了因成对计算带来的评估不准确问题。在此基础上，本文提出一种基于多变量AUC组合特征互补的特征选择算法，将候选特征可以提供的新分类信息和原始被选特征子集保留的类相关信息均考虑在内，选取最具互补性的特征子集。在UCI和TCGA数据集上的大量实验验证了本文方法的有效性。此外，本文提供了一个与前列腺癌相关的基因集，并探讨其生物学意义。"
2021,基于三维点云的曲面工件表面缺陷检测方法研究,计算机学院,李宇萌,邵秀丽,Robotics,0.2797,"国家“十四五”规划中指出要坚定不移走质量强国之路，加快工业智能化进程。工厂企业要求实现工件表面自动化质量检测。目前，基于二维图像的平面工件表面检测较为简单，而曲面工件因其自身特有的深度信息而难以实现自动检测。因此，本文以鼓风机转子为研究对象，以检测工件表面凹凸型缺陷为目标，开展了基于三维点云的曲面工件表面缺陷检测方法的研究工作。


       针对传统基于点云的缺陷检测方法耗时长且精度不高的问题，本文将改进的点云分类和分割深度学习模型应用于曲面工件表面缺陷检测中。并且，提出先通过粗粒度缺陷比例预测判断缺陷程度，再通过缺陷分割完成点级凹凸型缺陷标注的研究思路。为此，本文开展了以下三方面工作：


    （1）曲面工件表面的三维重建与去噪插补：基于双目立体视觉搭建三维重建平台重建曲面工件表面三维点云。在根据主成分分析法对点云空间矫正后，采用直通滤波去除离群点，基于动态空间栅格去除噪点，并基于邻域统计的三次样条插值算法插补表面孔洞。旨在提高点云质量，为缺陷检测提供可靠的数据支撑。


    （2）基于融合投影-点集网络的缺陷比例预测：在缺陷分割前，基于点云切片策略构建数据集。针对PointNet只关注关键点且忽略多视角特征的问题，用平均池化替代最大池化，并融合曲面点云切片的方位信息进行多特征学习。同时提出包含投影镜像转换、密度划分和二维卷积模块的投影网络，并将融合网络命名为FPP-PreNet。融合网络将投影网络和PointNet得到的全局特征描述符聚合进而完成了缺陷比例预测任务，达到了判断曲面工件表面缺陷程度的目的。


    （3）引入偏置注意力机制的凹凸型缺陷分割：在缺陷比例预测后，视缺陷程度进行分割。针对预测模型只提取了全局特征而无法进行点级标注的问题，提出融合投影-点集网络的点云缺陷分割模型。同时，引入偏置注意力机制重新分配局部特征权重，并将模型命名为OAFPP-SegNet。分割旨在实现曲面工件表面凹凸型缺陷点级标注。实验结果表明，相对于PointNet，OAFPP-SegNet在精准率、召回率和F1-Score这三个评价指标上都有大约20%的提升。"
2021,面向推荐系统重排序任务的级联Top-­K推荐框架研究,计算机学院,曹丁元,宫晓利,SE,0.2837,"推荐系统已经融入人们日常生活的方方面面，经典的推荐系统算法将排序任务简化为点击率（或转换率）预测任务，将表现更好的项目放置在推荐列表中靠前的位置。然而很多现象表明，用户在一个集合做出一个决策（例如点击某个网页）是由整个序列决定的，即推荐系统要从序列角度来推荐一个最优的K组合。推荐系统重排序环节的目的就是将项目推荐的问题转化为组合推荐的问题。但是，由于寻找最优组合的解空间过于庞大，模型直接从候选集中找到接近最优解的推荐组合时，会产生巨大的采样和训练开销，这对于任何一个具有实时性要求的在线推荐系统都是难以接受的。


因此，本文针对推荐系统重排序中最优组合序列探索空间巨大的问题，提出了级联Top-K推荐框架（Cascade Top-K Recommendation Framework）CTRF，用于平衡重排序的准确率和效率。首先，本文针对重排序问题进行了两个前期分析和实验。对静态数据集在研究重排序问题上的缺陷分析表明数据集与评价指标的选择是模型离线实验的关键问题。真实的线上重排序实验表明，组合列表推荐效果与项目的精排分呈正相关，与项目之间的相似度呈负相关，为框架设计提供了思路与方向。接下来，本文分析了不同的重排序建模方法的探索空间，并提出了包含上下两层结构的级联Top­-K推荐框架CTRF。CTRF上层采用考虑候选集上下文信息的多臂赌博机方法，建模为搜索有限空间内最优参数组合的问题。CTRF下层采用更加精确的考虑项目顺序的模型，对排列顺序进行更加细粒度的优化，最终实现生成的组合接近最优组合的目标。


本文分别基于离线数据集和真实电商推荐系统线上场景实现了CTRF。实验证明，CTRF在离线数据集上指标均超过了基准，且在平衡准确率和效率方面具有优越表现。在真实的推荐系统场景（淘宝的“搜索发现”）里的A/B测试结果显示，相比于基准桶CTRF提高至少了2.16%的UV（访问人数）。此外，模型奖励设置的研究实验为模型奖励函数的选择提供了支持，并对强化学习实际应用中的奖励选择进行了讨论与分析。"
2021,一种基于统计学习的DGA域名检测方法,计算机学院,李颖,王志,NLP,0.2697,"随着互联网的飞速发展和大范围的普及，它已经成为了人们生活的重要组成部分，然而互联网为人们的生活提供便利的同时也面临着各种各样的网络安全问题，僵尸网络是目前网络安全的主要威胁之一。大多数僵尸网络利用域名生成算法构建隐蔽的通信网络，因此有效检测域名生成算法（Domain Generation Algorithms，DGA）的对于破坏僵尸网络、维护网络安全具有重要意义。


       相比于早期基于逆向工程、域名系统（Domain Name System，DNS）活动数据的检测方法，以域名字符串为研究对象的检测方法具有更强的灵活性和更广的应用前景。但这些DGA域名检测方法大多利用单一算法实现域名检测，对DGA域名的分析角度单一，并且传统的基于概率的决策方式在预测未知样本时存在偏差。


       针对上述问题，本文提出了一种基于统计学习的DGA域名检测方法，以域名为研究对象集成多种异构方法实现轻量级、实时的域名检测。本文从34个人工特征和神经网络高级特征多角度全面分析域名字符串，分别利用传统机器学习方法和深度学习方法检测域名。本文在Conformal Evaluator的基础上提出了一种基于统计学习的多模型决策方法，该决策方法根据样本与每一类训练样本子集的拟合情况度量样本相似度，并结合显著水平确定基础模型的预测标签，然后通过统计分析多个基础模型的预测标签，并利用投票、综合考虑置信度和可信度等策略生成最终决策结果。


       实验结果表明，基于统计学习的多模型决策机制可以有效提升检测效果，基于统计学习的DGA域名检测模型有更强的综合检测能力和鲁棒性。另外，将该检测方法应用于真实DNS数据时，该方法可以准确检测DGA域名，帮助网络安全维护人员锁定危险主机，在现实应用场景中具有很高的实用性。"
2021,一种基于多模型的无监督网络入侵检测框架,计算机学院,邓琮弋,王志,NLP,0.2765,"网络入侵检测是网络安全的一个重要研究领域。随着网络规模和复杂性的增加以及恶意攻击模式的不断更新，通过监控网络流量高效地捕捉各种异常行为变得越来越具有挑战性。真实网络环境下，恶意攻击行为产生的异常流量往往隐藏在海量的正常流量之中，在这种背景下，获取准确标记的数据极难实现。因此，无监督学习方法比传统的有监督学习方法在入侵检测领域的应用前景更可观。然而，受限于无标签数据下的训练机制，基于单一算法的模型检测能力和稳定性普遍欠佳。此外，高维的流量特征对检测器综合性能的影响也不可忽视。

       为了解决上述问题，本文提出了一种新颖的入侵检测框架。该框架支持多种基础无监督学习算法和可缩放的基础检测器窗口，在确保关键流程有效的基础上，保留一定灵活性，以适应不同的应用环境。本文主要的创新工作如下：

       1. 设计了基于多模型的无监督网络入侵检测框架。该框架提供更强的检测能力和可移植性，且不仅支持现有的基础无监督异常检测算法，对未来一段时间内出现的具有更好性能的算法依然具有适配性。

       2. 提出了基于统计思想的多模型决策机制。这一机制适用于集成多个异质基础检测器的检测结果，可以在最大化集成检测器的检测力和保证其对正常样本的识别能力之间达到良好平衡。

       3. 提出了一种基于后验知识的无监督特征选择算法。这种算法适用于大型数据，且可以为无监督的特征选择过程提供更强的可解释性，在保留足量数据信息的基础上剔除一些无关特征的影响，提高检测性能。

       在 NSL-­KDD 数据集上和 Kyoto 数据集上的实验结果显示，使用单一算法构建的检测器识别恶意流量的能力存在差异，且检测力难免会受到数据分布变化和数据环境变化的影响。根据本文提出的框架构建的集成检测器可以综合基础检测器的优势，在提高综合检测能力的同时，保证误报率和漏报率的平衡。并且，框架良好的可移植性为在不同数据环境中寻找入侵检测任务的最优解提供了可能性。"
2021,面向异构多核处理器的CNN流水线调度方法研究,计算机学院,闫美君,宫晓利,OS,0.2826,"在ARM移动平台上运行卷积神经网络变得越来越常见。卷积神经网络常用于计算机视觉领域，解决移动端的视频推理、连续的图片推理等问题。因此，提高卷积神经网络在移动平台上推理的吞吐量、降低其能耗是一个亟待解决的问题。ARM计算库可部署在移动端，是目前性能最高的深度学习计算库。它可以分离卷积神经网络的层，将不同层分给不同处理器执行，如CPU、GPU等。但ARM计算库默认将网络分配到一种处理器上执行，而其他处理器是闲置的，会浪费移动平台上有限的计算资源。


本文提出了一种卷积神经网络的层调度方法，将推理层调度到ARM异构多核处理器上运行，并通过流水线提高系统的吞吐量。本文提出三点技术的创新：（1）提出了一种基于决策树的性能模型，能够准确预测卷积神经网络主要构建层（卷积层、深度可分离卷积层和全连接层）在不同类型处理器，包括GPU、CPU大核、CPU小核及其组合上的执行时间，而不仅是卷积层和全连接层主要计算核心的执行时间；（2）流水线阶段的底层硬件是由移动GPU、异构移动CPU大核、CPU小核混合构建的。（3）利用该性能模型的预测结果并设计了一个流水线的调度算法，可以为给定移动SoC上的任何CNN推理模型构造一个优化的流水线来提高吞吐量。


本文在HiKey960上实现了此方法, HiKey960是一个基于ARM的SoC，由四个大的CPU核（Cortex-A73）、四个小的CPU核（Cortex-A53）和一个Mali GPU组成。本文采用了现有的五种CNN推理模型，包括AlexNet、GoogleNet、ResNet50、MobileNet和SqueezeNet来评估此方法。实验结果表明，与目前最先进的Pipe-it方法相比，本文提出的方法能够平均提高系统42.5%的吞吐量，最高可达51.9%。同时，本文比Pipe-it平均降低40.6%的能耗，最高可降低52.0%。此外，本文还从微体系结构的层面对方法进行了分析，以说明它可以提高吞吐量的原因。"
2021,基于智能处理器的深度神经网络量化压缩方法,计算机学院,刘方鑫,李涛,NLP,0.306,"模型量化压缩技术能够有效降低深度神经网络模型的尺寸，减少访存需求，因此近些年得到了广泛关注和应用。智能处理器这类神经网络专用计算芯片也纷纷支持或仅支持低位宽神经网络计算。但是由于它们通常仅有模型推理的能力，只能提供较为原始的离线量化算法。这样即使能利用低位宽计算提升硬件效率，但是会对很多网络造成一定的准确度损失，对智能医疗等精度敏感的应用十分不友好。另一方面，虽然各种量化算法层出不穷，但是很多量化算法没有考虑到这些常见的智能处理器的硬件计算原理，无法直接应用于这些设备上，需要自行设计推理计算单元或仅能在全精度操作数模拟条件下运行。

  针对上述问题，本文以国产思元系列智能处理器为例进行了一系列研究和探索：首先分析了智能处理器原始量化方法和量化模型计算原理，并且基于此设计了一种基于数据分布最小化量化损失的均匀对称在线量化算法（USOQ）。该量化算法不仅能够有效降低量化损失，而且适配底层推理框架的计算原理。为了将使用 USOQ 算法量化的深度神经网络模型部署到智能处理器上的推理框架上，本文设计了一个量化模型部署工具。该工具能够自动化的完成部署过程，屏蔽算法运行框架和底层推理框架之间的差异，高效且几乎无损地完成 USOQ 量化模型的部署。除此之外，该工具中还集成了推理优化技术，能进一步提升量化模型推理性能。实验评估结果显示，USOQ 算法相较于其他量化算法和全精度模型能有效地保持准确度，并且经 USOQ 算法量化后的网络模型在两款智能处理器上都能进行高效地推理，在与 CPU 推理 ResNet-18 全精度模型对比实验中，相同输入数据条件下推理时间缩减了约 224.9 倍。"
2021,基于机器学习的Fast-flux及良性域名检测技术研究,计算机学院,韩春雨,张永铮,NLP,0.2794,"域名系统（Domain Name System，DNS）是互联网的核心组成部分，承担着将域名解析为IP地址的重任。域名解析服务的应用范围日益广泛，与此同时，其也成为各类具有代表性的网络安全威胁的重要工具。僵尸网络作为最具有代表性的网络安全威胁之一，其通常使用Fast-flux技术提升自身隐蔽性，这使得对Fast-flux域名检测技术的研究具有重要意义。此外，形式多样的恶意域名也层出不穷，直接对种类繁多的恶意域名进行检测较为困难，因此考虑先进行良性域名检测，其余部分即很有可能是恶意域名。本文就以基于机器学习的Fast-flux及良性域名检测技术为出发点，并针对以往相关工作存在的问题，展开深入的研究和讨论。本文的主要研究内容及创新点如下：

       （1）基于地域特征的Fast-flux域名检测方法。传统的Fast-flux域名检测方法因在稳定性方面存在不足，导致其实际的召回率与精确率不高。为此，提出了一种基于地域特征的检测方法。首先，针对Fast-flux域名流量的特异性提出了流量异常过滤算法和关联匹配算法，提高了检测的稳定性；其次，提出了量化的地域特征及异常值特征，相较于传统的地域特征进行了更细化地处理，以更好地捕捉Fast-flux域名的标志性特点；最后，将传统检测方法使用的特征进行重新整合和特征选择，以训练检测效果相对更好的逻辑回归分类器。实验结果表明，本方法的平均召回率、精确率和ROC_AUC分别达到了99.86%、97.67%和99.29%，均高于传统方法，具有更好的稳定性。

       （2）基于深度神经网络的良性域名检测方法。传统基于流量的域名检测方法普遍因白名单不全面的问题而导致其实际的召回率与精确率不高。为此，提出了一种基于深度神经网络的良性域名检测方法。首先结合数据统计论证了本文提出的多途径收集方法所得到的白名单相对更全面、更准确，然后依据良性域名与非良性域名在时间序列特性上的不同，提出“时间戳向量”特征，结合提取的其他特征训练了深度神经网络检测模型并进行了调优。实验结果表明，本方法的平均召回率、精确率和ROC_AUC分别达到了97.6%、98.4%和99.15%，均优于传统的决策树、SVM等算法在相同流量环境下的表现。

       （3）基于交叉特征的轻量级良性域名检测方法。大部分传统的基于流量的域名检测方法都需要在DNS流量中获得大量目标域名请求记录的重复样本以提取相关的统计特征。然而，在DNS流量中普遍存在着很多仅重复出现过很少次的域名，这些传统方法在这种情况下会难以进行有效检测。为此，提出了一种基于交叉特征的轻量级良性域名检测方法。首先选取若干不同时间段的轻量级流量（指相同域名的请求记录很少且总长度很短的DNS流量）。然后，由于在此场景下难于通过多个域名请求记录计算统计特征，因此通过同一条域名请求记录的不同特征进行交叉计算得到交叉特征并通过实验确定在该场景下相对更有效的基本特征及相应权重，同时确定了较适合的机器学习算法为朴素贝叶斯。实验结果表明，本方法的平均ROC_AUC达到了95.64%，较好地解决了轻量级域名检测问题。

       根据上述的各研究工作，设计并实现了Fast-flux及良性域名检测原型系统，结合实际应用案例验证了该原型系统的可行性。"
2021,基于深度学习的FASTA基因序列压缩,计算机学院,崔雯雯,刘晓光,NLP,0.3388,"随着基因测序技术的快速发展，在测序速度变快的同时，测序成本也在大幅降低，因此，大量的基因序列被产出，但也带来了一些关于数据存储、处理和传输方面的挑战。传统的压缩工作主要是为文本数据而设计，并不能在基因序列上得到良好的压缩性能，所以目前急需一个有效的、为基因序列定制的压缩方法。本文针对基因序列的FASTA标准数据格式，设计深度学习模型结合算术编码的方法压缩基因序列。


由于FASTA数据格式包含标头和基因序列两个部分，本文对标头采用直接复制的方法，对序列部分则设计了基于深度学习模型的预测器，包括卷积神经网络、双向循环神经网络和层次注意力机制三部分，能有效挖掘基因序列重复特征，并为算术编码提供碱基概率进行压缩。针对由于引入深度学习模型而增加额外的存储、计算开销等问题，本文设计了基于知识蒸馏的模型压缩方法，从参数初始化和训练时的目标函数两个方面进行改进，以减小深度学习模型规模的同时保持较好的预测性能。


本文在多个真实FASTA数据集上验证了新算法相对于其他算法在压缩性能的优势，并且当数据集相似度越高时，新算法的压缩率越好。模型自身评估阶段，本文通过消融实验印证了本文针对基因序列特点设计的模型不同模块的有效性，通过注意力权重特征可视化验证基因片断对于碱基预测结果的不同贡献度。模型对比评估阶段，本文从训练过程和在测试集上的表现两个方面与最先进的模型DeepDNA进行对比，实验结果展示了相对于DeepDNA，本文模型收敛过程更加稳定、预测性能更好。最后，本文对基于知识蒸馏的模型压缩方法进行了实验，结果表明，本文方法可以在损失较少的预测精度的同时将模型大小减半。"
2021,多变元多项式快速乘法算法,计算机学院,王灿灿,苏明,SE,0.2399,"多项式乘法是符号计算中的一个基础问题，其广泛应用于公钥密码体制、天体力学和粒子加速器等领域。针对单变元多项式乘法，学者们已探索出多种成熟的快速乘法算法，比如FFT 和Toom-Cook 算法。为了加速多变元多项式乘法，我们采用基于多项式归约的多变元多项式乘法机制，其主要包括三个步骤：1. 将多变元多项式归约成单变元多项式；2. 利用单变元多项式快速乘法算法计算步骤１中所述单变元多项式的乘积；3. 由单变元多项式乘积恢复出所求的多变元多项式乘积。该乘法机制充分利用已有的、高度优化了的单变元多项式乘法算法和开源库，达到加速多变元多项式乘法的目的。


本文给出三种可逆的多项式归约算法，旨在降低归约后的单变元多项式的次数，从而减少后续单变元多项式乘法的运行时间。首先是迭代 Kronecker 归约算法，在标准Kronecker 归约的基础上，改变替换指数的选取方式，从而降低归约后的单变元多项式的次数；本文通过数学归纳法给出了归约后该次数的取值范围。另外，本文发现该算法的最优变元归约顺序具有特殊结构，大幅度降低了最优归约顺序的搜索空间，并据此给出一个启发式的归约顺序。其次本文给出CRT 归约算法，利用中国剩余定理将多变元多项式可逆地归约为单变元多项式。之后本文提出混合归约算法，充分结合迭代 Kronecker 归约和 CRT 归约两者的优势。另外，本文就计算复杂度和归约后多项式的次数两个方面对多种归约算法进行了理论上的分析对比。实验结果证明：混合归约下的单变元多项式次数可以降低到标准Kronecker 归约下的 3%，意味着步骤 2 中单变元多项式乘法的运行时间大幅减少；基于多项式归约的多变元多项式乘法比直接乘法和matlab 中的多变元多项式乘法都快。


本文将多变元多项式快速乘法应用到Gröbner 基的计算过程中。计算Gröbner 基的过程中含有大量的多变元多项式除法运算，本文将多变元多项式除法转化成乘法，从而利用多变元多项式快速乘法算法加速Gröbner 基的计算。"
2021,融合医疗偏差的电子医疗记录缺失值填充算法研究,计算机学院,欧阳嘉伟,袁晓洁,NLP,0.2882,"随着医疗数据规模的极速扩张与医疗数据研究的不断推进，电子医疗记录已成为承载巨量医学信息的通用数据存储形式。但由于数据收集、存储、传输等操作不可靠，电子医疗记录含有大量缺失值，这会严重影响医疗分析与研究。近年来许多填充方法被用于解决电子医疗记录缺失值问题，但它们普遍只根据数据的数值规律推断缺失值，未考虑医疗数据中的医疗偏差。医疗偏差是体现数据非随机缺失性的医学现象，其反映的非随机缺失规律与患者身体状况相关，可帮助填充方法实现更准确合理的填充。综上，融合医疗偏差的电子医疗记录缺失值填充对医学研究大有裨益，具有重要的理论意义与广泛的实用价值。


本文整理调研电子医疗记录缺失值填充方法的国内外研究现状，发现当前填充方法通常没有分析医疗偏差的缺失规律，并缺乏利用医疗偏差的有效策略。这导致医疗偏差在模型中无法充分发挥填充效用，对模型填充效果造成瓶颈。同时，现有填充方法的模型分析与缺失值填充过于耦合，严重限制模型的扩展。


因此，本文研究提出一种融合医疗偏差的电子医疗记录缺失值填充算法。本文搭建包含两个循环神经网络的联合模型，对医学特征与医疗偏差协同建模，以同时捕获医学特征的数值规律与医疗偏差的缺失规律。之后，本文构造一个承接的信息融合模块，利用信息融合策略关联数值规律与缺失规律，进而生成可代表患者身体状况的潜在表征。最后，本文实现一个解耦的缺失值填充模块，结合原始未缺失数据的数值信息与信息融合模块的潜在表征，以填充数据中的缺失值并生成完整的电子医疗记录。总而言之，本文研究的算法可以有效融合医疗偏差以实现准确合理的电子医疗记录缺失值填充，而且填充后的完整电子医疗记录有助于促进下游医疗任务的分析与研究。


本文在两个真实电子医疗记录数据集上执行大量实验。通过填充对比实验证明本文算法的填充效果优于目前国内外主流的填充方法，通过预测对比实验证明经本文算法填充后的数据集可以帮助各类下游模型实现最优的预测效果。除此之外，参数分析实验与性能分析实验可以全面地对本文算法效用进行分析。最后消融实验明确证实融合医疗偏差对电子医疗记录缺失值填充大有裨益。"
2021,基于自步学习的细粒度图像识别研究,计算机学院,伍小平,杨巨峰,NLP,0.2769,"细粒度图像识别不论在学术研究还是工业应用方面都具有非常重要的意义。受自步学习范式的启发，本文对人类从易至难的学习模式在细粒度图像识别研究中的应用进行了探索。众所周知，近些年深度学习技术极大地推动了细粒度图像识别领域的发展，然而仍有两点局限是不可忽视的。


首先是数据稀缺问题。深度卷积神经网络的训练需要大量标注良好的数据，而获取这些数据是既昂贵又费时的，因此从免费的网络数据中进行学习成为一种流行策略，但数据噪声的存在增加了学习的难度。为了缓解这个问题，本文提出了一种双向自步学习框架，通过有意义的学习顺序来降低噪声的影响。该框架包含两个基本步骤：a) 根据网络样本与有标签的源数据样本之间的距离，双向自步学习框架优先对具有更短距离的简单网络样本进行采样以形成新的训练集；b) 模型最初使用所有的简单样本和困难样本进行训练，以此获得更高的稳定性，然后随着训练过程的进行逐渐减少困难样本以减少潜在噪声对模型训练的负面影响。通过迭代地交替进行这些步骤，模型最终能够收敛到一个更好的解。


其次是数据分布不平衡问题。在众多细粒度图像识别任务中，部分罕见类别先天地存在数据稀少的问题，从而导致识别结果偏向于训练样本较多的类别。因此本文提出了一种自步平衡学习算法。具体地说，我们首先引入了一个综合的度量，称为图像类别的复杂度，它是样本数和识别难度的结合。其中，各个类别的复杂度由自步学习算法第一次迭代的模型进行初始化；接着我们给每一个类别分配一个惩罚权重，复杂度越高的类别惩罚权重越大，复杂度越低的类别惩罚权重越小；最后通过重新排列训练样本来重构自步学习课程，以此平衡每一步中各类别训练样本的复杂度。因此，自步平衡学习机制可以学习到平衡的具有高区分度的特征表示。"
2021,基于语义关联感知的知识图谱广义关系学习研究,计算机学院,张旭,杨征路,NLP,0.3342,"近年来，知识图谱因其能够为推荐系统、信息检索、问答系统等智能应用提供基础知识支撑服务而受到了学术界和工业界的广泛关注。当前的许多大规模知识图谱(如Freebase、Yago等)虽然已经包含大量知识事实，但还远远不到完备的程度，而且知识事实随着时间发展也在不断扩增。因此，面对始终存在的知识图谱不完整的问题，知识图谱补全任务成为了近年来研究的热点。


       知识图谱采用三元组的形式来表示知识事实，即(头实体，关系，尾实体)。知识图谱补全模型致力于根据已经存在于知识图谱中的知识事实，来预测不完整的知识事实中的实体信息或者关系信息。一系列基于传递、乘法或神经网络的模型被相应提出，并且在知识图谱补全任务上实现了不错的效果。但是，当前主流的知识图谱补全模型面临着两个关键问题。第一，{\jiacu{现实数据集中关系分布不平衡}}。关系所对应的训练样本数量往往呈现长尾分布，存在大量的关系在数据集中仅有少量样本，而模型需要依赖大量的样本对关系的嵌入表征进行学习，对于少样本关系而言，模型的学习能力则会受到限制。第二，{\jiacu{无法应对零样本关系}}。现实世界是在不断演变的，由于新出现的零样本关系在训练集中没有支持样本，现有模型将无法学习到其对应的嵌入表征，也就无法进行相关不完整事实的预测。这两个问题严重影响了现有知识图谱补全模型在现实智能应用中的适用性，也进一步限制了相关智能应用的性能提升。


       为了解决以上问题，本文提出应该对知识图谱中的多样本、少样本和零样本关系进行联合学习，即广义关系学习(Generalized Relation Learning, GRL)。本文认为关系之间的语义关联性可以成为连接不同类型关系的嵌入表征学习之间的桥梁，因此通过关系之间的语义交互来设计并实现了{\jiacu{基于语义关联感知的广义关系学习模型GRL}}，此模型可以作为一个即插即用型模块，作用于现有的知识图谱补全模型上，对多样本、少样本和零样本关系进行联合学习。GRL通过一个基于关系分类感知的损失函数，采用注意力机制使关系嵌入进行充分交互，最终将关系之间的语义关联嵌入到其向量表征中。通过GRL使得语义相似度高的关系在向量空间中距离更紧密，语义相似度低的关系距离更远，不同类型关系的嵌入表征将得到优化，最终提高模型在整体知识图谱补全任务上的性能。


       本文通过将GRL插入到两个基础知识图谱补全模型DistMult和ConvE上进行实验来验证所提出模型的有效性。首先在5个当前主流的知识图谱数据集上进行知识图谱补全任务，结果表明经过GRL增强后的基础模型在多个数据集上的MRR指标均有明显提升，特别地，DistMult和ConvE基础模型在所有数据集上平均提升了3.84个百分点和1.08个百分点。其次，本文还单独在少样本关系和零样本关系上进行了实验，在少样本学习任务中的实验结果证明了GRL模型能够有效处理不平衡数据，缓解了少样本部分对整体效果的影响，同时，在零样本学习任务中，GRL模型也展示了优越的学习能力。所有实验结果证明了GRL模型对多样本关系、少样本关系和零样本关系进行联合学习的有效性。"
2021,低通信量恶意多方隐私交集运算技术研究,计算机学院,卫煜,刘哲理,SE,0.2386,"安全多方计算技术是应用密码学的一个重要研究领域。在数据驱动科技和经济发展的当代，安全多方计算技术是解决独立机构间不互通的数据在分享，流通和使用中面临的数据安全和隐私问题的重要手段。本文关注的恶意多方隐私交集运算是安全多方计算技术中的最重要和基础的问题之一。当前恶意多方隐私交集运算主流技术面临通信代价高和可扩展性差的挑战。具体表现在通信复杂度和参与方的数量成平方关系；难以将恶意多方隐私交集运算作为其他大型安全协议子协议使用，适配性差。


    为了解决上述问题，本文引入了新的恶意多方交集运算技术。并且在该技术上提出了新的恶意多方交集运算方案TH-PSI。TH-PSI的通信复杂度与计算参与方个数、计算数据量多少线性相关，达到了精确集合求交运算的最优通信复杂度。TH-PSI在计算千万量级数据集的交集时，实际运算时间只需数秒，比当前最优方案快一个数量级。此外，TH-PSI可以扩展提供灵活高效的数据分析功能，更好的和现有数据分析技术结合。


    TH-PSI是一个基于密码学技术和可信执行环境的混合协议。基于对可信硬件的安全建模，本文为TH-PSI提供了完备的恶意安全安全性证明。在和当前最优密码学协议可比较的前提下，本文用实验证明了TH-PSI在通信复杂度，计算开销和可扩展性上有大幅的提升。"
2021,视频人脸表情对电影口碑与票房的影响效应研究,计算机学院,余伯恺,杨巨峰,NLP,0.293,"电影丰富了人类的娱乐生活，越来越多的研究者希望找到影响电影受欢迎程度的关键因素。传统方法在研究影响电影口碑、票房因素的过程中，大多倾向于关注观众评论中的情感、电影导演的知名程度等外部因素，而忽略了电影本身所传达的丰富的视觉信息。


本文选取能够凸显电影剧情氛围的视频人脸表情来研究其对电影口碑、票房的影响效应。基于Ekman情感类别模型，本文使用深度卷积神经网络识别视频中演员的表情类别，包括开心、惊讶、愤怒、恐惧、悲伤、厌恶和中性。然后通过统计各类表情在电影中出现的频次及其在对应电影所有表情数量中所占的比例来研究对电影口碑、票房的影响效应，我们计算不同类别人脸表情信息与电影口碑、票房相关性系数值。


  本文收集了由31部电影共6,757,200帧图像组成的Movie-31数据集，该数据集的电影覆盖了喜剧、科幻、恐怖等常见影片类型，涉及了亚、欧、美的众多国家和地区。此外，这些电影横跨上世纪70年代至今约半个世纪的长短片，并且在票房收入、评分、评分人数、时长等方面均具有多样性。我们在公开数据集FER-2013上进行人脸表情识别模型训练，然后利用OpenCV中的人脸识别模型逐帧地捕获Movie-31数据集中的演员面部图像，并使用卷积神经网络模型对视频演员面部图像表情识别，之后统计各类表情数量。本文通过计算不同类型表情占比与电影口碑、票房的Pearson、Spearman和Kendall相关性系数值及其显著性，研究演员表情对影片口碑、票房的影响效应。结果显示，生气、消极表情占比与电影评分呈显著性正相关，中性表情占比与电影评分呈显著性负相关；消极表情占比与电影票房呈显著性正相关。"
2021,基于同态加密的推荐算法隐私保护研究,计算机学院,王雅飞,贾春福,SE,0.2561,"在大数据时代，用户希望能快速在海量数据中筛选出自己想要的信息。推荐算法作为一种个性化分析算法具有广泛的应用场景。随着云计算的快速发展，数据外包计算愈发普遍，其安全问题愈发受到关注。相较于现有其他隐私保护方式安全性较低、数据后续使用性差等弊端。基于同态加密的隐私保护方案能够在保证数据安全性的同时不影响数据的后续使用，但其在应用过程中也存在有计算复杂度较高、支持运算种类受限、用户端开销过大等诸多问题。因此，针对上述推荐算法隐私保护需求和同态加密应用问题，论文主要工作如下：


（1）论文提出了一种使用同态聚类和代理重加密的跨域推荐算法隐私保护方案，实现了TOPT推荐和基于协同过滤的评分预测。方案解决了训练方与请求方不为同一主体的场景下的隐私保护问题，为实现跨域推荐的目的，方案设计了适用于基于近似数计算同态加密算法的代理重加密方案，并对其正确性与安全性进行证明。为实现数据拥有者上传数据后的离线状态，降低用户端开销，方案设计了适合该场景的安全框架，引入密码服务提供商。接下来，为辅助构建基于用户协同过滤的同态推荐算法，方案设计了密文判断协议、密文除法协议以及密文排序协议，并证明了协议安全性。在同态评分预测方案中，方案设计了同态聚类算法，对训练数据集进行预处理，降低后续同态预测的计算量，减少请求用户等待时间。


（2）论文提出了一种基于矩阵分解的外包推荐算法隐私保护方案，实现了用户未评分项目的评分预测。为实现数据拥有者上传数据后的离线状态，方案设计了具有密码服务提供商的安全框架。为进一步提高安全性，方案设计了数据拥有者在加密数据前对其进行的混淆运算。为降低运算过程中的通信开销，方案设计了由推荐服务器自主执行的同态矩阵分解算法。在迭代停止判断阶段，设计了有、无通信的两种判断方式，能够满足算法需求。最后，通过实验对方案的科学性和可行性进行验证，与现有方案相比，论文提出的方案将大大降低服务器与密码服务提供商之间的通信开销，具有较高准确率。"
2021,基于深度学习的眼底多疾病分类方法研究,计算机学院,李宁,李涛,Robotics,0.2777,"眼科筛查对眼科疾病的防治至关重要。临床上，眼科医生通常根据患者双眼的信息对其进行多种疾病的诊断，随着深度学习技术的发展，面向眼科疾病的研究为实现自动的眼科筛查提供了强大助力。但是，现有的眼科疾病研究工作大多基于患者的单目图像和一种疾病进行，同时，现存用于眼科多疾病研究的数据集严重缺乏。本文从双目多疾病数据集的提出到多疾病分类模型的设计展开研究，主要内容如下：


针对现存可用于多疾病分析的双眼眼底数据集缺少的问题，本文提出了一个包含8种标注的多标签数据集ODIR。该数据集包含来自5000名患者双眼的10000张眼底图像，此外，数据集还提供了患者年龄、性别、眼底诊断关键词等多模态信息。针对ODIR多标签分类任务，本文设计了一个双眼为输入的基准模型，使用九种常见的神经网络进行多标签分类，并提供可参考的基准结果。实验结果显示单纯地增加网络的规模并不能为ODIR带来分类性能的提升，且需要一种针对双眼特征学习的结构来提升多标签分类的性能。


针对基准模型在分类过程中未考虑双眼特征相关性的问题，本文设计了一个用于双眼特征交互学习的DBSPNet网络，通过在基准网络中嵌入DBSP模块，可以实现使用一只眼的特征信息指导另一只眼的特征学习。实验表明，将Vgg16、ResNet18和ResNet50作为主干网络，DBSPNet较基准模型在Off-site验证集和On-site测试集的平均Final-score上提升了2.43%、3.17%和1.04%。


针对基准模型在分类过程中未考虑每个标签独特性特征的问题，本文设计了一个基于单标签注意力特征学习的MANet，通过在每个标签分类前加入一个注意力学习模块，验证了不同注意力模块在不同主干网络上的效果。实验表明，将Vgg16和ResNet18作为主干网络，MANet相对基准模型在Off-site验证集和On-site测试集的平均Final-score上提升了2.925%和3.495%。


最后，本文设计了一个同时考虑DBSPNet和MANet具有互补性的集成模型MSLT-AI，较使用三种不同主干网络的基准模型，其在两个数据集上的平均Final-score分别获得了4.51%、7.16%和5.265%的提升。"
2021,基于图神经网络的服务质量预测方法研究,计算机学院,候晓磊,许静,SE,0.3137,"随着面向服务的架构在技术和内容层面的发展，基于服务的软件系统种类和数量均急剧增加。选择合适的Web服务以满足终端用户的需求和偏好逐渐成为一项重要且具有挑战性的任务。服务质量（Qualify of Service，QoS）作为服务选择的通用度量，被广泛的用于评估和描述Web服务的非功能属性（例如可靠性、响应时间等）。因此，近年来的相关诸多工作旨在研究服务选择中服务质量的影响，并提出了一些典型的QoS感知选择模型。然而，一来传统的基于协同过滤的QoS预测方法，作为其中最流行的方法之一，无法捕获到用户和服务之间的高维、非线性交互，并受到现实世界中数据稀疏性的影响；二来这些工作未能在选择准确性和服务多样性之间提供合理的平衡，未考虑到用户对服务的非功能属性的潜在偏好，导致选择列表中存在大量类似或者冗余的Web服务。


为了解决上述问题，本文从用户角度出发，考虑多场景下的QoS预测，主要贡献如下：


（1）针对协同过滤方法的局限性，提出了基于位置感知的图神经协同过滤服务选择模型（Location-Aware Graph Collaborative Filtering，LGCF），以面对数据特征较少，用户对服务多样性不敏感的场景。该模型通过引入用户和服务的位置信息，利用图结构的消息传播聚合机制捕获用户和服务之间的高维、非线性交互，以提高QoS预测精度，选择出符合用户隐式偏好的云服务集合。


（2）针对服务多样性存在的QoS预测精度低的问题，提出了多样性服务选择网络模型（Diversified Service Selection Network，DSSN），以面对数据特征较多，用户对服务多样性要求较高的场景。该模型引入了一种改进的基于图的卷积计算，以捕获用户隐式的服务偏好，提高QoS预测效果。接着，在此基础上提出了一种基于服务距离的注意力机制来处理服务多样性，从而使模型训练倾向于选择具有不同属性的服务。


为了验证所提出两个方法的可行性和有效性，本文在公共的真实世界Web服务数据集WS-Dream上进行了广泛的比较实验，并在多种数据密度下将原始数据集进行稀疏化，完成对响应时间和吞吐量两种QoS属性的预测。"
2021,对偶生成式富内容网络表示学习研究,计算机学院,李娜,刘杰,NLP,0.3301,"当前世界处于信息爆炸的时代，互联网积累了海量的数据，其中很多数据是以网络的形式存在，结构复杂且难以理解，比如社交网络、生物网络、信息网络。网络分析与应用任务的基础是学习到准确有效的节点的表示。因此，网络表示学习，也叫网络嵌入，是数据挖掘领域的一个研究热点和难点。


       近年来，研究者在网络表示学习任务上取得了诸多成就，包括保持网络拓扑结构的网络表示学习方法、考虑伴随信息的网络表示学习方法等。然而现有工作在考虑伴随信息的网络表示学习方法方面还存在一些不足。一方面，现有的方法通常考虑固定短距离范围内的邻居信息来学习网络的拓扑结构信息，难以捕获长距离的邻居信息，并且考虑邻居范围时依赖先验知识，不能自适应地学习网络的拓扑结构信息。另一方面，现有的方法在融合节点特征和网络结构信息方面存在缺陷，通常分别学习节点的特征表示和结构表示，两者的融合不够充分。因此，本文围绕着带有节点特征的网络（富内容网络）的表示学习任务展开研究，从节点特征和网络结构信息的融合出发，将网络问题转化为序列问题，采用编码解码框架来自适应地学习序列信息，具体的研究内容如下：


       首先，本文针对富内容网络表示学习任务提出基于对偶学习的富内容网络表示学习模型（Dual Learning based content-rich Network Representation Learning model, DLNRL）。本文对网络进行随机游走，将网络问题转化为序列问题，通过对采样的序列使用LSTM建模来自适应地学习网络结构信息。节点特征和网络结构信息被看作节点的内容和标识两种模态，采用机器翻译的思想，通过模态间的翻译过程实现信息融合。针对提出的节点识别和内容生成任务，实现从内容模态到标识模态的翻译和从标识模态到内容模态的翻译，充分利用两个模态数据的对偶性。此外，本文设计对偶学习框架，尝试了不同的融合方式，在损失函数加入超参数，使两个子模型更灵活地相互促进、相互学习。


       其次，为了使节点识别模型和内容生成模型融合更充分，利用两个模型之间的对偶性，本文在DLNRL模型的基础上提出模型级对偶的富内容网络表示学习模型（Model level Dual content-rich Network Representation Learning model, MDNRL），同时建模数据对偶性和模型对偶性，将两个子模型中功能相通的模块进行参数共享，减少模型整体的参数量，增加参数更新次数，使两个子模型融合更加紧密。


       最后，针对对偶学习框架存在的模型训练不均衡问题，本文在前两个模型的基础上提出基于自编码的富内容网络表示学习模型（AutoEncoding based content-rich Network Representation Learning model, AENRL），将模型进一步扩展为自编码学习框架，将两个子模型融合为一个模型，建模了从节点内容模态到节点标识模态的相互映射，并对模型中的功能相通模块进行参数共享，建模数据和模型级对偶性，实现模型各部分均衡训练。


       综上，本文以富内容网络表示学习中的拓扑结构和节点内容的融合为切入点，提出了一系列的富内容网络表示学习模型，通过建模融合节点识别和内容生成这两个对偶任务，实现两种信息的充分融合，学习到更有效的节点的表示。"
2021,基于迭代探索框架的遥感图像道路提取研究,计算机学院,谭永强,任博,Robotics,0.2848,"在遥感技术领域，从航拍图像中提取感兴趣区域（如房屋和道路）是一个非常重要的研究课题。其中，道路提取有着非常广泛的应用，如车辆导航、城乡规划、地质灾害应急等。过去依靠人工识别与标注的方式花费了大量的人力物力，而且在时间上也有很大的滞后性。同时，由于采集的遥感图像中有很多遮挡情况和噪声区域，且不同地区道路的形态和路面情况有很大的复杂性，所以在遥感图像中提取道路是一个巨大的挑战。为了能够让机器自动化地完成该目标，前人提出了一些启发式的传统方法，如基于种子点连线、分割区域中心线、区域模板匹配等方法，但这些方法往往无法完整地描述复杂多样的道路，所以识别率很难达到要求。


近年来，一种迭代探索式框架成为了研究热点。该框架利用神经网络自动化地提取起始点，裁剪图像中以起始点为中心的包围盒区域，与已探索的路径一起送入卷积神经网络，以预测下一步的起始顶点，迭代地构建道路图。该框架是一种全自动方法，能够端到端地构建道路的图结构。


基于迭代探索框架，本文提出了一种输出点分割图的模型设计方案，该方案包括了动态可变步长、分割线索指引和轨迹探索三种技术。为了减少从网络输出到预测顶点坐标转换之间的误差，本文使用了点分割图的输出形式，其中点簇的峰值点作为预测顶点的坐标。首先，基于点分割图的输出形式，本文将可学习的步长信息引入神经网络的训练过程中，以期望网络能够根据遥感图像和已探索的路径，精确地生成最有利于道路连接的预测顶点位置。其次，基于点分割图，本文还提出了轨迹探索技术，使网络预测顺序的多个时间步的顶点位置，从而增强网络对道路整体信息的感知和应用能力。最后，本文提出了两种分割线索方案，赋予网络对整个图像补丁中道路分布和交汇点分布的预测能力，从而增加网络预测顶点的可信度。


在实验部分，本文基于由谷歌地图和OpenStreetMap组织标注构成的公开数据集，证明了所提出各个技术方案的有效性，并在与各种最新技术的比较中取得了基于图结构的三种评测指标的最高性能。"
2021,基于主动视觉的野生物种检测与跟踪技术研究,计算机学院,代婉,辛运帏,Robotics,0.3303,"随着人工智能与机器人技术的持续发展，搭载视觉单元的自主无人系统工 作于野外环境，进行持续性、自主式遍历覆盖任务与跟踪任务已成为可能。在 由自主无人系统与视觉系统组成的主动视觉智能监测系统作业时，一方面需要 对运动平台与云台相机进行联合运动规划与控制，一方面视觉算法对野生物种 进行检测并反馈定位跟踪信息，其中智能系统自主感知、规划与控制算法间的 同步协调机制与整体性能优化是关键性技术。本文基于上述需求，从算法设计、 系统设计与实现方面开展基于主动视觉的野生物种检测与跟踪技术研究。主要 工作如下：


(1) 针对在野外复杂环境下，遮挡和相似性会造成数据整体性能下降的问题， 提出基于抗干扰网络的受遮挡及相似目标分割与识别方法。本文首先收集并制 作了野生物种标准数据集 Far_Eco；其次针对遮挡和相似性问题设计了网络结 构，该网络主要有两部分组成，一是跟踪网络 SiamFc，作为基础架构，用于跟 踪目标；二是显著性网络 EGNet，作为主体结构，通过对于目标的显著性特征 以及边缘特征进行提取。最后在数据集中通过实验验证所提网络结构的有效性。


(2) 针对在野外复杂环境下小目标的检测问题，提出了基于增强网络的野外 杂乱环境下小目标检测方法。基于 Far_Eco 数据集中目标物种的数据分布，进 行了锚框和层级优化。锚框的优化是通过 K­means 算法，得到最合适的先验框； 层级设计是出于对网络复杂性考虑；接着是对网络结构的设计，提出了特征增 强模块，在不改变检测尺度的前提下，增强了检测网络结构的语义特征，来提 高小目标的检测能力。最后通过实验验证了所提网络结构的有效性。


(3) 以课题组前期工作中目标检测、目标识别、目标跟踪成果为基础，通过 对于野生物种的特性进行分析与研究，设计了基于主动视觉的野生物种检测与 跟踪一体化框架，并设计了四个模块，分别是学习模块、目标检测模块、目标跟 踪模块、控制模块；然后设计了主动视觉的覆盖与跟踪控制任务；最后由于目前 还未在真实实验条件下做实验，所以仅在课题组开发的虚拟仿真平台 CyberEarth 上完成了模型构建与系统调试过程，用于测试所提框架结构的可行性。"
2021,视觉问题中弱监督信号的探索与利用,计算机学院,张宇,王恺,Robotics,0.2631,"弱监督学习技术旨在利用比较弱的监督信号构建机器学习模型。计算机视 觉中的弱监督学习的研究起源于人们对节省标注成本的迫切需求：细粒度的视 觉任务中需要的强监督标注成本非常高昂，如何利用低成本的数据标注来训练 细粒度的视觉任务成为近年来的研究热点。除了能够减少对标注的需求，弱监 督学习技术更重要的意义还在于，从弱标注甚至无标注的开放数据中，挖掘图 像或视频中的视觉信息，并基于这种视觉信息的组合与推理完成复杂的认知任 务。这对于让机器模拟人类认知过程，形成更加通用的智能系统至关重要。本 文关注计算机视觉中的弱监督技术，在图像和视频两个不同的场景中分别研究 了如何利用弱监督的数据标注挖掘像素和物体两个层级的视觉信息，并利用从 弱监督信号中挖掘的视觉信息进行更高层的认知任务。


在视频场景中，存在视频问答中的视频和句子的配对这一弱监督信息学习， 针对视频问答任务中难以进行物体级别场景分析的问题，本文提出利用这种弱 监督信息学习物体级别的视频区域和物体词语的对应关系。利用这种物体级别 的视觉信息，本文设计了一种基于物体的注意力图生成模块，该模块通过问题 中的物体词语引导，关注视频中的相关区域。为了关注到对回答起关键作用的 物体区域，本文还提出一个注意力控制模块，该模块通过将注意力在问题中不 同的词语之间转移实现视频关注区域的转移。在公开数据集上的实验表明了本 文从粗糙的监督中挖掘到的物体级别视觉信息的有效性。 


在图像场景中，本文研究了用户个性化图像的语义分割问题。个性化图像 是指来自同一用户的图像集合。针对现有方法无法利用用户图像的个性化特点 的问题，本文将图像属于同一用户这一弱监督信息具体化为用户图片之间的相 互关联性，并将用户的图片聚类成组内图像高度相关的图像分组。本文提出一 个组上下文语义辅助模块，在对组内图像进行分割时，通过组内图片之间的上 下文语义补充，为图片中每一个像素的类别判定提供辅助。本文还收集了一个 包含众多用户图片的个性化图像数据集。在数据集上的实验表明，本文的方法 能够有效地利用用户个性化这一弱监督信息，提高分割性能。"
2021,基于代码克隆的提高代码质量的应用研究,计算机学院,杨卉,许静,SE,0.382,"随着计算机软件使用量的增多，人们对软件产品的质量要求也在提高。同时随着计算机软件功能的复杂化和软件规模的不断扩大，代码质量对软件产品的影响也越来越明显。开发和维护作为软件生命周期中对代码质量影响最大的两个阶段，吸引了大量研究者的关注。在开发阶段，通常会通过代码推荐工具获取相关代码来提高代码质量，目前的代码推荐主要集中在API级别，但是在这类方法中，代码中的上下文信息没有得到有效利用，导致推荐的代码层次不高，功能不全。在维护阶段，开发人员通常基于用户评论对代码进行改进，目前基于评论的研究主要集中在那些拥有大量评论的软件项目（s-projects）上，然而实际应用中，大部分的软件项目是缺乏用户评论（f-projects）的。


为了解决上述问题，本文从开发者角度出发，考虑在开发阶段和维护阶段提高代码质量，主要工作如下：


（1）针对API级别的代码推荐方法中，代码的上下文信息利用不充分，推荐代码层次不高，功能不全的问题，本文提出了一种基于代码克隆检测的类级别的代码推荐方法，以在开发过程中为用户推荐高质量的类级别的代码。该方法将深度森林（gcForest）引入克隆检测阶段，利用其中的级联结构和多粒度扫描充分获取代码中的上下文和结构信息，并对其中的多粒度扫描模块进行改进，使其能够处理代码向量。


（2）针对大量软件项目及代码片段用户评论不足的问题，本文提出了基于代码克隆的用户评论共享方法，利用半监督的卷积神经网络模型在s-projects和f-projects中建立一个评论共享通道将s-projects的评论共享给f-projects，同时将评论分类和主题提取整合到评论共享框架中以获取更好的共享效果。


为了验证所提出两个方法的可行性和有效性，本文从Stack Overflow和Github上收集了大量的代码片段和用户评论，并在大型代码克隆数据集BigCloneBench上进行了代码克隆检测的比较实验，接着对代码推荐结果和评论共享结果进行了分析比较，完成对代码推荐和用户评论共享方法的整体评估。


关键词：代码克隆检测；代码质量；代码推荐；评论共享"
2021,基于微服务架构的电信SCRM系统的设计与实现,计算机学院,王科,邵秀丽,SE,0.2859,"随着新一代信息与通信技术的飞速发展与不断成熟，技术正以前所未有的力量驱动着数字化社会的形成，推动数字化转型已经成为全球电信运营商的战略共识。另一方面，电信运营商在行业内竞争愈加激烈的同时，还面临着来自互联网公司异常严峻的跨界竞争压力，数字化转型迫在眉睫。在这样的背景下，电信运营商开启了数字化转型之路。


    某省电信分公司从2019年开始数字化转型，打造了数据驱动的新一代CRM（Customer Relationship Management）系统，转型初见成效。但随着疫情的冲击，某省电信发现，需要通过加快线上线下一体化协同服务，以及建立稳定的、双向沟通的客户关系，来解决客户和企业之间连接不够稳固的转型问题。为此，本文在对相关理论和需求的研究基础上，提出了基于微服务架构的电信SCRM（Social Customer Relationship Management）系统的解决方案，并对方案开展了如下工作：


    首先在理论研究阶段，通过大量文献的研读，本文对SCRM系统和私域流量的定义、优点、实现方式进行了研究，在此基础上，结合企业微信提供的能力，提出了通过企业微信来沉淀线下线上核心客户到私域流量客户池，并进行精细化运营的电信SCRM系统建设思路。并对系统的软件架构和关键技术展开了研究，确认了使用Spring Cloud微服务框架的技术路线。


    接着在分析阶段，本文将电信SCRM系统的建设目标设定为构建私域流量客户数字化运营平台，通过对企业微信和SCRM系统的能力集成，打通线上商品及服务、线下渠道及人员、电信客户三者之间的联系，形成线上线下协同的面向私域流量客户的全流程闭环跟踪服务体系。然后将具体需求梳理为基础数据同步、移动端应用、运营管理后台、对外开放接口四大类功能模块，并从功能和非功能两方面对需求进行了细化的分析。


    最后在设计和实现阶段，本文对系统的功能架构和技术架构进行了总体设计，并对客户事件同步、私域流量客户池构建、内容库转发、客户画像查看、精准营销转发、扫码登录、私域流量客户查询这七个核心功能模块，采用结构图、流程图、时序图、E-R图等多种设计方法展开了较为详细的功能设计、数据库设计工作，最后对这些功能模块的代码和界面逻辑进行了开发与测试。


    通过本文的工作，目前基于微服务架构的电信SCRM系统已经成功在某省电信落地。经过初期的运营，系统实现了系统建设目标，取得了良好的效果，对解决某省电信数字化转型中出现的问题，开创社交化运营新模式，实现营销与服务的线上线下协同，从而助力企业用户规模及业务发展双增长做出了应有的贡献。"
2019,跨模态生成式网络嵌入研究,计算机学院,何志成,黄亚楼,ML,0.3136,"网络是一种表征能力极强的数据类型，普遍存在于各种研究与应用领域中，如社交网络、交通网络、生物组织网络等。与图像、文本等数据类型不同，网络结构数据（以下简称为“网络数据”）最大的特点在于样本之间存在拓扑关系，蕴含着丰富的语义知识。在当今大数据时代，随着数据体量的飞速增长，网络数据的拓扑结构日益复杂，包含的知识也日益丰富，传统的网络数据分析方法面临着复杂度过高、模型失效等问题。如何学习有效的网络数据的特征表示以便于后续分析与应用的开展，是当前数据挖掘与机器学习领域亟待解决的研究问题。


近年来，网络嵌入模型已经被公认为是目前最有效的网络数据表征学习方案，受到了研究者们的广泛关注。网络嵌入的研究目的是在有效保留网络结构信息的前提下，为节点学习低维的、连续的向量表征。基于学到的节点表征，成熟的机器学习算法可以被快速地应用到后续的网络数据分析任务中。然而，目前的网络嵌入研究还存在两方面的不足。其一，现有的网络嵌入模型不能自适应地学习拓扑结构信息。其二，缺乏拓扑结构与节点内容的融合学习方面的研究。


针对如上问题，本文研究富内容网络的表征学习，每个节点不仅包含表达其唯一的身份信息的节点标识，还包含丰富的内容。通过充分考虑结构与内容之间的语义关联关系，设计基于跨模态语义映射的表征学习框架，提出一系列跨模态生成式网络嵌入模型，解决拓扑结构的自适应学习问题以及结构与内容信息的融合学习问题，为进一步的网络数据挖掘与分析任务提供有效的表征学习方案。所提出框架具体表述如下：


首先，针对网络数据中近邻结构的自适应学习问题、结构与内容信息的融合学习问题，提出基于跨模态映射的生成式网络嵌入模型，称之为自我翻译网络嵌入模型（Self-Translation Network Embedding，STNE）。基于随机节点序列的网络嵌入模型是目前最流行的研究范式。在本文中，我们将随机节点序列拆分为平行的节点内容序列与节点标识序列，采用在机器翻译研究中最流行的序列到序列（Sequence-to-Sequence，Seq2Seq）模型来建模从节点内容序列到节点标识序列的自我翻译过程，从而实现跨模态的语义融合。一方面，STNE模型的双向长短期记忆模型（Long Short-Term Memory，LSTM）编码器能够无缝融合随机节点序列中的内容与结构信息。另一方面，LSTM的记忆机制使得模型能够自适应地学习不同尺度的近邻结构信息。基于跨模态的翻译机制，实现网络结构与节点内容信息的无缝融合。


其次，为了进一步促进网络结构与节点内容信息的融合，提高节点嵌入的质量，在提出的跨模态生成式STNE模型基础之上，研究网络结构模态与节点内容模态之间的对偶语义映射关系，提出对偶的生成式网络嵌入模型（Dual GEnerative Network Embedding，DGENE）。基于结构与内容这两种模态间的对偶性，制定富内容信息网络中对偶的跨模态生成任务：从内容模态到结构模态映射的节点识别（Node Identification，NI）任务，从结构模态到内容模态映射的内容生成（Content Generation，CG）任务。一方面，采用已提出的STNE模型来解决节点识别任务。另一方面，为内容生成任务提出一种新的基于Seq2Seq架构的层次化生成模型，建模从节点标识序列到节点内容序列的生成过程，为序列中的每个节点分别生成文本内容。在联合学习框架下，利用两个任务之间的对偶性与互补性，采用参数共享机制来实现模型之间的知识共享，深化不同模态信息的融合，提高节点表征的质量。


最后，针对现有的网络嵌入模型无法完整地建模节点的结构上下文的问题，研究基于平行网络的网络嵌入模型，提出从网络到网络的网络嵌入模型（Network-to-Network Network Embedding，Net2Net-NE），实现网络中高阶结构信息的自适应学习。包括STNE与DGENE在内，基于随机节点序列的模型采用随机游走算法来对网络结构进行简化处理，将复杂的网络结构转化为了定长的随机节点序列。这种“格式化”的操作虽然具有高效性和简洁性，但是也破坏了节点完整的结构上下文，忽略了网络中普遍存在的非序列结构。因此，为了更全面、更准确地建模网络结构信息，在平行序列的基础上，进一步提出平行网络的概念，即只包含节点内容而不包含节点标识的内容网络以及只包含节点标识的标识网络。然后，构建节点的自我网络（Ego Network）来建模完整的局部结构上下文，提出具有递归组合特性的自我网络编码器，实现从局部到全局的结构关系建模与融合，在整体网络层面学习从内容到结构的跨模态语义映射过程。


综上，本文以网络拓扑结构与节点内容之间的跨模态语义映射关系为切入点，提出了一系列富内容网络数据表征学习的新方法和新思路。具体地，通过建立跨模态的生成过程，实现结构与内容信息的深度融合，学习节点表征。有效节点表征是网络数据研究与分析的基础，目前的富内容网络表征学习仍处于初步阶段，有很多问题需要解决，希望本文的研究能推动这一领域的发展。"
2019,面向德州扑克的机器博弈平台服务器设计与实现,计算机学院,刘国勇,王玮,Security,0.277,"近年来，随着计算机技术的飞速发展，人工智能领域不断突破创新，作为其问题来源和检验方法的机器博弈研究受到了越来越多的关注。2016至2017年，谷歌公司研发的AlphaGo与李世石、柯洁的进行围棋人机博弈并连续获胜，宣告了在围棋领域计算机对人类的制霸，同时也掀起了全球范围的人工智能热潮。


伴随着机器博弈领域的研究发展，机器博弈平台开始出现、发展。其作为人工智能与机器博弈研究的重要工具，给相关研究人员带来更高的执行效率、更方便的操作方法和必要的规则评判，使机器博弈对局更加公平、公正和高效。


本文的主要研究内容为德州扑克人机博弈平台服务器系统的设计与实现。该系统支持多客户端并发访问，为相应客户端提供了创建对局、请求决策以及维护AI实例等功能。


本文首先根据功能和性能需求对服务器进行解构，将其划分为转发服务器和AI服务器两部分，转发服务器负责处理客户端的并发访问并将请求转发到相应AI服务器进行计算。之后，根据系统的扩展性需求，将转发表从转发服务器中独立出来，又把转发服务器分为转发服务器主体与数据库服务器两部分，使得转发服务器得以弹性增加或减少以适应不同的并发量需求。与此同时，为了便于控制和管理AI引擎，AI服务器也自然地划分为负责调度的AI管理服务器和负责运算的AI引擎服务器。本文针对上述四部分构成的多端分布式服务器架构，进行通讯方式和处理流程的设计和实现，使其彼此之间可以顺畅的交互，并且能处理客户端的各种请求。


此外，本系统设计和实现了两种内置的德州扑克决策算法。以蒙特卡洛模拟方法进行牌力计算，以此作为决策的依据；和先验行为树指导，将一位职业德州扑克牌手的翻牌前圈策略植入，来帮助AI进行决策。


本系统的实现可以为机器博弈研究者带来便利，同时也可以帮助德州扑克初学者快速入门，提高自身水平。"
2019,肿瘤消融系统三维重建关键技术研究,计算机学院,田松,辛运帏,ML,0.2344,"随着计算机视觉理论的成熟和计算机图形图像处理技术的快速发展，三维重建成为计算机视觉领域的热点研究方向之一，尤其是基于图像序列的三维重建技术。因其重建成本低、周期短、适用范围广特点，已被广泛应用到和图像三维模拟与重建相关的各个领域中。

本论文旨在对三维重建可视化技术进行精细化研究，以提高医疗影像三维重建视觉效果。通过小波变换技术与深度学习技术对医学图像进行前处理，之后再三维重建，实现了三维可视化效果的较大提升。

首先对医学图像存储文件进行研究，便于后续对DICOM文件的操作与修改，之后主要通过序列图像层间插帧技术与深度学习技术提高空间分辨率，通过深度学习语义分割技术区分医疗影像的组织器官，层间插帧部分应用小波变换技术对医学图像高频部分和低频部分分别插帧，并通过构建小型深度学习网络进行层间插帧。语义分割部分主要借助以u_net为基准的编码解码模式的深度学习框架进行器官、组织分割。之后分析对比了三维重建绘制效果较好的体绘制算法，同时研发了三维重建双界面系统。最后将层间插帧与语义分割前处理部分与三维重建相结合，提高了三维重建的视觉表现力。"
2019,基于多因素融合的招聘信息分析与应用,计算机学院,亢鸣哲,刘杰,ML,0.2957,"人-职业匹配是人力资源管理中很重要的一部分，统计及分析招聘信息的特点，捕获技能和职业之间的关联，实现人-职业匹配，可以满足招聘者和求职者双方不同的需求，具有实际的应用价值和广泛的前景。


本文以互联网招聘信息为研究对象，针对招聘数据，以深度学习框架为研究基础，进行信息提取、技能信息生成等任务，主要研究内容为：


首先，本文主要使用python语言搭建网络爬虫模型，获取相关招聘网站的招聘信息，作为本文的数据。在此基础上，构建具有半结构化特征学习能力的信息提取模型。本文的招聘信息数据包含职业名称、类型、公司、地点等结构化属性，也包含非结构化的职业描述信息。针对职业描述信息，本文主要使用BILSTM-ATTENTION-CRF模型，提取职业描述中的技能实体，将提取的信息作为后期建模的重要特征之一，有助于后续应用模型的建立。


其次，本文深入分析招聘大数据，深度挖掘数据之间的特性，并制作网页展现数据可视化操作。


最后，本文使用基于语义控制的文本生成模型框架，实现人-职业之间的匹配，可以满足招聘者和求职者双方不同的需求。针对招聘者，在给定相关的属性，如公司名称、地点、职业名称、类型、职责描述等，生成相应的符合市场规律的关于技能的任职要求，有助于减少招聘者对财力、物力、人力的投入，可以有效减少成本，具有很重要的现实应用价值。同时，求职者可以利用此功能生成更符合相应职位的技能简历，可以节省求职者大量的时间和精力，有助于求职者求职的成功，有助于人-岗匹配的发展。


本文的研究很有现实意义，成果能够应用在现实中，具有重要的商业价值、实用价值。"
2019,非平稳随机信号的数据处理与特征选择,计算机学院,刘亚静,殷爱茹,ML,0.2406,"在信号处理领域，越来越多的处理方法被应用到实际信号中，而在现实生活中提取或传输的信号多为非平稳随机信号，通常会先将其转化为平稳信号，再使用经典的信号处理方法进行分析，但大多效果不好。本文针对这一类信号，并结合该领域中的两个典型数据集，包括医疗信号和语音信号，进行信号预处理和特征提取方法的研究，为信号滤除干扰、去除噪声、增强有效信号以及信号特征的表征方式提供一定作用。


本文对非平稳随机信号的处理包括预处理和特征提取两个部分，并从时域、频域以及时频域三个方面进行分析，增强信号的处理机制，完善信号的表征方式，为信号分类识别的准确率提供保障。信号的预处理是要对未经处理的原始信号进行预先的处理，如进行信号增强以方便后续操作中的有效信息提取、进行信号滤波以消除噪声信号对有效信号的干扰、进行信号的端点处理以提高信号的分析和研究效率等。统计信号的共性、定位信号的差异，能够对某一类的信号进行更深入的研究，用特征向量表征信号也是信号处理的一项重要内容。本文采用三种特征提取的方法研究非平稳随机信号，包括基于小波变换、韦尔奇功率谱和希尔伯特-黄变换的方法，将这些方法更多地应用到实际信号中，可以提高信号表征的质量。


最后，本文设计并实现了两个子系统，原型系统实现了对一般非平稳随机信号的应用，将信号处理的诸多方法与之相结合，更符合学以致用的原则。另一方面，对本文的肺音信号做深入探索，将信号处理的流程融入肺音分诊平台中，不仅能够体现信号处理的价值，更具有一定的现实意义。"
2019,面向结构化数据的分析性文本自动生成方法研究,计算机学院,邓君怡,刘杰,ML,0.2882,"结构化数据具有规则且简单的数据格式，便于存储与记录，因此是很多重要信息的主要存储形式。但是由于此类数据缺乏语境信息，并且具有领域性强，数据量大等特点，因此不便于人类阅读以及理解。与结构化数据相比，文本形式的数据更符合人类的阅读习惯，因此实现结构化数据到文本数据的转换，可以辅助人们有效的理解数据内容，把握事件重点。然而现有Data-to-text工作的目标大多为描述性文本生成，虽然此类文本能够在一定程度上满足部分用户需求，但仅能做到信息的传递而不能带来任何增益。在实际生活中更有价值的是对结构化数据深层次的分析与解读，而这一部分内容在传统任务中并未体现。


针对上述问题，本文以表格为研究对象，提出了一种扩展的数据到分析性文本生成任务，并针对相关的数据集及建模方法展开研究，主要研究内容如下：


本文首先根据实际应用需求以及现有研究，提出了扩展的数据到分析性文本生成任务，该任务旨在实现结构化数据到分析性文本的自动生成。为了实现该任务的建模，本文根据任务需求构建了中英文两种数据集，并对数据集的爬取与清洗过程进行介绍，在此基础上通过一定的计算策略对数据的统计信息以及数据质量进行分析和评估，分析结果证明了数据集的适用性。


其次，本文从数据定性分析的角度出发，引入关于数据表类型的主题信息，提出基于主题感知的数据到分析性文本生成模型，从而在建模过程保持分析性文本与原始数据记录表的主题一致性。在此基础上，模型还引入了数值嵌入表示学习模块，从而更好地量化原始数据中的数值型特征，最后通过在真实数据集上的实验验证了模型有效性。


最后，为了提高生成的精准性，满足对结构化数据定量分析的需求，本文进一步提出基于细粒度主题信息的分析性文本精准生成模型。模型首先利用主题模型学习关于记录的主题词词表，其次通过上下文注意力与主题注意力相结合的方式计算文本生成概率，使模型在生成过程中考虑关于记录的主题词分布，从而提升用词的精准程度。在此基础上，模型还引入了扩展的复制机制，提升数值结果的精准程度，并在真实数据集上验证了该方法的有效性。"
2019,基于UWB的井下定位及数据传输系统设计与实现,计算机学院,赵洋,宫晓利,Security,0.2593,"随着计算机技术的快速发展，越来越多的领域开始逐渐地与计算机相结合，矿产资源开采就是其中之一。矿井开采和计算机技术相结合，能够保障工作人员和设备的安全，降低矿井塌方引起的安全事故。因此，对井下工作人员及矿车的高精度定位迫在眉睫。在空气湿度大、电磁干扰强的矿井工作环境中如何获取高精度定位是本文的重点工作，如何在矿井中减少工人的参与程度是本文的另一项工作。本文结合UWB定位精度高，抗干扰能力强，穿透力好的特点实现井下的高精度定位。并利用UWB数据传输速度快的特点，发送控制信息以达到自动控制矿车的目的。


本系统定位是基于测距实现的，使用对称双边双向对称测距方法，针对测距中产生的抖动，分别使用滑动窗口滤波，中位值平均滤波，卡尔曼滤波进行消抖比较。通过对比三种滤波的噪声值、RMSE（均方根误差） 和方差等指标，最终选择滑动窗口滤波对测距结果进行抖动消除。其次根据矿井下特殊的工作环境，对常用的三边定位法进行改进，并利用多个基站对标签UWB设备进行定位计算，减少定位误差。对最终的定位坐标x，y值分别使用滑动窗口滤波消除抖动，过滤掉计算极端值。


同时，利用UWB脉冲信号数据传输技术，设计一套适用于UWB网络的可靠传输协议。针对矿井UWB个域网的特点，设计发送数据和响应数据必需字段，并设计超时重传机制、重复接收机制等，保证在UWB组成的网络中发送的数据能够及时到达，从而达到对设备的远程操作的目的。


最后利用UWB定位和数据传输的功能，设计基站UWB的定位功能。保证所有的基站能够顺利的移动到指定位置，计算移动后的坐标，并更新移动后基站的坐标信息。


本系统使用的是基于DW1000芯片的UWB设备，采用MATLAB进行模拟仿真实验。系统分为上位机和单片机两部分，用C#语言开发上位机程序，C语言开发单片机程序。经过实验，在6米范围内，标签和基站间的测距误差控制在实际距离值的15%以内，标签定位的相对误差控制在实际坐标值的20%左右，并且在UWB网络范围内，保证所有数据传输都能正常到达。"
2019,基于改进 U-net 神经网络的显微图像微结构分割研究,计算机学院,王钰晴,赵新,ML,0.2533,"显微图像分析是显微操作技术和生物医学科学研究的重要手段之一，其中，显微图像中的一些微小结构，例如，卵母细胞显微图像中的细胞极体、大脑血管显微图像中的微血管等，这些微结构是图像中重要的研究对象。现有的图像分析方法大多基于以卷积神经网络为代表的深度学习模型，但是这些模型大多只用于检测和识别图像中的主体和显著性物体，而对小物体小结构的关注度不够，无法满足显微图像处理的需求。基于上述问题，本文对显微图像中的微结构分割方法进行了深入的研究。本文的主要内容包括以下几个方面：


      首先，针对现有神经网络对图像中小目标识别检测和分割的不足，本文提出了一种面向图像微结构分割的改进U-net神经网络。该网络通过将不同大小的卷积核组成的模块嵌入U-net神经网络，使得同一层次的神经元具有不同大小的图像感受野，从而更好的实现不同尺度物体的特征提取；在此基础上，通过设计不同的损失函数的组合来更好的训练网络。可视化实验表明，相对于原有的U-net网络，本文提出的网络能够更好的关注到图像中不同大小的物体，因而更适用于显微图像中微小结构的图像分析。


      其次，针对细胞核移植操作中，卵母细胞极体这一细胞微结构检测的需求，本文提出了一种基于上述改进U-net网络的细胞极体分割和检测的方法。首先，为了学习极体和背景之间的差异，本文将极体检测问题转换为利用神经网络进行二值分割的问题；其次，设计了多种图像变换方法以模拟更多拨动过程中的图像来扩充训练数据集；最后，由于神经网络分割可能产生多个可能区域，本文利用灰度和面积信息进行非极大抑制，以返回最终的极体位置，实现极体的检测过程。实验结果表明，本文方法在准确率和速度上都优于现有极体检测方法，同时，能够很好的应对细胞操作过程中出现的图像离焦、极体变形以及不同发育状态极体的形态差异等问题，因而更适用于实际显微操作的需求。


      最后，针对膜片钳操作过程中，大脑微血管这一微结构重建的需求，本文提出了一种基于上述改进U-net网络的大脑血管三维分割与重建方法。原始数据为针对同一血管不同深度下获取到的多光子显微图像切片序列。为了应对图像序列水平方向之间的偏移，本文首先使用基于塑性变换的模型对图像序列进行配准；其次，为了实现不同深度图像之间的融合和互补，本文将改进U-net神经网络扩充至三维，相应设计了不同尺度大小的三维卷积核，以学习三维微结构的特征，实现三维数据的分割；再次，针对分割后出现的独立的体素等噪声，本文采用三维中值滤波进行处理，并利用阈值化操作将体素数据分为血管和背景两部分；最后，对处理后的三维数据进行三维重建可视化。数据分割的指标和三维重建的效果表明了方法的有效性。"
2019,一种分布式内存缓存系统的高效容错架构,计算机学院,赵帅兵,王刚,Security,0.2611,"随着信息时代的高速发展，数据库系统应该为我们提供实时、准确和高性能的服务。为了满足人们对性能日益增长的需求，许多大型数据库应用开始采用分布式内存缓存系统来提高用户体验。其中，Memcached就是一种典型的分布式内存缓存系统。分布式内存缓存系统将用户经常访问到的数据存放在内存中，极大地提高了这部分数据的访问效率。然而，传统的分布式内存缓存系统没有容错能力。当系统中的一个节点发生故障时，该节点存放的数据将会丢失。用户如果想访问这部分数据，就需要重新从远端服务器或者磁盘中读取。


为了提升分布式内存缓存系统的可靠性和可用性，Cocytus将Reed-Solomon（RS）编码和分布式协议加入到了分布式Memcached中，使得Memcached具有容错能力。然而，Reed-Solomon编码涉及到复杂的有限域运算，因此计算性能较低；另外，在数据恢复过程中，Reed-Solomon编码需要大量的数据传输开销。


针对RS编码的两个缺点，在先前的工作中，已将Row-Diagonal Parity（RDP）编码加入到分布式Memcached系统中来替代Cocytus采用的RS编码。然后，又采用Row-Diagonal Optimal Recovery（RDOR）模型来加速数据恢复过程。


在此基础之上，本文又采用了Collective Reconstruction Read（CRR）模型从网络并行的角度对数据恢复过程进行加速。RDOR和CRR分别从两个方面对数据恢复过程进行优化。其中，RDOR恢复优化模型可以减少数据恢复过程中传输的数据量。而CRR恢复优化模型的理念是将串行传输变成并行传输，即由多个节点同时参与数据传输。它可以将数据传输过程分配到不同的节点上并行完成，以此来达到加速数据恢复的目的。实验表明，这两种方案在数据更新和数据正常访问的性能和Cocytus接近。在4个数据节点和2个校验节点的情况下，当网络状态较差时，采用RDOR模型可以比Cocytus降低25%左右的数据恢复时间；当网络状态良好时，采用CRR模型可以比Cocytus降低30%左右的数据恢复时间。此外，本文对将RDOR和CRR结合起来进行了可行性探讨，证明了将二者结合起来的性能与单独使用CRR的性能接近。"
2019,基于CUDA的系统发育分析库的设计与实现,计算机学院,黄小敏,王刚,Security,0.2754,"系统发育分析（Phylogenetic Analysis）是一种根据现存有机体的遗传数据重构生物进化历史的过程，对于人类理解基因的进化方式、监测和控制疾病的传播以及对物种进行鉴别和分类等都有重要意义。人类基因组计划的提出以及DNA 测序技术的发展所带来的大量高质量的基因数据进一步促进了分子水平的系统发育分析研究的发展。但另一方面，大规模的输入数据也对系统的计算能力提出了更高的要求。传统的串行算法乃至基于多核CPU 的并行算法已无法满足相关研究工作者对程序运行时间的需求，越来越多的研究者开始将目光转向具有高浮点计算能力和显存带宽的图形处理器（Graphics Processing Unit, GPU）。


目前已有一些研究工作将GPU 用于特定系统发育分析软件并行优化的实践中并取得了明显的加速效果。然而，可用的系统发育分析软件数量众多、受众不同，为每个软件分别设计一套合适的GPU 并行优化算法需花费巨大的精力，也不具备实际可行性。因而，设计并实现一个可被多种发育推断软件调用的高性能库具有重要的研究意义。


本文针对两种常用的系统发育推断方法——最大似然法和贝叶斯方法的共同计算瓶颈，开发了一个基于CUDA 的系统发育推断的并行优化库——CuPhylo。 CuPhylo 利用似然值计算过程本身蕴含的多层并发性，设计了一种多级并发算法：基于CUDA 流（CUDA stream）的高层次的并发、基于多操作内核函数（multi-operation kernel）的中间层次的并发以及基于CUDA 线程（CUDA thread）的低层次的并发。鉴于系统可能存在的访存方面的性能瓶颈，CuPhylo 对CUDA 线程的访存模式进行了精心设计并合理地利用了GPU 端具有低访问延迟的存储空间——共享内存（shared memory）和寄存器。在任务分配方面，CuPhylo 对各种不同的状态数采用了不同的分配方案以实现可并发线程数与线程任务粒度的平衡。为减少内核函数（kernel）内部的条件分支、提高执行效率，CuPhylo 可选地对状态数和位点数（输入序列长度）进行了填充。此外，CuPhylo 还对CPU-GPU 之间的通信进行了优化以减少额外开销。与CPU 的比较结果表明，在多种规模的数据集上，无论输入序列为核苷酸、氨基酸还是密码子，CuPhylo 均可以取得良好的加速效果。与另一个系统发育分析的并行优化库——BEAGLE 的对比结果表明，在状态数为4 （核苷酸序列）的情况下，CuPhylo 在规模较小的数据集上的优化效果不如BEAGLE，但当数据规模达到一定程度时，CuPhylo 相比BEAGLE 的性能优势开始逐渐体现。在状态数为20（氨基酸序列）的情况下，CuPhylo 只在序列长度最短的几个数据集上处于劣势，在其余数据集上的优化效果则远好于BEAGLE。当状态数为61，也即输入序列为密码子序列时，CuPhylo 的性能全面优于BEAGLE，并且最高取得了相比BEAGLE 3.86倍的相对加速比。"
2019,基于CT图像的不可逆电穿孔肿瘤消融系统的关键技术研究,计算机学院,赵倩羽,辛运帏,Security,0.2415,"随着计算机图形图像处理技术的发展与医疗水平的提高，计算机辅助外科手术作为外科介入治疗术的一个分支，得到了长足的进步。该领域的研究内容也在逐年得到拓展。目前，使用医疗影像设备可以采集到人体不同部位的二维横断解剖图序列。在此基础上，通过引入计算机辅助外科手术系统能够对采集到的数据进行解析观察从而确定病灶。病灶位置的确定有助于在实际手术前对手术路径进行预规划，在实际手术中进行实时的导航。同时，系统中将一组人体的二维横断解剖图序列数据转化为三维立体图像优化了系统的可视化体验，提高了手术实施的精准度，降低了由于医护人员经验不足等人工问题所导致手术失败问题的可能性，具有研究意义与临床应用前景。


“基于CT图像的不可逆电穿孔肿瘤消融系统”是一套可用于计算机辅助外科手术上的对肿瘤进行微创穿刺进而达到消融目的的软件系统，论文针对消融系统中的一些关键技术进行了研究。在系统中实现了提取腹腔CT图像，对腹腔CT图像进行解析，重建腹腔CT图像中的三维数据场，使用三维重建技术完成腹腔的三维模型建立。并在此基础上实现手动消融坐标点拾取，规划出最佳消融路径，形成手术预案。


本论文的研究工作主要集中在对CT图像的自定义读取与解析以及将其转化为清晰地三维图像上，并在此基础上研究出消融路径的规划。论文主要内容包括为首先从软件系统的总体设计出发，对系统工作流程以及设计结构进行了介绍，分为术前与术中两个阶段对系统进行了阐述。然后将CT图像转换为设备无关位图进行处理，实现CT图像的提取与解析，将二维断层信息转换为三维数据场信息。同时对三维数据场信息进行处理，将其转化为清晰准确的三维可视化图像。完成对人体腹腔器官清晰地三维重建。最后根据Observer/Command的交互方式对三维重建图像进行空间变换并改进了点拾取功能，实现通过屏幕手动拾取三维图像上的坐标点并将其转换为路径点，完成三维视图下的路径生成。"
2019,僵尸网络检测中的概念漂移识别方法,计算机学院,田美琦,贾春福,Security,0.3754,"僵尸网络已经成为网络安全的主要威胁之一。接入互联网的僵尸主机数量

巨大，僵尸家族变种繁多，越来越多的检测系统依赖于机器学习算法。机器学习的基本假设是训练数据与检测数据的分布一致。但是攻击者们在利益驱动下不断开发新的躲避技术，有针对性地对抗检测，例如模仿攻击、梯度下降攻击、投毒攻击、变更C&C 服务器等。这些对恶意代码的升级以及新变种的传播，会引发概念漂移现象。概念漂移导致测试数据与训练数据的分布不一致，破坏了机器学习的基本假设，造成检测系统的准确率随时间下降以及模型退化，机器学习本身成为整个系统中最薄弱的一环。缓解僵尸网络检测中的概念漂移问题对网络安全有重要意义。

本文提出一种针对僵尸网络检测领域的概念漂移识别方法。首先，区别于

传统的基于阈值的检测方法，本方法基于统计学习和用户给出的可接受最大出错概率，动态地对概念漂移样本进行识别，并指导模型重训练。其次，本文分析了僵尸家族变种中的概念漂移类型，主要针对常见但难以被发现的温和型概念漂移，将漂移范围限定在虚拟概念漂移之内，避免真实概念漂移的发生，从而有效缓解模型退化问题。最后，本文针对僵尸家族变种的特性，提出改进的一致性预测框架，通过添加聚类操作，提高分析精度，降低计算时间开销，并且缓解了原框架下存在的无法识别和错误识别某类样本的误报问题。

本文选取BotFinder 作为一致性度量方法，使用p-value 进行统计学习, 并基于可信度给出概念漂移分析结果。本文在五个僵尸家族的公开数

据集上进行实验，从时间开销、识别效果、检测效果三方面对识别方法的

效率和有效性进行了验证。实验结果表明本方法可以及早识别概念漂移样本，并缓解模型退化。"
2019,电力系统规划及故障模拟危机控制系统开发的可靠性研究,计算机学院,王振,陈增强,Security,0.296,"电力网络运行过程中，电网企业会在一定的时间对电网运行相关设备进行周期性检查和维修，或是在电网改建扩建过程中进行计划停电接火工作。随着电力供电网络的不断完善、发展，电网停电计划也在逐步增加，这种计划的电力中断属于预知的中断，在进行停电之前电力系统运维人员已将用电负荷进行转带，因此不会造成严重的电力损失，会将停电所带来的影响降低至最低。然而，电力系统运行过程中大部分停电属于不可预见的电力事故，常见的故障主要有接地、短路造成的电力供应中断，这种故障时间不可预知，故障地点及影响范围无法进行控制，会造成电力系统及用户的经济损失，严重的会造成区域电力系统运行稳定，甚至影响用户人身安全及电力设备的安全。


目前电力系统故障监测及事故信息确定主要依靠电力系统调度部门实现数据收集、汇总、分析及安排事故处理。通过故障监测与分析系统的研究，可以缩短电力系统故障处理时间，快速及时的恢复电力供应。故障监测与分析系统通常通过微机控制装置集成电网数据，硬件的整体设计主要包括主用CPU 系统、模拟量采集系统、信息串行通信系统、人机交互界面系统。通过嵌入式子站的软件设计框架，Data Server接口通信包、IED Data Acquisition Server接口通信包、Data Server任务管理包、IED任务管理包、SIADS任务管理包、时钟管理、EOS 管理等工作包实现了系统设计。


综上所述本人结合电网企业现状和电网供电可靠性运行的要求，对电力系统规划及故障模拟危机控制系统研发进行研究，通过系统的实际应用案例，进一步分析和查找系统优化提升的方向，进一步提高系统实用性、创新性、可靠性。"
2019,面向大规模网络的VoIP流量实时识别研究,计算机学院,王荣康,张建忠,Security,0.3044,"近年来，VoIP应用凭借其丰富的服务和低廉的价格已经成为了重要的通信服务手段。不幸的是，VoIP服务为人类带来便利性的同时，也造成了诸多的社会问题。诈骗分子利用VoIP所提供的便利的服务实施犯罪活动，对我国诈骗案件的侦破造成了极大的困难。我国反VoIP作案面临严峻的局势，为了使VoIP进一步服务于人类，对恶意VoIP的监管是必要的。


主流的流量识别方法包括基于端口的识别方法，基于深度包检测技术的模式识别方法，基于统计分析和行为分析的识别方法等。由于VoIP技术所涉及网际协议种类繁杂，数据传输普遍采用加密技术和P2P技术，这些特征导致以上分类方法很难单独应用于VoIP流量识别，目前较为成熟的VoIP流量识别方法一般采用多种方法结合的策略。另一方面，VoIP流量识别要求较高的实时性，这意味着基于VoIP流特征的离线识别方法不能被应用于在线实时识别。VoIP流量识别领域仍有很多问题亟待解决。


本文针对上述问题进行研究，第一，简要的从信令传输和媒体传输两个阶段介绍了各类VoIP技术，并归纳了各种VoIP技术的出现对VoIP流量识别所造成的影响；第二，本文围绕VoIP应用类型采用CLNN模型提取特征，旨在自动化的提取比人为提取特征具有更高识别精度的特征；第三，基于所提取的特征，本文设计并实现了一个VoIP实时识别系统，该系统采用Apache Storm作为流处理计算引擎，支持在大规模的网络中采集并识别VoIP流量。实验结果表明，该系统可以实时的准确识别VoIP流量。"
2019,基于LIBS光谱数据定量分析实验平台的研发,计算机学院,黄山,谢茂强,Security,0.2569,"根据世界卫生组织的数据显示，每年空气污染问题会导致我国数十万人死亡。我国的大气污染治理问题在世界上没有成熟经验可以借鉴，找到合适的办法解决我国复杂的大气污染问题具有重要的意义。其中，如果可以准确了解各污染物在大气中浓度的时空分布，可以为我们找到针对性的解决办法提供重要依据。

要了解污染物的浓度分布，可以依靠先进的对地观测技术获得其光谱，然后通过分析其光谱反演出气体浓度。但是用于大气污染分子成分和浓度测量的对地观测技术主要依赖于吸收光谱法，大气污染物成分复杂，形态多样，传统方法无法在现有对地观测技术的基础上准确分析其成分和浓度。为此提出了基于光丝激光雷达的办法去获得大气污染物的LIBS光谱数据，并通过分析LIBS光谱获得对应的组份浓度。为了顺利的推动方案，本课题基于该目标搭建了基于LIBS光谱数据定量分析的实验平台，主要用于以下几个目的：1分类存储整理不断获得的测量的光谱数据；2 批量合成LIBS光谱数据用于在前期相关光谱数据严重匮乏的情况下推进算法；3 探索不同的数据处理方法、算法在LIBS光谱数据上的综合表现。

本文实现了相关实验平台，给出了基于NIST LIBS数据库中光谱的标准数据给出了批量合成光谱数据的方案，并根据合成的数据探讨了其他类型光谱或是信号处理方法在相关的LIBS合成数据上的实用性。在合成数据的基础上，提出了名为DWCNN的光谱定量分析算法，并与其他多种对比方法比较证明了算法的可靠性与实用性。"
2019,基于分布式存储系统的纠删码解码性能优化,计算机学院,彭昊,任明明,Security,0.2732,"随着当今数据量的不断增大，分布式存储系统逐渐成为了存储数据的选择，同时，存储数据的可靠性称为分布式存储系统需要考虑的重点问题。而用于确保数据可靠性的方式一般有两种，一种是使用副本的方式进行保护，但是副本的方式会消耗太多的空间，另一种方式则是使用纠删码的方式，而纠删码虽然能够消耗较少的存储空间，却存在着计算开销等额外开销，很可能因为没有及时恢复损失的数据，从而导致数据的丢失。从而提高解码的速度就能够提升纠删码系统的可靠性，而根据调查发现，当今各种系统发生故障98%的情况都是单个节点发生故障，因此提升单故障的解码速度也就是重点研究对象。


本文将RDP码和PIT码实现到了一个自带RAID-6码的NCFS文件系统上，利用RDOR和PITR完成对两种编码的单故障解码优化，后面又通过采用连续读的方式提高两种编码在解码过程中读入数据的效率，从而提高单故障解码速度。利用NCFS文件系统中自带的RAID-6编码作为两种新添加编码的比较对象，在单机模式下和网络模式下分别对三种编码进行了实验，比较它们在不同的条件下的单故障解码速度。又针对采用连续读策略后的RDP码和PIT码在相同条件下进行了实验，比较它们相比于之前的单故障解码速度的提升，以及与相同条件下的RAID-6单故障解码速度的比较。通过最后的优化之后，与RAID-6有着相同容错率的RDP码能够在单故障解码速度上由于RAID-6，而比RAID-6有着更高容错率的PIT码，也能够达到接近RAID-6的单故障解码速度，甚至在某些条件下能够优于RAID-6，所以在需要高容错率和较快的解码速度的系统中，完全可以考虑PIT码。"
2019,基于层次LSTM模型的短文本流会话抽取技术研究,计算机学院,金毅,温延龙,Security,0.2708,"近年来，网民主要通过移动社交网络应用对社会热点事件和热门话题发表观点和评论。在此过程中，产生海量具有经济价值、社会价值的短文本流数据。由于具有稀疏性、交错性、动态性等特点，短文本流数据分析与挖掘极具挑战性，已成为自然语言处理、信息检索等领域研究热点。短文本流会话抽取是短文本流数据分析与挖掘的关键任务之一，旨在从短文本流数据中提取特定主题会话，以便于进一步从中提取有价值信息，为舆情分析等领域服务。因此，快速、有效地抽取短文本流会话，具有重要理论意义和实际应用价值。


本文深入分析短文本流数据会话抽取研究现状，在此基础上，充分利用短文本流潜在的时序特性，结合层次LSTM神经网络和时序短文本分类技术，提出一种基于层次LSTM神经网络的短文本流会话抽取方案。该方案对短文本流数据进行向量化表示，用于提取短文本流数据的时序特征；根据短文本流数据向量化表示识别会话内容的对话行为，有效判定会话整体语义；设计高效的主题分类器，将短文本流会话抽取问题转化成为时序标签分类问题；给出短文本流数据内部特征定义，将其作为主题分类器修正项，辅助提升会话抽取效果。本文在中文QQ群聊数据集和英文Twitter数据集上进行大量实验，实验结果表明本文提出的方案能够有效解决稀疏性、交错性给短文本流会话抽取带来的影响，在准确率上具有明显优势，可广泛应用于短文本流分析与挖掘领域。"
2019,基于复合先验回归与自反馈LSTM的大气颗粒物源解析研究,计算机学院,马若雨,李岳,ML,0.3284,"近年来，基于机器学习与深度学习的研究与应用层出不穷，相关研究者设计并实现了众多模型，以解决各个研究领域内复杂应用场景下的问题，并取得了令人瞩目的成效。在大气环境治理领域，大气颗粒物作为大气环境中化学组成最为复杂、环境影响最为巨大的污染物，一直是大气尘污染的重要的研究对象。为了有效控制大气颗粒物的污染，对大气颗粒物的来源进行定性及定量的分析与研究，大气颗粒物来源解析技术应运而生。本文尝试将机器学习与深度学习中的相关技术应用到大气颗粒物来源解析的研究当中，设计并实现定制化的解析模型，使机器学习与深度学习的优势在该领域得到充分的发挥。

本文选取单颗粒气溶胶质谱数据作为主要研究对象，该数据与其他大气颗粒物数据相比具有时效性强、时间分辨率高以及适用于不同种类与大小的颗粒物等特点。本文的研究内容主要包括以下几个方面：1）针对大气污染物缺失数据插值的Bagging算法研究；2）基于复合先验回归模型的单颗粒源解析研究；3）基于自反馈LSTM回归模型的单颗粒源解析研究。

在针对大气污染物缺失数据插值的Bagging算法研究当中，本文设计并实现了一个基于梯度变化率指标的Bagging模型，融合了包括CART插值算法、Bayes插值算法、ARIMA插值算法在内的3种插值算法，解决了不同数据分布类型与不同缺失情况下的复杂插值问题。在基于复合先验回归模型的单颗粒源解析研究当中，针对无源贡献度真值，模型效果评估较困难的情况，根据大气颗粒物来源解析研究的实际需求，提出并设计了准确性、稳定性、稀疏性和正定性四个评估指标，将源解析算法的评估工作具体化与量化，可以更直观地对源解析技术进行评估；同时，依据四个评估指标设计并实现了复合先验回归模型，其结果在四个指标上的表现较好，且与传统的相似度对比源解析法相比计算时长大幅度降低，资源占用大幅度降低。在基于自反馈LSTM回归模型的单颗粒源解析研究当中，使用循环网络结构对大气颗粒物受体数据的时序信息进行学习与训练，解决复合先验回归模型当中每个颗粒物受体独立静态地进行解析的局限性，同时，本文提出的深度学习架构当中的自主反馈机制使得模型具有对初始输入自动评估的功能，并将评估结果反馈到后续的模型训练当中，对初始输入结果进行优化，得出最终的颗粒物来源解析结果，使得该结果同时具有对时序信息、专家经验以及同步观测信息等多因素的影响，更符合实际环境情况，具有更高的可靠性。

关键词： 自反馈LSTM；RNN；复合先验；Bagging；颗粒物源解析"
2019,面向机器学习任务的边缘计算调度系统设计与实现,计算机学院,尹腾召,宫晓利,AI,0.3026,"随着计算机科学的飞速发展，机器学习成为目前最火爆的前沿领域，人工智能也开始走进寻常百姓家，影响人们的生活。由于机器学习是建立在大数据之上的，神经网络的训练、推理都需要强大的算力支撑，对硬件有着较高的要求。但目前市面上终端设备的运算能力、功耗参差不齐，这就使本地AI程序的大规模应用受到了极大制约，“边缘深度学习”所处的境地非常尴尬。为了解决这个问题，使AI应用走进千家万户，计算机科学家提供了许多解决方案。云计算的出现曾使人们看到了曙光，但其缺点也很快暴露无遗：脱机状态下无法使用、高功耗、隐私与安全问题、延迟高等。


为了将人工智能应用从云端迁移到边缘设备，各种AI加速器和高性能显卡如雨后春笋般出现。与显卡相比，AI加速器在性价比、易用性等方面具有明显的优势，适合与低性能家用终端配合使用，其低功耗、即插即用、扩展性好的特点很好地解决了边缘设备运算能力不足的问题，Intel于2017年发布的Intel® Movidius™ Neural Compute Stick（NCS，即神经计算棒）便是一款针对低性能边缘设备的AI加速器。本文提出了“低性能设备+NCS”的方案解决边缘设备算力不足的问题。


为了研究NCS在边缘深度学习领域的前景，本文对NCS的性能和并行性进行了测试，分析神经计算棒取代显卡的可行性；然后根据我们测试所得出的结论，设计并实现了基于神经计算棒的任务调度系统，实现任务的动态分配，探索了系统所能发挥的最大效率，使低性能边缘设备具备了处理大规模运算任务的能力；接下来测试了算法在不同负载下的性能，验证算法的有效性，结果表明调度算法可以有效提升系统在不同负载下的性能，达到了预期目标。 


最后本文将算法整合到系统中，设计并实现了任务调度系统。"
2019,基于机器视觉的传送带拥堵检测系统的设计与实现,计算机学院,骆圣丽,邵秀丽,Security,0.246,某制药企业的自动生产线上有一道生产工序是采用多股道传送带进行药盒传送，在传送过程中会因为摩擦力等原因造成药盒拥堵情况，需要采用智能方法来替代人工干预，因此，有必要搭建传送带拥堵检测系统，缓解传送带拥堵问题，从而辅助企业进一步实现智能制造。 为此，论文开展了生产线传送带上药盒拥堵检测与拥堵均衡问题的研究，采用机器视觉技术对传送带工业生产图像进行预处理和目标检测，基于目标检测结果设计了拥堵检测策略和拥堵均衡策略，完成拥堵调度。首先，为了减少原始图像中的噪声等无关信息，本文对图像进行预处理，采用灰度化、腐蚀化、二值化等方法增强图像数据的可检测性。其次，论文提出了基于边缘检测的药盒定位策略和基于灰度取反的空位识别策略，完成动态背景下的药盒与空位检测，并基于OpenCV中的cvBox求取最小外接矩形，用不同颜色的矩形框标记出药盒与空位。另外，论文设计了基于空位分布的拥堵检测策略，判断每条股道的拥堵情况，并确定最拥堵的股道以及股道上待移出的药盒Move_Pack。最后，给出了拥堵均衡策略，即把最拥堵股道上的待移出药盒Move_Pack移动到最空闲股道的待移入空位Move_Vacancy上，确定调度移动路径。此外，还将本文提出的机器视觉目标检测策略与基于Faster RCNN深度学习网络模型目标检测算法的实现结果进行了对比，在准确度和实时性上，机器视觉目标检测策略的实现效果均会更好。基于机器视觉的传送带拥堵检测系统的硬件由机器视觉系统、预警装置、工控机、调度机械手四模块组成，分别用于图像采集、拥堵检测和调度执行，工控机上部署软件系统，进行图像处理，并进行信号传输。 此外，论文还设计实现了包括参数设置、图像预处理、目标检测、拥堵检测、拥堵均衡等软件功能模块的传送带拥堵检测系统，并针对系统的主要功能，设计了相关测试用例，完成了对系统的功能测试，运行效果良好。
2019,Android恶意代码的多模型协同分析系统,计算机学院,朱文焌,王志,Security,0.3539,"随着智能化设备的普及,以智能手机为代表的移动设备逐渐成为生活的必

需品,极大地提高了人们的沟通效率。当前,Android 是主流的手机操作系统,

占据了大量的市场份额。由于 Android 手机用户的不断增多,Android 应用的数量也呈现上升的趋势。人们可以从 Android 应用商店免费下载、安装各种应用程序,甚至安装了一些存在安全威胁的恶意应用。为了防止 Android 恶意应用窃取用户的隐私数据,研究者提出了大量的 Android 恶意应用检测方法。但这些检测方法大多是依赖于单模型,无法保证模型在一段时间后不会出现性能衰减的现象。


针对单模型的分类精度随时间下降的问题,本文设计了 Android 恶意应用的多模型协同分析系统。该系统利用了一致性评估的方法,在 SVM 和 XGBoost模型的基础上实现的协同分析,与单模型相比,突破了传统机器学习“是”与 “不是”的学习模式,能及时发现模型退化的现象,并采用有效的措施缓解这种现象。

本文的实验结果显示:多模型协同分析系统好于单个模型的性能,并且能

够缓解模型退化的现象,为 Android 恶意应用的检测提供了新的思路。除此之外,本文还对协同分析系统的预测结果进行了合理的解释,可以帮助研究者加深对恶意应用行为的理解。最后,编程实现了多模型协同分析的原型系统。该原型系统可以应用到实际的检测环境中。"
2019,基于SDN多交换机的流量异常检测与处理,计算机学院,靖桠楠,徐敬东,Security,0.3315,"分布式拒绝服务作为一种常见的网络攻击方式，具有分布性、伪装性、种类繁多、攻击门槛低和攻击效果显著等特性。在传统网络中DDoS攻击的防御主要是通过引入第三方插件、设置专门的流量清洗平台以及在设备端进行流量过滤等机制实现的。这些方式成本高、可视化程度低、灵活性较差。


软件定义网络能够通过编程的方式动态获取网络状态信息且快速实现应用的需求，具有可视性和和编程性。它的核心架构将网络转发层和控制层分离，网络的控制权交给控制器，解决了传统网络的诸多隐患。但SDN仍然面临着DDoS攻击的威胁。


本文结合DDoS攻击与SDN架构的特点，采用端口速率和传统的熵值算法来检测网络中流量的异常情况。本文的主要内容包括：


本文介绍了DDoS攻击和SDN的相关背景知识。首先介绍了SDN的框架、OpenFlow协议以及SDN控制器，其次介绍了DDoS的攻击原理以及在传统网络中DDoS攻击预防的方式。


利用OpenFlow协议采集网络拓扑信息和端口速率信息，通过获得的信息建立主机白名单，同时当端口速率出现异常时，触发异常检测模块。本文提出在SDN网络中基于熵值的流量异常检测方法。在多交换机中利用SDN控制器从流表项信息中提取关键信息，采用信息熵的方式对信息进行处理，获得熵值。通过将熵值与设定好的阈值进行比较完成攻击的检测。在异常缓解阶段通过控制器下发新的流表项，来控制数据包的转发。


在Mininet仿真平台中搭建SDN网络环境。本文采用Ryu控制器，并在其上部署了DDoS攻击检测与缓解的方法。实验中使用Scapy模拟正常流量与异常流量的产生。通过仿真实验对结果进行分析评估，验证了文本提出的DDoS检测与缓解方法的正确性与有效性。"
2019,基于动态方向接口模型的车载命名数据网络传输协议研究,计算机学院,唐莉莉,许昱玮,Security,0.2705,"随着无线通信与传感技术的发展，车联网(Internet of Vehicles, IoV)的概念应运而生。近年来随着车联网应用需求的爆发式增长，如何在车载环境下实现高质量的数据传输是IoV发展亟待解决的问题。现有的从无线局域网络演变而来的IEEE 802.11p以及从移动通信网络发展而来的3G、4G、5G技术都不能很好的为车联网提供高质量数据传输服务。近年来，命名数据网络(Named Data Networking, NDN)的提出受到了业界的广泛关注。NDN所具备的以数据为中心、多源多路径、网内缓存等特点，使得其非常适合于解决车联网中的数据传输问题。车载命名数据网络(Vehicular Named Data Networking, VNDN)就是在当前背景下提出的。目前，相关的研究方兴未艾，前期的研究工作忽略了接口在命名数据网络中的关键作用，从而易造成数据查询效率低、传输开销大等问题。因此在城市场景下，实现高效的命名数据传输是一个既重要又极具挑战性的问题。


      针对以上问题，本文在深入分析车辆GPS运动轨迹的基础上构建了动态方向接口模型(Dynamic Directional Interface Model，DDIM)。本文研究工作分为以下四部分：(1)为了确保合理性，本文对北京、武汉以及圣迭戈三座城市的出租车GPS轨迹数据集进行处理分析，总结出规律以支持动态方向接口模型的划分。(2)本文针对车载命名数据的传输需求提出了DDIM，并以此为基础设计了新的命名数据传输协议。该协议沿袭了原始NDN的设计方案，本文在此基础上进行适当的修改，以保证兴趣包以及数据包能够依据方向接口实现高效的转发。针对“先听后发”策略的不足，提出一种改进的基于延迟的分布式广播算法，以减少网络开销与命名数据传输延迟。(3)为了保持转发信息库(Forwarding Information Base, FIB)与挂起兴趣表(Pending Interest Table, PIT)的有效性，提出了对方向接口的重映射操作，同时对FIB以及PIT提出一种管理方案。(4)为了验证所提出协议的有效性，本文采用基于ns-3的ndnSIM(Named Data Networking Simulator)和WAVE(Wireless Access in Vehicular Environments)模块与SUMO(Simulation of Urban Mobility)仿真软件结合的方式进行仿真实验。对比实验表明，本文所提出的协议具有较高的兴趣包满足率、较低的传输延时和较少的跳步数。"
2019,基于密集子图发现的交通拥堵预测方法研究,计算机学院,邢雨菲,张海威,Security,0.2579,"摘  要


随着城市道路的不断发展和人们出行需求的日益多样化，交通预测技术已成为“互联网+交通”领域的热点研究问题，在“互联网+”的时代背景下，具有重要的研究意义和应用价值。


论文将面向动态交通网络，对于拥堵区域的预测方法展开研究。论文的工作可为车辆及行人提供更加科学的出行参考，方便人们的日常出行，疏导车流人流，为繁华路段减小交通负荷，有效避免拥塞，从而达到安全出行、高效出行、绿色出行的目的。


目前，已有的研究主要集中在交通流预测、交通拥堵预测以及交通状况影响因素预测等几个方面。本文以实时变化的动态交通网络为研究对象，以预测未来短时间内可能发生拥堵的区域为研究目标，设计有效的解决方案。该方案由交通流预测和密集子图发现两个阶段组成，首先，将动态交通网络抽象成一个动态图，并针对图中的每条边，利用K近邻（KNN）算法筛选出对其影响最大的若干条相邻边；然后，构建长短期记忆网络（LSTM）模型，将筛选后的边和目标边按一定比例加权作为模型的输入，对交通流进行了预测，预测结果作为未来短时间内的交通流；最后，在交通流预测结果的基础上，利用优化的密集子图发现方法计算出可能发生拥堵的区域。


论文分别在真实和人工合成的数据集上进行了实验，并与一些传统方法进行了对比。实验结果表明，本文设计的交通拥堵预测方案中，KNN和LSTM相结合的方法针对时间序列预测具有较高的准确率，优化的密集子图发现算法可以大幅度减少时间代价，能够发现质量较好的密集子图，并且具有一定的可扩展性，因此本文的工作能够有效预测动态交通网络的拥堵区域。


关键词：交通预测；交通流预测；神经网络；密集子图发现"
2019,基于改进的指令集随机化的软件安全保护方案,计算机学院,程晓阳,贾春福,Security,0.3835,"软件作为IT产业最主要的资产形式，在信息技术发展的数十年内受到了极大的冲击。本文为应对目前对软件产生威胁的一些攻击模型，设计实现了新的软件安全保护方案。


在软件安全保护领域中的经典威胁模型——MATE（Mate-At-The-End）攻击模型下，攻击者作为合法用户获得软件的控制权，以此为基础对软件进行恶意分析和篡改。代码虚拟化技术在对抗这种软件侵权行为中发挥了不可替代的重要作用。而近年来针对代码虚拟化的破解策略也层出不穷，与此同时，本文也发现目前的代码虚拟化技术有一个共同的缺陷——虚拟代码和本地代码之间的映射是静态产生的，并且进一步针对这一缺陷设计并部署了一个以频率分析为基础的攻击，实验证明目前的代码虚拟化技术在抵御这些攻击时显得捉襟见肘。

本文将指令集随机化和密钥共享的思想相结合提出了一种新的代码虚拟化技术\name 以更好地应对这些攻击。实验证明\name 能够以很小的时间和空间成本作为代价，抵抗目前文献中针对代码虚拟化保护方案的攻击方式和本文首次提出的频率攻击模型，同时\name 也让反向分析变得难于进行。


而在当前的实践中，很多软件也在遭受着远程攻击所带来的巨大损失。本文在代码虚拟化新技术的设计中将指令集随机化方案进行优化，其优化结果是只有合法的控制流可以获得指令集随机化的映射规则，这在思想上与保护程序不受远程控制流劫持攻击的控制流完整性的目标较为一致。利用优化的指令集随机化这种性质，本文将其应用于控制流完整性的保护——分别以程序内置VM和二进制动态分析框架PIN作为执行环境，设计并实现了控制流完整性的保护工具\namee 的两个版本。实验证明，两版\namee 相较于目前文献中基于插桩的CFI保护方案都能提供更高的安全保护，与此同时产生的开销也是可以被接受的，两版保护工具各自更为合适的应用场景，用户可以根据不同的需求选择不同的版本对软件进行保护。"
2019,基于外部知识库的多注意力机制短故事理解的神经网络方法研究,计算机学院,李茜,杨征路,AI,0.3256,"人工智能的飞速发展给人们的生活带来了巨大的变化，人工智能因此成为了当前研究的热点。虽然大量的研究人员参与到了人工智能研究中给该领域带来了巨大的推动，但是如何解决机器常识理解仍是该领域尚未解决的难点。本文主要对基于常识理解的短故事理解方向短故事结局填空（Story Cloze Test，SCT）这一任务进行了研究。


       SCT这一任务旨在测试机器对于故事理解的能力，具体的该任务要求机器通过给出的前四句故事情节从候选的两个不同的故事结局中选出一个符合常识和逻辑的正确的结局。这一任务不仅需要机器具有对故事语义的理解能力还需机器具有一定的逻辑推理能力和常识知识积累。


       本文通过神经网络的方法，一方面利用多种注意力机制优化模型故事语义理解能力和逻辑推理能力，另一方面，通过在模型中加入外部知识库的方法增加模型的常识积累，同时增强模型的可扩展性。


       实验结果表明，在SCT任务上通过加入多种注意力机制的方法取得了78.3%的结果，该结果不仅超过了过去工作中神经网络方法的最好结果，也超过了过去工作的最好结果。在加入外部知识库之后，模型进一步将结果提升至84.7%。"
2019,土壤理化性质的筛选及稻米镉超标概率分类研究,计算机学院,李生启,李岳,ML,0.2789,"~随着我国工农业的快速发展，土壤重金属污染变成一个日益严峻的环境问题。镉是环境中毒性最强的5毒（汞、铅、镉、砷、铬）元素之一，镉污染不仅会引起土壤功能的失调、土质的下降，还会不同程度的影响植物的生长发育。同时镉通过植物吸收，尤其是水稻，富集而转移进入食物链危害人类的生命和健康。所以研究土壤理化性质的筛选以及水稻中镉含量是否超标的检测就显得越发的重要。上述研究得到的土壤理化性质的筛选不仅可以更好的指导土壤污染类型划分的研究，还能在镉污染的监测和防治上节省因大量的土壤理化性质监测所带来的巨大成本。

本论文主要对土壤理化性质的筛选和水稻镉超标概率的分类进行了研究，主要包含以下研究内容：1）基于土壤污染类型划分结果的理化性质筛选的研究。2）基于镉污染的理化性质筛选的研究。3）基于稻米镉含量是否超标数据的分类研究。

本论文在土壤理化性质的筛选的研究中，首先做了土壤污染类型的聚类研究，采用经典聚类算法的代表K-Means，及其改进算法ISOData和神经网络聚类算法代表SOMNet等聚类算法进行了对比研究，得到较好的土壤污染类型的聚类结果。在土壤理化性质的特征筛选上，对聚类的结果做土壤理化性质的对于聚类划分结果的维度重要性排序，并通过土壤理化性质的维度重要性排序结果进行分类来验证排序结果的合理性，并且进行理化性质的筛选，最终使得筛选维度可以达到土壤污染类型划分的要求。

在现实应用中，要求对于不同的研究对同一区域土壤多次采样是不现实的。并且在农业中镉是五种重金属中毒性最大的，在土壤中种出来的作物，镉含量是否超标是一个非常重要的关注点。因此本文针对土壤污染类型下的镉污染进行维度筛选的研究，在本阶段的研究中，我们将上述问题简化一个最基本的约束条件，即筛选的特征应该和镉活性有较高的相关性，能够在一定程度上保证后续研究中，分类器的准确率，维度筛选结果应该对于该类型土壤所种植的稻米超标预测具有较好的指示意义。为该土地类型下的稻米中的镉含量是否超标的分类研究打下基础。

最后针对稻米镉检测数据研究了稻米镉含量是否超标的分类这一课题。针对稻米镉是否超标的分类效果差这一科学问题，分别对稻米镉检测数据做了标准化研究、数据分布研究及前置处理、单特征研究等一系列研究，以及插值对稻米镉是否超标的分类影响的研究。最终根据上述一系列的研究结果，设计实现了一个基于加权贝叶斯融合模型的自适应分类算法，对算法进行了理论推导以及算法的验证，证明了基于加权贝叶斯融合模型的自适应分类算法对稻米镉是否超标数据分类的有效性。"
2019,短文本作者身份识别关键技术研究,计算机学院,张浩文,袁晓洁,Security,0.3076,随着移动互联网的发展与社交网络的兴起，电子邮件、博客、微信、微博等 社交工具逐渐成为人们交换消息的最主要方式。社交工具的广泛应用给人们工作、 生活带来便利，同时也带来诸如谣言传播、电子诈骗等现实问题。社交工具蕴含 海量匿名短文本，其作者身份难以识别，给网络空间治理带来巨大挑战。因此， 有效识别社交网络短文本的作者身份，具有重要理论意义与实际应用价值。本文 在深入分析相关领域研究现状基础上，根据应用场景不同将短文本作者身份识别 分为两类，即 1）微博、论坛等社交媒体舆论领袖身份识别：舆论领袖在社交场 合十分活跃，借助社交媒体的快速传播，对舆情影响越来越大，需从匿名短文本 识别舆论领袖以防止其控制舆论；2）匿名邮件、商品在线评论等作者身份识别： 由于无法事先明确划分作者范围，需对整个语料库建模进行作者身份识别。由于 上述两种应用场景下语料库的数据特点不同，短文本作者身份识别方案难以同时 满足上述应用场景需求。因此，本文针对两种应用场景分别提出了对应解决方案： 第一，提出一种短文本舆论领袖身份识别方案。该方案抽取语料库的词特征 和字符级 n-gram 特征，用于作为短文本的特征表示并进行特征选择；考虑意见 领袖语料库作者数量少、文档数量多，采用基于快速文本分类器 FastText 进行舆 论领袖身份识别。本文选取英文博客数据集 Blog 语料丰富的 50 位作者构建舆论 领袖数据集进行大量实验。实验结果表明本文提出的舆论领袖身份识别方案，由 于 FastText 结构较为简单，在保持神经网络的特性同时，又可以有效避免过拟合， 适用于舆论领袖的身份识别问题。 第二，提出一种基于层次主题模型的短文本作者身份识别方案。该方案基于 作者和文档两个层次，利用主题模型对语料库进行建模，从文档和作者两个层次 训练语料库，实现了特征降维，用于处理短文本特征表达高度稀疏的问题；利用 Gibbs 采样算法对训练集的作者-主题、作者主题-单词、文档-主题、文档主题-单 词和测试集的文档主题-单词五个概率分布矩阵进行参数估计，同时避免陷入局 部最优；在作者身份识别过程，提出一种融合作者信息和文档信息的文档相似度 计算方案。实验结果表明，本文提出的解决方案在 Blog 和 Pan11 两个数据集上 均取得最优的表现。
2019,面向移动浏览器的软件能耗建模与优化方法研究,计算机学院,刘振,张金,Security,0.2921,"浏览器是互联网用户最常使用的应用程序之一，随着移动互联网时代的到来，与桌面浏览器相比，移动设备上的浏览器已经能满足用户大部分的使用需求，因此用户越来越依赖移动浏览器访问互联网。与此同时，随着Web技术的发展，如今网页的结构内容相比前些年变得更加复杂，移动浏览器在加载渲染网页时需要更多的硬件资源。


    近年来移动设备的性能越来越强大，而电池的发展相比于移动设备上其他硬件则慢了许多。这导致当今智能移动设备的续航时间比较短，非常影响用户的使用体验。移动浏览器都针对性能做了很多的优化，使得浏览器性能得到了大幅的提高。但是随着性能的提升，设备的耗电量也越来越高，这给靠电池供电的移动设备带来了沉重的负担。目前针对移动浏览器能耗优化的研究比较少。


    针对上述问题，本文对移动浏览器的能耗问题进行研究。为了更好的量化用户体验，本文采用首次有效绘制时间作为衡量用户体验的指标。然后通过实验研究首次有效绘制时间与CPU频率、网页结构、网速之间的关系。结合浏览器原理和实验分析发现，在网络环境一定时，CPU的频率存在一个阈值，当CPU频率超过该阈值时，网页的首次有效绘制时间几乎不再降低，本文定义该阈值为CPU最优频率，并且不同网页的CPU最优频率也可能不同。


    本文提出一种移动浏览器能耗优化模型，根据网页结构特征预测出加载该网页时的CPU最优频率，实现最佳的性能能耗权衡。首先爬取Aleax中国区排名前500位的网站首页源码并提取特征，然后使用自动化测试工具得到这些网页的CPU最优频率，最后将网页特征和网页对应的CPU最优频率构建数据集训练SVM模型。对于未知网页，提取网页特征输入模型，预测出网页的CPU最优频率，使用该频率进行CPU调度。实验结果表明，与系统默认CPU调度策略相比，本文提出的模型大幅降低了Odroid XU3开发板的能耗，并且首次有效绘制时间增加较少，对用户体验基本没有影响，验证了模型的有效性。"
2019,基于多层注意力机制和迁移学习的aspect-level情感分析,计算机学院,刘学达,刘晓光,ML,0.2893,"Aspect-level情感分析任务属于一种细粒度的情感分析任务，对句子中的不同aspect判断情感极性，aspect可以是句中所描述的事物或者事物某个方面的属性。在早期，解决该任务一般采用基于字典的方法，使用一些已知情感极性的种子词通过WordNet建立情感字典，每句话中的情感词通过情感字典得到相应的情感极性，句中的aspect再通过情感词的情感极性来得到自身的情感极性。之后，出现了情感字典和传统机器学习方法结合的方法，将字典信息作为一个特征加入到分类器，并手动地设计其他的特征，进行语义和句法分析。后来相关研究人员在该任务上引入基于深度学习的方法，使用递归神经网络与dependence tree结合进行端到端地训练，相对于传统机器学习方法在任务测量结果上有了不错的提升，并不再需要手动设计特征。自此之后，该任务上的研究方向越来越偏重于使用深度学习技术，并不断取得优异的效果。


本文同样采用基于深度学习的方法，提出了一个崭新的深度学习模型结构。本文的主要贡献有以下几点：第一，设计了多层注意力机制与相对位置信息结合使用的方法；第二，使用对抗训练的方法对传统的多任务学习方法进行改进；第三，将以上两点结合设计出了本文提出的模型，并通过预训练方法进行参数初始化。注意力机制与相对位置信息的结合使用可以看做是对注意力机制的改进，模型使用多层注意力机制与相对位置信息的目的是能更准确地捕捉到目标词和其上下文之间的语义关系；另一方面，通过对抗训练改进的多任务学习方法和使用的预训练方法可以统称为迁移学习的方法，是为了解决该任务上数据集数据量小，无法充分训练深度学习模型的问题。


SemEval14、SemEval15、SemEval16等相关数据集上的实验结果表明，本文提出的多层注意力机制相对于传统方法中使用的一层注意力机制对模型性能有了一定的提升。同时，结合多任务学习，对抗训练和预训练的方法，提高了模型在小数据集上的泛化能力。另外，本文通过对一些实例的注意力分布进行可视化表示，展示了模型在判断不同目标词的情况下可以很好的捕捉目标词和上下文之间的语义关系。"
2019,基于深度注意力因子分解机的安卓应用推荐系统,计算机学院,徐亦凡,许静,ML,0.3045,"移动设备已经成为人们日常工作与生活的重要组成部分，功能丰富的移动应用满足了人们各种不同的需求。然而随着移动应用市场中应用数量的不断增长，选择心仪的移动应用也增加了用户的时间成本。在这样的背景下，移动应用市场的推荐服务既可以帮助用户选择他们更加喜欢的应用，也可以增加应用的有效曝光度。


虽然近几年来，研究者提出了许多移动应用推荐模型，但是这些模型无法充分使用应用所有的特征，没能获得令人满意的推荐效果，原因可以归结为：1传统推荐模型（如协同过滤）仅关注有限的特征，对文本特征缺少有效的深度特征提取；2基于深度学习的推荐模型关注评论语义文本特征，一定程度上忽略了应用本身的特征。


本文提出了基于知识的深度因子分解机（knowledge-based deep factorization machine，KDFM），一种受因子分解机技术和深度学习注意力机制启发的推荐模型，并将它应用于移动应用的评分预测。KDFM由两个模块组成：用于进行多阶特征交互的深度因子分解机模块和用于提取文本类型特征的注意力编码器模块。通过构建基于评论上下文的主题预处理模块和基于注意力机制的深层特征交互模块，KDFM能够充分利用应用市场中丰富的分类型知识和文本型知识，包括用户评论、应用描述、申请权限和应用大小等，并提供准确的推荐服务。


本文从Google Play Store收集安卓移动应用数据，并进行了大量的实验，结果表明KDFM在评分预测方面与其他评分推荐模型（FOMF、DeepFM、DeepCoNN等）相比，准确度有明显的提升。KDFM在一定程度上解决了深度特征提取不足和文本型特征与分类型特征难以结合的问题。本文还通过对比实验证明了使用注意力机制和数据局部预处理能够有效的提升模型的表现。"
2019,基于Jetson TK1移动平台的目标检测系统设计与实现,计算机学院,宋秋迪,李敏,ML,0.2668,"辅助驾驶和智能交通的科学研究与市场化在近年来发展迅速，基于道路背景的目标（包括车辆、行人等）检测作为其中的重要基础技术，具有很大的研究与应用价值。道路场景复杂，道路中目标种类较多且互相遮挡等情况极易出现，同时，为了应用于辅助驾驶和智能交通系统的运行场景，目标检测算法需要在保持较高准确率的基础上，具有在移动嵌入式设备上快速运行的能力。因此，研究基于嵌入式设备的目标检测算法在科学研究和市场应用领域都具有重要意义。


卷积神经网络在图像检测领域取得了优秀的成绩，使用卷积神经网络实现道路目标检测功能具有可行性和先进性。本文对多种表现优异的检测网络进行了对比分析，根据道路目标检测和移动应用平台的特点，选择了单发多框目标检测器SSD作为基础检测方法。针对嵌入式移动设备量级小、计算资源有限的特点，本文利用轻量级网络结构MobileNet搭建了目标检测网络，压缩了模型的运算量和参数数量。基于特定的应用场景，本文通过分析道路数据集中各类别目标的数量与形状分布情况，调整了网络特征图上的区域候选框。实验表明，在嵌入式GPU上，本文提出的RM-SSD网络在保持了较高检测精度的情况下，运行速度达到了原始SSD网络的2.1倍。


本文基于辅助驾驶的研究需求与目标检测算法，开发了一个针对道路场景中车辆与行人目标的检测系统。为了模拟现实的行车环境和辅助驾驶功能，本文选择低功耗、高性能、高传输效率的NVIDIA Jetson TK1嵌入式开发板作为处理器，使用Arduino Mega 2560微控制器开发板、MC33926双轮电机控制板等控制与电子元器件以及可扩展万用实验平台等材料搭建了一个机器车移动平台。利用移动平台，本文在实际道路场景中进行了目标检测与运动实验，实验验证本文的研究方法具有有效性和优越性。"
2019,基于层次注意力模型的代码注释生成方法,计算机学院,曹昕雅,许静,Security,0.3004,"在社会经济的不断提高和快速发展下，社会生活的方方面面遍布着软件系统的应用。发布软件系统之后，一项不可或缺的环节就是对软件的维护，软件维护能够确保软件系统的运行保持正确性和有效性。代码注释在软件维护期间（提升代码的易读性和可维护性）起着举足轻重的作用，它可以指导软件开发及后期重构的过程，因为代码注释中包含代码的含义及作用，可以很好的帮助开发人员快速的理解目标代码的含义及作用，但是，对于绝大多数的源代码来说，充足详细的代码注释是较为缺乏的，这就为后续开发人员的维护和重构工作带来很大的麻烦。


为了能够达到提升软件维护的性能和效率这一目的，目前，许多研究者都在致力于代码注释自动生成的研究，但是，由于目标代码具有一定特殊的结构性，而生成的注释是一种自然语言形式的表示，所以，这项研究具有一定的困难。尽管如此，代码的特定结构也是有章可循的，随着各种开源社区的发展，网上有着大量的软件数据，这就为探寻代码特定结构带来了一定的可能性。


本文以编码-解码模型为框架，提出了基于层次注意力模型的代码注释生成方法，通过收集大量的开源软件代码，学习代码的特定结构及注释生成的内在规律，从而为给定的代码生成对应的代码注释。在编码器中，将输入的函数体表示为一个代码块，将代码块用词项表示，使用层次注意力模型学习词项对代码块的重要性，然后再使用一个分布式向量表示输入函数体；在解码器中，利用基于门控循环单元（Gated Recurrent Unit，GRU）的模型，依次预测最后的模型输出。本文通过学习代码注释中词语关于代码的条件概率分布，输出构成代码注释的词项序列，从而生成代码的注释。


实验结果表明，本文提出的基于层次注意力模型的代码注释生成方法相对于目前的其他方法而言，具有一定的优势来提高软件系统的可读性、可理解性和可维护性，从而节约了软件维护工作中的时间和成本，在一定程度上，提高了软件维护的效率。"
2019,面向场景文本检测的训练数据合成方法研究,计算机学院,刘子叶,王恺,ML,0.2674,"新时代计算机信息技术的发展与机器智能研究水平的提升，使得图片分析技术的研究和智能设备的研发越来越广泛。在场景图像中出现的文本带有很多的语义信息资源，因此高效率地提取自然场景里的文字信息有着很大的意义。近些年来，图像中文字的定位和识别技术高效地促进了依据内容进行的图片搜索和视频搜索、辅助视觉系统、智能交通系统、机器人视觉和自动化的发展。


深度学习对现实场景中文字定位和识别的研究起了很大的促进作用。然而，深度神经网络需要较大规模的数据集来进行训练。尽管现在计算机视觉领域已经有了很多公开的图像数据集，尤其是ImageNet数据集的提出极大地促进了深度学习的发展。但是带有标签的场景文本的图像数量仍然较少， ICDAR竞赛数据集只有几千张训练图像，coco-text等数据集有几万张训练图像、但粒度较粗且标注质量不高。依据这些小规模数据或低标注质量的中规模数据训练的深度学习模型，其输出的文本定位和识别结果难以满足实际应用需求。


在本篇论文中，提出了一种基于3D场景的场景文本图像合成方法，通过3D景像使用unity软件获得深度图像，并以此合成大量带有标签的场景文本图像。此3D场景文本图像合成方式在无人工干预下可以自动合成无限多的带有标签的场景文本图像，且能够得到各种级别的标注结果，比如单词级、字符级甚至像素级。将所合成的数据集应用于深度学习模型训练，有助于进一步提升场景文字定位和识别性能。


我们合成的数据集命名为3DSynthText，分为带灯光效果的和无灯光效果的合成图像，共生成了64个场景的图像，数据集的总数为800,000张。我们使用了2018年由Xiang Bai等人发表在ECCV上的文章《Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes》[1]中提出的Mask TextSpotter模型对我们合成的数据集进行评估。在我们合成的数据集上训练Mask TextSpotter模型后，分别使用Incidental Scene Text测试集和COCO-Text测试集对模型进行了测试。一方面，通过不同贴放文本平面比例和不同场景数量的对比实验，发现当仅在一个平面上贴放文本且使用的3D场景数目最多时，模型能够达到的效果最好。另一方面，通过与SynthText合成数据集和真实场景图像数据集上训练模型的对比实验可以发现：当与合成图像数据集进行比较，我们合成的数据集总体效果要略好于SynthText合成数据集；虽然我们的合成数据集效果略逊色于真实场景文本数据集，但在经过在合成数据集上预训练，在真实图像数据集上进行微调之后，我们合成数据集得出的总体效果好于真实图像数据集。"
2019,基于序列到序列神经网络模型的评论生成方法,计算机学院,吕克险,史广顺,Security,0.2757,"文本生成是自然语言处理中的一个重要研究方向。其研究方向主要为智能问答、机器翻译和新闻写作方面。本文主要针对新闻中的评论生成进行研究。评论生成的目的是可以增强用户对于新闻的理解，同时也可以用于评论模板，更有助于对于新闻评论文章的自动生成的研究。主要使用两种算法实现新闻的评论生成，并针对在单类型数据与多类型数据使用两种算法进行分析。本文主要工作如下：


第一、本文提出针对有标题的一般文本进行快速简单的评论生成方法，通过将新闻标题作为输入同时以评论作为标签训练带有注意力机制的encoder-decoder模型。


第二、由于新闻中存在虚假标题或者无标题的文本，本文提出基于文本摘要的评论生成算法，所以使用word2vec与TextRank算法生成文本摘要，将生成的摘要作为输入以评论为标签训练带有注意力机制的encoder-decoder模型。


第三、通过使用公开数据集和scrapy框架爬虫两种方式获取文章与评论。并且针对同一领域的新闻进行汇总，生成该领域词汇表，并按照上述两种方法训练评论生成模型，该方法可以提高模型的训练效率以及增强生成评论的质量。


本文中首先针对有标题的文本实现简单快速的评论生成模型，而后提出基于文本摘要的评论生成模型，增强评论生成模型的效果和适用性，最后实验主要针对新闻数据以及评论进行获取，并将文本进行简单的分类，针对同一领域的文本训练评论生成模型，提高生成的评论的质量。"
2019,手机指纹图像质量评估方法研究,计算机学院,邵军飞,史广顺,Security,0.305,"手机指纹识别技术需要解决多种问题，它不仅要解决手指干燥、湿滑、爆皮或操作不规范等造成的低质量指纹图像问题，还要解决各种类型攻击指纹带来的安全防护问题。在指纹识别系统最前端做好图像质量评估将有效降低这两类问题的影响。众多学者在指纹图像质量评估方面做了大量工作，但因为影响指纹图像质量的因素很多，基于公开数据集的质量评估方法研究成果难以适用于所有情况。本文针对一种包含拼接式攻击指纹的手机指纹图像数据集开展了质量评估方法研究，其主要工作描述如下：


1.本文针对传统指纹图像质量评估方法不适用攻击样本的问题，提出了一种手机指纹图像质量分层评估方案。方案使用多种纹理特征和多种机器学习方法对攻击指纹和活性指纹进行最优分类，将包含攻击指纹的复杂样本质量评估问题分解为攻击指纹特征检测和活性指纹质量评估两部分，有效解决了上述复杂样本的质量评估问题。


2.本文针对拼接式攻击指纹图像中的攻击特征检测问题，设计了一种基于褶皱的攻击特征检测方法。该方法实现了对拼接式攻击指纹的特征级别检测，有效的将一般性褶皱与攻击性褶皱识别分离，为包含拼接式攻击指纹的手机指纹图像质量评估提供了依据。


3.本文设计了一种基于方向差异性的指纹图像质量评估方法。本文研究发现两种不同方法对同一指纹图像进行脊线方向提取，其方向结果具有一定差异性，且这种差异性在不同质量图像上的表现具有一定区分性。基于这种区分性本文了定义的相关质量评估指标，并结合支持向量机进行了指纹图像质量评估，取得了良好效果。"
2019,指纹纹型分类及指纹遗传系统研究,计算机学院,贺彩玲,史广顺,Security,0.28,"生物特征应用一直是近几年研究的一个热门话题，生物特征（如指纹、掌纹、虹膜等）由于其唯一性和终身不变性被认为一种高效可靠的身份鉴别方法。其中，指纹由于方便采集和终身不变的特性，成为生物特征识别中使用最广泛的一种特征。近年来，大量研究成果表明，某些遗传病，特别是一些染色体病和先天畸形常伴有特殊的指纹类型。指纹遗传系统就是将输入的指纹按照一定的规则进行自动分类，应用统计规律进行数据统计，辅助相关研究人员进行指纹类型和一些遗传病的相关性研究的一个平台。关于指纹分类的研究，目前已经提出了很多方法，但是仍然存在着很多值得研究的问题。本文的主要研究工作如下：


1. 指纹是在人类进化的过程中形成的位于手指末梢由于手指皮肤凹凸不平形成的特殊纹路，具有终身不变性和可分类性。本文在前人对指纹分类研究的基础上，结合指纹遗传系统的具体应用，提出了使用指纹线特征进行指纹纹型分类的方法，将指纹分为弓形纹（A）、左箕形纹（L）、右箕形纹（R）、帐弓纹（T）和斗形纹（W）五种。


2. 通过对比分析基于奇异点的方法、基于脊线结构的方法和基于方向场的方法等指纹分类方法的优点和不足，提出了一种基于线特征的指纹纹型分类方法。通过纹线追踪算法提取指纹的纹线特征（包括纹线的起点、终点、纹线长度、最大曲率、最大曲率处的坐标以及纹线类型），通过纹线关系建立算法得到指纹纹线之间的关系特征，最后组合成完整的指纹线特征。根据指纹线特征，通过机器学习的分类方法对指纹进行分类。


3. 对于一些质量较差的指纹图像，通过一系列的预处理和线特征提取步骤，会丢失一部分原有的指纹图像数据，导致分类结果不准确。针对这一问题，本文利用卷积神经网络自动提取特征的能力，使用卷积神经网络在指纹原图的基础上对指纹进行纹型分类。


最后，本文将所研究的指纹纹型分类算法应用于指纹遗传系统，实现了核心处理流程和相应的算法应用。论文结尾处对本文的研究工作和取得的成果做了一个总结和展望，并且提出未来可以继续研究的方向。"
2019,基于生成对抗网络的时序数据缺失值填充算法研究,计算机学院,罗永洪,袁晓洁,ML,0.3095,"随着物联网和传感器技术的发展，现实生活中的大量时序数据，例如股票走势数据、气象观测数据和个人医疗数据等，被监测并记录下来。挖掘时序数据中的隐含信息并对时序数据进行分析具有重大的现实意义，比如股票价格分析和预测、天气预报以及未来健康状况预测等。然而，由于采集设备的不稳定性或者被干扰等原因，采集到的时序数据往往是不完整的，而数据的缺失部分则阻碍了对时序数据的深入分析。因此，缺失值的处理对于后续时序数据的分析变得格外重要。传统的缺失值处理方法包括直接删除法、直接填充统计数据法以及基于机器学习算法的填充法。然而上述缺失值处理方法均没有考虑到时序数据中的时间先后信息，故难以取得准确的填充效果。


本文利用时序数据的时间间隔特征，结合生成对抗网络技术，提出了一种基于生成对抗网络的时序数据缺失值填充方法。经过对抗式的训练，该方法能够生成符合原始数据集分布的新时序数据。针对每一条缺失时序数据，本文采用梯度下降算法寻找一个低维特征向量，使得以该低维特征向量为基础的生成样本与原始样本最相似，从而利用生成样本填充时序数据当中的缺失值。


       为了加快填充时序数据中的缺失值，本文进一步提出了端到端的时序数据缺失值填充方法。该方法充分利用了降噪自编码器的降维能力，自动的为每一条缺失时序数据寻找对应的低维特征向量。结合生成对抗网络技术，能够端到端的自动填充时序数据中的缺失值，具有更高的时间效率。


       真实数据集下的实验结果表明，本文提出的两种时序数据缺失值填充方法，在填充准确度上显著优于国际主流的填充方法，利用本文方法填充后的数据集也能够显著提高下游分类和回归任务的性能。"
2019,基于有损压缩的位置索引构建方法研究,计算机学院,郜姝妮,刘晓光,Database,0.2488,"随着大数据时代的来临，海量数据面临着存储和传输的巨大挑战，数据压缩的重要性不言而喻。在信息检索领域，倒排索引是搜索引擎中数据的组织形式，它实现了查询的快速检索，但同时也带来巨大的索引存储开销。因此，倒排列表的压缩一直是学者研究的焦点。


现代的搜索引擎不仅要实现检索结果的准确召回，更要保证在尽量短的时间内响应，然而结果召回时间（Efficiency）和结果准确性（Effectiveness）往往是互相制衡的。当代搜索系统应用了很多查询优化的技术来提高查询结果准确性，比如查询扩展，词项依赖模型等，这些操作需要利用额外的数据和计算来获取更加精确的检索结果。词项依赖模型（Term Dependency Model）也可称为邻近度模型，它考虑查询词在文档中出现的位置远近，往往查询词在一篇文档中出现的越紧凑，那么该文档与查询的相关性越大。计算邻近度所使用的位置索引大小大约是简化的倒排索引的3-5倍，所以计算查询词邻近度要耗费很大的I/O和CPU资源。


本文结合邻近度计算的特点，设计了倒排索引中位置列表的有损压缩方法。具体来说，本文挖掘位置信息聚集出现的属性，设计自适应粒度的聚类算法，并针对聚类后的数据提出有损压缩策略。有损压缩后的索引不仅降低了数据存储和传输的消耗，也缩短了邻近度模型中计算查询词位置距离的时间，即算分排序时间。除此之外，有损压缩后的位置信息减弱了邻近度模型带来的噪声，进而提高了平均检索准确性。然而，某些有损压缩的位置列表不可避免地会损害检索准确性，基于此，本文接下来提出了一种基于机器学习预测方法的混合索引生成机制，以弥补有损压缩带来的信息损失。通过训练好的模型来判别有损压缩后的某条位置列表是否会损害检索准确性，将可能损害检索准确性的有损压缩列表替换为其原始形式，最终的混合索引由原始和有损位置列表两部分组成。


为了验证方法的稳定性和通用性，本文使用了两个邻近度模型在两个查询集上分别评估有损压缩的位置索引和混合索引的表现，并从索引大小、检索算分时间和检索准确性三个方面对结果进行分析。"
2019,基于注意力机制优化的生成式文档摘要模型,计算机学院,桂敏,杨征路,Security,0.2819,"随着互联网的飞速发展，各种信息呈指数级增长，人类面临值信息超载的问题。对于这个问题，迫切需要一种工具可以即时的处理并消化理解这些信息展示给用户。目前的搜索引擎已经可以根据用户的查询语句向用户展示排好序的文档或者网页。但是，即使是功能非常强大的搜索引擎并装备有最新的信息检索技术，依然不能从多种信息源中用简洁但却包含信息量的返回结果展示给用户。这种情况下，一种可以即时总结各种信息源并理解内容向用户展现简洁明了的返回结果的工具显得尤为重要，这也就促使了自动文档摘要系统的发展。本文的研究方向是基于注意力机制优化的生成式文档摘要模型，文章通过对注意力机制的优化试图解决文档摘要中存在的显著性和重复问题，进而提升单文档场景下生成式摘要模型的性能的研究目标。在长文档摘要任务情景下，基础的注意力机制得到的注意力分布往往相对平坦，注意力被分散在各个位置上。但是直观上来说，一个性能比较好的注意力机制应该可以集中注意力在特定位置上，也就是说在某个时刻明确知道应该注意哪部分的内容，过往的模型没有对集中注意力这点进行研究。此外文档摘要模型中还存在较为严重的重复问题，进而影响摘要的可读性。通过细致的分析，本文认为以上问题都可以通过优化注意力机制得到解决。因此针对上述问题，本文提出三个方法对基础的注意力机制进行优化：

（1）注意力精炼单元（Attention Refinement Unit，ARU）用于模拟人类书写摘要时候的“揣摩”策略。在当前时刻得到初步的注意力分布之后，设置一个门单元对当前时刻重点相关的内容进行保留，同时忽略与当前时刻不相关的信息，这就相当于一个注意力提纯精炼的过程。从后文的实验结果分析中可以看出ARU可以较好地降低和当前时刻状态不相关部分的注意力，并突出相关的部分。

（2）局部方差损失函数（Local Variance Loss）对每一个时刻的注意力分布进行监督，激励模型在当前时刻将注意力放在少数位置上，降低不相关非重要位置的注意力。也就是说该损失函数将模型朝着注意力分布比较“sharp”的方向训练，这个方法的有效性在实验部分也有进行验证，证明经过局部方差损失函数直接监督的注意力分布可以集中注意力关注重点内容。

（3）全局方差损失函数（Global Variance Loss）试图通过一种较为简单的方式对重复问题进行解决。本文希望通过全局方差损失函数使得注意力分布更加分散，不对同一位置重复分配较高注意力。笔者观察到如果不出现重复的问题，除去每个位置历史所得最大注意力之后其他时刻注意力分布总和应该是平坦的，也就是方差比较小。根据这一观察提出的全局方差损失函数也确实对改善生成式摘要模型中的重复问题起到了明显的作用。"
2019,C语言内存泄漏静态检测工具适用性研究,计算机学院,张森,许静,Security,0.3456,"C语言允许开发者手动进行内存管理，使程序能够有更高的执行效率。但是由于内存管理的不当，在软件开发过程中，经常会发生内存泄漏漏洞，严重威胁着软件安全。近年来，由于用户对软件可靠性要求越来越高，对于内存泄漏漏洞的研究一直都是工业界和学术界的热点。


当前，对于内存泄漏的检测主要分为动态和静态两种方法。动态检测通常运行开销较大，且容易依赖测试用例质量；相反，静态检测由于不用运行代码，执行效率高，因此受到许多使用者的欢迎。在实际应用中，使用者通常希望能够通过快速简便的方法，从多样的静态检测工具中找出适合检测待测程序的工具。为了评估检测工具的适用性和有效性，传统的方法通过对特定数据集进行检测，得到误报率、漏报率等性能指标来进行对比分析。这种方法虽然能大致评估工具的有效性，但由于内存泄漏漏洞的多样性和检测工具静态分析策略的不同，各工具对不同类型的内存泄漏代码的检测能力也各不相同。传统的评估方法未能区分内存泄漏漏洞代码的类型，因此很难对工具进行细粒度的评估。


为了解决这个问题，本文提出了一种基于漏洞类型的测试用例生成方法，从而细粒度地评估内存泄漏检测工具的有效性和适用性。首先，本文根据内存泄漏漏洞的代码模式，基于堆内存行为和程序结构，将内存泄漏漏洞划分为11类，并给出每个子类的形式化描述。根据此分类，使用代码自动生成技术，基于Velocity模板引擎，设计并实现了内存泄漏测试用例自动生成系统，生成了一个新的测试数据集HPMD对检测工具进行评估。


实验中，本文使用三款主流静态检测工具，分别对两个已有数据集和HPMD数据集进行检测，通过分析对比实验结果，证明传统评估检测工具方法的局限性，得出不同的静态检测工具具有不同的特性，工具对于代码检测有一定的适用性的结论。同时，通过对HPMD数据集进行检测并分析，可得到各检测工具对不同类型的内存泄漏代码的检测能力，从而为用户推荐合适的工具。"
2019,针对恶意代码检测模型的后门嵌入攻击,计算机学院,田建文,王志,Security,0.3694,"虽然机器学习（ML）模型越来越受信任，可以在不同的领域做出决策，但使用此类模型的系统的安全性却越来越受到关注。与其他机器学习模型类似，深层神经网络最近已经被证明对攻击者精心设计的输入缺乏鲁棒性。这些输入源于常规输入，通过微小但精心选择的干扰，对神经网络进行欺骗。且ML模型通常是针对数据进行训练的，最近的研究表明，通过将精心制作的样本插入到训练集中，为攻击者提供了机会来改变模型，允许对手将后门或特洛伊木马插入到模型中，从而在进行预测时使用简单的外部后门触发器和模型本身的黑盒透视图启用恶意行为。检测这种类型的攻击非常具有挑战性，因为后门模型与正常模型几乎一模一样，唯有当只有投毒者所知道的后门触发器时被触发时，才会发生意外行为。


这两种攻击对模型都有很大的危害性，但是同时攻击的实施也存在一些限制。本文结合神经网络与恶意代码分析，认真研究了神经网络以及已有攻击方法的特点与不足，提出一种针对安卓恶意软件分析领域中的深度学习框架的后门攻击方法。该方法结合对抗性攻击与投毒攻击两种手段，设计了一种更高效的攻击策略。主要贡献包括以下三个方面：


1.通过研究图像识别上的通用对抗攻击策略，提出一种可应用于恶意代码的通用对抗攻击方法，以及一种高效的筛选策略来选择对标签变化作用最大的特征；


2.提出一种不控制标签的投毒策略，对特征进行针对性的毒化，使得样本可以通过插入该特征来躲避检测；


3.在不同场景下，提出不同的应对策略达到可观的攻击效果。


通过在训练集中注入极少量的样本便能构造一组后门特征，将该后门特征注入到恶意样本集上可达到很高的攻击成功率。"
2019,缩减PIT编码构造方法研究,计算机学院,刘博,刘晓光,Security,0.2792,"纠删码是一种容错编码技术，通过引入一定量的数据冗余来保证源数据的 安全性。与副本策略相比，纠删码技术能够通过更少的数据冗余实现同样的容错 能力，因此被广泛应用于大型存储系统之中。近年来，随着终端技术的不断发展， 越来越多的数据被广泛地收集、存储。数据规模的激增对利用纠删码恢复丢失数 据的效率提出了更高的要求。当故障发生时，系统需要更快速地获取对应的源数 据和校验数据、更高效地计算出丢失数据，尽可能避免因故障带来的高延迟。     一项科学统计表明，实际故障恢复案例中单磁盘故障恢复案例高达 99.75%。 这意味着提升一个存储系统的单故障恢复性能将会提升整个存储系统的响应性 能。本文以部署了 PIT 编码的存储系统的单故障恢复场景为背景展开探讨。传统 的 PIT 编码试图通过将一个阵列的最后若干列缩减来降低单位故障恢复所需数 据量。虽然这种做法有效，但是不足够高效。对一个特定的 PIT 阵列来说存在着 许多种缩减方案和不同的恢复策略，其不同的组合会带来不同的收益。怎样的组 合能令收益最大化是一个复杂而有价值的问题。     为了解决这个问题，我们首先寻找到了两种非常高效的缩减手段，称为行缩 减和列缩减；然后圈定了两种可行的恢复方案，称为 PQR 和 PRQ；最后提出了 一种启发式算法，用于快速求解较优的组合方案，称为 CHRS 算法。上述改进的 PIT 编码被称为 RC-PIT 编码，包含三维参数，分别为阵列规模 p、行缩减数目 r 和列缩减数目 c。本文从理论上证明了搭配 CHRS 算法的 RC-PIT 编码方案的高 效性，并通过模拟实验和真实系统实验加以验证。     模拟实验表明，CHRS 算法能够在线性时间内求解出次优甚至最优的解决方 案，且通过模拟实验可以快速得出在确定存储容量下的最优{,,}组合；真实系 统实验表明，与其他几种较新的基于异或运算的阵列码相比，最优参数下搭配 CHRS 算法的 RC-PIT 编码能够提升 20.2%~96.2%的吞吐，并降低平均 44.8%的 延迟。尤其是在分布式环境下的存储系统中，相较于对照组中的最好情况，RCPIT 编码仍能够降低超过 30%的单故障恢复所需的数据传输时间。上述结果表明 本文的工作是有效的，并为进一步优化传统编码问题开拓了新的视野。"
2024,Accelerating Flow-Based Sampling for Large-𝑁 Gauge Theories,MIT CSAIL,MIT Student,MIT Advisor,Database,0.1871,"Due to its consistency with numerous experimental observations, the Standard Model of particle physics is widely accepted as the best known formulation of elementary particles and their interactions. However, making experimental predictions using the Standard Model involves mathematical and computational challenges due to its complexity. Quantum chromodynamics (QCD), which can be described as an SU(3) gauge theory due to the 3 quark colors and 8 gluon types, is one sector of the Standard Model for which computing solutions is especially challenging. A natural theoretical generalization of QCD is the class of all SU(𝑁) gauge theories; these theories also provide a method for some QCD computations in the 𝑁 → ∞ limit. To study these theories numerically, approximations are calculated from configuration samples due to the mathematical complexity and lack of analytical solutions.

In this thesis, we explore asymptotically efficient flow-based sampling algorithms for the twisted Eguchi-Kawai (TEK) model, a method for analyzing large-𝑁 QCD numerically. We introduce an original architecture based on SU(2) matrix multiplication that allows for efficient Jacobian computation. In addition, we explore the possibility of transfer learning with respect to the number of colors 𝑁 and demonstrate that a model trained quickly on the SU(𝑁) setting also provides useful information in SU(𝑁′), 𝑁′ > 𝑁 cases."
2024,Relative Robot Localization and Frame Alignment for Multi-Robot Collaboration,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2828,"The growing field of collaborative robotics has the potential to enable and improve the execution of many challenging robot applications. For instance, with teamwork between multiple agents, dynamic object tracking can more completely cover an environment and trajectory planning becomes safer. However, for robots to share the quickly changing spatial information involved in these tasks, robots need to be able to express information originally sensed or planned in their own frame into the frame of neighboring agents. This can be challenging in cases where robots have no global pose information resulting in steady accumulation of error, or drift, in their local pose estimates. To mitigate the effects of drift, neighboring agents must make up-to-date estimates of the alignment between their frames, which can be difficult due to ambiguous alignments and the presence of outlier measurements. To address these issues, the first contribution of this thesis is a method for performing fast incremental frame alignment between pairs of robots, enabling collaborative multiple object tracking (MOT), the task of monitoring the locations of dynamic objects in an environment. To perform frame alignment, robots build up maps of recently seen static objects and use these maps and the detections of tracked dynamic objects to correct for frame drift. Using frame alignment estimates, agents share object detection information and account for additional uncertainty associated with the alignment estimate. The second contribution of this thesis presents a method to perform frame alignment with no initial guess. Many potential frame alignments are computed and we develop a filter that uses temporal consistency to reject outlier alignments and only accept a series of alignments that are consistent over time. We demonstrate in hardware experiments our ability to perform frame alignment in difficult scenarios and improve the quality of collaborative object tracking onboard real robots."
2024,Driving Emerging Technologies From Concept to Reality: A Case Study of Carbon Nanotubes,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2263,"The evolution of electronics has transformed nearly every aspect of society and has been fueled by decades of relentless device scaling. However, electronics is facing a paradigm shift as serious obstacles challenge future progress. Continued scaling is growing increasingly difficult and already yields diminishing energy efficiency benefits. At the same time, other obstacles such as data bandwidth bottlenecks, interconnect density limitations, and reliability are limiting computing performance. Therefore, traditional routes to progress are insufficient, and new approaches must be investigated if we are to continue the technological advancement society has come to expect.

One major thrust toward overcoming these obstacles is the search for alternative, beyond-silicon technologies. Yet despite the promise of these emerging nanotechnologies, their nascency has made their integration into practical and useful electronic systems challenging. In my thesis, I aim to tackle this challenge and present a roadmap for how such new and immature nanotechnologies can be leveraged to not only set the foundation for futuristic next-generation hardware, but also realize practical systems that can have an impact today. As a case study, I use carbon nanotubes (CNTs) to demonstrate a realistic roadmap for commercially realizing these next-generation technologies. First, I show, for the first time, that every type of today’s conventional circuitry (digital, analog, and mixed-signal circuits) can be fabricated with CNT field-effect transistors (CNFETs). This provides a pathway for adopting these futuristic technologies today. Second, to show how CNFETs can play a role in the next generation of computing systems, I leverage the unique low-temperature fabrication of CNFETs alongside emerging memory technologies to achieve the finest 3D integration of emerging technologies to date, which I further use to enable a new circuit design technique. Third, to show how CNFETs can enable futuristic electronic systems that can impact application spaces beyond conventional computing, I leverage VLSI-compatible foundry fabrication of CNFETs to realize BioSensor chips capable of detecting and identifying infectious pathogens in liquid. These experimental demonstrations of CNFETs in today’s (conventional circuitry), tomorrow’s (dense fine-grained 3D systems), and futuristic (healthcare diagnostics) applications explicitly demonstrate a practical roadmap for how emerging nanotechnologies can be developed for near-term adoption while providing longer-term motivation for enabling next-generation electronic systems."
2024,Perturbation-invariant Speech Representation Learning by Online Clustering,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.275,"Despite success across various tasks, self-supervised speech models face significant challenges in enhancing content-related performance with unlabeled data, requiring substantial computational resources. Meanwhile, learning from clustered discrete units has been shown to facilitate accurate phonetic representations. Thus, this thesis investigates speaker and noise-invariant speech representations. First, Speaker-invariant Clustering (Spin) is proposed to extract content representations through online clustering and speaker-invariant cross-view prediction. Second, Robust Spin (R-Spin) is devised to extend Spin to handle more distorted speech signals by leveraging acoustic pieces. Furthermore, this thesis includes a diverse set of evaluation and visualization techniques to quantitatively and qualitatively analyze the perturbation invariability of the proposed methods. This thesis offers approaches to producing perturbation-invariant speech representations and deeply investigates the characteristics of the learned representations, providing insights into these models and cultivating future extension possibilities."
2024,Toward In-Context Teaching,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3272,"When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill. Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students’ changing state of knowledge. There is increasing interest in using computational models, particularly large language models, as pedagogical tools. As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples. But how effectively can these models adapt as teachers to students of different types? To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods. We additionally introduce (3) AToM, a new probabilistic method for adaptive teaching that jointly infers students’ past beliefs and optimizes for the correctness of future beliefs. In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models. In human experiments, both AToM and LLMs outperform non-adaptive random example selection. Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it."
2024,Standardization of Electronic Component Datasheets to Improve Systematic Data Extraction,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2855,"This thesis addresses the challenge of standardizing electronic component datasheets to improve systematic data extraction. The absence of uniformity in datasheet design complicates the process of systematically extracting critical information, leading to significant manual effort and potential errors. This research explores the current state of datasheet standardization and examines existing systematic data extraction efforts from semi-structured documents. It highlights the limitations of current methods and emphasizes the need for further standardization to facilitate accurate and efficient data extraction. The thesis proposes a detailed methodology for transitioning electronic component datasheets from semistructured to structured formats through standardization. By defining common standards and specific structures for different types of datasheets, this approach aims to enhance both human readability and machine processing. The thesis concludes by discussing the broader implications of these standards and their potential applications in other fields. Through this work, the goal is to streamline the datasheet creation process, reduce manual intervention, and ultimately improve the accuracy and efficiency of systematic data extraction in the electronic components industry."
2024,Graphs of Convex Sets with Applications to Optimal Control and Motion Planning,MIT CSAIL,MIT Student,MIT Advisor,CV,0.1991,"This thesis introduces a new class of problems at the interface of combinatorial and convex optimization. We consider graphs where each vertex is paired with a convex program, and each edge couples two programs through additional convex costs and constraints. We call such a graph a Graph of Convex Sets (GCS). Over a GCS we can formulate any optimization problem that we can formulate over an ordinary weighted graph, with scalar costs on the vertices and edges. In fact, for any fixed choice of the variables in the convex programs, a GCS reduces to a weighted graph where we can seek, e.g., a path, a matching, a tour, or a spanning tree of minimum cost. The challenge in a GCS problem lies in solving the discrete and the continuous components of the problem jointly. By combining the modelling power of graphs and convex optimization, GCSs are a flexible framework to formulate and solve many real-world problems. The graph and the combinatorial goal (e.g., finding a path or a tour) model the high-level discrete skeleton of a problem. The convex costs and constraints fill in the low-level continuous details. The primary contribution of this thesis is an efficient and unified method for solving any GCS problem. Starting from an integer-linear-programming formulation of an optimization problem over a weighted graph, this method formulates the corresponding GCS problem as an efficient Mixed-Integer Convex Program (MICP). This MICP can then be solved to global optimality using common branch-and-bound solvers, or approximately by rounding the solution of its convex relaxation. Importantly, both the formulation of the MICP and its solution are fully automatic, and a user of our framework does not need any expertise in mixed-integer optimization. We first describe the GCS framework and the formulation of our MICP in general terms, without presupposing the specific combinatorial problem to be solved over the GCS. We illustrate our techniques through multiple examples spanning logistics, transportation, scheduling, navigation, and computational geometry. Then we focus on the Shortest-Path Problem (SPP) in GCS. This problem is particularly interesting since it generalizes a wide variety of multi-stage decision-making problems and, using our techniques, it can be solved very effectively. We consider two main applications of the SPP in GCS: optimal control of dynamical systems and collision-free motion iplanning. In these two areas, our techniques either generalize or significantly improve upon algorithms and optimization methods that have been developed for decades and are widely used in academia and industry. Lastly, the techniques introduced in this thesis are implemented in the software packages Drake and gcspy. The former is a large and mature software for robotics. It is open-source and widely used by the community. The second is a very simple and lightweight Python package which is also open source. In this thesis, we will illustrate the usage of gcspy through multiple basic examples."
2024,Improving Generative Models for 3D Molecular Structures,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2522,"Generative models have recently emerged as a promising avenue for navigating the high-dimensional space of molecular structures. Such models must be designed carefully to respect the rotation and translation symmetries of molecules. In this thesis, we first provide an overview of existing methods and techniques in this rapidly developing field. Next, we present Symphony, an𝐸(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments, improving upon existing autoregressive models for molecule generation and approaching the performance of diffusion models. The material in this thesis is primarily sourced from the publication “Symphony: SymmetryEquivariant Point-Centered Spherical Harmonics for 3D Molecule Generation"" [13] authored by Ameya Daigavane, Song Kim, Mario Geiger and Tess Smidt, and published at the International Conference on Learning Representations (ICLR), 2024."
2024,"Design of an Affordable, Precise Irrigation Controller that Lowers the Barrier to Water- and Energy-Sustainable Agriculture",MIT CSAIL,MIT Student,MIT Advisor,CV,0.2606,"With climate change and population growth exacerbating global food insecurity, it has become urgent to establish more water- and energy-efficient means to raise agricultural production. Available techniques to bolster crop productivity, such as solar-powered drip irrigation (SPDI) and precision irrigation, are currently cost-prohibitive for farmers in low-and middle-income countries (LMICs), where food insecurity will be most severe. This thesis demonstrates one method to reduce the barrier to these systems, by pairing them with a Predictive Optimal Water and Energy Irrigation (POWEIr) controller that optimizes irrigation schedules to make efficient use of solar and water resources for maximum crop yield. In doing so, POWEIr also decreases SPDI system costs.

First, this work confirms the hypothesis that scheduling irrigation activity to match the availability of variable solar power enables SPDI cost savings. For a fixed irrigation system, a SPDI full-season operation simulation study was conducted and the impact of adjusting the pumping load dynamically to match solar power availability was assessed. When evaluated against conventional operation, this process of profile matching enabled a power system lifetime cost decrease of >18% while delivering 100% of the required irrigation for a simulated two-hectare Kenyan tomato farm with over 50 m well depth.

To exploit these cost and reliability benefits, this work proposes the POWEIr controller. The POWEIr controller leverages machine learning and utilizes a small set of inexpensive sensors to optimize irrigation schedules based on solar energy and crop water demand pre-dictions. The performance of the POWEIr controller was evaluated with an experimental SPDI prototype and compared to simulated typical farming practices. For the same irrigation delivered, a six-fold decrease in the required battery capacity was observed. With no batteries, the POWEIr controller still satisfied a greater fraction of the irrigation demand. Overall, compared to typical practice, the controller provided more reliable irrigation using solar power, with minimal battery usage.

High reliability at low cost necessitates that the POWEIr controller’s irrigation schedules are robust to errors in agronomy inputs and weather data. Sensitivity to these errors was assessed by evaluating the impact on simulated irrigation amounts and crop yield. It was found possible to rely on weather data from an economical station, costing $190, 83% less than a better-equipped research-quality alternative, with negligible consequences to crop yields. This conclusion held steadfast across diverse crop and soil types. The crop coefficient was the most significant factor affecting irrigation performance, thereby pointing to the need for calibration of this factor alone. This underscores the POWEIr controller’s capability to accurately optimize irrigation schedules for only essential water use while relying on affordable sensors and minimal calibration.

Finally, the POWEIr controller was piloted on farms in Jordan and Morocco and performance was benchmarked against measured local, conventional drip irrigation practices on similar farms. It provided up to 44% and 43% savings in water use and pumping energy consumption, respectively, for similar crop yields. This result demonstrates theory to practice of accessible precision agriculture technology and offers tangible evidence of the POWEIr controller’s potential to raise agricultural sustainability."
2024,"Automated Engineering Design for Reusable Concrete
Building Structures",MIT CSAIL,MIT Student,MIT Advisor,CV,0.206,"Concrete contributes to 8% of global CO2 emission through reinforced concrete (RC) structural system. Unlike steel and timber structures, RC components are rarely reused due to the inseparable phase between concrete and steel. This results in down cycling of the components into aggregates or landfill material. The Pixelframe structural system [1] was proposed to facilitate the reusability of concrete components by implementing the existing external post-tensioning system in bridge structures and fiber reinforced system to design building beams and columns. This work presents an automated engineering design workflow for Pixelframe, including a engineering mechanics of the system that conforms to ACI 318- 19 [2] and fib Model Code 2010 [3], half-scale tests to verify the preliminary behavior of the system, and a scalable design algorithm for minimum embodied carbon designs. The workflow also uncovers new insights on choosing ranges of concrete strengths based on the element lengths and potential carbon reduction from refining the number of different concrete strengths in a building. This work demonstrates the utilization of existing building systems in the context of reusability and the potential of automated computational structures in aiding the design decisions to facilitate the circular economy of concrete structures."
2024,Robot Graph Grammars: Towards Custom Robots for Every Task,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2337,"As robots find broader applications outside factory floors, they face an increasing number of challenges. For example, they must accommodate rugged terrain, limited battery capacity, and complex dynamics. Existing robots are largely designed by hand to meet a given set of specifications. While highly capable, these manually-designed robots tend to leave performance on the table. These difficulties have motivated research into automatic robot design tools. Early tools were often limited in the range of robot topologies they can explore, however. Current graph-based robot representations can expand the space of possible designs, but it is not always clear how the resulting designs can be fabricated.

To enable efficient design exploration and ensure fabricability, we propose graph grammars as a universal robot design representation. Graph grammars use rewriting rules to incrementally add complexity or select among distinct design alternatives. Because only fabricable components and connections are expressed in the grammar, the generated robot topologies are valid by construction. Through recursion and branching, graph grammars can also generate a large variety of possible designs. To tackle this expansive search space, we propose a specialized learning-based search algorithm called Graph Heuristic Search (GHS). GHS focuses limited simulation resources on the most promising designs. We compare GHS to random search and Monte-Carlo tree search baselines, showing that GHS finds higher-performing designs in less wall-clock time. We combine graph grammars and GHS with other techniques such as differentiable simulation to efficiently optimize multiple types of mobile robots. In doing so, we show that graph grammars are a principled yet general design representation for robot co-design. Their efficiency and versatility brings us one step closer to the dream of generating custom robots for every task."
2024,Methods for Extracting and Analyzing Political Content on TikTok,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.2924,"In this thesis, I investigate the dynamics of political discourse on TikTok, with a focus on crafting a comprehensive methodology for extracting and analyzing political content related to the 2024 U.S. Presidential Election. This research utilizes a blend of advanced computational tools and crowd-sourced evaluations to delve into the mechanisms through which political influence is both exerted and perceived on the platform. For data collection, the study employed TikAPI, a tool designed for systematic scraping of TikTok videos, which targeted specific political hashtags to amass a substantial dataset. This dataset was analyzed using a variety of innovative methods, including snowball sampling to ensure a representative range of political engagement, and integration with Python to automate the data collection process. Additionally, I utilized Large Language Models (LLMs) to evaluate the relevance and persuasive impact of the content, and these machine-generated insights were then benchmarked against human judgments. Overall, the findings indicate a slight preference for Republican discourse on TikTok. Moreover, I demonstrate that OpenAI’s GPT can effectively classify videos by topic, although human input remains essential for more nuanced tasks such as stance detection and evaluation of persuasive effect. This exploration into the political landscape of TikTok represents one of the first of its kind, with the primary aim of this thesis being to develop a methodology that will support future research in this field."
2024,Exploiting irregular parallelism to accelerate FPGA routing,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2472,"In the era of hardware specialization, field-programmable gate arrays (FPGAs) provide a promising platform for computer architects, combining the programmability of software with the speed and performance of hardware. Despite this, compiling hardware programs onto FPGAs can be incredibly time-consuming, making it hard to develop and iterate on complex FPGA programs. Of particular relevance is the routing phase, which takes a circuit’s technology-mapped netlist and routes its signals using the switches and wires present on a given FPGA architecture, often with a target of minimizing critical path delay. This optimization problem is known to be NP-hard, and existing algorithms for approximating it exhibit very little regular parallelism.
This thesis accelerates the routing phase of VTR 8.0, a commonly used, open-source research tool for FPGA CAD flow. We show that despite the lack of regular parallelism, routing still exhibits significant irregular parallelism. This parallelism can be exploited on parallel architectures that provide hardware support for ordered tasks and fine-grained speculation, such as the Swarm architecture. Using Swarm, we exploit the parallelism present at the core of VTR’s algorithm, achieving a 35.9x speedup on a single routing iteration of a large benchmark (cholesky_mc) on 256 cores."
2024,Equitable Bus Route Electrification Based on a Mixed Integer Linear Programming Approach,MIT CSAIL,MIT Student,MIT Advisor,Security,0.1749,"While public transportation has seen improvement over time with advancements in vehicle technology and urban planning, low-income populations do not see the full benefit of these advancements. The common approach to transportation planning is to distribute benefits in the most cost-efficient manner, meaning neighborhoods with the best existing infrastructure are likely to receive more timely benefits than low-income areas that require more costly updates. This disparity can be thought of as a lack of equity in transportation planning, where equity means that the population that needs a public service the most should benefit the most from improvement of that service.

This work focuses on improving equity within the proposed electrification of the Massachusetts Bay Transportation Authority (MBTA) bus network in Boston. We are interested in which routes should receive updates first to maximize equity, while understanding that focusing on equity poses an inherent cost trade-off. To solve this problem, an optimal subset of routes must be selected for electrification using an objective function that prioritizes routes with the lowest income riders and the highest levels of pollution from diesel buses.

Assuming an optimal cost structure for the full transition to battery-electric buses, and also assuming that not all depots and routes will be electrified on the same time scale, we use Mixed Integer Linear Programming (MILP) methods and a quantification of transportation equity in various objective functions to decide which bus routes originating from the Cabot depot should be prioritized for electrification benefits from an equity standpoint. We then analyze the sensitivity of our results to changes in the cost constraint and conclude the degree to which equity factors correspond to higher energy transition costs. The results show that high-pollution routes are less attractive from a cost standpoint than low-income ridership routes. It is also shown that a given percentage of total electrification costs can electrify a subset of routes with even larger percentages of total pollution and low-income ridership, meaning that the benefits of including equity factors are high for given cost levels in our problem scope."
2024,ZeroWD: Supporting Zero-Waste Garment Design with Linked Edits,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2216,"In traditional garment manufacturing, the way fabrics are cut produces significant waste due to inefficiencies in the design and layout of the garment panels on fabric. Recently fashion designers have begun to explore different ways to design and layout garment panels in more efficient ways. An extreme example of this efficient fashion design process is zero waste fashion design, which aims to use all available fabric in the resulting garment. Currently many zero waste fashion designers manually cut out the 2D patterns and experiment with their 3D shape. With zero-waste design being inherently strictly constrained by the dimensions of the fabric, designers need to perform meticulous calculations for tasks as resizing and restyling. In our work, we propose ZeroWD, a novel interactive design tool that assists zero-waste fashion design by bringing pattern layout and cutting earlier in the design process. With our tool, designers can design zero waste garment panels and simulate the garment’s 3D shape with realtime feedback. By embedding zero-waste design constraints into the system, we enable designers to focus on the creative design rather than tedious constraint solving. Our user study demonstrates that ZeroWD can help fashion designers create garments with minimal waste."
2024,"Illuminating the Nature of Dark Matter through Observation, Simulation and Machine Learning",MIT CSAIL,MIT Student,MIT Advisor,CV,0.2297,"Dark matter constitutes 85% of the matter content in the universe, yet its microscopic nature remains elusive. Discovering the nature of dark matter will not only greatly further our understanding of the universe but will almost certainly shed light on what lies beyond the Standard Model of particle physics. In this thesis, I discuss the progress I have made in the hunt for dark matter by proposing direct observation strategies in the present-day universe, building simulations for dark matter energy injection imprints in the early universe, and using Machine Learning to address the unique challenges in both of these tasks. Specifically, I explore using the echoes of astrophysical radio sources to probe axion dark matter, self-consistently simulating dark matter energy injection in the era of reionization, employing simple Neural Networks to improve these early universe simulations, and utilizing Machine Learning-powered inference techniques to tackle the problem of the Galactic Center 1-ray Excess."
2024,"Advancements in Management Science: Applications to Online Retail, Healthcare, and Non-Profit Fundraising",MIT CSAIL,MIT Student,MIT Advisor,Database,0.2417,"Management science is an evolving-field that requires novel models and algorithms, combining methods from statistics, optimization, and machine learning. This thesis presents advancements in management science across three domains: revenue management, healthcare, and non-profit funding platforms. The chapters in this thesis develop rigorous algorithms and techniques which are relevant in practice, and present data-driven insights into each of the application areas. 

Chapter 2 studies a personalized dynamic pricing problem commonly faced by online retailers. Customers arrive sequentially to the selling platform, and for each arrival the seller must make an immediate pricing decision for that customer. The seller aims to learn the demand as a function of price and customer covariates through price experimentation, while simultaneously earning as much total revenue as possible. Previous work on this topic have adopted a classical online learning setup, where the retailer begins the selling horizon with no information about the problem and gains all knowledge about the demand function from the online selling phase. However, this assumption is often not true in practice. Many retailers already possess some information about their product's demand from market research or previous sales data, and not utilizing this information is clearly suboptimal. The chapter develops a novel framework that allows the seller to incorporate historical data on pricing decisions and realized demand, and moreover enables one to study the effect that certain characteristics of this historical dataset have on online selling performance. Using this framework, a dynamic pricing algorithm is proposed which effectively uses both historical and real time data, and achieves provably optimal performance. Furthermore, a new distance measure is developed to quantify how close the historical pricing decisions are to being optimal. Using this distance measure, the chapter shows a surprising inverse relationship between this measure and the achievable online performance.  

Chapter 3 focuses on applying causal inference techniques to study the treatment efficacy of different antibiotics on patients with urinary tract infection. Up to 50% of women will experience a urinary tract infection (UTI) in their lifetime, making it the third most common indication for antibiotic treatment in the United States. Though national treatment guidelines encourage using one of three antibiotics as the first-line treatment, other second-line and alternative antibiotics are still commonly prescribed in practice. Studies on the efficacy of first-line versus second-line and alternative antibiotics for UTI are limited and dated. The chapter presents a retrospective cohort study using the claims database from Independence Blue Cross to determine the relative efficacy and adverse event rates between different categories of antibiotics. By combining causal inference techniques with automated feature extraction using the Observational Medical Outcomes Partnership (OMOP) common data model, evidence is found which supports the use of guideline-recommended first-line treatments for uncomplicated UTI. Specifically, the rate of treatment efficacy is higher for first-line antibiotics relative to alternatives. Surprisingly, the analysis also finds evidence which supports increased efficacy of first line agents relative to second-line antibiotics, which are of broader spectrum, albeit the effect difference is smaller compared to the comparison between first-line antibiotics and alternatives. This large-scale cohort study which includes a comprehensive collection of covariates provides much-needed evidence to support the continued recommendation of first-line drugs for the treatment of UTI. The chapter also suggests the feasibility for performing complex causal inference analyses using automated feature engineering packages for OMOP-formatted datasets.

Chapter 4 studies an online matching problem where sequentially arriving donors must be matched to projects needing funding on peer-to-peer philanthropic crowdfunding platforms such as DonorsChoose.org. Empirical studies have shown that (i) donors have heterogeneous preferences over the projects, and (ii) many return to make more than one donation. Facing such donors, the platform’s aim is to match each donor to one of their preferred projects so as to maximize the total donation without over-funding any projects and without knowing the arrival pattern. Previous work in the literature have not studied the effect of returning donors on algorithm performance. The chapter shows an upper bound on the best achievable worst-case performance of any online algorithm which reveals the relationship between donor return rate and algorithm performance. Furthermore, numerical analysis shows that a simple known algorithm achieves a performance that improves with the number of returning donors without differentiating between the original and return donors. The algorithm is intuitive and straightforward to implement, and the results shed light on the practical value that returning traffic can bring for fundraising platforms."
2024,Dynamics of Gradient Flow with Contrastive Learning,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3027,"Contrastive learning (CL), in di erent forms, has been shown to learn discriminatory representations for downstream tasks without the need of human labeling. In the representation space learnt via CL, each class collapses to a distinct vertex of a simplex on a hypersphere during training. This property, also seen in other types of learning tasks, might explain why CL works as well as it does. Having class collapse on the test distribution, which determines how well the model generalizes to new samples and new classes, is tied to class collapse on the training distribution under certain conditions as studied by Galanti et al. (2022). In the case of CL, minimizing the contrastive loss has been shown to lead to collapse during training by Graf et al. (2021). In a recent study, Xue et al. (2023) show that the minimizing the contrastive loss is not enough to observe class collapse in the representation space for a single layer linear model and that we need minimum norm minimizers for the collapse to happen. However, their results don't explain how class collapse can occur without adding an explicit bias. The implicit bias of the gradient descent is a likely candidate to explain this phenomena. Here, we investigate the gradient ow of the spectral contrastive loss and give a theoretical description of the learning dynamics."
2024,Classical Commitments to Quantum States,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2617,"We define the notion of a classical commitment to quantum state scheme, which allows a quantum prover to compute a classical commitment to a quantum state and later open each qubit of the state in either the standard or Hadamard basis, while limiting communication with the verifier to a classical channel. Our scheme strengthens the notion of a measurement protocol from [Mah18], which is binding only in the standard basis. We construct our commitment scheme from the post-quantum Learning With Errors (LWE) assumption, and rely directly on any noisy trapdoor claw-free function family that satisfies the adaptive hardcore bit property first introduced in [Bra+18]."
2024,Machine Learning for Sepsis Prognosis: Prediction Models and Dissecting Electronic Health Records,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3125,"Sepsis is the body's extreme response to an infection. It is a life-threatening medical emergency. Given the heavy burden sepsis has posed on the health care system, extensive research in the area has been performed to facilitate sepsis diagnosis. Sepsis prognosis can support the assessment of the likely progression of the disease and thus inform treatment decisions, but it is much less explored. Here I present two approaches to build sepsis prognosis models. First, I introduced the idea of assessing neutrophil function from simple-to-obtain phase microscopy images. I developed an experimental pipeline using measurement of reactive oxygen species genera=on as a label of neutrophil function. I generated a large neutrophil imaging dataset and explored different deep learning approaches to predict neutrophil activation state. Second, I developed machine learning models to predict sepsis patient future clinical score using electronic health records. As part of the effort, I developed a multidatabase extraction pipeline to facilitate electronic health records extraction process. My work demonstrates the potential of using deep learning models to evaluate functional aspects of the immune system and to predict sepsis patient future state, which could provide significant insight into sepsis prognostic monitoring and is easy to adapt in clinical settings. It is of great significance to understand the input data in developing reliable and generalizable machine learning for healthcare models. It is also increasingly apparent that machine learning for healthcare models can predict patient sensitive information from data that does not explicitly encode it. However, we lack a clear understanding of the extent of the problem: what types of sensitive information can be predicted and how it generalizes to different models or different datasets. We lack approaches to develop models that can make clinical inferences but not infer sensitive information. Critically, we also lack approaches to explain such data encoding. Using electronic health records, I thoroughly investigated the ability of machine learning models to encode a wide range of patient sensitive information. I developed a strategy to ensure that clinical prediction is minimally based on patient-sensitive information. I presented an approach that can explain feature importance in patient sensitive information encoding. This set of studies not only allows us to gain deep understanding of the sepsis patient clinical score prediction model but also are applicable to a variety of machine learning models utilizing time-series data."
2024,Harvesting Innovation: Exploring the Potential of an AI-Enabled Platform to Revolutionize Agricultural Labor Markets,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2488,"Labor shortages in agriculture are a global problem that cause revenue losses and resource waste. In the US, immigration is the main source of such laborers, but labor immigration has decreased by 75% in recent years and costs due to unharvested crops due to labor shortage in  agriculture were estimated at USD 3.1 Billion per year in 2014.

This thesis investigates the persistent labor shortages in the agricultural sectors of the southern United States and Mexico, exploring the feasibility of alleviating these shortages through a labor matching platform enhanced by artificial intelligence (AI). With a focus on the economic implications and structural deficiencies in agricultural labor markets, the study examines how a digital platform can bridge the gap between supply and demand for agricultural labor. 

The research employs a multi-dimensional approach that includes extensive literature review, in-depth interviews with stakeholders, system dynamics modeling, and action research involving the launch of a company and release of a Minimum Viable Product (MVP). The MVP, a foundational component of the proposed digital platform, has been tested in the market to gather quantitative data and insights using web advertising. 

The findings highlight the platform’s potential to streamline labor matching processes, improve transparency, and increase efficiency in the agricultural labor market. Additionally, the integration of AI provides intelligent matchmaking capabilities, predicting and aligning labor needs with available workers more effectively. 

Not only does this thesis provide a potential business model to tackle a critical economic problem, but it also contributes to the broader discourse on the role of technology in transforming traditional industries in advanced and emerging economies."
2024,Machine-Learning based Ship Traffic Prediction in the Suez Canal,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3307,"This study implements and evaluates two approaches for predicting the average annual daily ship traffic (AADT) of ships within the Suez Canal with a focus on evaluating how deep-learning techniques can be leveraged for both approaches. The first approach is a novel method that utilizes both satellite imagery and AIS technology to predict the AADT. In order to do so, a 2-stage model is implemented that combines an image detection model followed by a correction factor model. The image detection model employs Mask R-CNN, a deep-learning neural network, and the correction factor model utilizes Long Short-Term Memory (LSTM), a recurrent neural network, to train on historical AIS data. Results of the 2-stage model using LSTM demonstrate positive indication of technical feasibility for the approach due to ground-truth AADT values falling within the interquartile range of predictions for all validation sets. Furthermore, although the interquartile ranges have considerable variation, the 2-stage model with LSTM had a mean absolute percentage error (MAPE) of 13.2% based on its median AADT predictions; this is a successful outcome especially when considering the high variance of vessel traffic and the noisiness that comes with satellite imagery’s small sampling rate as just snapshot moments in time. In addition to the 2-stage model, this study also implements a second approach involving a discrete-event simulation (DES) to estimate AADT, and we evaluate how the DES can benefit from using deep-learning techniques like LSTM. Results from the DES model with LSTM indicate an improved 90.8% reduction on the interquartile range of AADT predictions in comparison to that of the 2-stage model. Additionally, the DES model with LSTM had an MAPE of 3.8% for its median AADT predictions, demonstrating strong predictive accuracy. Overall, patterns within the AIS data indicate that despite the effects of Covid-19 in 2020, there is an increase in traffic in subsequent years especially in 2022 due to a rebound effect."
2024,Force Feedback and Tactile Sensing for Robotic Teleoperation of Contact Rich Manipulation Tasks,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2429,"Imitation learning has shown promising results in teaching robots new skills. We propose augmenting the ALOHA bimanual teleoperation system with haptic feedback to obtain higher quality expert demonstrations. We add two types of haptic feedback: force feedback and cutaneous feedback in both a real and simulation teleoperation system. Additionally, we propose to add tactile sensors to observe the impact of tactile data to imitation learning models in solving fine manipulation tasks."
2024,Simulating Dynamical Systems from Data,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2533,"The ever-increasing availability of data from dynamical systems offers an opportunity for automated data-driven decision-making in various domains. However, a significant barrier to realizing this potential is the issues inherent to these datasets: high-dimensionality, noise, sparsity, and confounding. In this thesis, we propose methods to exploit the richness in the structure of such datasets to overcome the above-mentioned problems while undertaking various inference tasks.

Central to these methods is a key factorization characterizing the function governing the dynamics. Specifically, we harness trajectories from different, yet related, dynamical systems. We posit that the function governing the dynamics of each individual system can be factorized into a linear combination of latent separable functions of the state and action. Crucially, these latent functions are shared across the different dynamical systems. This principled factorization structure provides guidance on how to devise theoretically sound methods that perform well empirically across a variety of tasks. These tasks include time series imputation and forecasting, change point detection, reinforcement learning, and trace-driven simulation in networked systems.

Exploiting the principled factorization structure has paved the way for the contributions we make in different tasks. First, we propose and analyze algorithms for mean and variance estimation and forecasting of time series with varying noise models, data missingness patterns, and assumptions on the factorization structure. These algorithms employ variants of the classical multivariate singular spectrum analysis (mSSA) algorithm and establish a link between time series analysis and Matrix/Tensor Completion. Second, we develop and analyze an algorithm for change point detection inspired by the factorization structure and based on the cumulative sum (CUSUM) statistic. This work extends the analysis of CUSUM statistics traditionally done for the setting of independent observations. Finally, we explore the potential gains of considering the factorization structure in simulating Markov Decision Processes (MDPs). We then build upon this approach to accommodate MDPs with time varying parameters with the specific application of trace-driven simulation in networked systems."
2024,Modeling Control Signals for Reconstruction-based Time Series Anomaly Detection,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3257,"Automated time series anomaly detection methods can provide insights while reducing the load placed on human experts in a variety of settings. Machine-generated signals, such as those produced by sensors, often contains control signals in addition to the target observation signal. These signals may provide additional insight about the normal vs. abnormal properties of the observation signal. Despite this fact, even recent anomaly detection methods using deep learning give limited consideration to the relationship between observation and control signals, often failing to handle the control signal at all. This work proposes pre-processing, modeling, and evaluation methods for multivariate, heterogeneous time series to examine how using information from the control signal can improve anomaly detection. We develop a deep learning reconstruction-based pipeline and test its performance on the NASA Soil Moisture Active Passive (SMAP) satellite and the Mars Science Laboratory (MSL) Rover, which contains heterogeneous sensing data from exploratory missions. The pipeline follows the Sintel machine learning framework and is accessible through the Meissa library, which builds on the capabilities of the open-source library Orion for end-to-end unsupervised time series anomaly detection pipelines."
2024,Design and Fabrication of High Frequency Electromagnetic Coil for Magnetic Particle Imaging,MIT CSAIL,MIT Student,MIT Advisor,CV,0.1792,"Magnetic Particle Imaging (MPI) is a promising modality which uses Magnetic Nanoparticles (MNPs) for tracer-based imaging in biomedical applications. Aside from their use in imaging, MNPs are increasingly being utilized for therapeutics, controlled targeted drug delivery, and diagnostics. These techniques depend on the behavior of MNPs when exposed to alternating magnetic field of a certain frequency and amplitude. However, the frequency typically used for imaging is 25kHz, while the transduction behaviors desired for these biomedical applications are seen at low radio-frequencies and higher amplitude fields than ones used for imaging. This work presents a high frequency electromagnetic coil which fulfills operational, safety, and geometric parameters necessary for incorporation in a custom MPI system and will allow us to simultaneously image and stimulate at specific locations within the body of a mouse. Optimization of the instrument is done through experimentation and electromagnetic theory, with focuses on parasitic elements and metallurgical phenomena. A resonant tank and direct cooling with a water pump allows for increased field strength while maintaining thermal and radio-frequency energy absorption standards for in vivo experiments."
2024,"Fast Multistage Compilation of Machine Learning
Computation Graphs",MIT CSAIL,MIT Student,MIT Advisor,NLP,0.2629,"Machine learning applications are increasingly requiring fast and more computational power. Many applications like language models have become so large that they are run on distributed systems in parallel. However, getting into the details of optimally scheduling or even just running machine learning models on distributed systems can be a distraction for researchers ideating models. Hence there has been development of abstractions to facilitate running machine learning models in parallel on distributed systems. We present a compiler for the StreamIt language- a language made for abstract signal processing and multicore programming. We use that abstraction as a way to distribute the computation of machine learning models programmed in PyTorch."
2024,Understanding the Mechanisms of Shock-Induced Deformation in Polymeric Systems,MIT CSAIL,MIT Student,MIT Advisor,Security,0.1966,"Shock waves, commonly resulting from ballistic or explosive impact in military scenarios, are highly destructive and pose significant health risks due to their rapid, intense energy transfer. To mitigate these effects, it is crucial to design armor capable of absorbing and dissipating shock wave energy, thus safeguarding the wearer. Traditional materials like woods and metals often fall short in meeting these requirements due to their low toughness, high weight, limited flexibility, or manufacturing challenges. Furthermore, even when suitable materials are identified, they often have unclear physical deformation mechanisms contributing to their dissipative properties; thus, it is difficult to generalize the benefits of one material in order to find alternatives.

This thesis focuses on the study of polymeric materials, which often outperform traditional materials in terms of shock energy absorption. The vast design space of polymers affords them a diverse set of physical properties and adaptability to different applications. Researchers can tailor these properties by altering the polymers' type, composition, or structure. In order to obtain detailed mechanistic understanding of the shock response of some polymeric materials, atomic and molecular scale simulations are leveraged and the configurational and vibrational responses of the systems analyzed. The thesis work consists of three major projects: (1) a theoretical study of the statistics of Gaussian polymer chains under harmonic applied fields, (2) a computational study of semicrystalline and crystalline models of polyethylene (PE) undergoing shock deformation, and (3) a computational study of liquid ethanol systems undergoing shock deformation.

The theoretical study of Gaussian chains generalizes previous literature models and is shown to well approximate a variety of applications involving polymer confinement. A wave equation is derived from the force-compression relationship of the Gaussian chains--with the addition of a viscous damping fluid--and is connected to the well-studied Korteweg de Vries equation. The effects of material properties (dispersion, dissipation, and nonlinear elasticity) on the propagation of a shock wave are demonstrated explicitly.

The computational study of semicrystalline PE considers systems of three different crystallinity fractions; their configurational changes under shock are analyzed, distinguishing the responses of the crystalline and noncrystalline regions of the systems. Physical mechanisms underlying the deformation response of these materials, including crystallographic slip and loss of nematic order, are identified as a function of shock pressure. The prominence of these mechanisms is shown to be a non-monotonic function of lamellar thickness and crystallinity fraction, and they each contribute to shock energy storage.

The computational study of ethanol reveals the configurational and vibrational changes undergone by the liquid in response to elevated shock pressure and temperature. Shifts of vibrational peaks are observed and compared with experimental results from our collaborators. The hydrogen bonding network responds to energy from the shock wave, resulting in structural and dynamical alterations which may have implications for thermal conductivity and reduction of hot spot formation from shocks.

Overall, this thesis presents a multipronged and detailed approach to understanding shock deformation in polymeric and organic systems, offering mechanistic insights and methodological frameworks applicable to a broader range of materials."
2024,Toward Real-time Earth Observation with Satellite Constellation Crosslinks and Propulsion,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2373,"The development of remote sensing small satellite constellations has created the potential for high-resolution Earth-observation data to reach end users faster. This work investigates how propulsion and intersatellite links enable constellations to continuously collect and deliver data faster than constellations without these capabilities.

This work has four contributions. The first contribution is a constellation simulation framework that is based on open-source libraries. This simulation framework can propagate satellites and execute propulsive maneuvers. The second contribution is a planning and scheduling algorithm for propulsive maneuvers, target observation times, and optimal data routing paths. The third contribution is the development of high performance constellation designs with respect to constellation cost and the following metrics: age of information, system response time, and total pass time. The cost model is developed from two separate models: the Small Satellite Cost Model (SSCM) and a launch cost model developed in this work. The fourth contribution is a set of cost-estimating relationships (CERs) that models the trade-off between cost and system performance in terms of the aforementioned metrics.

The new simulation framework of contribution 1 is verified against the industry-standard software Systems Tool Kit (STK). The simulation framework is used to run 21 different constellation designs, 3 different satellite models, and 432 distinct ground targets. These scenarios are run during each of the four seasons to eliminate geometric biases for a total of 108,864 individual scenario simulations. A single satellite executing the reconfiguration algorithm produces up to a 125% increase in pass time over seven days when compared to an identical satellite without propulsive capabilities. For an access cone with a nadir half-angle of 20 ,thereconfigurationalgorithm produces a 67% increase in pass time. Comparing the cost of inter-satellite link (ISL) and reconfiguration-capable satellites versus (i) only ISL-capable satellites and (ii) a baseline satellite without ISL or reconfigurable capabilities, a Pareto optimal analysis revealed 29% of designs had both propulsion and intersatellite link capabilities when optimizing for age of information, 7% of designs had both propulsion and intersatellite link capabilities when optimizing for system response time, and 33% of designs had both propulsion and intersatellite link capabilities when optimizing for total pass time. The CERs show that for constellations costing between $150M and $1B (FY24), age of information can be reduced by 32 seconds for every million dollars spent, system response time can be reduced by 35 seconds for every million dollars spent, and total pass time over 3 days can be increased by 2 seconds for every million dollars spent."
2024,"Generative Design Tools: Implications on Design Process, Designer Behavior, and Design Outcomes",MIT CSAIL,MIT Student,MIT Advisor,Database,0.2301,"Generative design tools, empowered by recent advancements in computational algorithms, offer the opportunity for human designers and design tools to collaborate in new, more advanced modes throughout various stages of the product design process to facilitate the creation of higher performing and more complex products. Much of the research focuses on the technical development and application of these tools, while less attention has been paid to how generative design tools are used from the designer’s perspective. Three main contributions of this dissertation include a development of a generative design process, observations of the implications of the use of generative design tools, and an understanding of how designers balance multiple objectives throughout a generative design process.  A grounded theory approach based on the experiences of designers was first used to develop a generative design process. Six in-depth interviews were conducted with experienced designers from different disciplines who use commercial generative design tools in their work, detailing the design processes they followed. A qualitative-based coding and analysis of the interviews was used to generate 161 coded themes describing the design process. Through these themes, a provisional process diagram for generative design and its uses in the early-stage design process is proposed to outline explicit and implicit stages of the design process.  Several implications of the use of generative design tools on the design process and designer behavior were developed through additional analysis of the interviews. The early 5  stages of defining tool inputs bring about a constraint-driven process in which designers focus on the abstraction of the design problem. Designers will iterate through the inputs to improve both quantitative and qualitative metrics, such as engineering performance and product styling. This learning-through-iteration allows designers to gain a thorough understanding of the design problem and solution space. This can bring about creative applications of generative design tools in early-stage design to provide guidance for traditionally designed products.  It was observed that generative design tools primarily allow for quantitative inputs to the tool while qualitative metrics, in particular aesthetics, are considered indirectly by designers. To explore this further, controlled lab experiments were conducted to understand how designers balance quantitative and qualitative objectives while using generative design tools. Thirty-four participants completed two design tasks (with and without generative design tools) with the same qualitative and quantitative objectives. Counterintuitively, designs created in the task without generative design tools had a statistically higher quantitative performance than those created with generative design tools. On the other hand, the designs produced with generative design tools displayed a greater aesthetic diversity and expanded a larger portion of the objective space. Participants also expressed the ability to focus on the qualitative objectives by delegating the quantitative objective to the generative design tool. This showcases the potential for generative design tools to assist in the design process and leveraging the expertise of both the human designer and the generative design tool to allow for greater consideration of various objectives throughout the design process."
2024,Essays in Industrial Organization and Labor Economics,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2538,"This thesis consists of three chapters, two in Industrial Organization and one in Labor Economics. The first and second chapters study industrial technologies: the first explores how machine learning changes decision-making of heavy duty truck technicians, while the second studies technological switching in the shale industry. The third chapter studies wage garnishment in the United States.

The first chapter (joint with Adam Harris) uses observational data to explore how a predictive algorithm changes human decision-making. Using a novel, rich decision-level data set from the maintenance of heavy-duty trucks, we document how skilled technicians' decision-making is changed by the introduction of an algorithm designed to predict the risk of truck breakdowns. We develop and estimate a model of technician decision-making that accounts for variation in monetary and non-monetary costs. Using an embedded neural network, we flexibly estimate technicians' beliefs about the probability of truck breakdowns both before and after the introduction of the algorithm. Comparing these estimated beliefs with an objective breakdown probability, we find that the algorithm significantly improves technicians' ability to predict breakdowns: the algorithm narrows the gap between actual and optimal costs by 79%. All of this gain comes from decreased repair costs, suggesting that the algorithm primarily helps technicians avoid low value repairs.

The second chapter studies technology in a different setting: the U.S. shale industry.  In late 2014, global oil prices dropped precipitously, driving U.S. shale producers out of the market. As the number of new wells completed dwindled, productivity began to rise sharply, beginning a steepened upward descent that continued through 2019. This chapter draws on detailed well-level data from the Bakken shale play in North Dakota to tease apart several classic explanations for these trends, including Schumpeterian creative destruction and technological improvement. I document firm-level jumps from gel-based completions to slickwater after the price shock, with earlier jumps for mid cap and private firms. However, I find that improved geological targeting (or ""high-grading"") and slickwater adoption fail to account for over 70% of the productivity increase.

The third chapter (joint with Anthony DeFusco and Brandon Enriquez) uses administrative data to investigate wage garnishment in the United States. Wage garnishment allows creditors to deduct money directly from workers' paychecks to repay defaulted debts. We document new facts about wage garnishment between 2014 and 2019 using data from a large payroll processor who distributes paychecks to approximately 20% of U.S. private-sector workers. As of 2019, over one in every 100 workers was being garnished for delinquent debt. The average garnished worker experiences garnishment for five months, during which approximately 11% of gross earnings is remitted to their creditor(s). The beginning of a new garnishment is associated with an increase in job turnover rates but no intensive margin change in hours worked."
2024,Optical Property Prediction and Molecular Discovery through Multi-Fidelity Deep Learning and Computational Chemistry,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2911,"Optical properties are crucial for the design of molecules for numerous applications, including for display technologies and biological imaging. The accurate prediction of these properties has been the subject of decades of work in both physics-based approaches and statistical modeling. Recently, large datasets of both computed and experimental optical properties have become available, along with the advent of powerful deep learning approaches cable of learning meaningful representations from these large datasets. This thesis presents new approaches for predicting optical properties by fusing the experimental and computational data in multi-fidelity models that achieve greater accuracy and generalizability than previous methods. Additionally, it conducts a thorough benchmark of various strategies for handling multi-fidelity data to inform the modeling choices of future practitioners working with optical properties and beyond. Despite the greater availability of optical property data recently, the near-infrared (NIR) region of the spectrum remains more data-sparse despite its promise in many applications. This thesis demonstrates the shortcomings of existing methods for predicting optical properties in this region of chemical space and recommends best practices for future research in this area. Finally, this thesis highlights successful usage of data-driven optical property prediction for the discovery of novel molecules for specific applications."
2024,Broadband single and multimode quantum light generation using optical nonlinearities,MIT CSAIL,MIT Student,MIT Advisor,CV,0.1816,"There is a growing effort in many fields of physics to bridge the classical and quantum realms. To our best understanding, our world is governed by the laws of quantum mechanics, but some of its most interesting features - such as the ability to morph uncertainty and noise - are washed out when system sizes become too large. Light is the ideal playground to investigate the interplay between the classical and quantum domains, with its well-known particle-wave duality and diverse behaviors at both the classical wave and single photon levels. To this end, there is significant interest in generating quantum states of light that can be harnessed for applications in the classical world we are most familiar with. However, maintaining ""quantumness'' as the number of photons grows large has proved challenging due to the detrimental effects of loss. In this thesis, I describe two theoretical proposals to make macroscopic quantum light a reality. I focus on bright intensity squeezed states of light that have intensity noise far below the standard quantum limit. If realized, these states would bring the quantum mechanical phenomenon of squeezing to macroscopic intensities, which in turn could pave the way towards widespread quantum light sources that offer enhanced signal to noise ratios. I describe two distinct methods that use tools from nonlinear optics and dissipation engineering to realize broadband squeezing in both single and multiple frequency modes. I show that the squeezing can be tunable across a wide range of the electromagnetic spectrum that spans frequencies where quantum light has never been generated."
2024,The Impact of Thermostat Automation and Retail Rate Designs on Cooling and Heating Flexibility: Balancing Consumer Preferences and an Efficient Grid,MIT CSAIL,MIT Student,MIT Advisor,Security,0.1736,"Flexibility in household energy consumption is crucial for improving grid efficiency and reducing peak electricity demand. The ongoing impact of climate change and the move towards electrification worsen these challenges, emphasizing the need for effective peak demand reduction strategies. Current approaches often involve peak pricing retail tariffs, behavioral responses to grid operator notifications, or expensive technologies such as demand-side batteries. However, these methodologies rely on unpredictable consumer participation or substantial capital investments. On the other hand, the growing use of smart thermostats presents an opportunity for passive, efficient control of household energy consumption. Combining smart thermostats with appropriate price signals creates an opportunity to optimize the balance between energy cost and thermal comfort. This work examines the role of smart thermostat automation and dynamic retail rate designs in maximizing heating and cooling flexibility while ensuring consumer comfort. The research introduces a new approach to demand-side management by using reinforcement learning (RL) to optimize thermostat settings based on individual thermal preferences and price signals. A comprehensive testbed simulation framework was developed to analyze these effects, incorporating bottom-up energy modeling, individualized thermal comfort profiles using smart thermostat data, and advanced thermostat controls to investigate the impacts of various rate designs on residential energy demand. The study evaluates these impacts at a population level, considering the effects on over 80 household archetypes across a localized region. Key findings show that partitioned time-of-use rates with moderate pricing shifts effectively reduce energy usage without creating new peaks, unlike more aggressive pricing strategies that can lead to pre-cooling-induced new peaks. These insights offer valuable guidance for policymakers and utility operators in designing rate frameworks that decrease overall electricity consumption and peak demand without compromising personal comfort."
2024,"Modeling spatial mapping, memory and their underlying mechanisms in the hippocampal complex",MIT CSAIL,MIT Student,MIT Advisor,Database,0.2446,"Humans form mental representations of the space and environment around them. This ability is fundamental to tasks such as navigation, spatial reasoning, and understanding the relationships between objects in the environment. Spatial mapping in humans involves several cognitive processes, including perception, memory, and spatial reasoning. Memory plays a crucial role in spatial mapping. As individuals move through an environment, they encode and store information about the spatial layout, which they can later recall to navigate or perform tasks. Further, spatial memory involves similar brain regions as those implicated in sequential episodic memories. Research on human spatial mapping has greatly advanced our understanding of how humans form these mental representations, but leaves us some ways from a complete understanding. In particular, it has been difficult to understand what makes human spatial representations generalizable enabling few-shot learning of maps of novel spaces, how humans store the vast amount of spatial information (maps) experienced through their lifetimes, and what is the connection between spatial memory and episodic memory in the brain, and why is it significant? In this thesis, I aim to answer these questions. First, I ask whether hierarchical spatial representations form the basis of generalizable spatial representations leading to efficient exploration of novel spaces. I present a Map Induction framework that uses a compositional hierarchy to represent spaces, and present results on its utility for exploring novel spaces. Second, I ask how humans store the vast amount of information (e.g., compositional map primitives required to form hierarchical spatial representations) experienced through their lifetimes. I present a neural model called MESH (motivated by brain’s entorhinal-hippocampal system), that has an exponential capacity and shows a gradual decay in retrieval quality with an increase in the number of stored memories rather than a catastrophic drop. Third, I present Vector-HaSH, a model of the entorhinal-hippocampal circuit that forms an instance of MESH, preserving all its properties. This model unifies-general associative memory, spatial memory and episodic memory providing a computational hypothesis for the unification of spatial and episodic memory roles of the hippocampal complex. Overall this research bridges the computational, algorithmic and implementation levels of analyses for explaining how humans represent and reason about spaces."
2024,Design and Synthesis of Stimuli-Responsive Polymers with Programmable Cleavability,MIT CSAIL,MIT Student,MIT Advisor,Database,0.1928,"Polymers comprise a large portion of modern-day materials, from everyday plastics that we can hold and use, to nanomaterials imperceptible to the naked eye. Applying synthetic chemistry to impart structural changes to established polymers offers a promising path to introduce novel functionalities for applications ranging from biology to sustainability. In particular, this thesis explores the synthesis, characterization and evaluation of polymeric platforms containing rational incorporation of moieties that can undergo chemical cleavage, effecting enhancements in their design and performance. In the first half, we explore advancements to linker design and controlled release of payloads from molecular bottlebrush polymers. The first chapter introduces bottlebrush polymers as nanocarriers for therapeutics, and provides a detailed literature analysis of the synthetic and architectural developments that have been reported for these constructs, as well as outlooks for the future. The second chapter reports the first synthesis of peptide-containing bivalent bottlebrush (co)polymers (BBPs), featuring caspase-3-cleavable peptides linked to fluorogenic probes that provide a “turnon” signal upon enzymatic cleavage. The impacts of different architectural features of these polymers on enzyme access reveal insights into the interactions of enzymes with BBPs, and provide design criteria for future therapeutic systems leveraging this approach. The third chapter investigates a synergistic approach to treating pancreatic ductal adenocarcinoma (PDAC) with drug-loaded BBPs by leveraging multiple facets of structural modularity, including linker and drug identities and concentration ratios. This mechanism-guided approach to combination therapy is validated with the translation of in vitro studies that identify synergy across axes of both drug release timing and mechanism of action to in vivo validation of enhanced therapeutic efficacy of the combination BBP system. The remaining two chapters are a departure from BBPs, instead introducing a novel approach to cleavable comonomers for improving plastic end-of-life sustainability. The fourth chapter thus provides detailed background on the current plastic waste outlook, vinyl polymers and their synthesis, radical ring-opening polymerization, and current approaches to cleavable comonomers and the end-of-life options they offer commodity polymers. The fifth and final chapter reports the first “mixed” cleavable comonomer approach to degradable polymers towards a polyacrylic acid system optimized for biodegradability. A computational model offers parameters for controlling degradation fragment molecular weight and dispersity that are validated experimentally, and the material performance properties of the homopolymer are retained for its cleavable analog. Overall, this thesis leverages structure-activity relationships of cleavable functionalities in stimuli-responsive polymers, and expands the scope under which they can be utilized during their productive lifetime or processed thereafter."
2024,Numerical and Analytical Methods in Low-Dimensional Strongly Correlated Quantum Systems,MIT CSAIL,MIT Student,MIT Advisor,Database,0.1641,"The study of low-dimensional strongly correlated quantum systems lies at the intersection of intricate theoretical models and practical numerical methods, offering deep insights into condensed matter physics. This thesis explores the application of various numerical and analytical methods to these systems. It addresses universal behaviors and phase transitions, exemplified by the phenomenon of multiversality. Specifically, the transition from a 1D Luttinger liquid to a charge density wave insulator, characterized by partly Kosterlitz-Thouless transition and partly Ising transition, is analyzed using both analytical renormalization group calculations and numerical density matrix renormalization group simulations. Additionally, the thesis introduces a statistical smoothing spline method to pinpoint transition points systematically. The work extends to quantum dynamics, presenting a generic theoretical framework for analyzing quantum-classical adiabatic dynamics with learning algorithms. A provably efficient adiabatic learning (PEAL) algorithm with favorable scaling properties is developed. The algorithm is numerically validated on the 1D Holstein model, demonstrating its precision in predicting dynamics. Furthermore, the thesis derives a Hamiltonian lattice formulation for the 2+1D compact Maxwell-Chern-Simons theory, providing an analytical solution that aligns with continuum theories and facilitating future numerical applications. Through these explorations, the thesis underscores the complementary roles of numerical and analytical methods in advancing the understanding of complex quantum systems."
2024,Process Digitalization: 3D Deep Learning in Manufacturing Applications,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2744,"The surge in artificial intelligence (AI) popularity and investment has significantly impacted various sectors, including automotive, aerospace, and defense. Smaller companies at the base of these supply chains often lack the resources and knowledge for AI implementation compared to larger original equipment manufacturers, creating a unique opportunity for these smaller companies to leverage AI for growth. However, many AI initiatives in these smaller firms stall at the prototyping phase. This research outlines, from planning to execution, steps and considerations for implementing an AI initiative at a small to medium sized manufacturing company. As well, given the importance of 3D data in the industry, the research also conducts a deep dive on working with, analyzing, and integrating 3D data into an AI model using various techniques, from statistical analysis to 3D deep learning. Discussion on the different
data representations including point clouds, voxels, polygon meshes, depth maps, and boundary representations, and their trade-offs help with the determination of which representation is best for different use-cases. Most of the techniques apply to various unstructured data types to enable multi-modal inputs to a descriptive, predictive, or prescriptive AI model. Additionally, beyond the technical requirements, an entire section is dedicated to discussing the human element in this whole process, focusing on a company’s personnel and cultural aspects, which is often where initiatives can succeed or fail."
2024,LLM-Directed Agent Models in Cyberspace,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3764,"Network penetration testing, a proactive method for identifying vulnerabilities in cy- berspace, has long been the domain of human experts. However, rapid advancements in machine learning have opened up new possibilities for automating many of these tasks. This thesis aims to explore the application of Large Language Models (LLMs) for automating penetration tests and Cyber Capture the Flag (CTF) challenges, bridging the gap between static tools and dynamic human intuition in cybersecurity.
This work provides an evaluation framework for assessing the performance of LLMs in autonomously solving CTF challenges, with an emphasis on understanding the capabilities, limitations, and best prompting strategies for LLMs in this domain. Notably, this thesis presents an agent configuration that offers a 102% improvement in challenge completion on a database of PicoCTF challenges compared to the published baseline. By analyzing a variety of agent strategies, response formats, and historical action representations in the context of CTF challenges, this work aims to provide insights into the best practices and limitations in leveraging LLMs for cybersecurity tasks. Additionally, this work proposes a hierarchical architecture to guide an LLM-enabled agent in performing complex, multi-step penetration testing tasks with strategic foresight. This proof of concept approach shows success in entry level challenges. While LLMs exhibit impressive capabilities, they are limited out of the box in their ability to solve complex, multi-step tasks requiring exploration, necessitating approaches such as those described in this work to improve performance in these areas."
2024,Geo-UNet: A Geometrically Constrained Neural Framework for Clinical-Grade Lumen Segmentation in Intravascular Ultrasound,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3599,"Precisely estimating lumen boundaries in intravascular ultrasound (IVUS) is needed for sizing interventional stents to treat deep vein thrombosis (DVT). Unfortunately, current segmentation networks like the UNet lack the precision required for clinical adoption in IVUS workflows. This arises due to the difficulty of automatically learning accurate lumen contour from limited training data while accounting for the radial geometry of IVUS imaging. We propose the Geo-UNet framework to address these issues via a design informed by the geometry of the lumen contour segmentation task, building anatomical constraints directly into the architecture. We first convert the input data and segmentation targets from Cartesian to polar coordinates. Starting from a convUNet feature extractor, we propose a two-task setup, one for conventional pixel-wise labeling and the other for single boundary lumen-contour localization. We directly combine the two predictions by passing the predicted lumen contour through a new activation (named CDFeLU) to filter out spurious pixel-wise predictions. Our unified loss function carefully balances area-based, distance-based, and contour-based penalties to provide near clinical-grade generalization in unseen patient data. We also introduce a lightweight, inference-time technique to enhance segmentation smoothness. The efficacy of our framework on a venous IVUS dataset is shown against state-of-the-art models. We will make the code repository for this project available soon after approval from industry collaborators."
2024,Harnessing Intelligent Audio-Gesture Interfaces For Wearables As A Sleep Aid,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2416,"Insomnia—difficulty in initiating and maintaining sleep—affects a significant portion of the global population. The mainstream adoption of wearable computing presents a unique opportunity to study and aid sleep at an individual level. Here we introduce Zzzonic, a smart sleep-aid application designed for smartwatches that leverages cognitive psychology and human-computerinteraction (HCI) to facilitate sleep onset by engaging users in audio tasks as a formof intrusive thought control. A significant aspect of Zzzonic's functionality is its adaptive control system, which estimates sleep onset latency in realtime by monitoring indicators such as motion anduser response. The system then progressively modifies the characteristics of the audio tasks to minimize sleep onset latency. This thesis evaluates Zzzonic through a series of user trials conducted throughout the development of the app, accessing the capacity to predict and control sleep onset. The results indicate accurately predicting sleep onset latency in realtime as a control signal is possible but there was no evidence indicating the system could minimize slope onset latency. The inclusion of more indicator signals and machine learning techniques is likely to significantly improve realtime sleep onset latency prediction. Future work on computer-modulated intrusive thought control would benefit from the evaluation of task design, intrusive thought indicators and identifying an adequate control framework."
2024,Thermal Interaction of Inert Additives in Energetic Materials,MIT CSAIL,MIT Student,MIT Advisor,Security,0.1918,"Energetic materials are used for a variety of applications, including airbag deployment and solid rocket fuels, that require high energy density and various energy release rates. The energy release rate, determined by how fast the material burns, is often thought to be proportional to the bulk thermal diffusivity of the material. However, the inclusion of insulating inert particles in energetic materials has shown burning rate enhancement in certain cases. Flame front corrugation that increases the reaction front area observed at micron to sub-millimeter scales was proposed previously to explain the phenomenon. However, a recent simulation study observed a significant temperature gradient within the inert particle, implying that the residence time of the inert particle in the flame front could play a role in the thermal interaction between additives and surrounding energetic materials. In this work, we tested these hypotheses by employing a high-speed microscopic imaging system to quantify the burning rate and flame morphology of Al/CuO nanothermites with various SiO2 particle sizes and mass loading. Additionally, we performed flame propagation simulations to quantify the thermal interactions between the energetic materials and the embedded single inert particle. The experimental results show that the burning rate depends on the particle size as well as mass loading. Specifically, as the SiO2 particle size increases from 100 nm to 100 μm, the burning rate is enhanced by 26% at a mass loading of 7.5%. Further computational studies reveal that flame corrugation may not be the sole factor to alter the burning rate. Non-dimensional analyses show that energy absorption and temperature non-uniformity in inert particles have strong correlations with particle diameter. When the characteristic time of heating the inert particle is shorter than the flame residence time, the inert particle acts as a heat sink, leading to a negative impact on burning rates due to the heat removal from the surrounding energetic materials. Experimental studies reveal that additive particle size has an impact on the nanothermite burn rate. Insight into why this may occur is provided by computational studies of a single particle inclusion, as well as images captured of the burn rate experiments, showing the flame front morphology and particle size effects on heat transfer may play a key role in burn rate alteration by inert additives."
2024,Modeling the Future Space Debris Population and Orbital Capacity,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2326,"Increased investments and technological advances in satellite manufacturing and launch services have led to a newly vitalized Low Earth Orbit (LEO) environment. Megaconstellations consisting of hundreds to hundreds of thousands of satellites have been proposed, with SpaceX’s Starlink satellite constellation now reaching more than 5400 operational satellites. This denser LEO environment underscores the urgent need for models to predict and manage the risk of collisions and the sustainable use of space. Many models have been proposed over the years to quantify the risk of collisions between resident space objects, including the seminal paper by Kessler that described the runaway conditions for which LEO could become unusable. In this thesis, the development of the MIT Orbital Capacity Analysis Tool (MOCAT) is described along with conclusions and insights. MOCAT is a novel open-source approach to evaluating the LEO environment and comprises of a Source Sink Evolutionary Model (SSEM) and a Monte Carlo (MC) method. The SSEM simplifies the complex dynamics of space-object interactions into deterministic equations, focusing on the long-term evolution of orbital populations across different altitude shells. The simplified nature of the SSEM allows for computational efficiency, which enables optimization routines such as the exploration of equilibrium solutions for LEO carrying capacity. The improvements to the SSEM in this work through binning in the physical dimension as well as inclusion of Delta-V dynamics from the collision dynamics increases the fidelity of the SSEM. In comparison, MOCAT-MC offers a comprehensive means to simulate the individual interactions between RSOs. The MOCAT-MC tool propagates the orbits of low-earth orbit objects and models their interactions including collisions and explosions, and provides insights into the evolving trends of the LEO population. Of particular note is the computational efficiency of the model, which is essential for managing the complexities inherent in orbital dynamics and the potential large number of objects centuries into the future. Validation results and a range of simulations, including no-future launch scenarios and the launch of proposed megaconstellations totaling more than 80,000 active payloads are explored, resulting in millions of trackable objects. Despite the much fewer megaconstellations planned at the higher altitudes, even a small fraction of failures in post-mission disposal or collision avoidance maneuvers result in an outsized effect on orbital debris accumulation. MOCAT-MC is able to simulate Lethal Non-Trackable (LNT) objects, which comprise the vast majority of the orbital population today. These lethal non-trackable object population will only grow as more payloads and debris are launched into orbit and increase the collision rate. The effect of these objects are modeled and discussed. These two models offer different approaches to modeling the future orbital environment each with its strengths and weaknesses. Validation against existing models in literature shows the utility of MOCAT in informing future space traffic management and constellation design. The MOCAT tool has been created such that researchers can use a common model that is validated, robust, and efficient, allowing for advancement in our ability to forecast and mitigate the risks associated with the increasing density of LEO while advocating for a more sustainable approach to space exploration and utilization."
2024,Practical Considerations For the Deployment of Clinical NLP Systems,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3788,"Although recent advances in scaling large language models (LLMs) have resulted in improvements on many NLP tasks, it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as healthcare. A healthcare system attempting to automate a clinical task must weigh all approaches with respect to safety, efficacy, and efficiency. This thesis investigates the challenges and implications of implementing LLMs in clinical settings, focusing on the three considerations listed above: safety, efficacy, and efficiency. We first explore the potential biases that might be introduced in downstream patient safety by using LLMs in a zero or few-shot setting and find that LLMs can propagate, or even amplify, harmful societal biases in a number of clinical tasks. Then, we examine the privacy considerations of pretraining a language model on protected health information (PHI) bearing clinical text and find that simple probing methods are unable to meaningfully extract sensitive information from an encoder-only language model pretrained on non-deidentified electronic health record (EHR) notes. Finally, we conduct an extensive empirical analysis of 12 language models, ranging from 220M to 175B parameters, measuring their performance on 3 different clinical tasks that test their ability to parse and reason over electronic health records. We show that relatively small specialized clinical models are substantially more effective than larger models trained on general text used through in-context learning. Further, we find that pretraining on clinical tokens allows for smaller, more parameter-efficient models that either match or outperform much larger language models trained on general text. We argue that using a clinical text-specific pretrained language model allows for an efficient, effective, and privacy-conscious approach, enabling a tailored and ethically responsible application of AI in healthcare."
2024,The definition and implementation of a computer programming language based on constraints,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2821,"Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 1980"
2024,Exploring Developmental Change in Ego-Motion Experience Across Infancy,MIT CSAIL,MIT Student,MIT Advisor,CV,0.262,"Humans flexibly and intuitively use vision to plan and guide navigation through the local environment. How does this ability develop in infancy? One possibility is that the development of visual representations for navigation is driven by passive exposure to the visual statistics of scenes. Another possibility is that active navigation experience using vision to plan and guide locomotion is the driving factor. In order to distinguish between these two hypotheses, it is necessary to understand the nature of infants’ early visual scene experience itself. Surprisingly little prior work has characterized infants’ early experiences with ego-motion through scenes, before and after learning to locomote. We use ecological momentary assessments to quantify infants’ exposure to ego-motion through scenes, and how that changes with locomotor experience. We found that pre-crawling infants who have never independently navigated already experience significant passive visual exposure to forward-facing ego-motion through scenes. Nevertheless, this experience increases substantially with age and locomotor status."
2024,The Power of Perception in Human-AI Interaction: Investigating Psychological Factors and Cognitive Biases that Shape User Belief and Behavior,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2521,"This thesis investigates the psychological factors that influence belief in AI predictions, comparing them to belief in astrology- and personality-based predictions, and examines the ""personal validation effect"" in the context of AI, particularly with Large Language Models (LLMs). Through two interconnected studies involving 238 participants, the first study explores how cognitive style, paranormal beliefs, AI attitudes, and personality traits impact perceptions of the validity, reliability, usefulness, and personalization of predictions from different sources. The study finds a positive correlation between belief in AI predictions and belief in astrology- and personality-based predictions, highlighting a ""rational superstition"" phenomenon where belief is more influenced by mental heuristics and intuition than by critical evaluation. Interestingly, cognitive style did not significantly affect belief in predictions, while paranormal beliefs, positive AI attitudes, and conscientiousness played significant roles. The second study reveals that positive predictions are perceived as significantly more valid, personalized, reliable, and useful than negative ones, emphasizing the strong influence of prediction valence on user perceptions. This underscores the need for AI systems to manage user expectations and foster balanced trust. The thesis concludes with a proposal for future research on how belief in AI predictions influences actual user behavior, exploring it through the lens of self-fulfilling prophecy. Overall, this thesis enhances understanding of human-AI interaction and provides insights for developing AI systems across various applications."
2024,Assessing United States Energy Poverty Policy: Regulatory Design Alternatives and Resource Allocation,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2171,"Guaranteeing sufficient and affordable access to energy services is increasingly critical as climate change continues to worsen, energy costs increase due to the need to meet decarbonization goals, and the trend in general inequality among citizens persists. To ensure the affordability of energy services, in this thesis, I analyze the design of policies and programs addressing energy poverty according to the four strategy decisions that I argue must be made during their ideation: assistance, targeting, funding, and governance. I focus on the strategies designed and implemented in the US and the EU and discuss the benefits and disadvantages of the different approaches followed in both contexts. Based on this comparative analysis, I find there are changes to US federal policy design that should be implemented to better serve households living in energy poverty. Specifically, current allocations under the US Low Income Home Energy Assistance Program (LIHEAP) to states have been nearly static since 1984, while the distribution of energy poverty is dynamic in location and time. To improve the allocation of federal resources, I produce a novel machine learning approach based on sociodemographic and geographical information to estimate energy burden in each US census tract for 2015 and 2020. This analysis reveals an increase in the average household energy burdens, and the range of households experiencing energy poverty broadened. To improve the targeting strategy of LIHEAP, I design an optimized allocation structure that illustrates a shift in funding to the southern US from northern states. To better match household assistance needs, this analysis urges policy makers to revise the distribution of resources to reflect where concentrations of energy poverty exist in the US."
2024,Aquaculture Basket Detection and Tracking for Autonomous Surface Vehicles,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2861,"With the global population on the rise, there is an increased demand for seafood, underscoring the crucial role of aquaculture- the practice of farming aquatic organisms [1]. In the realm of aquaculture, oyster farming is relatively low maintenance, except for the challenge of manually flipping heavy oyster-laden bags. To address this issue, MIT Sea Grant introduced the Oystermaran, an autonomous catamaran specifically designed for this task. This thesis presents contributions to the electronics, controls, and perception systems of the Oystermaran project. In particular, it presents an oyster basket detection and tracking method using the object detector You Only Look Once (YOLO) [2]. In addition, the electronics system has been updated and new manual controllers were created to enable the use of a new f lipping mechanism developed this year. This system is evaluated on data from field testing at Ward Aquafarms, a Cape Cod-based oyster farming business. The results show that oyster baskets can be robustly detected in new environments, despite environmental factors. This marks a significant step towards real-time viability for autonomous oyster farming."
2024,Maintenance and Metalearning of Time Interval Representations,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2098,"When we perform actions in the world, we estimate what is happening around us. That information goes through a series of transformations in the brain in order to execute an action that meets our goals. For example, we might remember the speed of a car in order to decide when to cross the road. These transformations can be simple, for example based on physics models of speed and time, like the car example, or they can be complex and built around evolutionary and experience-based statistical regularities in the world. This thesis uses a sensorimotor time production task to investigate different types of transformation and noise that exist between observation and action. First, I will propose a task which utilizes memory of a time interval in order to probe memory noise, memory storage, and inference over internal noise. To do this, monkeys perform a delayed time reproduction task. I find that the behavior is consistent with the the brain storing the memory as a function of time, and that the inference does not mitigate the internal memory noise. Second, I investigate how estimated prior distributions change when the statistical regularities of the world change. Monkeys perform a blocked time reproduction task, and behavior across policy transitions shows fast adaptation to new policies. I apply this algorithm to a model and fit it to behavioral data.  Third, I display some preliminary neural data gathered during these tasks as well as hypotheses for neural implementation. With these experiments, I utilize a simple task to pick apart transformations that occur between observation and action."
2023,Systemic risk in the interbank lending market,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3466,"Our goal is to understand the functioning of the interbank lending market in times of market stress. Working towards this goal, we conduct theoretical analysis and simulation to study the effects of network structure and shock scenarios on systemic risk in the market. We consider shocks of various sizes at both global and local scales. In terms of risk measures, we study relative systemic loss and the default rate, separating the latter quantity into fundamental default and contagion. Our simulations suggest that all systemic risk measures are similar on the well-studied directed Erdős-Rényi model and the more complex fitness model if we match the mean density and the mean edge weight of these two models. We show through both derivations and simulations that the network size has little effect on systemic risk when the network is sufficiently large. Moreover, as the mean degree grows, the different default rates considered all increase, while relative systemic loss decreases. Furthermore, simulations suggest that local shocks tend to cause more harm than global shocks of the same total size. We also derive upper and lower bounds on a bank's probability of default, only using its neighbors' information. For implementation, we build a method for real-time, automatic, interpretable assessment of financial systemic risk, which only requires temporal snapshots of observable data. Our algorithm takes in partial data, inferring a random graph model, and then generates empirical distributions for risk measures. The first part relies on inferring a fitness model that is compatible with observed information. For the second part, we use simulations to obtain empirical distributions for systemic risk that arises from interbank clearing. We test our method on synthetic data and apply it to the federal funds market using empirical data. Our method is fast enough to be incorporated into algorithms that produce intraday time trajectories of risk prediction. The data requirement is practical for investors as well as regulators, policy-makers, and financial institutions."
2023,Accelerated algorithms for constrained optimization and control,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2427,"Nonlinear optimization with equality and inequality constraints is a ubiquitous problem in several optimization and control problems in large-scale systems. Ensuring feasibility along with reasonable convergence to optimal solution remains an open and pressing problem in this area. 

A class of high-order tuners was recently proposed in adaptive control literature with an effort to lead to accelerated convergence for the case when no constraints are present. In this thesis, we propose a new high-order tuner based algorithm that can
accommodate the presence of equality and inequality constraints. We leverage the linear dependence in solution space to guarantee that equality constraints are always satisfied. We further ensure feasibility with respect to inequality constraints for the specific case of box constraints by introducing time-varying gains in the high-order tuner while retaining the attractive accelerated convergence properties. Theoretical guarantees pertaining to stability are also provided for time-varying regressors. These theoretical propositions are validated by applying them to several categories of optimization problems, in the form of academic examples, power flow optimization and neural network optimization.

We devote special attention to analyze a special case of neural network optimization, namely, linear neural network training problem, to understand the dynamics of nonconvex optimization governed by gradient flow and provide lyapunov stability guarantees for LNNs."
2023,"First-principles control of zeolite synthesis, transformations, and intergrowth",MIT CSAIL,MIT Student,MIT Advisor,Database,0.2573,"Designing new materials enabling of sustainable catalysis and separations is essential to fully decarbonize the industrial sector, but materials discovery is hindered by labor-intensive experimentation. Computational methods such as high-throughput screening or machine learning can filter structures and compositions prior to experiments. Nevertheless, finding synthesis routes to realize computationally proposed materials still relies on trial-and-error. This thesis describes how materials discovery can be streamlined using high-throughput simulations, physics-based representations, and machine learning. In particular, this work analyzes the synthesis, phase transformations, and intergrowth of zeolites, which are industrially relevant catalysts and molecular sieves known for their hard-to-control phase competition and polymorphism.

Part I of this thesis describes the development of methods to simulate and analyze zeolite transformations and template-based synthesis. Diffusionless phase transformations and intergrowths are predicted by using graph-based representations of zeolite frameworks. Moreover, a data-driven analysis indicates how  structural factors are often insufficient to predict outcomes in their synthesis. To address this knowledge gap, computational tools are developed to simulate template-based synthesis conditions for zeolites. The proposed methods accelerate the prediction of binding energies between molecular templates and zeolite hosts, reducing computational costs by up to two orders of magnitude while increasing the reproducibility of the calculations. These tools are then used to simulate hundreds of thousands of template-zeolite pairs in a high-throughput screening pipeline. The simulation results explain thousands of synthesis outcomes from the zeolite literature from the past six decades and recall archetypical templates purely from physical principles.

Part II of this thesis leverages these methods to design new zeolite synthesis routes, thus enabling the control of phase competition, intergrowth, and catalytic properties of the materials. The computational tools guide the experimental synthesis of zeolites with improved catalytic behavior, including pure-phase frameworks with low-cost templates and disordered structures with tunable polymorph selectivity. Finally, the computational approach maps numerous possibilities for further discovery based on over 35 million data points generated using machine learning, which is used to decode this “zeolite genome”. This work provides a roadmap on how synthesis routes for zeolites can be controlled a priori using theory and computation."
2023,"DNA sequence design of non-orthogonal binding networks, and application to DNA data storage",MIT CSAIL,MIT Student,MIT Advisor,Network,0.2921,"DNA has proven itself a powerful tool in a diverse array of nanotechnology-related domains, including molecular computation, nanostructure fabrication, and data storage. Most DNA-based systems focus on using sets of DNA sequences that are orthogonal to each other, such that each DNA sequence has a dedicated binding partner, its complementary sequence. This design approach reduces the number of interactions that must be considered when predicting how a system will behave, at the cost of reducing the information-gathering ability of each molecular unit.

Relatively little research has attempted to solve the problem of designing promiscuous, or non-orthogonal, DNA sequences, which are characterized by their ability to bind to several distinct partners with variable binding affinities. Yet there are many situations in which this type of dense interaction network can be useful. For example, in neural networks, a node will often take inputs from hundreds or thousands of upstream nodes, allowing it to condense large amounts of information into a single output value. While naturally occurring biological networks often make use of promiscuous binding behavior, the field of molecular computing currently lacks a general-purpose and efficient method for non-orthogonal DNA sequence design.

In this thesis, I describe a novel, robust, and broadly applicable method for designing small or large sets of non-orthogonal DNA sequences. This method takes an arbitrary matrix of pairwise binding affinities, and attempts to design DNA sequences such that the differential binding affinity between any two pairs of sequences is proportional to the difference in the corresponding elements of the matrix. The key innovation of this method is the reformulation of the matrix via a binary embedding, which reduces the design specification to a set of binary strings that permit relatively straightforward sequence design.

Not all matrices permit a binary embedding and I consider three cases here: when a binary embedding exists, when it is unknown if it exists, and when it does not exist. When it exists, I show through both simulation and experiment that DNA sequences can be designed with high precision. When it is unknown if a binary embedding exists, I give novel conditions for determining existence via representation of the matrix in a weighted graph. Finally, when an exact binary embedding does not exist, I develop an alternative method using approximate binary embeddings. To demonstrate the power of this method, I apply to the task of similarity searching in a large, simulated DNA databases, where I show that it outperforms the existing state of the art. I hope that this work opens the door to further innovations in designing and applying non-orthogonal DNA sequences to DNA nanotechnology."
2023,Identifying functional groups in microbial communities based on ecological patterns,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2764,"Recent development in sequencing technologies has greatly advanced our understandings of structure and function of microbial communities in various ecosystems. In microbial communities, a metabolic function is often performed by a group of multiple species (i.e., a functional group) at the same time. However, identifying these functional groups remains to be a major challenge for structure-function mapping in microbiome studies. Instead of relying on annotation-based methods that are highly biased for a few model microorganisms, here I tackle this challenge by developing a novel annotation-free approach. In chapter two, I develop the mathematical framework behind the new approach – which we call EQO – and show its power by applying it to a few existing microbiome datasets. I show that, based solely on the patterns of statistical variation in species abundances, EQO identifies functional groups in soil, ocean and animal gut microbiome. The following two chapters discuss an application of this method, which has led to the discovery a potential new form of interaction between bacteria in animal guts, and an unexpected finding in the lab regarding the ecological dynamics of phage-plasmids in marine bacterial populations. In chapter three, I show how applying EQO to an aquaculture dataset leads us to identify potential pathogen-inhibiting groups of bacteria in an animal-associated microbiome. Guided by the computational prediction, I successfully isolate a member of this group that is a novel species with a broad spectrum of interaction against various Vibrio pathogens. By synthesizing and secreting polysaccharides, the novel species causes limited dispersion and reduced virulence of Vibrio. My efforts to understand the ecology of marine bacteria also lead me to study the role of widely distributed phage-plasmids. Combining mathematical models and experimental evidence, I show that loss-of-function mutations and segregational drift recurrently drive productive infections of phage-plasmids within marine bacterial populations. Together, this thesis provides a simple yet powerful approach to abstract functional groups from taxonomic composition in complex microbiome. As a useful hypothesis generating tool, this approach will pave the way for more mechanistic studies of microbiome in the future."
2023,On Semi-supervised Estimation of Distributions,MIT CSAIL,MIT Student,MIT Advisor,Network,0.27,"We study the problem of estimating the joint probability mass function (pmf) over two random variables. In particular, the estimation is based on the observation of 𝑚 samples containing both variables and 𝑛 samples missing one fixed variable. We adopt the minimax framework with [notation] loss functions, and we show that the composition of uni-variate minimax estimators achieves minimax risk with the optimal first-order constant for 𝑝 ≥ 2, in the regime 𝑚 = 𝑜(𝑛)."
2023,Physics-Inspired Deep Learning for Inverse Problems in MRI,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3926,"We demonstrate the power of combining the forward image acquisition model with deep learning solutions for inverse problems in magnetic resonance imaging (MRI), from individual network layers to the network architecture design and inference procedure.

First, we propose neural network layers that combine image space representations with representations in Fourier space, where MRI data is acquired. These layers can be used as drop-in replacements for standard image space convolutions in a variety of network architectures and yield higher quality reconstructions across a wide range of MR imaging tasks.

Next, we demonstrate a deep learning framework for rigid-body motion correction in MRI, where the forward imaging model informs both the network architecture and the inference procedure. Our method incorporates potentially unknown motion parameters as inputs to the network and then optimizes them for each test example. The optimization is performed via an objective function that forces the reconstructed image and estimated motion parameters to be consistent with the acquired data. This approach reduces the joint image-motion parameter search used by most motion correction strategies to an inference-time search over motion parameters alone, greatly simplifying the complexity of the optimization problem to be solved for a novel image. Our hybrid method achieves the high reconstruction quality metrics that characterize deep learning solutions while retaining the benefits of explicit model-based optimization – in particular, the ability to reject examples where the network produces poor reconstructions. Experiments demonstrate the advantages of this combined approach over purely learning or model-based reconstruction techniques."
2023,The Coordination Imperative: A Comprehensive Approach to Align Customer Demand and Inventory Management for Superior Customer Experience in Retail,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2646,"The rapid growth of customers traversing different channels during their buying journey presents both opportunities and challenges for organizations. Fragmented decision-making and siloed communication between marketing and supply chain teams can lead to inefficiencies and negatively impact customer experience. This thesis proposes a conceptual framework to align customer demand and inventory management. The framework is examined in the empirical context of the fashion industry, focusing on the US market and insights from Brazil and Japan. By introducing a PDCA (Plan Do Check Action) process and cross-functional metrics, such as NPS (Net Promoter Score) and OTIF (On time in Full), this study seeks to encourage cooperation between departments and coalesce decision-making around enhancing customer experience. The research will explore the quantitative and qualitative aspects of the retail industry focusing on fashion and identify opportunities to leverage technology, marketing, and supply chain management for improved performance. Our study validated the existence of siloed operations and the drawbacks caused by silos in today’s business. Through 16 expert interviews, we identify three key factors that contribute to silos between marketing and supply chain. They are technology fragmentation, lack of integrated KPIs, and complexity of multiple channels. Further, the interviews helped uncover how the experts tackled these challenges in daily operations.

The expected deliverable is a framework that combines analyzed customer journeys with cross-functional metrics to support decision-makers in day-to-day operations. The goal is to deliver a world-class customer experience by aligning decisions to coordinate actions. There is potential to incorporate machine learning to suggest experiments and further optimize value delivery for 4  customers by retailers through multiple channels. Our conceptual framework applies to various businesses struggling with coordination between demand generation and fulfillment."
2023,Studying Electronic Textures with Coherent Lensless Imaging,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2689,"X-ray microscopes have opened our collective eyes to the richness of nanoscale texture in systems such as correlated and quantum materials. These microscopes draw their power from the combination of short wavelengths, which provide high resolution, and interaction with atomic resonances, which makes them sensitive to subtle changes in electronic structure. However, x-ray microscopy remains an area where the main limits are technological rather than fundamental. Therefore, major progress is still possible with methodological improvements.

In the past twenty years, research has exploded into the use of coherent x-ray light to improve the quality and resolution of x-ray microscopes. In many cases, using coherent light makes it possible to remove the objective lens in a microscope, replacing it with an algorithmic analysis of the direct scattering data. This can increase the quality and resolution of the resulting quantitative images.

In this thesis, I present the results from a collection of projects aimed at using coherent imaging methods to study the real-space texture of electronic phases of matter with soft x-ray light. I first discuss the methods we developed and implemented to counteract the experimental errors that we found to be ubiquitous in our data, focusing on ptychography, the most commonly used lensless imaging method. Then, I turn to the development of an entirely new single-shot lensless imaging method, randomized probe imaging (RPI).

RPI has proven to be reliable and robust across a broad range of scenarios. The remainder of the thesis is devoted to applications of RPI at a free electron laser and a synchrotron. Also reported are further projects designed to improve the method, as well as attempts to expand our understanding of the mechanisms behind it and its limitations. I sincerely hope that the availability of RPI will help bring x-ray imaging to a broader group of scientists and lead to a better understanding of the nanoscale details of electronic texture."
2023,Systematic Modeling and Design of Sparse Deep Neural Network Accelerators,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3105,"Sparse deep neural networks (DNNs) are an important computation kernel in many data and computation-intensive applications (e.g., image classification, speech recognition, and language processing). The sparsity in such kernels has motivated the development of many sparse DNN accelerators. However, despite the abundant existing proposals, there has not been a systematic way to understand, model, and develop various sparse DNN accelerators. 

To address these limitations, this thesis first presents a taxonomy of sparsity-related acceleration features to allow a systematic understanding of the sparse DNN accelerator design space. Based on the taxonomy, it proposes Sparseloop, the first analytical modeling tool for fast, accurate, and flexible evaluations of sparse DNN accelerators, enabling early-stage exploration of the large and diverse sparse DNN accelerator design space. Across representative accelerator designs and workloads, Sparseloop achieves over 2000× faster modeling speed than cycle-level simulations, maintains relative performance trends, and achieves ≤ 8% average modeling error. 

Employing Sparseloop, this thesis studies the design space and presents HighLight, an efficient and flexible sparse DNN accelerator. Specifically, HighLight accelerates DNNs with a novel sparsity pattern, called hierarchical structured sparsity, with the key insight that we can efficiently accelerate diverse degrees of sparsity (including dense) by having them hierarchically composed of simple sparsity patterns. Compared to existing works, HighLight achieves a geomean of upto 6.4× better energy-delay product (EDP) across workloads with diverse sparsity degrees, and always sits on the EDP-accuracy Pareto frontier for representative DNNs."
2023,SenseMate: An AI-Based Platform to Support Qualitative Coding,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3523,"Unstructured data can be analyzed numerically or qualitatively through methods like sensemaking. One of the key stages of sensemaking is qualitative coding, where the data is divided into units, and each unit is assigned a category or code. Unfortunately, coding is tedious and time-consuming when carried out manually. Finding a balance between manual and fully-automated coding can help increase efficiency while allowing human judgment and preventing systematic machine errors. In this thesis, I propose an accessible semi-automated approach to qualitative coding. First, I apply a novel machine learning method, rationale extraction models, to qualitative coding. These models recommend themes for each unit of analysis in qualitative data and tend to perform better with less ambiguous themes. Through an online experiment, I find that assistance from rationale extraction models increases coding performance and reliability. Next, I execute an iterative, human-centered design process to create SenseMate, an AI-based platform for qualitative coding. After 13 user testing sessions and 3 design iterations, I observe that model overreliance can be minimized through cognitive forcing functions and easy-to-understand model explanations. I also design several ways for users to efficiently provide feedback on machine-generated rationales. To connect my model and design evaluations, I implement a prototype of SenseMate and conduct a summative user evaluation through an online experiment. The evaluation reveals that participants with access to AI assistance have higher coding performances but spend more time on the platform. The effectiveness of various design decisions within SenseMate is also explored. Finally, I discuss a myriad of future work possibilities. Overall, this thesis offers a practical and accessible solution to analyzing unstructured data, which has broad applications for researchers and organizations across various fields."
2023,1/f noise in MOSFETs with ultrathin gate dielectrics,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3394,"Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 1992"
2023,A Robust and Efficient Framework for Slice-to-Volume Reconstruction: Application to Fetal MRI,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3714,"Volumetric reconstruction in presence of motion is a challenging problem in medical imaging. When imaging moving targets, many modalities are limited to fast 2D imaging techniques that provide cross-sectional snapshots (2D images) of the subject with an attempt to ""freeze'' in-plane motion. However, inter-slice movement results in slice misalignment in 3D space, i.e., each image being an independent slice that fails to form a coherent volume for diagnosis and analysis. bTo this end, slice-to-volume reconstruction (SVR) has been proposed to reconstruct a high-quality 3D volume from misaligned 2D observations by performing inter-slice motion correction and super-resolution reconstruction. Existing SVR algorithms, however, have a limited capture range of slice motion and are time-consuming, particularly when producing high-resolution volumes.

This thesis proposes a motion-robust and efficient machine learning framework for SVR, motivated by the application of magnetic resonance imaging (MRI) in assessing fetal brain development. We first introduce a slice-to-volume registration transformer that models input slices as a sequence and performs inter-slice motion correction by simultaneously predicting rigid transformations of all images in 3D space. We then reformulate the reconstruction problem using implicit neural representation, where the underlying volume is represented by a continuous function of 3D coordinates. This resolution-agnostic approach allows efficient reconstruction of high-resolution volumes. Finally, we extend this method to data that suffer from non-rigid motion by introducing an implicit motion field that captures slice-dependent deformation. These advances together enable robust and efficient 3D reconstruction and visualization in fetal MRI, benefiting diagnosis and downstream analysis. Additionally, the proposed framework has the potential for broader clinical implications in various applications that involve similar volumetric reconstruction problems."
2023,A computational framework for emotion understanding,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2876,"The organizing principle of this thesis is that human emotion understanding reflects a model-based solution to a large class of ill-posed inverse problems. To interpret someone's expression, or predict how that person would react in a future situation, observers reason over a logically- and causally-structured intuitive theory of other minds. For this work, I chose a domain that is perceptually and socially rich, yet highly constrained: a real-life high-stakes televised one-shot prisoner's dilemma.

In the first set of studies, I illustrate that forward predictions play a critical role in emotion understanding. Intuitive hypotheses about what someone is likely to feel guide how observers interpret and reason about expressive behavior. By simulating human causal reasoning as abductive inference over latent emotion representations, a parameter-free Bayesian model captured surprising patterns of social cognition.

In the second set of studies, I formalize emotion prediction as a probabilistic generative model. Mental contents inferred via the inversion of an intuitive theory of mind generate the basis for inferring how others will evaluate, or 'appraise', a situation. The Inferred Appraisals model extends inverse planning to simulate how observers infer others' reactions, in the terms of utilities, prediction errors, and counterfactuals on rich social preferences for fairness and reputation. I show that the joint posterior distribution of inferred appraisals provides a powerful method for discovering the latent structure of the human intuitive theory of emotions.

In the third set of studies, I build a stimulus-computable model of emotion understanding. This work emphasizes the importance of testing whether computational models can use emotion-relevant information in service of social cognition. I suggest that building computer systems that approach human-level emotional intelligence requires generative models, where inferred appraisals function as latent causal explanations that link behavior, mental contents, and world states."
2023,"A Closer Look at Classical Measurement, an Algorithm for Deliberation in Rodents, and a Conjecture on Intertemporal Choice",MIT CSAIL,MIT Student,MIT Advisor,CV,0.2565,"In this three-part thesis, Part I is an examination of the measurement process in classical Hamiltonian mechanics. This part is concerned with the tradeoff that exists, when measuring any observable of a system, between the disturbance inflicted upon the system and the information that can be extracted. The main result takes the form of a Heisenberg-like precision-disturbance relation: measuring an observable leaves all compatible observables undisturbed but inevitably disturbs all incompatible observables. The magnitude of the disturbance (the analogue of Ò) is found to be proportional, in a sense that is made precise, to one’s initial uncertainty in the ready-state of the apparatus—a quantity that relates to the temperature of the apparatus.

Part II of this thesis develops a model of the computations taking place in the deliberative decision-making system of rodents, during wakefulness and sleep, with focus on the role of hippocampus (HPC). In this model, medial prefrontal cortex performs high-level planning, and then tasks HPC with fleshing out the details of the plan, as needed. We describe this planning task of HPC as an optimal control problem, which allows us to draw insights from the powerful mathematics of optimal control theory. The model makes novel testable predictions, provides insights into memory consolidation during sleep, and offers a paradigm capable of accommodating a wide range of observed phenomena, such as the theta rhythm, the slow oscillation, spindle oscillations, sharp wave-ripples, θ-sequences, for-ward and reverse SWR-sequences, the formation and strengthening of episodic memories, and a need for two modes of operation—online and offline.

The two parts described above are the main content of this thesis. Part I falls within the purview of classical theoretical physics, while Part II falls in that of computational neuroscience. The two may seem unrelated; however, while each part is self-contained, I see the two as connected. Part III of this thesis is my attempt to provide an outline of a bigger picture, which sees the foregoing as lines of inquiry towards the same far-reaching conjecture—one which has had a strong pull on my imagination during my PhD, and which I hope to be able to address in the future. This conjecture is that the probability calculus of quantum mechanics holds a kind of normative status for a class of decision problems involving intertemporal choice under uncertainty—a class of problems of great importance to artificial intelligence, brain sciences, economics, and, I argue, to physics too."
2023,Visible-Light Integrated Photonics for 3D-Printing and Trapped-Ion Systems,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2347,"Silicon photonics has enabled next-generation optical technologies that have facilitated revolutionary advances for numerous fields spanning science and engineering, including computing, communications, sensing, and quantum engineering. In recent years, the advent of visible-light integrated photonics platforms has opened up the potential for further diverse applications. This thesis builds upon these recent technologies to demonstrate novel applications of visible-light integrated photonics.

First, we combine the fields of silicon photonics and photochemistry to propose the first chip-based 3D printer, consisting of only a single millimeter-scale photonic chip without any moving parts that emits reconfigurable visible-light holograms up into a simple stationary resin well to enable non-mechanical volumetric 3D printing. This work presents a highly-compact, portable, and low-cost solution for the next generation of 3D printers.

Next, we propose integrated-photonics-based system architectures and the design of key integrated-photonics components for both polarization-gradient and electromagnetically-induced-transparency cooling of trapped ions. Further, we experimentally demonstrate a pair of polarization-diverse gratings and design the first integrated polarization rotators and splitters at blue wavelengths, representing a fundamental stepping stone on the path to advanced operations for integrated-photonics-based trapped-ion quantum systems involving multiple polarizations.

Finally, we demonstrate optical trapping and tweezing of microspheres and cancer cells using an integrated optical phased array for the first time, representing a two-orders-of-magnitude increase in the standoff distance of integrated optical tweezers and the first cell experiments using single-beam integrated optical tweezers."
2023,Computational and experimental methods for CRISPR-based saturation mutagenesis screens,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2827,"Genetic variation is a powerful framework for functional characterization of the human genome. The emergence of CRISPR technology has enabled the efficient and diverse installation of genetic variation in situ, leading to its widespread use in functional genomics. The application of high-throughput CRISPR saturation mutagenesis screens for the functional interrogation of the coding and non-coding genome holds great promise in accelerating our understanding of how static DNA sequences encode and influence dynamic processes in human development and disease.

In this thesis, we focus on the development of computational and experimental methods for CRISPR-based saturation mutagenesis screens. First, we developed CRISPR screening uncharacterized region function (CRISPR-SURF), a deconvolution framework for the analysis of CRISPR saturation mutagenesis screens. Drawing inspiration from the field of signal processing, we propose the modeling of CRISPR perturbations across an underlying genomic regulatory signal by means of a convolution operation and apply CRISPR-SURF for the discovery of non-coding regulatory elements involved in gene regulation. Second, we developed PrimeDesign to facilitate the rapid design of prime editing (PE) guide RNAs and demonstrate its utility by using recommended designs to install pathogenic variants in human cells. Complementing PrimeDesign, we developed pegPool as a high-throughput pooled screening strategy for prime editing guide RNA (pegRNA) optimization. We demonstrate the generalizability of pegPool by assessing a total of >18,000 pegRNA designs, with up to 210 designs in a single pool, to identify high efficiency pegRNA constructs targeting genomic sites. Finally, we developed multiplexing of site-specific alterations for in situ characterization (MOSAIC) as a rapid non-viral method for saturation mutagenesis screens at single-nucleotide and codon resolution. Using MOSAIC, we demonstrate in situ saturation mutagenesis of the BCR-ABL1 oncogene to identify drug resistant variants and IRF1 untranslated region (UTR) to map non-coding regulatory elements involved in transcriptional initiation."
2023,Prospects for Quantum Equivariant Neural Networks,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2796,"Convolutional neural networks (CNNs) exploit translational invariance within images. Group equivariant neural networks comprise a natural generalization of convolutional neural networks by exploiting other symmetries arising through different group actions. Informally, a linear map is equivariant if it transfers symmetries from its input space into its output space. Equivariant neural networks guarantee equivariance for arbitrary groups, reducing the system design complexity. Motivated by the theoretical/experimental development of quantum computing, in particular with the quantum advantage derived from other quantum algorithms/subroutines for group theoretic and linear algebraic problems, we explore the potential of quantum computers to realize these structures in machine learning. This work reviews the mathematical machinery necessary from group representation theory, surveys the theory of equivariance, and combines results in non-commutative harmonic analysis and geometric deep learning. Convolutions and cross-correlations are examples of functions which are equivariant to the actions of a group. We present efficient quantum algorithms for performing linear finite-group convolutions and cross-correlations on data stored as quantum states. Potential implementations and quantizations of the infinite group cases also discussed."
2023,Improving Predictability of Wind Power Generation,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2675,"Wind energy plays an important role in decarbonizing the economy and increasingly accounts for a growing share of electricity supply in the United States. However, availability of wind resource is highly dependent on variable factors such as weather and local geographies, making wind power generation forecast a particularly difficult task. This adds to the challenge of grid management, which requires that the supply of electricity equates the demand at all times. Complicating the effort to improve wind power predicitability is a lack of empirical data, since wind power generation data are proprietary and often considered business secrets. To address this lack of empirical study, this thesis uses actual generation data between 2016 to 2021 from seven anonymized wind farms in Midwestern United States that range from 50MW to 235MW in size. The experiments demonstrate how machine learning methods can be used to forecast wind power generation at different time intervals, and how the accuracy of forecasting can be significantly improved when using a combination of newly extracted weather forecast data and weather measurement data. The economic benefits of more accurate forecasting are then studied using a using a simulation with market data from the Midcontinent Independent System Operator and the Southwest Power Pool. The thesis then explores whether predictability of wind power generation can be improved by placing weather stations closer to the wind forecast sites. Implications of these findings can inform investment decisions regarding weather monitoring stations and forecasting models, which can help electricity market participants adapt to a grid with an increasing share of renewable resources."
2023,"Proliferated Low Earth Orbit (pLEO) Satellite
Constellation Handover Cost Analysis",MIT CSAIL,MIT Student,MIT Advisor,Network,0.3335,"In any mobile network, handovers between routing nodes generally cause a reduction in available resources for users. This is very true of proliferated Low Earth orbit (pLEO) satellite constellation networks in which both the satellite and the user are mobile with respect to each other. As satellites travel in their obits, they move into and out of ground users’ views every few minutes [4], and mobile users can move into and out of satellite spot beams frequently as well. When existing communication between a user and its serving satellites (uplink and downlink) terminate, user data must be relayed to the next serving satellite, possibly incurring additional data transmissions and overhead in the form of network management and control actions for acquisition in the network. This issue is becoming more relevant as commercial companies building their own satellite networks must figure out an efficient handover strategy to reduce unnecessary data transmissions and handover overhead. In this thesis, I estimate the satellite handover cost by quantifying the number of transmission hops required to relay existing queued data to/from the next serving satellite. The handover cost of a satellite network will depend on factors such as the network topology and the handover algorithm itself. I will quantify the impact of the aforementioned factors on the satellite network handover cost. A lower handover cost generally implies that the overall monetary cost (capital expenditure and operational expenditure) of a network to the provider (and also the user) is lower as well."
2023,A Data-Driven Approach to Improve Optical Fiber Manufacturing: Focus on Core Deposition,MIT CSAIL,MIT Student,MIT Advisor,CV,0.325,"This thesis presents an in-depth investigation on characterization of optical fiber preform core manufacturing and the identification of underlying trends in measured production data. While walking through the different operations involved in the process, we explained the challenges associated with insuring refractive index profile precision and glass purity. Starting with unsupervised learning, process by process, we applied linear and non linear dimensionality reduction algorithms (PCA and t-sne) to features matrices created from time series data and have been able to connect data clusters with context information like machines or month of the year. Then considering the core fabrication process as a whole, we studied the propagation of trends in the data sets up to quality measurements using Dice’s statistic to gauge similarities between samples sets. Finally, we developed some data-driven regression models in order to predict the refractive index measured at the end using data from all processes. As a result, Kernel algorithms performed the best and almost as well on raw statistics from all processes as on encoded information about machine sequences and dates. This supervised approach demonstrated some great potential for the development of prediction tools which could help design the optimized production line. An underlying objective is to support Sterlite Technologies Limited in using data-driven approach applied to process control for its plant in Waluj and Shendra starting by implementing good practices for variables measurement, logging and tracking."
2023,Learning Z-Order Indexes with Dynamic Bit Allocation,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2658,"The Z-order curve is a space-filling curve that maps multi-dimensional data to singledimensional values. Z-order has been used in databases to sort multi-dimensional data. Modern data management systems such as Amazon Redshift and Databricks Delta Lake give users the ability to sort on multiple columns using Z-order. However, the Z-order is difficult to tune, with tunable parameters such as which columns to include in the Z-order. Currently, users must specify the columns for Z-order when using the systems and might not necessarily achieve the best performance, as the choice of columns has a significant impact on performance. Another drawback is that the systems give equal weight to the columns, which often does not result in the best performance due to the unequal impact columns have on query performance. Our work aims to automatically determine the best Z-order configuration for a particular dataset and workload. In this thesis, we introduce learning Z-order indexes using an approach we refer to as dynamic bit allocation, which considers not only which columns to include, but also the weight to put on each column. Our learned Z-order indexes outperform existing techniques by up to 11× in query time and up to 30.2× in rows scanned, revealing the potential of tuning Z-order to improve query performance."
2023,The Science and Art of Human and Artificial Intelligence Collaboration,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3451,"While artificial intelligence (AI) appears to be surpassing the performance of human experts on a wide variety of games and real-world tasks, these algorithms are prone to systematic and surprising failures when deployed. In contrast to today’s state-of-the-art algorithms, humans are highly capable of adapting to new contexts. The different strengths and weaknesses of humans and AI motivate a guiding research question for the emerging field of human-AI collaboration: When, where, why, and how does the combination of human problem solving and AI systems lead to a hybrid system that surpasses (or fails to surpass) the performance of either humans or the machine alone? This dissertation addresses various dimensions of this guiding question by conducting large-scale, digital experiments across three distinct tasks and domains: deepfake detection, dermatology diagnosis, and Wordle. First, the experiments in deepfake detection examine the similarities and differences between human and machine vision in identifying visual manipulations of people’s faces in videos and identify important performance trade-offs between hybrid systems and human or AI only systems for deepfake detection. Second, the experiments in dermatology diagnosis reveal that non-visual information is often essential for diagnosing skin disease, diagnostic accuracy disparities across skin color exist in image-only store-and-forward teledermatology, and clinical decision support based on a fair deep learning system can significantly increase physicians’ diagnostic accuracy in this experimental setting. Third, the experiment on Wordle demonstrates that digitally mediated expressions of empathy can counteract the negative effect of anger on human creative problem solving. In addition to these digital experiments, this dissertation presents two algorithmic audits on clinical dermatology images to reveal where systematic errors arise in state-of-the-art algorithms, examines how context influences automated affect recognition, and proposes methods for more effectively incorporating context in applied machine learning. Together, these contributions provide empirical evidence for why human-AI collaborations succeed and fail across a variety of tasks and domains, insights into how to design human-AI collaborations more effectively, and a framework for when and where hybrid systems should rely on human problem solving."
2023,Integral Quadratic Constraints and Safety Certificates for Uncertainty Characterization and Control Safety-Aware Filtering of Proximity Operations Between Satellites,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2515,"Techniques in robust optimization and formal verification methods are used (1) to examine the stability and robust performance of a satellite controller that considers six-dimensional, uncertain state, and often unmodeled dynamics during rendezvous and proximity operations, and (2) to explore the synthesis of control Lyapunov/barrier functions (CLFs/CBFs) using neural networks and stochastic gradient descent to provide safety-aware filtering for the fuel-optimal control policies. A linear quadratic regulator controller for a servicer satellite (Servicer) is analyzed via the dissipativity inequality principle and quadratic constraints. This method allows the capture of unmodeled dynamics to reduce system uncertainty of proximity operations among the Servicer, client satellite (Client), and unsafe regions (e.g., obstacle). The same controller is implemented with a finite time horizon (i.e., model predictive controller) to filter out unsafe control output during an autonomous inspection of a Client. This framework mitigates the collision risk based on integral quadratic constraints (IQCs) worst bounds recommendation, miss distance, Mahalanobis distance, and Probability of Collision (Pc) metrics. Innovative deterministic reachability methods based on integral quadratic constraints and neural Lyapunov functions are compared and connected. The novel contributions of this work focus on formulating mathematical safety guarantees, modeling controller output, and reducing uncertainty on system performance when designing fuel-optimal and safe maneuvers of Servicer around the Client while avoiding unsafe regions in LEO."
2023,Neighborhood Transformation Marginalization forOOD Detection,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3827,"Out-of-distribution (OOD) detection is an important part of enabling the real world deployment of machine learning models. Many recent methods developed to perform OOD detection rely on calculating a score function on a given test point then thresholding the value to classify the point as in-distribution (ID) or OOD. However, calculating a score function on a single example may give biased or inaccurate estimates, especially as examples are sampled further and further OOD. In this paper we propose TraM: Transformation Neighborhood Marginalization, a method to improve the estimation of score functions used for OOD detection by calculating their expectation over a transformation neighborhood. TraM demonstrates improvements on a subset of commonly used OOD score functions in the OpenOOD benchmark, improving a baseline ODIN score function by up to 6 AUROC. However, it is not found to improve other baseline metrics signficantly, indicating the need for further research on this topic."
2023,Top-Down Synthesis for Library Learning,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3442,"This thesis introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus. We present an implementation of the approach in a tool called Stitch and evaluate it against the state-of-the-art deductive library learning algorithm from DreamCoder. Our evaluation shows that Stitch is 3-4 orders of magnitude faster and uses 2 orders of magnitude less memory while maintaining comparable or better library quality (as measured by compressivity). We also demonstrate Stitch’s scalability on corpora containing hundreds of complex programs that are intractable with prior deductive approaches and show empirically that it is robust to terminating the search procedure early—further allowing it to scale to challenging datasets by means of early stopping. We publish the code, the documentation, a tutorial, and a Python library for interfacing with our for our Rust implementation of Stitch.

Tutorial & Documentation (Python Library): https://stitch-bindings.read thedocs.io/en/stable/intro/tutorial.html 

Rust Implementation: https://github.com/mlb2251/stitch 

Artifact (Awarded: Reusable): https://github.com/mlb2251/stitch-artifact"
2023,"Embedding StarLogo Nova into WISE for a Seamless
Student Experience",MIT CSAIL,MIT Student,MIT Advisor,SE,0.2959,"Support for teaching computational thinking has been increasing throughout K-12 schools as the world is being more utilized by computer technology [2]. The Scheller Teacher Education Program (STEP) at MIT uses educational technologies to create innovative learning experiences. An example project is StarLogo Nova, a block-based programming environment that facilitates the creation of agent-based models to study complex systems [17]. Currently StarLogo Nova is a website where students independently login and make projects for their models. However, the overall experience of using StarLogo Nova can be improved as there is no guidance when a student makes a model. In this thesis, we will augment StarLogo such that there will be a concept of activities, a user experience for students to receive instructions and answer questions while still allowing convenient interactions with a StarLogo project, such as editing or viewing a model. To do this, we integrate StarLogo into a platform called WISE (Web-based Inquiry Science Environment)."
2023,Scalable sketching and indexing algorithms for large biological datasets,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2798,"DNA sequencing data continues to progress towards longer sequencing reads with increasingly lower error rates. In order to efficiently process the ever-growing collections of sequencing data, there is a crucial need for more time- and memory-efficient algorithms and data structures. In this thesis, we propose several ways to represent DNA sequences in order to mitigate some of these challenges in practical biological tasks. Firstly, we expand upon an existing k-mer (a substring of length k) -based approach, a universal hitting set (UHS), to sample a subset of locations on a DNA sequence. We show that UHSs can be efficiently constructed using a randomized parallel algorithm, and propose ways in which UHSs can be used in sketching and indexing sequences for downstream analysis. Secondly, we introduce the concept of minimizer-space sequencing data analysis, where a set of minimizers, rather than DNA nucleotides, are the atomic tokens of the alphabet. We propose that minimizer-space representations can be seamlessly applied to the problem of genome assembly, the task of reconstructing a genome from a collection of DNA sequences. By projecting sequences into ordered lists of minimizers, we claim that we can achieve orders-of-magnitude improvement in runtime and memory usage over existing methods without much loss of accuracy. We expect these approaches to be essential for downstream bioinformatics applications, such as read mapping, metagenomics, and pangenomics, as well as to provide ways to better store, search, and compress large collections of sequencing data."
2023,Privacy Law in Practice: Exploring Challenges to Modern Privacy Compliance,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3282,"Modern privacy legislation covers a broad data scope and introduces technically challenging data management requirements. Computer science research has emerged to resolve technical challenges, but proposed system designs could benefit from deeper understandings of user workflows. Existing qualitative work to understand privacy compliance on the ground gives both reason for optimism and alarm. There is a growing community of knowledgeable privacy professionals, but their effectiveness is hindered by organizational dynamics. We conduct 10 semi-structured interviews of privacy experts to further understand challenges faced by privacy practitioners. We find key challenges arising primarily from misaligned organizational incentives and difficulty in policy interpretation. We urge organizations to invest in and empower privacy engineers, researchers to explore different design directions, and policymakers to enable greater user recourse against corporations. We hope our work can help enable privacy respecting institutions and systems."
2023,Data Attribution: From Classifiers to Generative Models,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3657,"The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets. Moreover, existing methods are often tailored to the supervised learning setting, and are not well-defined for generative models.

In this thesis, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We first demonstrate the utility of TRAK across various modalities and scales in the supervised setting: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). Then, we extend TRAK to the generative setting, and show that it can be used to attribute different classes of diffusion models (DDPMs and LDMs)."
2023,Deep-learning Enabled Accurate Bruch’s Membrane Segmentation in Ultrahigh-Resolution Spectral Domain and Ultrahigh-Speed Swept Source Optical Coherence Tomography,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3853,"Aged-related macular degeneration (AMD) and diabetic retinopathy (DR), the leading cause of significant vision loss worldwide, alter the retinal structure and capillary blood flow in eyes. Optical coherence tomography (OCT) and angiography (OCTA), the gold standard imaging modalities in ophthalmic clinics, enable the micrometer-scale visualization of retinal structure and vasculature and provide the ability for early detection and progression monitoring of retinal disease. Ultrahigh resolution, spectral domain OCT prototype (UHR SD-OCT) and ultrahigh speed, swept source OCT prototype (UHS SS-OCT) developed by our group provide the ability to visualize the fine structural changes in the outer retina and vascular changes in the retina respectively, which occur with the disease progression. A few of the most important clinical findings with AMD and DR, such as drusen and choriocapillaris (CC) blood flow deficit, are located adjacent to the Bruch’s membrane (BrM). BrM is a very thin (2–6 µm) extracellular matrix, which is generally not resolved in commercial OCT instrument and therefore challenging to perform segmentation and analysis. It is even more challenging when pathologic changes in retina distort its appearance and contrast. To qualitatively and quantitatively assess the pathologic changes adjacent to BrM, an accurate segmentation is required for robust analysis. This thesis presents an advanced automatic, deep learning-based segmentation framework. The study aims to generate an accurate BrM segmentation for quantitative analysis. The performance of the segmentation is evaluated on both healthy eyes and eyes with retinal diseases, and reproducibility / repeatability is assessed through consecutive repeated imaging sessions on patients as well as longitudinal imaging of patients. This study will facilitate the investigation of in vivo early structural / vascular biomarker for AMD and DR progression."
2023,Towards Precision Oncology: A Predictive and Causal Lens,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2899,"Precision oncology promises personalized care for each patient based on a holistic view of their data. However, several methodological and translational advances are required for successful implementation of this vision in the clinic. These include building temporal models to predict a patient’s survival outcomes in response to therapy, validating these methods with experimental data from Randomized Controlled Trials (RCTs), quantifying the uncertainty in the predictions, and finally, exploring how these elements can be woven together into a clinical decision support tool. In this thesis, I explore each of these aspects in turn: i) first, I build different models of clinical time-series data, with a focus on prediction of survival outcomes and forecasting of core biomarkers, ii) next, I design methods to give additional “context” for these models, including uncertainty quantification of causal estimates and validation of these estimates using RCT data, and iii) finally, I study how these elements affect treatment decision-making via a controlled user study of a decision support tool prototype."
2023,Computational Modeling of Elastic and Transformation Incompatibility at Grain Boundaries in Shape Memory Materials,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2156,"Shape memory alloys (SMAs) and zirconia-based ceramics (SMCs) find a wide range of applications in various fields due to their unique properties such as superelasticity and shape memory effect. Desirable superelastic properties of shape memory materials are realized to their maximum extent in single crystalline structures due to the absence of internal constraints. By contrast, in polycrystalline forms, superelasticity is significantly compromised by severe premature intergranular fracture originated at grain boundaries. This limitation has drawn significant research interests in developing microstructures that can preserve the properties of single crystals while avoiding the production cost and manufacturing limitations of single-crystal processing.

The overarching goal of the thesis is to improve our understanding of the competition between martensitic transformation, grain boundary constraints, and intergranular fracture in shape memory materials through comprehensive computational modeling. To this end, we developed a finite-element based framework for modeling martensitic transformation at the continuum level incorporating details of the micromechanical information. A single-crystal model is implemented to provide a full mechanistic three-dimensional description of both the anisotropic elastic and martensitic transformation stress-strain response, including the non-Schmid behavior observed in some types of SMCs. We used the geometrically nonlinear theory of martensite to identify all possible transformation systems in SMAs and SMCs, based on the knowledge of lattice parameters of the single crystal. In the case of SMCs, the model was calibrated against data obtained from compression tests of zirconia micropillars in previously published literature. We conducted finite element simulations to obtain detailed information on the nucleation and evolution of martensite variants and stress distribution at grain boundaries in both SMAs and SMCs. The simulation results also provide insights on the competing mechanisms of elastic and transformation incompatibility leading to severe stress concentration at grain boundaries. We identified grain boundary configurations which result in very large stress 3 concentrations at very low deformations due to elastic incompatibility, as well as others where the elastic incompatibility is relatively low and stress concentrations only occur at large transformation strains. We also showed how this approach can be used to explore the misorientation space for quantifying the level of elastic and transformation incompatibility at grain boundaries in both SMAs and SMCs. In addition, we investigated the correlation between different types of incompatibilities and grain boundary characteristics. In the particular case of SMAs, we explored the role that a coincident site lattice (CSL) may have in affecting grain boundary incompatibilities. We demonstrated that grain boundaries with low CSL order exhibit low elastic incompatibilities in Cu-based SMAs, as previously suggested from experimental observations. However, high CSL order grain boundaries result in incompatibilities that are commensurate with those exhibited by random grain boundary configurations. This approach could be used to identify misorientations that reduce or minimize grain boundary incompatibilities, thus extend the superelastic range of the material."
2023,A Doppler Radar Lock-in Demodulation Algorithm for Machine Vibration Sensing,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2858,"Data-driven predictive maintenance of modern machinery has the potential to increase equipment lifespan and decrease manufacturing costs. Among various condition monitoring techniques, vibration analysis can effectively diagnose potential problems in machines. Doppler radar can be used as a sensor that provides non-contact, inexpensive real-time data collection without necessitating line-down time. Conventional Fast Fourier Transformation based vibration analysis requires large amounts of data to achieve high spectral resolution necessary for fault detection especially with radio frequency sampling, which can be computationally too expensive for analysis. In this work, we propose to use a sweeping lock-in amplifier to achieve high frequency resolution with small amounts of data by processing windowed sections of Doppler-shifted radio signals. This algorithm can reliably measure the Doppler shift frequency corresponding to the travelling speed of a low frequency moving object and identify the oscillation frequency with small amplitude, with the latter widely present in machine vibration. The distinguishing condition of the two cases is mathematically derived. The proposed algorithms are verified in simulation with triangular displacement waveform for simplicity of analysis and sinusoidal waveform for generic applications. For experimental verification, speaker vibration at a known frequency is analyzed to achieve an accuracy of 0.025 Hz within the known vibration frequency. This method is robust to the presence of noise frequencies and capable of detecting multiple frequencies."
2023,Integrated Heteroepitaxial Photodetectors,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2296,"Optical detection in the near-infrared and telecommunication bands has historically been performed using single-crystal bulk Ge, but the development of Ge-on-Si epitaxy reduced fabrication costs and opened doors for usage in applications including optical communications and infrared imaging. To reap the benefits of monolithic integration and incorporation in the back-end-of-line (BEOL) stack, low processing temperatures (< 450°) are required. Using novel processing methods and strategic anneals, we have demonstrated that low temperature Ge growths on silicon can achieve low defect densities required for high performance. In this work, Ge-on-Si p-i-n photodetectors illuminated under normal incidence have demonstrated comparable responsivity and dark current density to devices processed at high temperatures. Relatively low temperature anneals (500°C) increased performance, but as-grown diodes also showed a responsivity of 0.11 A/W and [formula]. Annealing conditions of 500°C 3 hr improved such performance to 0.15 A/W and [formula].

In the mid-wave infrared (MWIR), photodetection has been successfully implemented for decades using the II-VI material set, Hg₁₋ₓCdₓTe. Extensive research pushed HgCdTe to nearly reach its theoretical performance limit, while also highlighting its inherent shortcomings for commercialization. An upcoming material set, [formula]  has the potential to overcome such barriers while promising comparable performance. In this work, Lumerical simulations were performed to optimize a waveguide-integrated photodetector that incorporated an [formula] homojunction and was straightforward to fabricate, assuming successful epitaxy growths. The photodetector design promoted 30% light absorption after 20 𝜇m propagation into the detection region."
2023,"Between the Lines: Encoding Relations Through Body, Tool, and Algorithm",MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2499,"The tools architects use orchestrate the discipline in seen and unseen ways. In recent decades, we have swapped early forms of mechanical drawing instruments for digital tools with unimaginable computing power. While this increased level of computational literacy allows us to script and code architectural forms more efficiently, it has also created incongruities between the computationally described object and material constructions. At times the digital tools we depend on today go as far as defining the aesthetic of our buildings. To complicate this further, the digital tools most often solicited by the architectural practice are non-native imports adapted for their visual potential and practical uses. Meaning embedded within the programming of tools that shape our buildings are residual values of other disciplines. For example, we can trace the origins of CAD software back to engineers and mathematicians at Boeing and here at MIT, who sought to mechanize the construction of splines and irregular curved surfaces for the production of slipstream automobiles, toothbrushes, and even letterforms. And much like the hidden algorithms in the background of our digital tools, there is an apparatus of choreography surrounding our physical tools that encode instructions on how the body engages with the object. In other words, the machines we use produce not only drawings but gestures as well, keying us into the always-present yet rarely discussed embodied dimensions of tools. 

To expand upon the embodied dimensions of our tools today, we need to reconsider the machine as the site of intervention. Motion data and performance envelopes surrounding our tools extend beyond the projective reenactment of the machine and offer us a means to measure the derivative of what it takes to produce a drawing, a surface, or a construction. This thesis dislocates the spline from its formal geometry associated with slipstream construction and recasts it as a way to record the tumble-type inscriptions surrounding an object’s performance — a tactic to mutually mark and negotiate the activity between humans and machines."
2023,Technical and experimental design for electricity conservation policy : continuous information feedback,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3249,"Thesis: M.S., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 1986"
2023,Model development based on discrete particle simulations of partially- and fully-saturated granular media,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2246,"Granular materials are ubiquitous in industrial and geophysical scenarios. At a high computational expense, the discrete element method (DEM) simulates granular materials with a high accuracy by tracking individual particles. At the other extreme, empirical formulas based on dimensional analysis and continuum models are convenient to be applied to large scale problems, but calibrations may be needed. In this thesis, DEM simulations are carried out as virtual experiments to study the particle-scale physics and then guide the formulation of empirical relations or continuum models for two applications.

Dynamic similarity, commonly applied in fluid systems, has recently been extended to locomotion problems in granular media. Our previous research was limited to locomotors in cohesionless, flat beds of grains under the assumption of a simple frictional fluid rheology. However, many natural circumstances involve beds that are sloped or composed of cohesive grains. Expanded scaling relations are derived and DEM simulations are performed as validation, with inclined beds and cohesive grains using rotating “wheels” of various shape families, varying size and loading conditions. The data show a good agreement between scaled tests, suggesting the usage of these scalings as a potential design tool for off-road vehicles and extra-planetary rovers, and as an analysis tool for bio-locomotion in soils.

In the bedload sediment transport process, the variability in the relation between sediment flux and driving factors is not well understood. At a given Shields number, the observed dimensionless transport rate can vary over a wide range in controlled systems. A two-way coupled fluid-grain numerical scheme has been validated against physical experiments of spherical sediment particles. It is used to explore the parameter space controlling sediment transport in simple systems. Examination of fluid-grain interactions shows fluid torque is non-negligible near the threshold. And the simulations guide the formulation of continuum models for the bedload transport and the creep flow. Furthermore, a numerical scheme has been developed to simulate the transport of natural shaped sediment particles. Conglomerated spheres, approximating the real shapes from CT scanning, are constructed in DEM and coupled with the fluid solver. Agreement with the corresponding flume experiments is observed."
2023,Data Science in Investment Management,MIT CSAIL,MIT Student,MIT Advisor,Database,0.2354,"In this thesis, titled ""Data Science in Investment Management,"" we aim to explore the applications of data science and artificial intelligence across various dimensions of investment management, offering innovative solutions and insights to the industry. This thesis is composed of several parts, each addressing a different aspect of investment management and leveraging data science techniques to deliver valuable insights.

In first part, for industries and crypto-currencies, we develop a dynamic classification system that groups stocks according to quantified similarities from a wide variety of structured and unstructured data features. With the availability of big data, we were able to use artificial intelligence (AI) methods to extract relevant information about companies from various data sources and learn about their similarity in the future, according to market perception. In second part, we study ways of creating capital and portfolio management for fusion energy and biopharmaceutical investments. By leveraging computational techniques like portfolio approach, we provide novel insights into the optimal financing strategies for high-risk, high-reward ventures like fusion research and biopharmaceutical investing. We also quantify the impact of clinical trial results on the stock prices of the companies, that can aid biopharma investors in risk management. Given the increasing interest in ESG investing, we study the excess-returns of the ESG investing. We also develop the measure of the impact on patient lives due to the products of the biopharmaceutical companies that can attract ESG funds for biopharmaceutical companies. Next part of the thesis investigates the real-time psychophysiological analysis of financial risk processing, offering a deeper understanding of human behavior in the context of investment decision-making using a data driven approach. In the next part, we focus on the use of explainable Machine Learning for an important problem of consumer credit risk. In the final part, we conclude with the discussion about the future of Artificial Intelligence and Data Science in Finance."
2023,zk-Sigstore: System for Anonymous Certificate-Based Software Signing,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3446,"Most software developers get their software dependencies from online repositories, allowing for greater efficiency during the development process. However, downloading software from the internet comes with security concerns, and issues with open source software security have led to several high-profile attacks. In order to combat the problem, many repositories have implemented digital signatures for packages to verify the contributor’s identity, but with limited success due to well-documented usability issues surrounding key management. The digital signature primitive itself also does not provide an answer to which signers have the authority to sign which artifact. Proposals like Sigstore aimed at fixing the usability problems with digital signatures come with privacy concerns that have limited uptake, and though they provide some answers to the signing authority question, these come with scalability, verifiability, and privacy concerns.

This thesis presents zk-Sigstore, a system for usable (certificate-based) and anonymous digital signatures for software. zk-Sigstore is a certificate-based signature system, but instead of publishing identities in the clear, identities are obfuscated with a cryptographic commitment. Techniques from key transparency verifiable key directories inform a scalable, verifiable, and private authorization record for mapping digital artifacts to the maintainers with the authority to sign them.

Using zk-Sigstore for software signing, signing and verifying times are on the order of hundreds of microseconds even for the largest of software repositories, and deployment of zk-Sigstore requires minimal changes to existing infrastructure, making it a practical solution to this real-world problem."
2023,"Visualization and Behavioral Testing Of Common
Sense Generative Programs",MIT CSAIL,MIT Student,MIT Advisor,CV,0.3582,"Probabilistic generative programs are powerful tools that allow for modeling complex 3D worlds containing objects and agents. Recent advances in these programs have resulted in creation of rich models whose traces represent 3D scenes, but there exist challenges in using visualizations and simulation tools for practical implementations. In this thesis, I describe the development of infrastructure to accelerate research in this area. Specifically, I present a pipeline for synthetic data generation with physics simulation capabilities and a suite of rendering options. By leveraging existing scene graph generators and multiple visualization engines, photorealistic datasets can be produced to evaluate probabilistic generative programs and create stimuli for gathering information on human behavior. This framework allows fine-grained temporal tracking of object poses and velocities, both with and without occlusion, facilitating the collection of rich human behavioral data on dynamic object tracking. More broadly, the tools developed here provide visualization, debugging capabilities, and configurable synthetic datasets to benchmark future progress in 3D scene understanding. Development of this infrastructure is an investment in improved synthetic data generation and analysis frameworks is an important step toward robust probabilistic generative programs for 3D world modeling."
2023,Optimization of Throughput in Sheet Metal Manufacturing by Tuning the Sheet Metal Nesting Strategy Based on Sheet Utilization and Downstream Part Handling Costs,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2586,"Sheet metal fabrication has become a fundamental process in modern engineering due to its versatility and is used across a wide range of industries. Nesting a given set of sheet metal blanks onto raw material sheets is a major cost driver as it determines the amount of usable metal and the rest of the sheet is thrown away as scrap. Nesting algorithms are very effective at identifying the most efficient layout of a given set of parts to maximize the sheet utilization. Hence, material utilization of the sheet is mainly defined by the number of parts being nested and their geometries. On one hand, nesting algorithms would prefer having a large number of grouped parts that allow them to make more efficient sheet metal nests due to more possible combinations of parts on a given sheet. On the other hand, the downstream sorting process which sends the parts to their respective further processing stations would prefer having fewer number of grouped parts as the parts get nested randomly which increases the time spent on the non value add activity. Therefore, an effective nesting strategy between the two extremes is necessary to balance the sheet utilization with the intensive sorting requirements to make the process cost effective and meet the required throughput. In this thesis, a sheet metal nesting strategy is identified for a manufacturing operation with a wide variety of products and plant locations across the globe. Cost and throughput models are produced which inform the selection of a globally optimized nesting strategy. Regional differences in cost drivers such as varying labor rates and raw material costs are considered, and an optimized nesting strategy is validated for deployment across global plant locations. This work provides a detailed approach to optimizing sheet utilization in sheet metal manufacturing through selection of an optimized nesting strategy."
2023,Permutation-based Significance Tests for Multi-modal Hierarchical Dirichlet Processes with Application to Audio-visual Data,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3478,"Complex underlying distributions in multi-modal data motivate the need for data fusion methods that integrate observations of different modalities in a meaningful way. We explore the multi-modal hierarchical Dirichlet process (mmHDP) mixture model as a Bayesian non-parametric approach to data fusion. In particular, we elaborate on its censored-data perspective, which aligns groups of observations at a group level to accommodate for missing data in any modality. To explore the model behavior, we develop a processing pipeline that applies the mmHDP to audio-visual data, a common and practical multi-modal system. We apply this pipeline to musical data with known audio-visual relationships and provide in-depth qualitative analyses on the learned model parameters. Because of its non-parametric and unsupervised clustering nature, it can be difficult to quantify the significance of the learned mmHDP structure. We propose a novel permutation testing framework that empirically measures the significance of the mmHDP structure and demonstrate its viability using both synthetic and real audio-visual data. The results convey that the mmHDP model captures meaningful structure in the audio-visual data and that the permutation testing framework is a viable method for quantifying model significance."
2023,Integral Equation-Based Inverse Scattering and Coil Optimization in Magnetic Resonance Imaging,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2437,"One trend in Magnetic Resonance Imaging (MRI) over the years has been to steadily increase the static magnetic field strength and hence the frequency of operation, resulting in higher available signal-to-noise ratio that could be traded for shorter scan times and increased image quality. In the ultra-high field regime (≥7T), since the radiofrequency wavelength is comparable to the dimensions of body, quasi-static approaches cannot be used to simulate the interactions between electromagnetic field and biological tissue, which can result in unwanted energy deposition hot spots and in decreased image quality. The electrical properties of tissue (permittivity and conductivity) influence these interactions and the RF field distributions inside of the body. Although undesirable from the point of view of coil and pulse design, this dependence on EP opens the door to new imaging modalities using the same MR data. In this thesis, I detail how we applied highly accurate integral equation formulations to the tasks of 3D electrical properties estimation (inverse scattering) and parallel transmit (pTx) coil array optimization. I also present novel regularization strategies that are ideally suited for inverse problems. I also discuss how we validated these approaches with numerical examples, and the efforts that we undertook to estimate electrical properties of a phantom using data from an MR scanner."
2023,Magnetothermal Modulation of Nerve Growth,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2359,"Magnetic nanoparticles (MNPs) provide several mechanisms for wireless neuromodulation. MNPs under applied AC magnetic fields (AMFs) exhibit hysteresis loss, which can activate heat-sensitive ion channels such as TRPV1. This magnetothermal modulation requires AMFs with peak field strengths of up to 40 kA/m and frequencies of up to 580 kHz. While air–gap magnetic cores can achieve the necessary field parameters, their small size limits them to in–vitro experiments. A 10 cm coreless solenoid design generates the desired field parameters and is suitable for in–vivo experiments but requires several kilowatts of power. Here, we show the construction of a resonant tank inverter capable of delivering 6000 Watts of power at 600 V and 10 A to the tank circuit and generating the requisite AMF field strength and frequency inside the coil.

As a first experiment, we use the apparatus to demonstrate wireless, magnetothermal modulation of dorsal root ganglia (DRG) explants, sensory neuronal structures that are a critical target for nerve therapy. Calcium influx into neurons plays a key role in many processes necessary for axonal regeneratiton. Using magnetothermal modulation, we stimulate calcium uptake into DRG cells via TRPV1 ion channels, which are endogenously expressed and heat sensitive. By adjusting the pulse pat-tern of magnetic stimulation, we find the optimal conditions for inducing neurite outgrowth in DRG cultures."
2023,Data Augmentation and Conformal Prediction,MIT CSAIL,MIT Student,MIT Advisor,CV,0.4447,"Conformal prediction is a popular line of research in uncertainty quantification. Conformal predictors output sets of predictions accompanied by a guarantee that the set contains the true label. Conformal prediction is particularly promising because it makes no distributional assumptions and requires only a black-box classifier to produce sets with this type of guarantee. Unfortunately, existing conformal predictions can produce uninformatively large prediction sets for certain examples, which limits their applications to real-world contexts. In this thesis, we explore the impact of data augmentation, a popular computer vision technique, on the performance of conformal predictors. In particular, we present multiple ways of combining data augmentation with conformal prediction by introducing five methods of test-time-augmentation-enhanced conformal prediction (TTA-CP). We find that certain TTA-CP methods can improve upon the size and stability of prediction sets created by traditional conformal prediction. Using ImageNet and Fitzpatrick 17k, two datasets differing in size, complexity, and balance, we reveal dataset-dependent decisions that are key to improving performance in conformal prediction."
2023,Defio: Instance-Optimized Fusion of AWS Database Services,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2784,"Building large-scale data infrastructures is hard: There are often more than a single type of workloads and business requirements, but unfortunately, “one size does not fit all”. Modern database systems tend to specialize towards a specific type of workload, and thus organizations are left to integrate these differently-specialized database systems in order to achieve sufficient performance for all of their use cases and workloads.

This kind of hybrid architecture—also known as Data Mesh architecture—often leads to the increasing complexity of maintaining and utilizing database services, both for the data engineers and the end users. However, we believe that some of this complexity can be abstracted away from the end users, in particular with respect to query routing, i.e. determining where to execute each individual SQL query among the multiple database engines.

To overcome this challenge, we propose Defio, a unified interface to multiple specialized database engines that can intelligently handle myriads of workloads without having the end users think about the underlying execution of each query. Specifically, this thesis focuses on the design and implementation of an instance-optimized query router, which ultimately enables Defio to take advantage of the performance benefits of each specialized database in a Data Mesh architecture—resulting in what we call a fusion of database services."
2023,Multi-Modal Transit Time Prediction for E-Commerce Fulfillment Optimization and Carbon Emissions Reduction,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2769,"Consumers are purchasing an increasing amount of goods through digital channels as compared to brick and mortar and expect fast, reliable delivery. At the same time, society is facing the urgent challenge of reducing carbon emissions to limit global warming to levels considered safe by climate scientists. A global sportswear retailer is investing in improving the digital consumer experience while meeting its aggressive 2030 carbon reduction goals. This work studies how machine learning can be used to both improve the retailer’s digital fulfillment operations and reduce their carbon emissions footprint. It focuses on enhancing the decision-making used to select a distribution center to fulfill a consumer’s order from, and aims to do so by increasing the accuracy of a key input into that process. Specifically, the work targets accuracy improvement of transit time estimates, which quantify the number of days between a parcel’s carrier induction and delivery.

Machine learning techniques are leveraged to develop a model for predicting transit times. Model development begins with data preparation, which is inclusive of sourcing, cleaning, sampling and feature engineering. It then continues with a series of experiments to provide insights into favorable model design elements. A final model is created under consideration of experimentation results. This model is associated with an accuracy of 67%, which is a improvement beyond the current state accuracy of 45%. A counterfactual analysis is conducted to assess the impact of improved transit time estimates on key fulfillment metrics. On a one month sample, the model enables improved fulfillment decisions; namely ones that are associated with a 4.5% decrease in lead time, a 3% reduction in CO2 emissions, and a 1.5% reduction in cost."
2023,Provably near-optimal algorithms for multi-stage stochastic optimization models in operations management,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2597,"Many if not most of the core problems studied in operations management fall into the category of multi-stage stochastic optimization models, whereby one considers multiple, often correlated decisions to optimize a particular objective function under uncertainty on the system evolution over the future horizon. Unfortunately, computing the optimal policies is usually computationally intractable due to curse of dimensionality. This thesis is focused on providing provably near-optimal and tractable policies for some of these challenging models arising in the context of inventory control, capacity planning and revenue management; specifically, on the design of approximation algorithms that admit worst-case performance guarantees. In the first chapter, we develop new algorithmic approaches to compute provably near-optimal policies for multi-period stochastic lot-sizing inventory models with positive lead times, general demand distributions and dynamic forecast updates. The proposed policies have worst-case performance guarantees of 3 and typically perform very close to optimal in extensive computational experiments. We also describe a 6-approximation algorithm for the counterpart model under uniform capacity constraints. In the second chapter, we study a class of revenue management problems in systems with reusable resources and advanced reservations. A simple control policy called the class selection policy (CSP) is proposed based on solving a knapsack-type linear program (LP). We show that the CSP and its variants perform provably near-optimal in the Halfin- Whitt regime. The analysis is based on modeling the problem as loss network systems with advanced reservations. In particular, asymptotic upper bounds on the blocking probabilities are derived. In the third chapter, we examine the problem of capacity planning in joint ventures to meet stochastic demand in a newsvendor-type setting. When resources are heterogeneous, there exists a unique revenue-sharing contract such that the corresponding Nash Bargaining Solution, the Strong Nash Equilibrium, and the system optimal solution coincide. The optimal scheme rewards every participant proportionally to her marginal cost. When resources are homogeneous, there does not exist a revenue-sharing scheme which induces the system optimum. Nonetheless, we propose provably good revenue-sharing contracts which suggests that the reward should be inversely proportional to the marginal cost of each participant."
2023,First Step into A New Physics Realm: Search for the Majorana Nature of Neutrinos in the Inverted Mass Ordering Region,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2025,"The search for neutrinoless double-beta decay (0𝜈𝛽𝛽) is the only way to prove the Majorana nature of neutrinos and is thus a major area of interest for neutrino physics. Discovering 0𝜈𝛽𝛽 and measuring its half-life will be the first solid evidence for physics beyond the Standard Model (BSM) and lead to a plethora of new theoretical and experimental investigations.

This dissertation contains both theoretical and experimental work. The theoretical calculation of the non-perturbative nuclear matrix element for 0𝜈𝛽𝛽 is done using lattice quantum chromodynamics (LQCD) to interpret the experimental data. The preliminary results of 𝑛⁰𝑛⁰ → 𝑝⁺𝑝⁺𝑒⁻𝑒⁻ process for both long-range (light Majorana neutrino exchange) and short-range (heavy Majorana neutrino exchange) contributions are obtained.

The experimental work focused on KamLAND-Zen 800. It is one of the leading efforts with data from an exposure of 970 kg·yr of ¹³⁶Xe. Machine learning methods are employed to discriminate background events in data and generate new simulations for future study. With no 0𝜈𝛽𝛽 signal excess over the background expectation, statistical properties are extracted by a Bayesian analysis utilizing Markov Chain Monte Carlo (MCMC) algorithm. The lower limit for the 0𝜈𝛽𝛽 half-life of [formula] at 90% C.I., corresponding to the effective neutrino mass range of 38.4-160.0 meV, which is the first search in the inverted mass ordering region."
2022,Quasistatic computing environments,MIT CSAIL,MIT Student,MIT Advisor,Network,0.346,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1996."
2022,A plan for remodeling an industrial power plant,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2971,"Thesis (B.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1930."
2022,Synthesizing Object Models from Natural Language Specifications,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3764,"Program synthesis has traditionally excelled in tasks with precise specifications such as input-output examples and formal constraints by using structured and algorithmic approaches based on enumerative search and type inference. However, traditional synthesis techniques have no mechanism of incorporating real-world knowledge, which is commonplace in software engineering. Motivated by this, we introduce a new synthesis task known as specification reification: synthesizing concrete realizations of vague, high-level application specifications. We focus on a specific instance of this: generating object models from natural language application descriptions. Towards this goal, we present three approaches for object model synthesis that leverage domain knowledge from the GPT-3 language model. In addition, we design a scoring metric to evaluate the success of synthesized object models on seven sample tasks such as classroom management and pet store applications. We demonstrate that our language-model-based synthesizers generate object models that are comparable in quality to human-generated ones."
2022,Mathematical programming and electrical networks,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3142,"Thesis (Sc. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1959."
2022,A demonstration of a formal specification & requirements language : a case study,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3461,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2001."
2022,Dielectric Resonator Antennas : theory and design,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2241,"Theoretical models for the analysis of Dielectric Resonator Antenna (DRA) are developed. There are no exact solutions to many of the problems in analytical form, therefore a strong focus on the physical interpretation of the numerical results is presented alongside theoretical models. I have used the physical interpretation of the numerical results to lay down some important design rules. A few new inventions associated with the DRA are also included. These are the elliptical DRA, the DRA with a rectangular slot, the adjustable reactance feed, the triangular DRA and the dual band DRA-patch antenna."
2022,Magneto-thermal Transport and Machine Learning-assisted Investigation of Magnetic Materials,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2146,"Heat is carried by different types quasiparticles in crystals, including phonons, charge carriers, and magnetic excitations. In most materials, thermal transport can be understood as the flow of phonons and charge carriers; magnetic heat flow is less well-studied and less well understood.

Recently, the concept of the flat band, with a vanishing dispersion, has gained importance. Especially in electronic systems, many theories and experiments have proven that some structures such as kagome or honeycomb lattices hosts such flat bands with non-trivial topology. Even though a number of theories suggest that such dispersionless mode exist in magnonic bands under the framework of the Heisenberg spin model, few experiments indicate its existence. Not limited to these flat band effects, magnetic insulators can assume a variety of nontrivial topologies such as magnetic skyrmions. In this thesis, I investigate the highly frustrated magnetic system Y0.5Ca0.5BaCo4O7, where the kagome lattice could potentially lead to nontrivial thermal transport originated from its flat band. While we do not observe signatures of the flat band in thermal conductivity, the observed anomalous Hall effect in electrical transport and spin glass-like behavior suggest a complex magnetization-transport mechanism.

Motivated by the rapid advancement of artificial inteligence, the application of machine learning into materials exploration is recently investigated. Using a graphical representation of crystallines orginally suggested in Crystal Graphical Convolutional Neural Network (CGCNN), we developed the ML-asssited method to explore magnetic compounds. Our machine learning model can, so far, distiguish ferromagnet or antiferromagnet systems with over 70% accuracy based only on structual/elemental information. Prospects of studying more complex magnets are described."
2022,"Graphs, Principal Minors, and Eigenvalue Problems",MIT CSAIL,MIT Student,MIT Advisor,Network,0.262,"This thesis considers four independent topics within linear algebra: determinantal point processes, extremal problems in spectral graph theory, force-directed layouts, and eigenvalue algorithms. For determinantal point processes (DPPs), we consider the classes of symmetric and signed DPPs, respectively, and in both cases connect the problem of learning the parameters of a DPP to a related matrix recovery problem. Next, we consider two conjectures in spectral graph theory regarding the spread of a graph, and resolve both. For force-directed layouts of graphs, we connect the layout of the boundary of a Tutte spring embedding to trace theorems from the theory of elliptic PDEs, and we provide a rigorous theoretical analysis of the popular Kamada-Kawai objective, proving hardness of approximation and structural results regarding optimal layouts, and providing a polynomial time randomized approximation scheme for low diameter graphs. Finally, we consider the Lanczos method for computing extremal eigenvalues of a symmetric matrix and produce new error estimates for this algorithm."
2022,Predicting Audience Tweet Engagement,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3405,"Social media has become the ubiquitous infrastructure through which the world is connected. It allows people to interact not only with family members and friends but also with prominent figures like movie stars, presidential candidates, and even royalty. These celebrities have immense presences on social media, and each post they share has the potential to reach millions of people. As the sphere of social media influence grows increasingly large, it also becomes increasingly important to be able to understand how influencers on social media affect their audience. However, it is difficult for individuals with large social media platforms to gain insight into how their posts influence their followers. While social media platforms do provide influencers with some audience breakdowns and statistics, they are often not granular enough to be useful. In this thesis, we present methods to analyze an influencer’s tweets and audience. We then use these results to predict which segments of an influencers audience will interact with different types of posts. These insights can help determine which areas an influencer has the greatest potential to make an impact in and thus guide the direction and content of influencer campaigns."
2022,Design and Analysis of a Novel Wave Energy Converter With a Tension Leg Platform and Oscillating Proof Masses,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2441,"A design of novel wave energy converter with an oscillating proof mass and an electromagnetic power takeoff mechanism was considered. The wave energy converter has two parts, a tension leg platform connected by tether lines to the sea floor and inside of it, proof mass oscillators with motions which are coupled to those of the tension leg platform. In order to simplify the analysis, the system was constrained to only oscillate in the direction of surge. Complex hydrodynamic forces caused by ocean waves will excite the system and the surge motion of the proof mass relative to the tension leg platform will generate power via the electromagnetic power takeoff mechanism. First a model of the system with a linear restoring force exerted on the proof mass is analyzed using linear theory. Following the development of the linear theory, a more complex model with a nonlinear restoring force was considered. Using both a frequency-domain approach and a time-domain simulation, the average power of these systems were calculated. To further maximize power, a control circuit and control law are introduced which increase the average power by multiple factors. By introducing nonlinear restoring force and a control law, the performance of the system was shown to be further improved."
2022,Uncertainty-Based Design Optimization and Decision Options for Responsive Maneuvering of Reconfigurable Satellite Constellations,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2636,"There are many time-sensitive mission applications for persistent satellite coverage, including dynamic and unpredictable events such as natural disasters, oil spills, extreme weather events, or geopolitical conflicts, which may progress rapidly and require frequently-updated information to co-ordinate the ground response. Reconfigurable satellite constellations can provide on-demand regional coverage by maneuvering orbits to focus passes over the area of interest. In contrast, traditional satellite constellations cannot maneuver to pass over specific ground locations, meaning that achieving persistent coverage spanning all possible locations of interest globally results in a requirement for thousands of satellites. This would present prohibitive costs for many applications, as well as contributing to worsening issues of space traffic management and congestion in Low Earth Orbit (LEO).

Incorporating reconfigurability into constellation design allows for responsive maneuvering of satellites into repeating ground tracks (RGTs) over a location of interest, simultaneously reducing the required constellation size by improving the utilization of individual satellites and providing flexibility in the achievable ground coverage. Past work on reconfigurable constellations (ReCon) demonstrated average cost savings of 20-70% compared to iso-performance static constellations, although the complexity of the solution space for the design optimization process limited the maximum size of constellations that could be evaluated.

In this thesis, a probabilistic performance metric is developed to compare constellation designs, adopting principles of reliability-based design optimization to quantify the confidence level that reconfigurable designs will outperform iso-cost static alternatives and by what margin of performance. The results show that 74.2% of reconfigurable designs outperform iso-cost static designs with a confidence level of 90% or higher, and with a margin of at least 10% improvement in the level of performance achieved. Computational intensity of the model presents the major constraint upon the size and complexity of simulation cases that may be modelled, so variance reduction techniques are applied to lower the standard error of mean performance in the output, allowing for a reduction in optimization size and runtime while maintaining the same level of error in the predicted results. Decision options for the operational phase of a reconfigurable constellation are presented and assessed to characterize how satellite operators must weigh mission priorities to evaluate trade-offs between propellant conservation and improved coverage of high-value targets."
2022,Reducing intraday patient wait times through just-in-time bed assignment,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2804,"Massachusetts General Hospital (MGH) is the oldest and largest hospital in New England as well as the original and largest teaching hospital of the Harvard Medical School. The neuroscience units experience patient flow issues similar to those observed throughout MGH, including high bed utilization and long intraday patient wait times. This project focuses on the neuroscience units as a microcosm of the hospital. MGH consistently operates near capacity. Patients from the emergency department, the perioperative environment, intensive care units (ICUs) and other sources compete for beds. The admitting department manages the bed assignment process across MGH. Assignments are often made without access to all relevant information, such as expected admission, surgery and discharge timing. As a result of common procedures, patients are frequently assigned to a bed before they are clinically ready to move. Our analysis reveals that suboptimal bed assignment and patient transfer processes are among the leading root causes of intraday patient delays. The primary objective of the project is to develop a bed assignment policy to reduce intraday patient wait times. The policy consists of a bed assignment algorithm and enabling bed management processes. To account for patient acuity, the algorithm segments patients by movement (e.g., ED-to-ICU). The target maximum wait for each segment is the acceptable wait length (AWL). The algorithm ranks patients based on their ready times and the AWLs, and assigns beds primarily on a just-in-time (JIT) basis. The enabling bed management processes include small-scale early discharge and early transfer interventions to better align the intraday timing of demand for inpatient beds with available capacity. A simulation of neuroscience patient flow is used to evaluate different approaches. The model shows that adoption of the JIT policy would increase the percentage of patients who experience bed waits within the AWL for all movement types. Predicted bed waits for patients who require ICU-level care would be 30 minutes or less for 90% of ED patients and 95% of OR patients (improvements from historical baselines of 44% and 91%, respectively). Predicted bed waits for transfers to floor beds would be two hours or less for 81% of ED patients and 93% of OR patients (improvements from historical baselines of 63% and 84%, respectively). The solution significantly reduces intraday patient wait times without a major increase in hospital capacity."
2022,Double-gated field emission arrays,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2112,"There is a need for massively parallel, individually addressed and focused electron sources for applications such as flat panel displays, mass storage and multi-beam electron beam lithography. This project fabricates and characterizes double-gated field emission devices with high aspect ratio. One of the gates extracts the electrons while the second gate focuses the electrons into small spots. High aspect ratio silicon field emitters were defined by reactive ion etching of silicon followed by multiple depositions of polycrystalline oxide insulators and silicon gates. The layers were defined by a combination of lithography, chemical mechanical polishing and micromachining. We obtained devices with gate and focus apertures of 0.4[mu]m and 1.2[mu]m diameter. The anode current has very little dependence on the focus voltage and the ratio of the focus field factor to the gate field factor βF / βG is 0.015. Scanning electron micrographs of the devices, numerical simulation and spot size measurements on a phosphor screen confirmed these results. An e-beam resist, PMMA, was successfully exposed using the FEA device as an electron source."
2022,Optimal sizing of solar and battery assets in decentralized micro-grids with demand-side management,MIT CSAIL,MIT Student,MIT Advisor,Network,0.2625,"Solar-based community micro-grids and individual home systems have been recognized as key enablers of electricity provision to the over one billion people living without energy access to-date. Despite significant cost reductions in solar panels, these options can still be cost-prohibitive mainly due over-sizing of generation assets corresponding with a lack of ability to actively manage electricity demand. The main contribution shared is the methodology and optimization approach of least-cost combinations of generation asset sizes, in solar panels and batteries, subject to meeting reliability constraints; these results are based on a techno-economic modeling approach constructed for assessing decentralized micro-grids with demand-side management capabilities. The software model constructed is implemented to represent the technical characteristics of a low-voltage, direct current network architecture and computational capabilities of a power management device. The main use-case of the model presented is based on serving representative, aggregated, household-level load profiles combined with simulated power output from solar photovoltaic modules and the kinetic operating constraints of lead-acid batteries at hourly timesteps over year-long simulations. The state-space for solutions is based on available solar module and battery capacities from distributors in Jharkhand, India. Additional work presented also extends to real-time operation of such isolated micro-grids with requisite local computation. First, for load disaggregation and forecasting purposes, clustering algorithms and statistical learning techniques are applied on quantitative results from inferred load profiles based on data logged from off-grid solar home systems. Second, results from an optimization approach to accurately parametrize a lead-acid battery model for potential usage in real-time field implementation are also shared. Economic results, sensitivity analyses around key technical and financial input assumptions, and comparisons in cost reductions due to the optimization of solar and battery assets for decentralized micro-grids with demand-side management capabilities are subsequently presented. The work concludes with insights and policy implications on establishing differentiated willingness-to-pay, tiers of service, and dynamic price-setting in advanced micro-grids."
2022,Design and analysis of a 6/4-GHz receiver front end,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3468,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2022,Representing troubleshooting information for a high-volume production line,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3546,"Thesis (M.S.)--Massachusetts Institute of Technology, Sloan School of Management, 1994, and Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1994."
2022,Algorithms for simulating human pre-mRNA splicing decisions,MIT CSAIL,MIT Student,MIT Advisor,ML,0.26,"In this thesis, I developed a program, ExonScan, to simulate constitutive human pre-mRNA splicing. ExonScan includes several models for splicing components, including splice sites, exonic splicing enhancers, exonic splicing silencers, and intronic splicing enhancers. I used ExonScan to test various aspects of human splicing, including correlation of splicing signal strength with tissue expression levels, the effectiveness of experimentally determined exonic splicing silencers, and splice site identification."
2022,Modeling of micro-electro-mechanical integrated test structures,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3468,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2022,Multiple machine maintenance : applying a separable value function approximation to a variation of the multiarmed bandit,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3467,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2002."
2022,Improving equipment performance through queueing model applications,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3566,"Thesis (M.S.)--Massachusetts Institute of Technology, Sloan School of Management, 1995, and Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2022,A stacked full-bridge microinverter topology for photovoltaic applications,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2166,"Previous work has been done to develop a microinverter for solar photovoltaic applications consisting of a high-frequency series resonant inverter and transformer section connected to a a cycloconverter that modulates the resonant current into a single-phase 240 VRMS utility line. This thesis presents a new stacked full-bridge topology that improves upon the previous high-frequency inverter section. By utilizing new operating modes to reduce the reliance on frequency control and allowing for the use of lower blocking voltage transistors, the operating frequency range of the HF inverter is reduced and efficiency is increased, especially at low output powers and lower portions of the line cycle. The design of an experimental prototype to test the stacked full-bridge HF inverter topology is presented along with test results that demonstrate the success of the topology. Future improvements to increase performance are also suggested."
2022,Modeling the appearance of cloth,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3499,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1987."
2022,Real property portfolio management : a decision-support model,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3452,"In the 1980's corporate real estate has assumed a more active role in the strategic planning of American corporations. However, the tools to accurately evaluate the performance of corporate real property portfolios are still at a very rudimentary stage in their development. This thesis concentrates on the space inventory system of a large corporation and presents a model for determining fair comparisons between buildings across the portfolio. A technique is devised for identifying ""outliers"", that is, buildings whose performance is significantly different from other buildings of the same type. This technique shows how to classify buildings into groups, so that building class standards can be determined and trends identified. Artificial Intelligence tools such as decision-support systems can be helpful to encode the expertise for evaluating buildings' performance levels. Through the design of two working demos the thesis illustrates how that is possible, and points towards future alternatives. The author spent an academic semester as a consultant/ intern in the real estate division of a multinational corporation. For anonymity purposes, the corporation is called the Star Corporation. The Star Corp. provided the data used in the research, as well as the supervision and training in their in-house systems operation."
2022,Frequency domain model-based intracranial pressure estimation,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3213,"Elevation of intracranial pressure (ICP), the pressure of the fluid surrounding the brain, can require urgent medical attention. Current methods for determining ICP are invasive, require neurosurgical expertise, and can lead to infection. ICP measurement is therefore limited to the sickest patients, though many others could potentially benefit from availability of this vital sign. We present a frequency-domain approach to ICP estimation using a simple lumped, linear time-invariant model of cerebrovascular dynamics. Preliminary results from 28 records of patients with severe traumatic brain injury are presented and discussed. Suggestions for future work to improve the estimation algorithm are proposed."
2022,A multiscale framework for Bayesian inference in elliptic problems,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2806,"The Bayesian approach to inference problems provides a systematic way of updating prior knowledge with data. A likelihood function involving a forward model of the problem is used to incorporate data into a posterior distribution. The standard method of sampling this distribution is Markov chain Monte Carlo which can become inefficient in high dimensions, wasting many evaluations of the likelihood function. In many applications the likelihood function involves the solution of a partial differential equation so the large number of evaluations required by Markov chain Monte Carlo can quickly become computationally intractable. This work aims to reduce the computational cost of sampling the posterior by introducing a multiscale framework for inference problems involving elliptic forward problems. Through the construction of a low dimensional prior on a coarse scale and the use of iterative conditioning technique the scales are decouples and efficient inference can proceed. This work considers nonlinear mappings from a fine scale to a coarse scale based on the Multiscale Finite Element Method. Permeability characterization is the primary focus but a discussion of other applications is also provided. After some theoretical justification, several test problems are shown that demonstrate the efficiency of the multiscale framework."
2022,Asynchronous distributed flow control algorithms,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3457,"Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1984."
2022,Studies in discrete dynamic programming,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3138,"Thesis (Sc. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1958."
2022,A strategic perspective on the commercialization of artificial intelligence : a socio-technical analysis,MIT CSAIL,MIT Student,MIT Advisor,AI,0.3683,"Many companies are increasing their focus on Artificial Intelligence as they incorporate Machine Learning and Cognitive technologies into their current offerings. Industries ranging from healthcare, pharmaceuticals, finance, automotive, retail, manufacturing and so many others are all trying to deploy and scale enterprise Al systems while reducing their risk. Companies regularly struggle with finding appropriate and applicable use cases around Artificial Intelligence and Machine Learning projects. The field of Artificial Intelligence has a rich set of literature for modeling of technical systems that implement Machine Learning and Deep Learning methods. This thesis attempts to connect the literature for business and technology and for evolution and adoption of technology to the emergent properties of Artificial Intelligence systems. The aim of this research is to identify high and low value market segments and use cases within the industries, prognosticate the evolution of different Al technologies and begin to outline the implications of commercialization of such technologies for various stakeholders. This thesis also provides a framework to better prepare business owners to commercialize Artificial Intelligence technologies to satisfy their strategic goals."
2022,Global and Robust Optimization for Engineering Design,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2595,"There is a need to adapt and improve conceptual design methods through better optimization, in order to address the challenge of designing future engineered systems. Aerospace design problems are tightly-coupled optimization problems, and require all-at-once solution methods for design consensus and global optimality. Although the literature on design optimization has been growing, it has generally focused on the use of gradient-based and heuristic methods, which are limited to local and low-dimensional optimization respectively. There are significant benefits to leveraging structured mathematical optimization instead. Mathematical optimization provides guarantees of solution quality, and is fast, scalable, and compatible with using physics-based models in design. More importantly perhaps, there has been a wave of research in optimization and machine learning that provides new opportunities to improve the engineering design process. This thesis capitalizes on two such opportunities.

The first opportunity is to enable efficient all-at-once optimization over constraints and objectives that use arbitrary mathematical primitives. This work proposes a constraint sampling and learning approach for global optimization, leveraging developments in machine learning and mixed-integer optimization. More specifically, the feasible space of intractable constraints is sampled using existing and novel design of experiments methods, and learned using optimal classification trees with hyperplanes (OCT-Hs). OCT-Hs describe union-of-polyhedra approximations of intractable constraints, which are solved efficiently using commercial solvers to find near-feasible and near-optimal solutions to the global optimization problem. The constraints are then checked and the solution is repaired using projected gradient methods, ensuring feasibility and local optimality. The method is first tested on synthetic examples, where it finds the global optima for 9 out of 11 benchmarks, and high-performing solutions otherwise. Then it is applied to two real-world problems from the aerospace literature, and especially to a satellite on-orbit servicing problem that cannot be addressed via other global optimization methods. These applications demonstrate that decision tree driven optimization provides efficient, practical and optimal solutions to difficult global optimization problems present in aerospace design as well as other domains, regardless of the form of the underlying constraints.

The second opportunity is to optimize designs affected by parametric uncertainty in a tractable and deterministic manner, while providing guarantees of constraint satisfaction. Inspired by the wealth of literature on robust optimization, and specifically on robust geometric programming, this thesis proposes and implements robust signomial programming to solve engineering design problems under uncertainty. The methods are tested on a conceptual aircraft design problem, demonstrating that robust signomial programs are sufficiently general to address engineering design problems, solved efficiently by commercial solvers, and result in designs that protect deterministically against uncertain parameter outcomes from predefined sets. In addition, robust designs are found to be less conservative than designs with margins; robust aircraft demonstrate 9% better average performance than aircraft designed with margins over the same scenarios, while providing guarantees of constraint feasibility.

In anticipation of future aerospace design problems becoming increasingly coupled, complex and risky, this thesis provides a new perspective for dealing with design challenges using structured mathematical optimization. The proposed methods inject mathematical rigor into engineering design methods while keeping practical concerns for conceptual design in focus."
2022,"Dynamic responsiveness in the American states : legislators, constituents, and organized interests",MIT CSAIL,MIT Student,MIT Advisor,SE,0.3034,"In my second paper, I demonstrate the effectiveness of supervised machine learning methods in recognizing textual references to firms, organized interests, or any other political actors (an application of named entity recognition), and then resolving these references to real-world referents (an entity resolution task). Together, these methods make possible the large-scale measurement of political actors or their activity from sources such as diplomatic cables, transcripts, and administrative or legislative records. Organized interests are embedded in the legislative process in state capitols, writing bills and participating in committee meetings; they contribute stakeholder perspective and testify to the technical points of proposed legislation. Studying exactly which groups participate addresses a minimal standard for democratic governance. The third paper accomplishes this using the measurement strategy described in the second paper."
2022,Analysis of signal transduction networking using activation ratios,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3204,"The molecular processes by which information is incorporated and distributed within a cell are termed signal transduction. These pathways allow cells to interact with each other and with their environments and are critical to the proper cellular function in a variety of contexts. Previously developed methods for analyzing signaling networks have been largely ignored, most likely due to their mathematical complexity and difficulty in application. A novel analysis framework was developed to assist in the examination of signaling networks, both to facilitate the reconstruction of previously undetermined pathways as well as to quantitatively characterize interactions between components. This approach, termed activation ratio analysis, involves the ratio between active and inactive forms of signaling intermediates at steady state. The activation ratio of an intermediate is shown to depend linearly upon the concentration of the activating enzyme. The slope of the line is defined as the activation factor, and is determined by the kinetic parameters of activation and inactivation. The mathematical functionality of the activation ratio changes for other signaling network arrangements. It is therefore possible to extract the original network structure from a set of measured activation ratios, with activation factors yielding a measure of activation potential between intermediates. This framework was tested using computational simulations of a small-scale interconnected network, cascades with feedback, and in the presence of experimental noise. In the process, additional tools were developed to automate and evaluate the analysis."
2022,Autonomous data collection techniques for approximating marine vehicle kinematics,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.356,"Understanding vehicle kinematics is essential in allowing autonomous guidance algorithms to accurately assess short range encounters. Low cost, reconfigurable autonomous vehicles motivate using in-field online techniques rather than tow tank testing or Computational Fluid Dynamics (CFD). While the parameters of many physical dynamic models can be obtained using System Identification (SI) techniques, these models require knowledge of the vehicle actuators, which may not be the case in a ""backseat driver"" architecture using payload autonomy. Even when an identified physical model is available, using it to simulate trajectories requires insight into the design of the relevant controller, which may be proprietary or otherwise unknown to the back seat. This thesis develops a data collection procedure to obtain empirical kinematic trajectories for unmanned surface vehicles (USVs). A linear black box model of the USV yaw system is also developed, using only data available in the backseat. A prediction table for the M200 USV is developed with both techniques."
2022,Uncertainty-Aware Ensembling in Multi-Modal AI and its Applications in Digital Health for Neurodegenerative Disorders,MIT CSAIL,MIT Student,MIT Advisor,CV,0.356,"Common neurodegenerative disorders such as Alzheimer's dementia and Parkinson's disease are increasingly recognised as leading causes of death and disability with debilitating symptoms such as progressive cognitive decline, communication breakdown, motor dysfunction and accompanying psychiatric disorders. However, factors such as unavailability of efficient and cost-effective assessments for conclusive diagnosis, time-consuming test protocols, poor prognostic capabilities, and inadequate treatment options with accompanying side effects are all barriers to progress in providing faster and more effective intervention to individuals living with these life-altering disorders. In this thesis, we take a step towards using digital health and machine learning to improve diagnostic and prognostic capabilities and to address remote care via telemedicine in Alzheimer's dementia and Parkinson's disease. Our goal is to provide more cost-effective, non-invasive, and scalable technologies for risk stratification of Alzheimer's dementia using speech. We also aim to monitor drug response and disease progression for Parkinson's disease via telemedicine, allowing real time symptom tracking through wearables alongside a patient's treatment status, which will help facilitate remote care and dynamic and adaptive treatment plans. In addition to addressing the challenges in diagnosis and treatment of neurodegenerative disorders, we further propose a novel uncertainty aware boosting technique for multi-modal ensembling and evaluate it on healthcare tasks related to Alzheimer's dementia and Parkinson's disease. This presents manifold benefits, such as reducing the overall entropy of the system, making it more robust to heteroscedasticity, and improving calibration of each of the modalities along with high quality prediction intervals."
2022,Design of Nuclear-Targeting Peptides for Macromolecule Delivery via Machine Learning,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2753,"The effective design of functional peptide sequences remains a fundamental challenge in biomedicine. For example, cell-penetrating peptides (CPPs) are capable of delivering macromolecular cargo to intracellular targets that are otherwise inaccessible. However, design of novel CPPs with high activity and unique structure remains challenging. In this thesis, methods to design and characterize highly active CPPs for antisense oligonucleotide delivery were explored.

Machine learning is a promising method for de novo design of functional peptide sequences. A deep learning model inspired by directed evolution was used to optimize abiotic sequences that traffic antisense oligomers to the nucleus of cells. The model was able to predict activities beyond those in the training dataset, and simultaneously decipher and visualize sequence-activity predictions. The validated miniproteins (40-80 residues) were more effective than any previously known variant in cells. By augmenting the machine learning model to over-represent shorter sequence space, the model also predicted a short peptide (18-residues) with comparable activity to a positive control peptide. Empirical sequence-activity studies demonstrated reliance on the cationic residues as well as the C-terminal cysteine residue. These sequences were nontoxic, able to deliver other biomacromolecules to the cytosol, and efficiently delivered antisense cargo in mice.

A different approach to discover and characterize CPP sequences was also taken, by extracting peptides taken up into cells and analyzing their relative quantities or identifying their sequences by mass spectrometry. First, several mirror-image D-peptides had similar delivery activity to their native forms, while demonstrating complete proteolytic stability. Mixtures of fully intact antisense-peptide conjugates could be recovered from whole cell and cytosolic lysates, and relative concentrations were quantified by MALDI-TOF. This method was then extended to the discovery of de novo sequences from a combinatorial library of antisense-peptide conjugates containing unnatural residues. Following cell treatment with the biotinylated antisense-peptide library, the cytosol of cells was extracted and internalized peptides recovered via affinity capture. De novo sequencing was achieved by Orbitrap tandem mass spectrometry, and several unique, unnatural sequences were identified that could effectively deliver the antisense oligomer to the nucleus.

In summary, machine learning and mass spectrometry-based strategies to discover and characterize novel CPP sequences for antisense delivery were explored. In the future, we envision combining these methods in order to use lists of library hits to train a machine learning model to design sequences composed of fully unnatural amino acids."
2022,Optimal control of controllable switched systems,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2877,"Many of the existing techniques for controlling switched systems either require the solution to a complex optimization problem or significant sacrifices to either stability or performance to offer practical controllers. In [13], it is shown that stabilizing, practical controllers with meaningful performance guarantees can be constructed for a specific class of hybrid systems by parameterizing the controller actions by a finite set. We extend this approach to the control of controllable switched systems by constraining the switching portion of the control input and fixing the feedback controller for each subsystem. We show that, under reasonable assumptions, the resulting system is guaranteed to converge to the target while providing meaningful performance. We apply our approach to the direct-injection stratified charge (DISC) engine and compare the results to that of a model predictive controller designed for the same application."
2022,A framework for multi-modal input in a pervasive computing environment,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3011,"In this thesis, we propose a framework that uses multiple-domains and multi-modal techniques to disambiguate a variety of natural human input modes. This system is based on the input needs of pervasive computing users. The work extends the Galaxy architecture developed by the Spoken Language Systems group at MIT. Just as speech recognition disambiguates an input wave form by using a grammar to find the best matching phrase, we use the same mechanism to disambiguate other input forms, T9 in particular. A skeleton version of the framework was implemented to show this framework is possible and to explore some of the issues that might arise. The system currently works for both T9 and Speech modes. The framework also includes potential for any other type of input for which a recognizer can be built such as graffiti input."
2022,Active pixel sensors for X-ray astronomy,MIT CSAIL,MIT Student,MIT Advisor,CV,0.3299,"An active pixel sensor array, APS-1, has been fabricated for the purpose of scientific x-ray detection. This thesis presents the results of testing the device. Alternate design architectures are explored. Recommendations are made for a next-generation sensor. CCDs have been the dominant x-ray sensor in astronomy for over ten years. Limitations inherent to CCDs are starting to become important. Active pixel sensors (APS) provide an alternate architecture that may solve these problems. APS-1 is a first-generation sensor designed by Lincoln Laboratory's Advanced Silicon Technology Group. APS-1 is fabricated in a fully depleted silicon-on-insulator (FDSOI) technology. FDSOI is especially well-suited to produce a scientific x-ray imager. The device includes sixteen different pixel variations to determine the processing parameters that can produce the best imager. Dark current, noise, and responsivity of the various pixel designs was measured using an electronics system adapted from a CCD test system. X-rays were detected at room temperature. Ordinary active pixels have high noise levels ( 70 electrons). Many pixel designs capable of lower noise have been presented in the literature. Active reset, pixel-level CDS, and CTIA pixel designs are discussed in detail and simulated. A second-generation sensor from Lincoln Laboratory, using pixel-level CDS, is discussed. This device, APS-2, will be available for testing in 2006. APS-2 simulation results are presented. It is expected to have an input-referred noise of less than five electrons, near the performance of modern CCDs."
2022,Nonsmooth Methods for Process Integration,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2694,"Process integration is a promising method to improve sustainability and reduce waste in chemical processes by recovering excess resources such as heat, water, or other materials. However, calculating the maximum amount of resource that can be reused is challenging because resource sinks can only take in resource if it is of high enough quality. As a result, most current integration methods are either limited and heuristic or use large superstructure formulations that must assess all possible matches between the resource sources and sinks. 

Therefore, this thesis presents new computational methods for maximizing resource recovery that use nonsmooth functions to compactly describe the resource that is available at different qualities. This work can be divided into three main contributions that improve process integration for systems with different resources and assumptions:

1. A generalized approach to process integration that uses a system of two nonsmooth equations to describe optimal reuse for a wide variety of resources, including multiple resources simultaneously,

2. An extension of this general approach to more complex mass and water systems with multiple contaminants that can limit their reuse,

3. A non-smooth optimization formulation that applies our integration approach to design variable-temperature cogeneration systems that convert process waste heat into electricity. 

By utilizing non-smooth equations, each of these contributions exhibits improved scaling compared to other integration methods and have numbers of equations or constraints that remain the same regardless of the size and complexity of the system. In addition, unlike other methods, our approaches have the flexibility to either determine resource requirements or the process variables that achieve a given target.

This thesis describes the formulation and implementation of each of these non-smooth approaches and applies them to a wide of range of example applications. These applications include carbon-constrained energy planning, hydrogen conservation networks, water recovery from petroleum refining with multiple contaminants, and designing improved cogeneration systems for sulfuric acid and cement production processes. The results from these examples show the flexibility and scalability of our approaches and the breadth of improvements they can provide. Together, our contributions increase the applicability of computationally efficient process integration methods to improve the sustainability of a wide range of chemical processes."
2022,The design and construction of a special purpose computer for speech synthesis-by-rule.,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3327,Thesis. 1976. M.S.--Massachusetts Institute of Technology. Dept. of Electrical Engineering and Computer Science.
2022,Identifying and modeling unwanted traffic on the Internet,MIT CSAIL,MIT Student,MIT Advisor,Network,0.4237,"Accurate models of Internet traffic are important for successful testing of devices that provide network security. However, with the growth of the Internet. it has become increasingly difficult to develop and maintain accurate traffic models. While much internet traffic is legitimate, productive communications between users and services, a significant portion of Internet traffic is the result of unwanted messages sent to IP addresses without regard as to whether there is an active host at that address. In an effort to analyze unwanted traffic, tools were developed that generate statistics and plots on captured unwanted traffic to unused IP addresses. These tools were used on a four-day period of traffic received on an inactive IPv4 class A network address space. Each class B subnet in this address space received an average of 7 million packets corresponding to 21 packets per second. Analyses were performed on a range of class B and C subnets with the intent of discovering the types of variability that are characteristic of unwanted traffic. Traffic volume over time, number of scans, destinations ports, and traffic sources varied substantially across class B and C subnets."
2022,"A theoretical analysis of interstitial hydrogen : pressure-composition-temperature, chemical potential, enthalpy and entropy",MIT CSAIL,MIT Student,MIT Advisor,ML,0.2312,"We provide a first principles analysis of the physics and thermodynamics of interstitial hydrogen in metal. By utilizing recent advances in Density Functional Theory (DFT) to get state energies of the metal-hydrogen system, we are able to model the absorption process fairly accurately. A connection to experiment is made via Pressure-Composition-Temperature (PCT) isotherms, and thermodynamic molar quantities. In the model, we understand the excess entropy of absorbed hydrogen in terms of the change in its accessible microstates. A connection is also made between the entropy and electronic states of interstitial hydrogen. However, our model indicates that this connection is too small to account for experimental results. Therefore, a conclusion is made that the entropy of absorbed hydrogen is mostly (non-ideal) configurational in nature. To model the latter in a manner consistent with experiment, we have explored a new model that posits a weak binding between clusters of hydrogen atoms at neighboring sites. We have developed a formulation and fitted the results to experimental data. We find a least squares fitting of the model to the entropy and enthalpy results in model parameters which seem physically reasonable. The resulting model appears to provide a natural physical explanation for the dependence of the excess entropy on loading."
2022,Scheme for identifying and describing behavioral innovations embodied in computer programs,MIT CSAIL,MIT Student,MIT Advisor,Network,0.346,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1996."
2022,Nickel-catalyzed Suzuki-Miyaura reactions of unactivated halides with alkyl boranes and planar-chiral borabenzene catalysts for Diels-Alder reactions,MIT CSAIL,MIT Student,MIT Advisor,Network,0.234,"Part I describes the expansion in scope of a nickel-catalyzed coupling reaction of unactivated alkyl bromides and alkyl boranes to include unactivated alkyl chlorides. The new method is adapted for use outside of a glove box and is also found to be applicable not only to the coupling of primary chlorides, but also to the coupling of bromides and iodides, both primary and secondary. ... This coupling reaction of chlorides is further adapted to the of p-chloro aryl alkyl amines. This work constitutes an extension directing groups for the asymmetric Suzuki-Miyaura reactions halides. ... Part II details work towards an asymmetric Diels-Alder reaction between cyclopentadiene and methacrolein catalyzed by a planar-chiral boron Lewis acid. This system exhibits a level of turnover that is unprecedented in reactions mediated by planar chiral boron heterocycles. Computational studies shed light on the nature of the 7tsymmetry interaction between borabenzenes and complexed carbonyl groups. The selectivity of the borabenzene-catalyzed Diels-Alder reaction is also examined."
2022,Dynamic models for convective systems.,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3287,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1966. Ph.D.
2022,Designing and Testing a Mobile Creative Coding Application for Children,MIT CSAIL,MIT Student,MIT Advisor,SE,0.339,"Children are becoming increasingly engaged with applications on mobile phones. They use mobile apps to socialize, communicate, and play games. There is an opportunity to channel this familiarity and fascination with mobile phones in order to introduce children to computational thinking and creative expression. Towards that end, the Lifelong Kindergarten (LLK) Group is designing a free mobile application that will provide a motivating, creative, and accessible way for children to learn how to code through creating interactive animations that they can send to friends and family. This thesis identifies key design questions and challenges involved in the design of this new coding platform for children, reviews the strategies other mobile applications have employed in addressing related design challenges, and introduces a variety of solutions that our research team has developed, along with their affordances and limitations. This thesis also presents an analysis of data from conducting playtests and semi-structured interviews, and suggests lessons learned for other designers of mobile coding applications for children."
2022,A study of time-compressed speech.,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3388,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1974. Ph.D.
2022,Similarity Metrics for Biological Data: Algorithmic developments for high-dimensional datasets,MIT CSAIL,MIT Student,MIT Advisor,ML,0.3102,"Advances in experimental methods in biology have allowed researchers to gain an unprecedentedly high-resolution view of the molecular processes within cells, using so-called single-cell technologies. Every cell in the sample can be individually profiled — the amount of each type of protein or metabolite or other molecule of interest can be counted. Understanding the molecular basis that determines the differentiation of cell fates is thus the holy grail promised by these data.

However, the high-dimensional nature of the data, replete with correlations between features, noise, and heterogeneity means the computational work required to draw insights is significant. In particular, understanding the differences between cells requires a quantitative measure of similarity between the single-cell feature vectors of those cells. A vast array of existing methods, from those that cluster a given dataset to those that attempt to integrate multiple datasets or learn causal effects of perturbation, are built on this foundational notion of similarity.

In this dissertation, we delve into the question of similarity metrics for high-dimensional biological data generally, and single-cell RNA-seq data specifically. We work from a global perspective — where we find a distance function that applies across the entire dataset — to a local perspective — where each cell can learn its own similarity function. In particular, we first present Schema, a method for combining similarity information encoded by several types of data, which has proven useful in analyzing the burgeoning number of datasets which contain multiple modalities of information. We also present DensVis, a package of algorithms for visualizing single-cell data, which improve upon existing dimensionality-reduction methods that focus on local structure by accounting for density in high-dimensional space. Lastly, we zoom in on each datapoint, and show a new method for learning 𝑘-nearest neighbors graphs based on local decompositions.

Altogether, the works demonstrate the importance — through extensive validation on existing datasets — of understanding high-dimensional similarity."
2022,Optical studies of super-collimation in photonic crystals,MIT CSAIL,MIT Student,MIT Advisor,CV,0.2302,"Recent developments in material science and engineering have made possible the fabrication of photonic crystals for optical wavelengths. These periodic structures of alternating high-to-low index of refraction materials allow the observation of peculiar effects, in particular, the propagation of optical beams without spatial spreading. This effect, called super-collimation (also known as self-collimation), allows diffraction-free propagation of micron-sized beams over centimeter-scale distances. This linear effect is a natural result of the unique dispersive properties of photonic crystals. In this thesis, these dispersive properties are studied in a two-dimensional photonic crystal slab. Both qualitative and quantitative descriptions are presented. The beam propagation method was used to simulate the evolution of a Gaussian beam inside such structures. The wavelength dependence of the super-collimation effect was studied, and it was observed that the optimum wavelength for this device was around 1500 nm. A precise contact-mode near-field optical microscopy technique was used to obtain high-resolution images of the beam profile at different positions along the photonic crystal, and showed that a 2 [micro]m beam width was conserved over 3 mm. In addition, high-resolution confocal measurements confirmed the size of the beam after 5 mm of propagation."
2022,Optimization of optical characteristics of travelling wave modulators,MIT CSAIL,MIT Student,MIT Advisor,Network,0.3489,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1985."
2022,Cost-optimal design of a household batch electrodialysis desalination device,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2424,"This thesis investigates the pareto-optimal design of a household point-of-use batch electrodialysis (ED) system to provide a cost-effective replacement for existing reverse osmosis (RO) devices, for brackish water desalination of Indian groundwater, at lower energy consumption and higher recovery: 80-90% vs 25-40%. Target specifications derived from user-interviews, and RO products, guided the selection of a batch architecture, for which a coupled flow-mass transport model to predict desalination rate was developed, and validated using a lab-scale ED stack. The effects of varying the production rate (9-15 L/hr) and product concentration (100-300 mg/L) requirements on optimal selection of geometry, flow-rates, and applied voltage for total cost minimization were then explored using a multi-objective genetic algorithm. Given the low utilization of the system and the current cost of materials, the energetic cost was dominated by the capital-cost of the system. At a fixed feedwater concentration of 2000 mg/L, which is representative of the upper bound on groundwater salinity underlying much of India, and a recovery ratio of 90%, the capital cost sharply increased for systems targeted at 100 mg/L vs 200 mg/L and 300 mg/L: $141, $93, and $79, respectively averaged for systems that produced between 11.5 and 12.5 L/hr of desalinated water. Promising directions for additional cost reduction include voltage-regulation during the batch process and the development of inexpensive pumps. In addition, a candidate cost-optimal design was prototyped and tested to verify that the measured desalination performance agreed with the modeled expectations."
2021,Exploration of alternative algorithms for multi-channel acoustic echo cancellation,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2495,"Multi-Channel Acoustic Echo cancellation (MCAEC) is a vital component of delivering clean speech to a virtual personal assistant through a smart speaker with multi-channel audio (stereophonic, etc). The use of the Kalman filter as an alternative adaptive filter methodology for this MCAEC application is explored in this work. The Normalized Least Mean Squares filter (NLMS) serves as a benchmark for the Kalman filter. Simulations using room recordings and measured room responses are employed in this exploration. Useful metrics such as the Word Error Rate (WER) and Echo Return Loss Enhancement (ERLE) help to distinguish performance among the two adaptive filter algorithms. For the single channel case, simulations confirm the cancellation and convergence rate advantage of the Kalman filter, in full-band, but the NLMS filter gives similar results in the sub-band domain, as measured by WER and ERLE. In the multi-channel case, both solutions achieve similar steady state cancellation, but the NLMS offers slightly faster convergence rates. In experiments where adaptation was not frozen, the Kalman filter effectively maintains high echo cancellation by tracking input signal statistics. In most cases, the Kalman filter does not present an appropriate alternative for the MCAEC application in this work."
2021,Developing software for compressed imaging transcriptomics,MIT CSAIL,MIT Student,MIT Advisor,SE,0.295,"Modern-day biological experimentation often necessitates a scale of data that is exponential with respect to the number of genes that are being measured, and this in turn leads to high latency and monetary cost during hypothesis testing. In addition to such practical constraints, some biological experiments are just physically infeasible due to fundamental limitations on the throughput of current technologies. However, because nearly all biological data are highly structured and can be described in terms of relatively few components, it is not necessary to measure each data point individually. Instead, using the framework of compressed sensing, it is possible to take advantage of this structure to gather the requisite data for an experiment while collecting only a fraction of the original number of measurements. In previous work, we have applied compressed sensing for the particular purpose of generating spatial gene expression profiles using fluorescence microscopy (i.e. imaging transcriptomics). In order to make this technique more accessible and user-friendly, we built CISIpy, an open-source software system that implements the pipeline's computational aspects. This system is designed to enable efficient compressed sensing workflows that is highly portable across platforms and especially amenable to cloud computation. The end result is a well-tested, open-source software package replete with functionality, documentation and examples."
2021,Sensing and timekeeping using a light-trapping diamond waveguide,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2201,"Solid-state quantum systems have emerged as promising sensing platforms. In particular, the spin properties of nitrogen vacancy (NV) color centers in diamond make them outstanding sensors of magnetic fields, electric fields, and temperature under ambient conditions. This thesis focuses on spin-based sensing using multimode diamond waveguide structures to efficiently use large ensembles of NV centers (> 10¹⁰). Temperature-stabilized precision magnetometry, thermometry, and electrometry are discussed. In addition, the precision characterization of the NV ground state structure under a transverse magnetic field and the use of NV-diamond for spin-based clocks are reported."
2021,ML-driven clinical documentation,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3676,"Electronic health records (EHRs) have irrevocably changed the practice of medicine by systematizing the collection of patient-level data. However, clinicians currently spend more time documenting information in EHRs than interacting directly with patients, and have adapted to time-intensive note-writing by authoring free-text notes overloaded with jargon and acronyms. Clinical notes are therefore difficult to parse and largely unstructured. This negatively impacts the ability of EHR systems to convey information between different clinicians and institutions, to communicate medical findings to patients, and to allow for programmatic ingestion of data to derive further automatically-learned insights. In this thesis, we present a new EHR system that addresses these problems by using novel machine learning methods to streamline the processes by which clinicians enter in new information and surface relevant details from past medical records. Our intelligent interface aids physicians as they type, allowing for automatic suggestion and live-tagging of clinical concepts to alleviate documentation burden, while simultaneously enabling clinical decision support and contextual information synthesis. Furthermore, as clinicians craft notes we automatically structure and curate their free-text inputs, allowing for further data-driven innovation and improvement. This EHR can reduce physician burnout, decrease diagnostic error, and improve patient outcomes, all while collecting a corpus of clean, labelled clinical data. Our system is currently deployed live at the Beth Israel Deaconess Medical Center Emergency Department and is in use by doctors."
2021,Algorithmic intervention to mitigate inventory and ordering amplification in multi-echelon supply chains,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2506,"The 'bullwhip effect' is a classic, yet persisting, problem with reverberating consequences in inventory management and refers to how forecast errors and safety stock builds yield increasing amplitudes in both orders and on-hand inventory positions the further one moves away from a source of order variability. The bullwhip effect is responsible for both excessive strain on real world inventory management systems, stock outs, and unnecessary capital reservation though safety stock building. In this paper, the author develops algorithmic approaches to mitigating bullwhip using simulation modeling, including cost minimization and amplification minimization, and then interprets the results in the context of existing models of human heuristics in ordering decisions. The algorithmic approaches are optimized as one member within a model of a human decision makers operating within a multi-echelon supply chain with imperfect information sharing and information delays."
2021,Efficient homomorphically encrypted privacy-preserving automated biometric classification,MIT CSAIL,MIT Student,MIT Advisor,SE,0.248,"This thesis investigates whether biometric recognition can be performed on encrypted data without decrypting the data. Borrowing the concept from machine learning, we develop approaches that cache as much computation as possible to a pre-computation step, allowing for efficient, homomorphically encrypted biometric recognition. We demonstrate two algorithms: an improved version of the k-ishNN algorithm originally designed by Shaul et. al. in [1] and a homomorphically encrypted implementation of a SVM classifier. We provide experimental demonstrations of the accuracy and practical efficiency of both of these algorithms."
2021,Autonomous navigation of distributed spacecraft using intersatellite laser communications,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2758,"Autonomous navigation refers to satellites performing on-board, real-time navigation without external input. As satellite systems evolve into more distributed architectures, autonomous navigation can help mitigate challenges in ground operations, such as determining and disseminating orbit solutions. Several autonomous navigation methods have been previously studied, using some combination of on-board sensors that can measure relative range or bearing to known bodies, such as horizon and star sensors (Hicks and Wiesel, 1992) or magnetometers and sun sensors (Psiaki, 1999), however these methods are typically limited to low Earth orbit (LEO) altitudes or other specific orbit cases. Another autonomous navigation method uses intersatellite data, or direct observations of the relative position vector from one satellite to another, to estimate the orbital positions of both spacecraft simultaneously."
2021,Loanwords and the perceptual map : a perspective from MaxEnt Learning,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3997,"It will be shown that the patterns of consonant deletion and vowel epenthesis used by speakers of Cantonese to adapt English words are compatible with the PMap, and can be modelled through the MaxEnt learners mentioned above. It will also be shown through a series of computational simulations that Wilson's (2006) learner fails to acquire the grammar necessary to account for the patterns of loanword adaptation, while White's (2013) learner succeeds. This is a result of the way in which the PMap is encoded within these learners. While both encode the PMap as a series of asymmetrical Gaussian distributions on the weights of constraints, Wilson (2006) encodes this asymmetry through the variances, or plasticities, of the distributions, while White (2013) encodes it through the means, or target weights. A grammar which encodes the PMap through asymmetrical plasticities must encounter evidence from the phonology of the language in order to alter the weights of constraints."
2021,Predicting post-surgical opioid consumption using perioperative surgical data,MIT CSAIL,MIT Student,MIT Advisor,AI,0.3222,"Improper consumption of prescription opioids is a massive public health issue in the United States currently. Here, we propose one approach of tackling this issue through using machine learning techniques to predict opioid consumption post discharge for surgical patients. Through the data collected from surgical patients at BIDMC, relevant features will be identified and used to predict if patients high, outlier consumption. Using logistic regression and gradient boosted decision trees, model performance were evaluated at AUCs of 0.7270 and 0.7289 respectively."
2021,Identifying patterns of learning : a case study of MIT's Introductory Programming Course (6.000x),MIT CSAIL,MIT Student,MIT Advisor,SE,0.3498,"The ever-increasingly relevant introductory programming course offered at MIT presents a unique opportunity to uncover student learning patterns and common behavioral motifs. The course 6.0001/6.0002 harbors a wealth of student interaction data on its companion MITx platform as well as associated grades. Although this course has been offered for the last twelve years, since 2008, little has been done to identify aspects of the course that best aid or hinder student success. This thesis will focus on finding various learner subpopulations to elucidate those materials that best aid certain students to allow for a more tailored teaching mode for future iterations of the course. In addition, this thesis will define an 'effort' statistic that encompasses the holistic engagement of a given student in order to provide an additional statistic to use when determining final grades. I begin with a course specific analysis of enrollment demonstrating the significance of this type of analysis."
2021,Tiresias : a peer-to-peer platform for privacy preserving machine learning,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.2456,"Big technology firms have a monopoly over user data. To remediate this, we propose a data science platform which allows users to collect their personal data and offer computations on them in a differentially private manner. This platform provides a mechanism for contributors to offer computations on their data in a privacy-preserving way and for requesters -- i.e. anyone who can benefit from applying machine learning to the users' data -- to request computations on user data they would otherwise not be able to collect. Through carefully designed differential privacy mechanisms, we can create a platform which gives people control over their data and enables new types of applications."
2021,Using natural language to predict bias and factuality in media with a study on rationalization,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.4094,"Fake news is a widespread problem due to the ease of information spread online, and its ability to deceive large populations with intentionally false information. The damage it causes is exacerbated by its political links and loaded language, which make it polarizing in nature, and preys on peoples' psychological biases to make it more believable and viral. In order to dampen the influence of fake news, organizations have begun to manually tag, or develop systems to automatically tag, false and biased information. However, manual efforts struggle to keep up with the rate at which content is published, and automated methods provide very little explanation to convince people of their validity. In an effort to address these issues, we present a system to classify media sources' political bias and factuality levels by analyzing the language that gives fake news its contagious and damaging power. Additionally, we survey potential approaches for increasing the transparency of black-box fake news detection methods."
2021,Past price and trend effects in promotion planning; from prediction to prescription,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2219,"Sales promotions are a popular type of marketing strategy. When undertaking a sales promotion, products are promoted using short-term price reductions to stimulate their demand and increase their sales. These sales promotions are widely used in practice by retailers. When undertaking a sales promotion, retailers must take into consideration both the direct and indirect effects of price promotions on consumers, and as a result, on the demand. In this thesis, we consider the impact of two of these indirect effects on the planning process of promotions. First, we consider the problem of the promotion planning process for fast-moving consumer goods. The main challenge when considering the promotion planning problem for fast-moving consumer goods is the negative indirect effect of promotions on future sales. While temporary price reductions substantially increase demand, in the following periods after a temporary price reduction, retailers observe a slowdown in sales."
2021,Design of a Phi-2 and a Class E inverter for underwater systems,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2601,"In Autonomous Underwater Vehicles (AUVs), many potential failure modes exist due to pressure housing and the need for connections between different pressure housings. Waterproof connectors do exist but drive up the price and weight of underwater systems, a costly disadvantage as mass and volume are at a premium for an underwater system. If we can remove the necessity for physical connectors, we can design cheaper, more robust submarines. This can be done with wireless power transfer (WPT), which can transmit power efficiently across mediums within the submarine, therefore eliminating the need for physical connections and making underwater systems more compact and light-weight. The thesis presents two WPT systems for an AUV with two different inverters that convert DC power to AC power that drive the WPT system. The first system presented uses a Class E Inverter, a common topology for DC-AC conversion, and the second system utilizes a Phi-2 Inverter, a topology that uses the inherent parasitic capacitances to substitute for physical components. The WPT system utilizes magnetic resonance coupling to transmit power from transmitter coils attached to the inverters to receiver coils attached to a load through a rectifier. Simulations show that, when correctly tuned, the two designs can give comparable performance in power transfer efficiency and range. The choice of design is likely to be decided by a combination of the size and weight of the finished system, along with the ease of design."
2021,Machine learning in housing design : exploration of generative adversarial network in site plan / floorplan generation,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3033,"Technology has always been an important factor that shapes the way we think about Architecture. In recent years, Machine Learning technology has been gaining more and more attention. Different from traditional types of programming that rely on explicit instructions, Machine Learning allows computers to learn to execute certain tasks ""by themselves"". This new technology has revolutionized many industries and showed much potential. Examples like AlphaGo and OpenAI Five had shown Machine Learning's capability in solving complex problems. The Architectural design industry is not an exception. Early-stage explorations of this technology are emerging and have shown potential in solving certain design problems. However, basic problems regarding the nature of Machine Learning and its role in Architecture design remain to be answered. What does Machine Learning mean to Architecture? What will be its role in Architectural design? Will it replace human architects? Will it merely be a design tool? Or is it relevant to Architecture at all? To answer these questions, this thesis explored with a specific type of Machine Learning algorithm called Pix2Pix to investigate what can and cannot be learned by a computer through Machine Learning, and to evaluate what Machine Learning means for architects. It concluded that Machine Learning cannot be a creative design agent, but can be a powerful tool in solving conventional design problems. On this basis, this thesis proposed a prototype pipeline of integrating the technology into the design process, which is a combination of Generative Adversarial Network (Pix2Pix), Bayesian Network and Evolutionary Algorithm."
2021,Expresso-AI : a framework for explainable video based deep learning models through gestures and expressions,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3537,"We have developed a framework for Analyzing Facial Videos and applying it to Automatic Depression Detection. We also developed a video based models We have developed a framework to analyze the decisions of Deep Neural Networks trained on facial videos. We test this framework on Automatic Depression Detection. We first train Deep Convolutional Neural Networks (DCNN) pre-trained on Action Recognition datasets and fine-tune on the facial videos. We interpret the model's saliency maps by analyzing face regions and temporal expression semantics. Our framework generates both visual and quantitative explanations on the model's decision. Simultaneously, our video based modeling has improved previous single-face benchmarks of visual Automatic Depression Detection (ADD). We conclude successfully that we have developed the ability to generate hypotheses from a facial model's decisions, and improved Automatic Depression Detection's predictive performance."
2021,Structural and algorithmic aspects of linear inequality systems,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2383,"Linear inequality systems play a foundational role in Operations Research, but many fundamental structural and algorithmic questions about linear inequality systems remain unanswered. This thesis considers and addresses some of these questions. In the first chapter, we reconsider the ellipsoid algorithm applied to solving a system of linear inequalities. Unlike the simplex method and interior point methods, the ellipsoid algorithm has no mechanism for proving that a system is infeasible (in the real model of computation). Motivated by this, we develop an ellipsoid algorithm that produces a solution to a system or provides a simple proof that no solution exists. Depending on the dimensions and on other natural condition measures, the computational complexity of our algorithm may be worse than, the same as, or better than that of the standard ellipsoid algorithm.. In the second chapter, we reduce the problem of solving a homogeneous linear inequality system to the problem of finding the unique sink of a unique sink orientation (USO) in the vertex evaluation model of computation. We show the USOs of interest satisfy a local property that is not satisfied by all USOs that satisfy the Holt-Klee property. This addresses an open question that is motivated by the idea that such local structure could be leveraged algorithmically to develop faster algorithms or a strongly polynomial algorithm. In the third chapter, we make progress on a conjecture about a particular class of linear inequality systems that have balanced constraint matrices. A balanced matrix is a 0-1 matrix that does not contain a square submatrix of odd order with two ones per row and column. The conjecture asserts that every nonzero balanced matrix contains an entry equal to 1, which upon setting to 0, leaves the matrix balanced."
2021,Scaling RFID positioning systems using distributed and split computing,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2778,"Fine-grained tracking of objects in the physical world at scale has a broad potential impact in health care, retail, manufacturing, supply chain, and consumer product industry. In this thesis, I focus on using RFID-based technology for such applications due to its low-cost and growing prevalence of RFID tags. In contrast to current RFID systems that focus on a monolithic reader, I propose a distributed sensor node architecture that can scale by combining distributed and split computing techniques. On the distributed computing front, I introduce an architecture that enables extending the operation range and coverage from an end user's perspective while improving the manageability aspect via high-level semantic API. On the split computing front, I develop a framework to offload expensive tasks to the cloud or an edge server; the framework enables the use of small, cheap commodity compute devices as hosts at the edge while maintaining the high accuracy of fine-grained positioning. The thesis describes the design and implementation of these techniques. Moreover, through a hybrid evaluation of simulation and practical systems, the thesis demonstrates how these techniques enable us to design a scalable, manageable, and accurate RFID positioning system."
2021,Discrete mechanical metamaterials,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3203,"Digital fabrication enables complex designs to be realized with improved speed, precision, and cost compared to manual techniques. Additive manufacturing, for example, is one of the leading methods for rapid prototyping and near net shape part production. Extension to full scale structures and systems, however, remains a challenge, as cost, speed and performance present orthogonal objectives that are inherently coupled to limited material options, stochastic process errors, and machine-based constraints. To address these issues, this thesis introduces new materials that physically embody attributes of digital systems, scalable methods for automating their assembly, and a portfolio of use cases with novel, full-scale structural and robotic platforms. First, I build on the topic of discrete materials, which showed a finite set of modular parts can be incrementally and reversibly assembled into larger functional structures."
2021,Temperature prediction using thermal fluctuations from wireless sensor networks in adaptive filter model,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3057,"In many scientific experiments, it is imperative to minimize the unintended effects of variables other than the independent variables. Temperature, pressure, and gas levels are factors controlled to a certain extent using expensive climate-controlling technology, yet the resolution for monitoring their levels is generally low. The downward scaling of communication-enabled electronics in size, cost, and energy provides a potential toolset for tracking such data with high spatial and temporal resolutions. We establish a data collection methodology through a low-cost, small footprint distributed network system of modules that records data in a remote server. The system architecture allows for increased spatial resolutions, demonstrates high precision of measurements, and investigates room dynamics. Modules are fabricated using commercial sensors such as the ESP8266, BME680, and TCS34725. In this paper, we propose a temperature prediction model using adaptive filter methodologies to learn the relationship between thermal fluctuations at distinct locations within a lab environment."
2021,G-Network for outcome prediction under dynamic intervention regimes,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3758,"Counterfactual prediction is useful in settings where one would like to know what would have happened had an alternative regime been followed, but one only knows the outcomes under the observational regime. Typically, the regimes are dynamic and time-varying. In these scenarios, G-computation can be used for counterfactual prediction. This work explores a novel recurrent neural network approach to G-computation, dubbed G-Net. Many implementations of G-Net were explored and compared to the baseline, linear regression. Two independent datasets were used to evaluate the performance of G-Net: one from a physiological simulator, CVSim, and another from the real-world MIMIC database. Results from the CVSim experiments suggest that G-Net outperforms the traditional linear regression approach to G-computation. The best G-Net model found from the CVSim experiments was then evaluated using the MIMIC dataset. The outcomes under a few different counterfactual strategies on the MIMIC cohort were explored and evaluated for clinical plausibility."
2021,Social and affective machine learning,MIT CSAIL,MIT Student,MIT Advisor,AI,0.3248,"Social learning is a crucial component of human intelligence, allowing us to rapidly adapt to new scenarios, learn new tasks, and communicate knowledge that can be built on by others. This dissertation argues that the ability of artificial intelligence to learn, adapt, and generalize to new environments can be enhanced by mechanisms that allow for social learning. I propose several novel deep- and reinforcement-learning methods that improve the social and affective capabilities of artificial intelligence (AI), through social learning both from humans and from other AI agents. First, I show how AI agents can learn from the causal influence of their actions on other agents, leading to enhanced coordination and communication in multi-agent reinforcement learning. Second, I investigate learning socially from humans, using non-verbal and implicit affective signals such as facial expressions and sentiment."
2021,Automatic modeling of machining processes,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3398,"3 axis CNC milling is a ubiquitous manufacturing method in industry due to its versatility and precision. The fundamental parameters that dictate cutting performance (""speeds, feeds, and engagement"") must be manually set by the machine programmer; proper operation therefore relies heavily on operator skill. In this thesis, an intelligent CNC controller is presented that uses low-cost sensors to fit an analytical model of cutting forces. The analytical nature of this model allows for favorable convergence characteristics and low computational costs. This is used to optimize cutting feeds with respect to process constraints for future movements; as more data is collected, the model continuously reinforced. This intelligent controller therefore abstracts out some of the complexities of machining and makes the process more approachable."
2021,Measuring justice in machine learning,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3125,"How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?"
2021,Investigating mechanisms of biophysical diversity between phasic and tonic motor neurons,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2234,"Neurons exhibit striking diversity in core neuronal properties (intrinsic biophysical and synaptic properties), which are the building blocks of brain function and computation. Despite the central role of these properties in brain function, the underlying molecular and biophysical mechanisms which generate this diversity remain incompletely understood. In the Drosophila larval motor system, phasic (1s) and tonic (1b) motor neurons (MNs) differ in their intrinsic biophysical properties, providing an ideal system to examine electrophysiological diversity across neuronal populations. To address this question, we combined in vivo whole-cell patch-clamp physiology with biophysical modeling. First, we characterized biophysical diversity between 1s and 1b MNs. To explore molecular mechanisms underlying such diversity, single-neuron PatchSeq RNA profiling experiments were carried out to correlate biophysical properties with differences in ion channel gene expression profiles. These experiments suggest that cyclic nucleotide- gated like (CNGL) ion channels are upregulated in 1b MNs several folds, which indicates that CNGL could be a candidate ion channel that might specify diversity in electrical properties . To test this hypothesis, we misoverexpress CNGL in 1s MNs so that we could investigate how this ion channel contributes to the diversity between them. We developed an analysis toolset in MATLAB that can be used to analyze whole-cell patch-clamp physiology data and obtain excitability properties. Using the Izhikevich model, we were able to quantify and predict the spiking properties of 1s and 1b MNs. Using a ball and stick model, we were able to reproduce the tonic firing pattern of 1b neurons and tested tonic firing patterns in different compartments of 1b neurons. Taken together, this thesis work laid the foundation to begin characterizing biophysical mechanisms of intrinsic diversity of Drosophila neurons by combining experimental data with modeling."
2021,Inductive logic programming with gradient descent for supervised binary classification,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3717,"As machine learning techniques have become more advanced, interpretability has become a major concern for models making important decisions. In contrast to Local Interpretable Model-Agnostic Explanations (LIME), this thesis seeks to develop an interpretable model using logical rules, rather than explaining existing blackbox models. We extend recent inductive logic programming methods developed by Evans and Grefenstette [3] to develop an gradient descent-based inductive logic programming technique for supervised binary classification. We start by developing our methodology for binary input data, and then extend the approach to numerical data using a threshold-gate based binarization technique. We test our implementations on datasets with varying pattern structures and noise levels, and select our best performing implementation. We then present an example where our method generates an accurate and interpretable rule set, whereas the LIME technique fails to generate a reasonable model. Further, we test our original methodology on the FICO Home Equity Line of Credit dataset. We run a hyperparameter search over differing number of rules and rule sizes. Our best performing model achieves a 71.7% accuracy, which is comparable to multilayer perceptron and randomized forest models. We conclude by suggesting directions for future applications and potential improvements."
2021,Learning causal graphs under interventions and applications to single-cell biological data analysis,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.279,"This thesis studies the problem of learning causal directed acyclic graphs (DAGs) in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. The identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes, has previously been studied. This thesis first extends these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them, by defining and characterizing the interventional Markov equivalence class that can be identified from general interventions. Subsequently, this thesis proposes the first provably consistent algorithm for learning DAGs in this setting. Finally, this algorithm as well as related work is applied to analyze biological datasets."
2021,Contrasting contrastive and supervised models interpretability,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3735,"In this thesis, we compare the representations of an unsupervised contrastive model to those of an equivalent supervised model using several deep neural network interpretability methods: network dissection, sparsity experiments, and saliency maps. Network dissections of self-supervised contrastive and supervised models show that the neurons of the contrastive model tend to learn about different parts of an object (ie. top-half of a dog or left-half of a person) while the neurons of the supervised model tend to learn about the entire object (ie. a dog or a person). Sparsity experiments show that the representations learned by the contrastive model are less sparse than the representations learned by the supervised counterpart model. Saliency maps show that the contrastive model focuses more on specific parts of the input image. Finally, we find that the contrastive model representations transfer better to finegrained classification tasks than the supervised model representations."
2021,Algorithms for learning to induce programs,MIT CSAIL,MIT Student,MIT Advisor,AI,0.3052,"The future of machine learning should have a knowledge representation that supports, at a minimum, several features: Expressivity, interpretability, the potential for reuse by both humans and machines, while also enabling sample-efficient generalization. Here we argue that programs-i.e., source code-are a knowledge representation which can contribute to the project of capturing these elements of intelligence. This research direction however requires new program synthesis algorithms which can induce programs solving a range of AI tasks. This program induction challenge confronts two primary obstacles: the space of all programs is infinite, so we need a strong inductive bias or prior to steer us toward the correct programs; and even if we have that prior, effectively searching through the vast combinatorial space of all programs is generally intractable. We introduce algorithms that learn to induce programs, with the goal of addressing these two primary obstacles. Focusing on case studies in vision, computational linguistics, and learning-to-learn, we develop an algorithmic toolkit for learning inductive biases over programs as well as learning to search for programs, drawing on probabilistic, neural, and symbolic methods. Together this toolkit suggests ways in which program induction can contribute to AI, and how we can use learning to improve program synthesis technologies."
2021,Phosphoproteomics analysis of Alzheimer's disease,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.2667,"Alzheimer's disease (AD) is a form of dementia characterized by the appearance of amyloid-[beta] plaques, neurofibrillary tangles, and inflammation in brain regions involved in memory. Despite numerous clinical trials, a limited understanding of disease pathogenesis has prevented the development of effective therapies. Several lines of genetic and biomolecular evidence indicate that AD progression involves cellular signaling through neuronal and glial protein phosphorylation networks. In order to understand which phosphorylation networks are dysregulated, I use mass spectrometry to characterize the phosphoproteome of post-mortem brain tissue from AD patients and multiple mouse models of AD. Using computational analysis, I identified several signaling pathways that are dysregulated before neurodegeneration occurs. Many of these signaling factors were expressed primarily in non-neuronal cell types, including microglia, astrocytes, and oligodendrocytes."
2021,Modeling and design of magnetic flux compression generators,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2276,"The explosively-pumped magnetic flux compression generator (FCG) is a pulsed-power current amplifier powered by an explosion. This thesis surveys FCGs, demonstrating their general operation; develops a new magnetic-field-strength-based model for FCGs in the form of a generalized cylinder that more accurately captures losses to magnetic diffusion than commonly employed circuit models, but maintains simplicity in the form of a low order DAE; develops a simplified means of calculating the inductance of FCGs, providing a bridge between the field-based and circuit models; presents a design of a full loop FCG system (a topology underserved by existing literature) and an experimental setup to verify the designed loop generator; and proposes a class of non-explosive magnetic flux compression generators. The designs and models herein provide new tools and jumping-off points for further research into FCGs, particularly in the miniaturized systems gaining popularity and in the potential for reusable flux compression power sources."
2021,Supernumerary robotic limbs for human augmentation in overhead assembly tasks,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.4112,"Manufacturing tasks are highly demanding of work, and there is an especially high prevalence of injury associated with overhead tasks which are taxing to the shoulder and upper body. To assist workers completing these tasks, and to increase overall productivity, safety and effectiveness, we introduce a novel design of Supernumerary Robotic Limb (SRL). This is a robotic arm worn on the shoulder of the technician/- worker which extends the human capability with implicit force control algorithms that allow for intuitive control and interface of the extra robot arm. Affectionately dubbed Aucto, the robotic arm can lift an object and hold it while the wearer is securing the object using a tool with both hands. The worker does not have to take a laborious posture for a long time, reducing fatigue and injuries. Furthermore, a single worker can execute the task, which would otherwise require two workers. Two technical challenges and novel solutions are presented. One is to make the wearable robot simple and lightweight with use of a new type of granular jamming gripper that can grasp diverse objects from an arbitrary direction. This eliminates the need for orienting the gripper against the object with three-axis wrist joints, reducing the number of degrees of freedom (DOF) from 6 to 3. The other is an effective control algorithm that allows the wearer to move freely while the robot on the shoulder is holding an object. Unlike a robot sitting on a floor, the SRL worn by a human is disturbed by the movement of the wearer. An admittance-based control algorithm allows the robot to hold the object stably and securely despite the human movement and changes in posture. A 3 DOF prototype robot with a new granular jamming gripper and an ergonomic body mounting gear is developed and tested. It is demonstrated that the robot can hold a large object securely in the overhead area despite the movement of the wearer while performing an assembly work."
2021,A zero kernel operating system : rethinking microkernel design by leveraging tagged architectures and memory-safe languages,MIT CSAIL,MIT Student,MIT Advisor,OS,0.3805,"A secure kernel is the keystone upon which all software systems are built. Historically, memory corruption errors have accounted for a large portion of kernel bugs. These bugs are difficult to detect and avoid in memory-unsafe languages such as C. To mitigate such bugs, we build on top of an operating system written in a memory-safe language, Rust. Rust provides memory-safety guarantees while remaining as fast and flexible as other systems languages. Yet, some operations within operating systems, such as hand-written assembly for interrupt handling, do not fit within the scope of a language memory-safety model. To reduce the scope of these errors, microkernels isolate and reduce privilege by moving much of the traditional kernel into userspace services. However, their effectiveness is limited by the inflexibility of modern hardware. The Zero Kernel Operating System (ZKOS) emphasizes the high-level ideas of compartmentalization and least privileges on a tagged architecture. In particular, instead of relying on the Ring model and paging, which coarsely limit privilege and isolation granularity, a tagged architecture allows ZKOS to isolate at the memory word level and provide truly disjoint privileges. To this end, ZKOS slices kernelspace and userspace into fine-grained components based on function. Then, ZKOS defines specific entry and exit points between components and composes policies to limit component transitions and privileges. This increases the precision of isolation and privilege, and complements the local compile-time and runtime checks Rust performs to reduce the scope of bugs."
2021,Understanding microRNA targeting with high-throughput biochemistry,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3018,"We therefore adapted a high-throughput biochemical platform utilizing random-sequence RNA libraries to obtain the vast quantity of affinity values required to predict miRNA targeting efficacy. Through a novel analytical approach, we assigned relative dissociation (K[subscript D]) constants to all binding sites </-12 nt in length, for six miRNAs. These analyses revealed unanticipated miRNA-specific differences in the affinity of similar sites, unique sites for different miRNAs, and a 100-fold influence of flanking dinucleotide context surrounding a site. These measurements informed a biochemical model of miRNA targeting that outperformed all existing models of miRNA targeting, which was extended to all miRNAs using a convolutional neural network (CNN) trained on both affinity and repression data. We also applied this high-throughput biochemical approach to understand the role of the miRNA 3' region using partially random RNA libraries."
2021,Leveraging machine learning to predict playcalling tendencies in the NFL,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3114,"In this thesis, we apply four machine learning models to NFL play-by-play data from 2009-2018 to predict whether a team will run or pass the ball on a given play. We tested our models using league-wide and team-specific data in five different situations on the field. Our best league-wide models achieved a test accuracy of 80% and our best team-specific models achieved a test accuracy of 86%. Relative to the baseline of the run-to-pass ratio, the best league-wide models achieved an increase in accuracy of 25% and the best team-specific models achieved an increase of 27%. Our models showed that the Tennessee Titans, the New York Jets, and the Cincinnati Bengals have been the most predictable offenses in the NFL over 10 years. We found that a team's in-game run-to-pass ratio and their win and score probabilities are the driving factors for offensive play-calling. Additionally, our results show that teams are more predictable later in games, and that less predictable teams tend to experience greater success offensively."
2021,Pareto Gamuts : exploring optimal designs across varying contexts,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2777,"Manufactured parts are meticulously engineered to perform well with respect to several conflicting metrics, like weight, stress, and cost. The best achievable trade-offs reside on the Pareto front, which can be discovered via performance-driven optimization. Objective functions used to define the Pareto front often incorporate assumptions about the context in which a part will be used, including loading conditions, environmental influences, material properties, or regions that must be preserved to interface with a surrounding assembly. Existing multi-objective optimization tools are only equipped to study one context at a time, so engineers must run independent optimizations for each context of interest. However, engineered parts frequently appear in many contexts: wind turbines must perform well in many wind speeds, and a bracket might be optimized several times with its bolt-holes fixed in different locations on each run. In this paper, we formulate a framework for variable-context multi-objective optimization. We introduce the Pareto gamut, which captures Pareto fronts over a range of contexts. We develop a global-local optimization algorithm to discover the Pareto gamut directly, rather than discovering a single fixed-context ""slice"" at a time. To validate our method, we adapt existing multi-objective optimization benchmarks to contextual scenarios. We also demonstrate the practical utility of Pareto gamut exploration for several engineering design problems."
2021,X-ray Micro-Computed Tomography and Deep Learning Segmentation of Progressive Damage in Hierarchical Nanoengineered Carbon Fiber Composites,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.273,"Advanced composite laminates comprised of carbon (micro) fiber reinforced polymer (CFRP) have become widespread in modern high-performance aerospace structures, providing high, tailorable mass-specific stiffness and strength. However, while underpinning such performance benefits, CFRP microstructural heterogeneity and mechanical property anisotropy concomitantly give rise to complex damage mechanisms that lead to difficult-to-predict failure, limiting CFRP understanding. Progressive damage mechanisms in CFRPs generally encompasses a spectrum of modalities, interactions, and sequences across multiple scales, exhibiting broad sensitivity to loading conditions. Dominant damage mechanisms have been identified generally as polymer matrix cracking within (intralaminar) and between (interlaminar, termed ‘delamination’) plies, fiber fracture, fiber bundle microbuckling, and fiber/matrix interfacial debonding. Two emerging solutions aiming to suppress or delay such mechanisms toward enhanced strength and stiffness are considered in this dissertation: (i) aligned carbon nanotube (A-CNT) interlaminar reinforcement (termed ‘nanostitch’) that primarily targets delaminations, and (ii) thin-ply morphology that targets intralaminar cracking and delaminations. Both solutions have demonstrated significant mechanical improvements via standard ex situ tests that lack underlying progressive damage understanding. In view of these limitations, this dissertation advances understanding of composite progressive damage by modern ex situ and state-of-the-art in situ X-ray micro-computed tomography (µCT) studies, including advancing experimental techniques via artificial intelligence (AI), in the context of aerospace-grade CFRP strengthening and toughening effects of nanostitch, thin-ply, and their combination."
2021,Motional state engineering for continuous-variable quantum computation,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2341,"The standard approach to quantum computation uses qubits, which are well-described as a two-level system. An alternative approach to quantum computation is continuous-variable quantum computation (CVQC), which uses physical observables, such as the strength of an electromagnetic field or the position of a particle in space, whose numerical values belong to continuous intervals. Trapped ions are well-developed for quantum computation, and they possess both qubit and continuous degrees of freedom that can be precisely controlled, making them a good candidate for a realization of CVQC. Although there exist software frameworks capable of simulating CVQC experiments, these frameworks do not incorporate realistic noise sources and cannot be tailored to a specific trapped-ion setup. In this work, we develop a computational framework for simulating CVQC operations using trapped ions in a realistic system with realistic noise sources. We do so first with ideal Hamiltonians and then with Hamiltonians generated directly from the electric potential and fields that can be applied to the trapped ion in a representative Paul trap. This allows for the direct simulation of a squeezing operation that can be implemented through application of voltages in trapped-ion experiments. These methods can be applied to other CVQC operations in order to allow for their direct simulation as well. We package these tools into a usable application with which we can load information about an experimental configuration and then use this simulation procedure to design and test experiments in CVQC achievable with an ion-trap setup, thus facilitating the experimental design process and eventually allowing for prediction of system behavior and comparison with experimental results."
2021,An optimizing C compiler for DSP architectures,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3298,"Thesis (S.B. and M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999."
2021,Translational design computation,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3111,"In Mediations, we present dynamic, synergetic, and emergent strategies for how computational mediations can occur within cocreation systems. The living and nonliving parts of any cocreation system may interact to form synergies. Combined, these synergies produce complexes that give rise to new macro-level organizations -- products of the synergies of the parts and not simply of the parts themselves. Thus, the mediation between physical, digital, and biological entities needs to address the design of dynamic relations guiding synergetic behaviors, the design of the synergetic behaviors themselves or ultimately, the design of emergent self-expression of the system. Throughout this thesis, the framework is developed theoretically and applied in practice. It is documented in publications such as Making Data Matter and Hybrid Living Materials and projects such as Wanderers, Living Mushtari, the Vespers Series, Rottlace, Lazarus, Totems, Fiberbots, and Silk Pavilion II."
2021,Observations of decision-making in the mechanical design process in a start-up company,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3213,"This thesis examines the effect that working at a start-up company has on decisions and considerations during the mechanical design process, and is based on the experience of the author while interning at an Al robotics start-up as part of an MIT graduate students' team. An overview of the company is provided, the different stages of the product development are introduced and Miso's approach to the design of the modules for its product is discussed. Advantages and disadvantages of the approach are examined with examples, and suggestions for improvement are provided. In particular, the role of first-order-analysis (FOA) as a powerful tool to predict problems early is presented, the need for order as a necessary condition for growth is discussed, and next steps for the future production ramp-up stage are shared."
2021,On the use of switched-capacitor multi-level inverters for electro-aerodynamic thrust applications,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2436,"A prototype HV DC-AC power converter was designed and tested for use on the dielectric barrier discharge(DBD) section of a decoupled thruster on an electro-aerodynamic(EAD) thrust plane application. The converter was a switched-capacitor multi-level inverter(SCMLI) capable of generating a variety of AC waveform shapes. The power draw of the DBD and the DC corona discharge current were measured for each waveform at a DBD voltage of 6kVpp and 10kHz, and used to estimate the thrust to DBD power characteristics of each waveform. The estimated thrust to DBD power for a sine wave and sawtooth wave were >76% higher than for a pulse wave, and >19% higher than for a square wave, indicating that a sine or sawtooth wave generating circuit such as a SCMLI may be a good choice for EAD flight applications."
2021,Deep learning methods for the design and understanding of solid materials,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2885,"The trend of open material data and automation in the past decade offers a unique opportunity for data-driven design of novel materials for various applications as well as fundamental scientific understanding, but it also poses a challenge for conventional machine learning approaches based on structure features. In this thesis, I develop a class of deep learning methods that solve various types of learning problems for solid materials, and demonstrate its application to both accelerate material design and understand scientific knowledge. First, I present a neural network architecture to learn the representations of an arbitrary solid material, which encodes several fundamental symmetries for solid materials as inductive biases. Then, I extend the approach to explore four different learning problems: 1) supervised learning to predict material properties from structures; 2) visualization to understand structure-property relations; 3) unsupervised learning to understand atomic scale dynamics from time series trajectories; 4) active learning to explore an unknown material space. In each learning problem, I demonstrate the performance of the approach compared with previous approaches, and apply it to solve several realistic materials design problems and extract scientific insights from data."
2021,An explorable electrotactile display.,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3147,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D.
2021,Made-up minds : a constructivist approach to artificial intelligence,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3181,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1989."
2021,A Multi-Track Elevator system for E-commerce fulfillment centers,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3031,"Fulfillment centers located in densely populated urban areas are an ever-growing need for leading online consumer websites. These urban fulfillment centers have limited land mass and must have innovative solutions to transport goods within the available vertical space. This work presents a Multi-Track Elevator (MTE) System, a competitive solution for rapid access and retrieval of goods in high-rise e-commerce fulfillment centers and warehouses. The MTE System consists of multiple vertical rails connected with angular traverse rails that allow multiple carriages to go up and down without collision. A novel turning point system switches track routes so that several carriages can move across the multiple rails for rapidly accessing many floors and collecting diverse goods. Unlike existing vertical-horizontal grid elevators and rail systems, the roller-coaster type, self-powered carriages on the MTE system do not have to stop at switching points, but can continually move across the network of rails. Further, this work describes the architecture of the rail network system and techniques for switching multiple rails, followed by the design of vertical turntables for smooth, continuous rail switching. Finally, outlining the use of a simple route optimization algorithm, diverse elevator systems are compared with respect to total traveling time and distance. A proof-of-concept prototype has been built and is presented."
2021,"Stimulation, speculation, simulation : the architecture of the captured city that the corporation gave us",MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.29,"For thousands of years, architecture built itself from a history of its own understanding of itself. A retracing over time, architecture was inherited from its past. Over time, sets of ideals were built collectively, with the city as its manifesto. It could be argued that the architecture built was in response to an architecture that already existed. the production of making architecture made cities, and further, showcased architecture as a way of thinking. Diagrammatic thinking produces diagrammatic architecture. Orthographic thinking produced orthographic architecture [1]. Machines have learned to compound a retracing of time through creating systems of algorithms that recursively produce patterns of understanding through their recordings of human behavior. The model is based on recording the past to predict the future. Increasingly, our cities have been non-stop recorded through various and ubiquitous sensing devices. The city is then re-represented afterward, through the eyes and interpretation of the machine. Or rather, by the eyes of the machine by the men who made the machine see the way they want the machine to see, building a proxy city, a representation of the real world, to help make choices in the distant and near future. How might we begin to imagine architecture as collective intelligence within this new system? Imagining architecture as a type of metadata. Mined through vision, camera, surveillance technology, connecting various strands of metadata produced surveillance capitalisms abilities. Imagine other connections mining this figural metadata could produce for a second just processing the knowns and unknowns, speculating on the possible city in Rumsfeldian ways, knowing from these systems, it is that they have the capabilities to find patterns and order not before seen. When past behavior is the basis of predicting future behavior, how might we revisit the city that was, to forge the city to come?"
2021,Improving the efficacy of teacher-facing analytics dashboards for game-based assessment and beyond,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3014,"Since the early 2000s, more academic instruction has been moved to take place online, which has caused a growing demand for dashboards--websites that educators can access to monitor and understand their student's performance. However, there is a glaring lack of research dedicated to creating design principles that effectively meet instructor needs. In this work, I contribute to this field of study by utilizing feedback from teachers to build and evaluate a dashboard prototype for Shadowspect, a game-based geometry assessment, and construct an accompanying set of design recommendations. I conducted a small user study with a local high-school mathematics teacher to assess the potential of the prototype. From this evaluation, I found preliminary evidence that suggests that the design principles that guided the development of the prototype, which emphasized actionability and transparency, were successful in addressing long-existing problems in current dashboards."
2021,Passive enhancement of air flow at pedestrian level in built environments,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2288,"The velocity profiles in the third to sixth canyons were measured with Acoustic Doppler Velocimetry. Compared with the reference case, void decks enhance near-ground flows in all measured canyons by up to a factor of two, but the enhancement effect weakens in downstream canyons. The wind catcher enhances the flow in the target canyon by 2.5 times with no significant effect in other canyons. The reversed wind catcher and the step-up/ step-down canyons reduce flows in the downstream canyons. The experimental data was used to validate computational fluid dynamics (CFD) models. CFD simulation results agree well with the experimental results for all cases. The validated CFD models were then used to study the void decks and the wind catcher in three-dimensional canyons. Void decks double near-ground flows in all canyons. The wind catcher increases near-ground flow in the target canyon by only 50% due to leakage at the sides."
2021,Scalable integrated screening tools for cardiovascular disease,MIT CSAIL,MIT Student,MIT Advisor,SE,0.288,"We have developed Android mobile apps and hardware capable of performing pulse wave analysis (PWA) and measuring pulse wave velocity (PWV) using PPG techniques. The analysis algorithms are configured to run on a custom server that is able to handle large amounts of medical data. In this thesis, I describe the PWA and PWV algorithms, the mobile applications associated with these measurements, and their integration with a custom server. To validate these new algorithms, data was used from two separate clinical studies conducted by our group. For PWA, I analyzed PPG waveforms from young athletic people, young non-athletic people, old healthy people, and old CAD patients, which resulted in median PWA Scores of 3.51 (0.57), 3.19 (0.78), 1.98 (0.66), and 1.81 (0.5) respectively. From these results, the PWA tool demonstrated sufficient sensitivity to distinguish between the four different cardiovascular health classifications."
2020,Machine learning methods for targeting and new product development,MIT CSAIL,MIT Student,MIT Advisor,SE,0.31,"We conduct two large-scale field experiments to evaluate seven methods widely used to design targeting policies. The findings compare the performance of the targeting methods and demonstrate how well the methods address common data challenges. The challenges we study are covariate shift, concept shift, information loss through aggregation, and imbalanced data. We show that model-driven methods perform better than distance-driven methods and classification methods when the training data is ideal. However, the performance advantage vanishes in the presence of the challenges that affect the quality of the training data. Chapter 3: Firms typically compare the performance of different targeting policies by implementing the champion versus challenger experimental design. These experiments randomly assign customers to receive marketing actions recommended by either the existing (champion) policy or the new (challenger) policy, and then compare the aggregate outcomes."
2020,Prescriptive analytics in operations problems : a tree ensemble approach,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2907,"In chapter 3, we show how to solve optimization problems with random forest objective functions and general polyhedral constraints. We show how to formulate this problem using MIO techniques and show this formulation can be decomposed and solved iteratively using Pareto-optimal Benders cuts. We also provide analytical guarantees on an approach that approximates a large scale random forest optimization problem by optimizing over a smaller forest, and develop heuristics based on ideas from cross validation. In chapter 4, we study a new problem where nurse practitioners need to be dynamically routed to patients' houses as service requests are received. We show how to solve using Approximate Dynamic Programming and develop methods to solve ADP's with combinatorial action spaces and non-linear cost-to-go functions approximated using a tree or tree ensemble approximation."
2020,Congestion-dependent pricing for a service provider,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2398,"A service provider who provides users access· to a communication network is constrained in the system by the limited amount of bandwidth that can be offered to users. We will discuss congestion-dependent pricing, a method of pricing connection access to the network that accounts for this bandwidth constraint. Different models for the system are analyzed and simulated. The case of multiple classes of users as well as the case of a probabilistic demand function are analyzed in detail. The dynamic congestion-dependent pricing policy that maximizes performance will be determined using dynamic programming. The steady state revenue generated for the service provider will be used as the measure of system performance. Additionally, approximation and estimation techniques to simplify analysis and implementation of different systems are analyzed."
2020,Accommodation-through-Bypassing : overcoming professionals' resistance to the implementation of algorithmic technology,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3632,"While algorithmic technologies are rapidly changing how work is performed in professional organizations, professional workers are resisting the implementation of these technologies in their workplaces. Previous studies of the development and implementation of workplace technologies suggest that managers or technology developers respond to workers' resistance in a way that is intended to make workers more amenable to using the technology, and that professionals' recalcitrance can ultimately impede the adoption of new technology despite managers' or developers' efforts. In my a 21-month field study of the development and implementation of three cases of machine learning-based algorithmic technology, I find that that the development and implementation of algorithmic technologies can proceed in the face of professionals' resistance when developers bypass the professional workers and repurpose the technology for use by another group of actors that is present in professional work settings: managers, administrators, and other ""central"" actors. Using the post-humanist concept of tuning -- which views technology development as a dialectic between resistance and accommodation -- I show that in order to carry out accommodation-through-bypassing, developers engage in a series of practices whereby they strategically manage relations with the resistant ""local"" actors, the technology itself, and the ""central"" actors. These findings highlight that the dialectic of resistance and accommodation that characterizes the technology development process can occur even in the face of strong professional recalcitrance, that accommodation can be strategically geared toward workers who are not the originally-intended users of a technology, and that technology developers can play a key role in influencing workplace relations in professional organizations."
2020,Building blocks for regenerative medicine : vascularized models and immunomodulation to engineer hepatic cell therapies,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2725,"By applying unsupervised machine learning techniques to scan the secretome in stimulated devices, we identified endothelial-derived mediators that can independently stimulate proliferation of human hepatocytes. Collectively, the data presented here underscore the importance of multicellular models that integrate tunable biochemical and fluid forces, and demonstrate that SHEAR devices can be used to discover and validate conditions that promote human liver regeneration. Limiting the allogeneic immune response is a major challenge in the implementation of cell-based therapies. To ameliorate this problem, we engineered an immune cloak around transplantable liver tissue by enabling trans-presentation of immune checkpoint pathways. This technology is called SHIELD (stealth hepatic immunotolerant ensembles for liver disease). SHIELD activates checkpoint pathways in supporting stromal cells and/or in endothelial cells lining the vasculature to induce immune cell exhaustion and anergy."
2020,The complexity of the future of work,MIT CSAIL,MIT Student,MIT Advisor,AI,0.2714,"Rapidly advancing cognitive technologies, such as artificial intelligence (AI), have the potential to drastically impact modern society and to shape the future of work. Accordingly, policy makers and researchers seek forecasts into technological change and labor trends, including growing job polarization and income inequality as well as decreasing career mobility and spatial mobility for workers. Although a given technology impacts demand for only a narrow set of workplace skills, modern empirical work relies on coarse labor distinctions between cognitive and physical or routine and non-routine work to explain employment trends. In this dissertation, I explore the complex ways that skills and employment undergird aggregate labor dynamics in the US. As a motivating example, I demonstrate how simple measures for skills within a labor market contribute to the differential impact of automation across US cities of different sizes."
2020,Molecular graph Self attention and graph convolution for drug discovery,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3227,"Drug development is an important, but complicated and expensive process. By utilizing the power of deep learning, we aim to improve the current process of drug development. We model molecules as undirected graphs and use graph convolutions and self-attention to predict molecular properties. With a series of ablation studies, we demonstrate the added value of several key components in our network. We analyze two standard datasets: BBBP, which includes classication data on whether molecules pass the blood-brain barrier, and ClinTox, which includes toxicity information. Using our architecture, we are able to match state of the art performance on the BBBP prediction task."
2020,Development of designer-relevant Lattice-Boltzmann Wind Field Model for urban canyons and their neighborhoods,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.2469,"Wind field analysis is one of the most important components for designers to achieve a thermally-comfortable and energy-efficient building design. Designers need a fast and relatively accurate wind field model to get integrated into the design workflow, but current platforms to work on are either costly and time-consuming conventional Computational Fluid Dynamics (CFD) tools or over-simplified data correlation factors, which makes the workflow undesirable for designers' use. In this thesis, a novel Lattice-Boltzmann Wind Field Model (LBWFM) is developed and integrated in a designer-relevant Rhino-based environment. Lattice-Boltzmann Method (LBM) is introduced as the solver due to its open-source and parallelism natures, and coded in C# language for three-dimensional urban airflows. Results of the model are validated with experimental measurements as well as conventional CFD tools for both wind velocity and pressure fields. To further enhance the computational efficiency, proper settings of inlet wind profile and optimal modeling domain size are investigated for the LBWFM. And the relative wind pressure coefficient calculated out of the model is then applied in the analysis of wind-driven natural ventilation potential with the indicator of air exchange flow rate. Finally the limitation of the model is stated and future work is discussed on the modifications of buoyancy effect and potential extension is addressed in the application of LBWFM."
2020,Data-driven optimization with behavioral considerations : applications to pricing,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2545,"In chapter 3, we study a retailer's optimal promotion strategy when demand is affected by different classes of customers' status in the rewards program and their heterogeneous redemption behavior. We formulate the retailer's problem as a dynamic program and prove a unique optimal threshold discounting policy. We also propose an approximation algorithm of the optimal price as a convex combination of the optimal prices for each class separately. Using data from a fast food chain, we assess the performance of the algorithm and the optimal pricing compared to current practice. In chapter 4, we are concerned with accurately estimating price sensitivity for listed tickets in the secondary market. In the presence of endogeneity, binary outcomes and non-linear interactions between ticket features, we introduce a novel loss function which can be solved using several off-the-shelf machine learning methods."
2020,Light source relighting for indoor scene photos with deep neural networks,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3109,"We seek to use deep neural networks to develop a method to detect the light sources in a given image of an indoor scene, computationally adjust their lighting intensity, and re-render the edited scene as an image. By doing so, we can visually relight the image--effectively turning the light source ""on"" or ""off"" in the image. This thesis introduces such a method by using Generative Adversarial Networks (GANs) and intervention techniques to this end. The method is composed of a pipeline of processing stages, from detecting the light sources to reconstructing the scene in GAN representation space to performing edits on the GAN representation to fine-grained control over the edited lighting, and we present its results here. The thesis work has a wide range of applications in the field of content creation and image editing."
2020,New optimization approaches to matrix factorization problems with connections to natural language processing,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.4209,"The new approach builds uncertainty sets using one-sided constraints and two hypothesis tests, uses alternating optimization and projected gradient methods, including Adam and mirror descent, to find good local optima. In computational experiments, we demonstrate that these models are better able to avoid over-fitting than LDA and PLSA, and result in more accurate reconstruction of the underlying topic matrices. In Chapter 5, we develop modifications to latent Dirichlet allocation to account for differences in the distribution of topics by authors. The chapter adds author-specific topic priors to the generative process and allows for co-authorship, providing the model with increased degrees of freedom and enabling it to model an enhanced set of problems. The code for the algorithms developed in each chapter in the Julia language is available freely on GitHub at https://github.com/lauren897"
2020,An adaptive partitioning scheme for ad-hoc and time-varying database analytics,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2406,"Data partitioning significantly improves query performance in distributed database systems. A large number of techniques have been proposed to efficiently partition a dataset, often focusing on finding the best partitioning for a particular query workload. However, many modern analytic applications involve ad-hoc or exploratory analysis where users do not have a representative query workload. Furthermore, workloads change over time as businesses evolve or as analysts gain better understanding of their data. Static workload-based data partitioning techniques are therefore not suitable for such settings. In this thesis, we present Amoeba, an adaptive distributed storage system for data skipping. It does not require an upfront query workload and adapts the data partitioning according to the queries posed by users over time. We present the data structures, partitioning algorithms, and an efficient implementation on top of Apache Spark and HDFS. Our experimental results show that the Amoeba storage system provides improved query performance for ad-hoc workloads, adapts to changes in the query workloads, and converges to a steady state in case of recurring workloads. On a real world workload, Amoeba reduces the total workload runtime by 1.8x compared to Spark with data partitioned and 3.4x compared to unmodified Spark."
2020,Coflow scheduling in data center networks,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2509,"In this work, I analyze the problem of coflow scheduling on a multi-stage cross bar switch with a set of intermediate ports. Using a multi-stage abstraction for the data center network enables development of algorithms that reduce the network latency for certain traffic patterns. In this context, I compare the capabilities of the multi-stage network with the cross-bar switch via simulations and theoretical calculations. A theoretical upper bound of 1/e was shown for the ratio of the difference between the minimum total coflow completion times (TCCT) on these two networks and the minimum TCCT on the cross-bar switch. On the other hand, simulations using approximation algorithms gave values of up to 8 percent for the average of the same ratio under randomly generated trac patterns."
2020,Adaptive sampling of transient environmental phenomena with autonomous mobile platforms,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3666,"is subsequently presented, which addresses the MSS problem and overcomes key technical challenges with planning in natural environments. Theoretical performance guarantees are derived for PLUMES, and empirical performance is demonstrated against canonical uniform search and state-of-the-art baselines in simulation and field trials. Ultimately, this thesis examines the challenges of autonomous informative sampling in the environmental and earth sciences. In order to create useful systems that perform diverse scientific objectives in natural environments, approaches from robotics planning, field design, Bayesian optimization, machine learning, and the sciences must be drawn together. PLUMES captures the breadth and depth required to solve a specific objective within adaptive sampling, and this work as a whole highlights the potential for mobile technologies to perform intelligent autonomous science in the future."
2020,Deep learning with physical and power-spectral priors for robust image inversion,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.2992,"Conversely, if more restricted examples are used as training examples, the training can be guided to undesirably ""remember"" to produce the ones similar as those in training, making the cross-domain generalization problematic. Next, we also propose to use deep learning to greatly accelerate the optical diffraction tomography algorithm. Unlike previous algorithms that involve iterative optimization algorithms, we present significant progresses towards 3D refractive index (RI) maps from a single-shot angle-multiplexing interferogram. Last but not least, we propose to use cascaded neural networks to incorporate the system physics directly into the machine learning algorithms, while leaving the trainable architectures to learn to function as the ideal Proximal mapping associated with the efficient regularization of the data. We show that this unrolled scheme significantly outperforms the End-to-End scheme, in low-light imaging applications."
2020,Dynamic and robust network resource allocation,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2569,"We formulate a dynamic robust optimization model that addresses this decision question, apply a tractable solution heuristic, and prove theoretical guarantees of the heuristic's performance. Our model is calibrated with publicly available data to generate insights on how the policymakers should balance investment between medical inventory and personnel training. The World Health Organization and regional public health authorities decide on the influenza (flu) vaccine type ahead of flu season every year. Vaccine effectiveness has been limited by the long lead time of vaccine production - during the production period, flu viruses may evolve and vaccines may become less effective. New vaccine technologies, with much shorter production lead times, have gone through clinical trials in recent years. We analyze the question of optimal vaccine selection under both fast and slow production technologies. We formulate the problem as a dynamic distributionally robust optimization model."
2020,A Comparison of artificial intelligence algorithms for dynamic power allocation in flexible high throughput satellites,MIT CSAIL,MIT Student,MIT Advisor,AI,0.2851,"Although multiple AI approaches have been proposed in the recent years, most of the analyses have been conducted under assumptions that do not entirely reflect the new operation scenarios' requirements, such as near-real time performance or high-dimensionality. Furthermore, little work has been done in thoroughly comparing the performance of different algorithms and characterizing them. This Thesis considers the Dynamic Power Allocation problem, a DRM subproblem, as a use case and compares nine different AI algorithms under the same near-real time operational assumptions, using the same satellite and link budget models, and four different demand datasets. The study focuses on Genetic Algorithms (GA), Simulated Annealing (SA), Particle Swarm Optimization (PSO), Deep Reinforcement Learning (DRL), and hybrid approaches, including a novel DRL-GA hybrid. The comparison considers the following characteristics: time convergence, continuous operability, scalability, and robustness."
2020,"Urban computing using call detail records : mobility pattern mining, next-location prediction and location recommendation",MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3152,"Urban computing fuses computer science with other fields, such as transportation, in the context of urban spaces by connecting ubiquitous sensing technologies, analytical models and visualizations to solve challenging problems in urban environment and operation systems. This paper focuses on Call Detail Records, one widely collected opportunistic sensing data source for billing purposes, to understand presence patterns, develop mobility prediction methods and reduce traffic congestions with location recommendations. Understanding human mobility and presence patterns at locations are the building blocks for behavior prediction, service design and system improvements. In the first part, this thesis focuses on 1) understanding presence patterns at user locations with a proposed metric Normalized Hourly Presence, 2) extracting common presence patterns across the population with Principal Component Analysis; 3) and infer home and workplaces using K-means Clustering and Fuzzy C-means Clustering. The proposed method was implemented on MIT Reality Mining data, by which we demonstrate that with inference rates of 56% and 82%, the method can improve 79% and 34% in accuracy respectively in home and workplace inference comparing to the baseline model. In addition, it was implemented on the CDR data collected in a crowded city in China to prove its scalability and applicability in real-world applications. With Fuzzy C-means Clustering, we could flexibly trade-off between inference rate and accuracy to understand the interplay between the two and apply it for various purposes. With an understanding of mobility patterns, the next crucial foundation in urban computing is mobility prediction, enabling transportation practitioners to take actions beforehand and commercial organizations to send location-based advertisements, etc. Specifically, this paper focuses on next-location prediction from Call Detail Records. Mobility traces was analogized to language models, mapping cell towers to words and individual location traces to sentences. Recurrent Neural Network is a successful tool in natural language processing, which is applied in mobility prediction due to its acceptance of sequential input, variable input length and ability to learn the 'meaning' of cell towers. By implementing the method on Call Detail Records collected in Andorra, we show that the method improved more than 40% over the baseline model, with 67% and 78% accuracy in next location at cell tower and merged cell tower level respectively. The 'meanings' of the cell tower could also be inferred, the same as learning the meanings of words in sentences, from the embedding layer of Recurrent Neural Network. The last project aims at tackling the challenge of severe traffic congestions with location recommendations. The availability of large-scale longitudinal geolocation data, such as Call Detail Records, offers planners and service providers an unprecedented opportunity to understand location preferences and alleviate traffic congestions. Location recommendation is a potential tool to achieve these two objectives. Previous research on location recommendations has focused on automatically and accurately inferring users' preferences, while little attention has been devoted to the constraints of service capacity. The ignorance may lead to congestion and long waiting time. We argue that Call Detail Records could help planners and authorities make interventions by providing personalized recommendations given the comprehensive urban-wide picture of historical behaviors and preferences. In this research, we propose a method to make location recommendations for system efficiency, defined as maximizing satisfactions toward recommendations subject to capacity constraints, exploiting travelers' choice flexibilities. We infer implicit location preferences based on sparse and passively-collected Call Detail Records. We then formulate an optimization model the defined system efficiency. As a proof-of-concept experiment, we implement the method in Andorra, a small European country heavily relying on tourism. By extensive simulations, we demonstrate that the method can reduce the travel time increased by congestion during peak hour from 11.73 minutes to 5.6 minutes with idealized trips under full compliance rates. We show that the average travel time increased by congestion is 6.17, 6.98, 8.37 and 10.98 minutes with 80%, 60%, 40% and 20% compliance rates. Overall, our results indicate that Call Detail Records can be used to make locations recommendation while reduce traffic congestion for system efficiency. The proposed method can be applied to other large-scale location traces and extended to other location or events recommendation applications."
2020,Design of an advanced sEMG processor for wearable robotics applications,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.4369,"This thesis presents the design and evaluation of a surface electromyography(sEMG) acquisition platform specialized for wearable robotic applications. While sEMG is one of the best ways to interpret human muscle and neural activity, the research field in wearable robotics has limitations in utilizing sEMG signals with commercially-available sEMG acquisition platforms. These limitations include a large and bulky electronics package, poor portability, insufficient electrode versatility, and limited reconfigurability. This thesis aims to present an effective design that provides solutions to these many difficulties while insuring robustness and acquisition signal quality. The thesis reasons and explains in detail every design decision and process among the system development and manufacturing. The evaluation of the manufactured system compared to a benchtop state-of-the-art sEMG recording platform is demonstrated. Practical utility of the developed sEMG measurement system is also demonstrated with a real world wearable robotic application."
2020,Mechanistic modeling of bacterial nutrient uptake strategies,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2465,"Bacteria have developed a variety of strategies to nd and consume the substrates necessary for both the cell's energy-consuming processes and for the additional biomass needed to replicate. A greater understanding of the diversity and regulation of these strategies can provide us with a number of insights relevant for a variety of applications, from predicting bacterial population dynamics and thus carbon-cycling rates in the ocean to bio-engineering bacteria into microscale robots. Here I use toy, mechanistic models of single-cell metabolism that allow me to quantify the costs and benefits of various nutrient uptake strategies. I find that: (i) a sensing-uptake trade-off governs E. coli's regulation of maltose uptake and chemotaxis to maltose; (ii) a rate-affinity trade-off in nutrient transport systems governs the speciation of marine oligotrophic and copiotrophic heterotrophs; and (iii) an exploration-conservation trade-off governs the prevalence of motility in the marine microbial world. This work thus provides new understanding of how both phenotypic diversity and cellular regulation are governed by trade-offs for maximizing growth rate in dierent environments."
2020,Non-contact ultrasound,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3067,"This thesis explores the design, development, and evaluation of two novel non-contact ultrasound imaging methods: immersion ultrasound and optical ultrasound. Immersion ultrasound utilizes traditional piezoelectric elements in a tomographic framework to develop new algorithms and acquisition methods for quantification of tissue geometry and properties in human proximal limbs. Bone is uniquely challenging for ultrasound due to the high impedance mismatch between bone and soft-tissue in the imaging domain. New imaging algorithms are necessary for both geometric and quantitative reconstruction of subjects with bone. Multiple immersion systems were designed and constructed using the framework presented in this thesis. Mechanical systems include a 4 degrees of freedom single element system and a fully flexible 36 degrees of freedom robotic system abbreviated MEDUSA (Mechanically Discrete Ultrasound Scanning Apparatus)."
2020,Long-wave infrared frequency combs based on quantum cascade lasers,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2187,"Ever since the invention of quantum cascade laser (QCL), the performance and the flexibility in design has made it a desirable source for a wide range of applications, such as trace-chemical sensing, health monitoring, frequency metrology, noninvasive imgaing and infrared countermeasures. The LWIR region (or mid-infrared region), roughly ranging from 2-20 [mu]m, is of particular importance to spectroscopy applications, since many molecular species have their strongest rotational-vibrational absorption bands in that area. Infrared laser spectroscopy began about 40 years ago and has been using a variety of different tunable laser-based sources, particularly lead salt diodes, color center lasers, difference frequency generation and optical parametric oscillators. The large tunabilitiy in the design (lasing frequency, tunability, power, material system, etc.) and the compactness in fabrication and packaging has made QCL an ideal source for laser-based spectroscopy. Traditional spectroscopy systems suffer from problems like large physical dimensions, long data-processing times and spectral resolution restrictions. Therefore the development of a simple, robust, compact and inexpensive optical source/system like QCL frequency combs can largely benefit spectroscopy systems. In the past few years, QCLs have proven to be able to form comb radiation in both LWIR and THz regions. And dual comb spectroscopy has been demonstrated using QCL frequency combs with very short acquisition time ([mu]s). The development of a broadband, high power, narrow linewidth and stable LWIR frequency comb based on quantum cascade laser is the key to realizing such broadband ultrafast spectrometer in the mid-infrared range. This thesis explores the design, fabrication and characterization techniques towards the development of LWIR QCL frequency comb devices for spectroscopic purposes. A complete wet etch epi-up fabrication process is reported, with preliminary results on the dry-etch technique to incorporate dispersion compensation strucutre and epi-down fabricaiton for high power CW mode QCL device. Formation of comb(-like) regime has been observed in two devices, with the Gires-Tournois Interferometer (GTI) mirror providing dispersion from the rear facet. In order to improve the comb performance of these devices, dispersion of the device is measured to provide essential information for the design of chirped top cladding for dispersion compensation. This thesis provides an important step towards the realization of a room temperature, broadband, CW mode LWIR QCL frequency comb device for spectroscopic purposes."
2020,Preventing IPC-facilitated type confusion in Rust,MIT CSAIL,MIT Student,MIT Advisor,OS,0.291,"Type-safe languages undertake to prevent the type confusion vulnerabilities that arise in type-unsafe languages such as C++. One such type-safe language is Rust, which provides powerful type safety guarantees [1]. However, these guarantees are valid only for a single compilation unit. That is, they may not hold when multiple separately compiled processes communicate. In this work, we explore how type confusion vulnerabilities can still arise when multiple separately compiled, internally type-safe processes share information through inter-process communication (IPC). We propose safeIPC, a tool for eliminating IPC-facilitated type confusion in Rust. safeIPC is a Rust compiler extension that detects communications over IPC and inserts runtime checks to ensure that type safety is maintained. Programs instrumented with safeIPC throw a runtime error if the type of any data received over IPC is not equivalent to the type expected. Our analysis shows that safeIPC is effective in preventing type confusion vulnerabilities not prevented by Rust alone."
2020,A stereo vision system with automatic brightness adaptation,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2718,"This thesis describes the development of an automatic brightness adaptive imaging system for use in stereo vision algorithms implemented for a variety of processing architectures. A 256 x 256 array of wide dynamic range pixels with on-chip A/D converters provides the digital data path for a feedback network which controls the charge integration parameters at each pixel. The first goal of the project was to build a real-time demonstration of the imager with configurable compression functions. Secondly, electronic irising was employed by controlling the global charge integration time based on the average intensity of the image. In addition to electronic-irising, the imaging system employs a linear or a logarithmic compression scheme based on the image data. The controller fits the compression function to the image by comparing the average intensities of many different regions within the image. Finally, a 3-camera stereo-vision system was developed with data transfer to a PC through the PCI bus at 60fps. The imagers are synchronized and controlled based on the center imager's data which allows for consistent object correlation in stereo vision algorithms."
2020,Infrastructures for secure multiparty computation,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2228,"We devise information theoretically secure protocols that allow additional pairs of parties to establish secure OT correlations using the help of other parties in the network in the presence of a dishonest majority. Our main technical contribution is an upper bound that matches known lower bounds regarding the number of OT channels necessary and sufficient for MPC. In particular, we characterize which n-party OT graphs G allow t-secure computation of OT correlations between all pairs of parties, showing that this is possible if and only if the complement of G does not contain the complete bipartite graph Kn-t,n-t as a subgraph. Finally, we study the problem of building an infrastructure for fair secure computation, where we guarantee that if any party receives the output of the secure computation, then all honest parties do as well."
2020,Modeling and tradespace exploration of a space suit hip bearing assembly using multi-degree-of-freedom range of motion analysis,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3312,"Space suits are crucial to human spaceflight, but can restrict motion, require additional energy, and increase injury risk. Previous planetary suits were largely based on flexible components, which generate additional forces on the occupant as they resist volumetric changes from flexing components. The NASA Mark III suit addresses this problem using a Hip Brief Assembly (HBA), composed of rigid, constant-volume sections connected by bearings. However, due to the rigid components and fixed degrees of freedom (DoFs), the HBA and other hard-component joint assemblies (HCJAs) have stricter bounds on motion. For example, previous analysis shows that the hip multi-DoF range of motion (ROM) for an HBA occupant is not well-aligned with the nominal hip ROM during gait (gait NHROM). In this thesis, a set of methods for describing HCJA geometry and the effect on occupant ROM is presented."
2020,Tracking engagement : a machine learning framework for estimating affective engagement,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3261,"Globally, construction fatality counts remain among the highest of all industries. As part of efforts to improve workers occupational health and safety, most companies provide workers with ongoing safety training. Yet accidents continue to take place, as there is a lack of understanding on how to increase the knowledge transfer that would help improve safety. The goal of this thesis is to automate and improve manual observation methods, presently used to determine construction workers' engagement during training courses by applying machine learning techniques to video images. This thesis proposes a framework to measure construction workers' engagement during training courses by unobtrusively analyzing engagement through body and pose estimation, codifying who is speaking and understating the predicted emotional state of a given worker through their facial expressions of emotion at specific lectures times through stateof- the-art computer vision techniques. The framework was prototyped on fifteen graduate and undergraduate students from a private university in the United States during four class sessions in a stadium set up classroom by three high definition cameras. The proposed system can enhance our understanding of learning processes within classroom contexts, while reducing the labor-intensive process of traditional observations methods, and allowing for the observation of a full class simultaneously. Further, the repeatability and standardization of objective observations will be improved as it will no longer depend on the skills of the observer and on his or her ability to capture and make sense of what was observed."
2020,Designing structures with tree forks : mechanical characterization and generalized computational design approach,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2939,"The thesis presents results that systematically test this methodology by studying how matching quality varies depending on the number and species of tree forks available in the library and relates this back to the mechanical properties of tree branches found through physical testing. Second, mechanical laboratory testing of tree fork nodes of various tree species (available locally in the area) is presented to quantify the structural capacity of these connections and observe the behavior under tree fork load transfers. A structural score is developed to characterize the tolerance of tree fork nodes to imperfect matches in terms of structural capacity; these resulting geometries are compared to the previous matching-based scoring system. The resulting approach is projected forward as a framework for a more general computational approach for designing with existing material systems and geometries that can also be expanded beyond tree forks."
2020,The effects of fuel volatility and operating conditions on sprays from pressure-swirl fuel injectors,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2233,"Optimal design of modern direct injection gasoline engines depends heavily on the fuel spray. Most of the studies published regarding these fuel sprays involve cold bench tests or motored optical engines, neglecting the roles of the fuel volatility and temperature. This study, therefore, was designed to describe changes in the spray properties due to fuel volatility and operating conditions using a firing optically-accessible engine. Planar laser-induced fluorescence and planar Mie scattering imaging experiments were performed to show changes in the spray structure, including its radial and axial penetration. Phase-Doppler particle analysis experiments were included to track the droplet diameter and velocity at various points throughout the spray. A computational fluid dynamics model was also used to study the physics leading to the observed changes. The results show that the spray structure changes with not only ambient gas density, which is often measured, but also fuel temperature and volatility. The mean droplet diameter was found to decrease substantially with increasing fuel temperature and decreasing ambient density. Under conditions of low potential for vaporization, the observed trends agree with published correlations for pressure-swirl atomizers. As ambient density decreases and fuel temperature increases, the volatile ends of multi-component fuels evaporate quickly, producing a vapor core along the axis of the spray. Beyond a certain point, evaporation is violent enough to cause additional breakup of the droplets. A fit to this volatility-induced breakup data provides an additional correlation for determining the mean diameter of volatile sprays. Coincident with the volatility-induced breakup trend is an increase in the initial cone angle of the spray. However, the reduced droplet diameter and rapid vapor generation under these superheated conditions result in a narrow spray with increased axial penetration. In the process of performing these experiments, insights were found regarding the operation of these diagnostics in high-density sprays."
2020,Prediction and analysis of degree of suicidal ideation in online content,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.4156,"Machine learning (ML) has increasingly been used to address the growing burden of mental illness and lack of access to quality mental health care. Recently such models have been applied to online data, such as social media postings to augment mental health screening. Despite the potential of these methods, online ML classifiers still perform poorly in multi-class settings. In this thesis, we propose the usage of novel document embeddings and mental health based user embeddings for triaged suicide risk screening. Machine learning to infer suicide risk and urgency is applied to a dataset of Reddit users in which the risk and urgency labels were derived from crowdsource consensus. We show that the document embedding approach outperforms count-based baselines and a method based on word importance, where important words were identified by domain experts. We examine interpretable features and methods that help to discern and explain risk labels. Finally, we find, using a Latent Dirichlet Allocation (LDA) topic model, that users labeled at-risk for suicide post about different topics to the rest of Reddit than non-suicidal users."
2020,Investigating the influence of biosphere-atmosphere interactions on atmospheric chemistry and composition,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2013,"The interactions between the biosphere and the atmosphere are an important controlling factor for regional to global atmospheric chemistry and composition. This ultimately has wide impacts on issues like air quality and climate change. However, there are still substantial uncertainties in the biosphere-atmosphere interaction processes that drive the global abundance and variability of many critically important atmospheric constituents, including ozone, aerosol, and Volatile Organic Compounds (VOCs). This thesis aims to address these uncertainties through a multifaceted approach, combining theory and data-driven models with observations. The scope of the research completed herein is introduced and described in Chapter 1. Chapter 2 is a case study of biosphere atmosphere interactions where the air quality impact of large-scale agricultural deforestation in Southeast Asia is investigating using global models."
2020,Applying Critical Chain to improve the management of uncertainty in projects,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2944,"In an ever intensifying global competitive market, the management of projects, particularly product development efforts, increasingly is one of the few areas which can produce a sustained competitive advantage. Firms that can bring products to market faster can extract higher initial margins, can be more responsive to their customers, and will have products with longer sales lives. Critical Chain is a new methodology that applies Eli Goldratt's Theory of Constraints to project management in order complete projects faster and with greater predictability while simultaneously making more efficient use of resources. The Critical Chain method accomplishes this by building project networks with average task durations, aggregating buffer at the end of projects where it can absorb unplanned iterations and other delays, and de-conflicting resources, both within and across projects. This new project management methodology was researched by spending seven months on site with ITT Night Vision and applying the method to two product development projects. In addition, benchmarking studies of previous product development efforts at the same site and of another lead user of the tool were conducted to provide both qualitative and quantitative comparison data. Critical Chain appears to minimize schedule risk while simultaneously minimizing project duration, and has the potential to improve both communication and employee morale."
2020,Precision measurement of and search for dark matter in the transverse momentum spectra of Z bosons,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.1762,"A measurement of the differential Z boson production cross section in proton-proton collisions is presented. It furnishes a precision test of the Standard Model, and constrains the parton distribution functions of the proton. Moreover, it is a building block for future measurements of the mass of the W± boson. A study of the efficiency of lepton identification algorithms is performed which drives the precision of the measurement at lower values of transverse momentum. In tandem, a search for new physics in events with a Z boson produced in association with large missing transverse momentum is presented. The results of this search are interpreted in the context of several dark matter models: generic spin-0 or spin-1 mediators, invisible decays of a Higgs-like boson, unparticles, and large extra spatial dimensions. A multivariate analysis was developed to enhance the sensitivity of the invisible Higgs interpretation. The theoretical uncertainty on the irreducible background from electroweak diboson processes is constrained by emulating the missing energy using pure control samples in the fully leptonic final states. The data were collected with the Compact Muon Solenoid detector at the Large Hadron Collider and correspond to an integrated luminosity of 35.9 fb-1. No significant deviations from the Standard Model are found."
2020,Steps towards proof construction using reinforcement learning : environments and models for hypothesis-posing as subtask creation,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3242,"Despite recent advances in reinforcement learning (RL) that have allowed AI algorithms to master games such as Go from scratch, scant progress has been made on applying RL to one of the first tasks seen as susceptible to automation: theorem proving. I present steps towards training agents to construct proofs through utilizing the ability to pose hypotheses as a way to uncover information and break tasks down into subtasks. To do so, I create a novel bitstring problem that retains many of the challenges posed by proof construction while dispensing with the need to parse grammars. I then assess the performance of well-known RL algorithms on tasks derived from this problem, demonstrating that it is non-trivial. Finally, I alter a model that successfully learns one of the bitstring tasks in order to acquire results on possible mechanisms for theorem-proving prototypes."
2020,Optimization of Patterned Surface Structures,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2713,"This thesis advances a recent work on the optimization of patterned surface structures used for architecture and structural engineering. On their own, well-designed surface structures--such as plates and shells--can be highly efficient, but by introducing specific aperture patterns, designers can further improve their potential for structural efficiency. Used as a way to invigorate an otherwise homogeneous architectural environment, even intricately patterned surfaces with highly complex geometries can be realized thanks to recent advancements in digital fabrication technologies. Most recent work on the integration of pattern design and structural optimization lacks general structural engineering applicability and does not address the incompatibility of traditional analysis methods with contemporary CAD environments."
2020,Lexical and Language Modeling of Diacritics and Morphemes in Arabic Automatic Speech Recognition,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.4234,"Arabic is a morphologically rich language which rarely displays diacritics. These two features of the language pose challenges when building Automatic Speech Recognition (ASR) systems. Morphological complexity leads to many possible combinations of stems and affixes to form words, and produces texts with high Out Of Vocabulary (OOV) rates. In addition, texts rarely display diacritics which informs the reader about short vowels, geminates, and nunnations (word ending /n/). A lack of diacritics means that 30% of textual information is missing, causing ambiguities in lexical and language modeling when attempting to model pronunciations, and the context of a particular pronunciation. Intuitively, from an English centric view, the phrase th'wrtr wrt n thwrt with 'morphological decomposition' is realized as, th wrtr wrt n th wrt. Including 'diacritics' produces, the writer wrote in the writ. Thus our investigations in this thesis are twofold. Firstly, we show the benefits and interactions between modeling all classes of diacritics (short vowels, geminates, nunnations) in the lexicon. On a Modern Standard Arabic (MSA) corpus of broadcast news, this provides a 1.9% absolute improvement in Word Error Rate (WER) (p < 0.001). We also extend this graphemic lexicon with pronunciation rules, yielding a significant improvement over a lexicon that does not explicitly nodel diacritics. This results in a of 2.4% absolute improvement in WER (p < 0.001). Secondly, we show the benefits of language modeling at the morphemic level with diacritics, over the commonly available, word-based, nondiacratized text. This yields an absolute WER improvement of 1.0% (p < 0.001)."
2020,Interpretable machine learning methods with applications to health care,MIT CSAIL,MIT Student,MIT Advisor,NLP,0.3055,"With data becoming increasingly available in recent years, black-box algorithms like boosting methods or neural networks play more important roles in the real world. However, interpretability is a severe need for several areas of applications, like health care or business. Doctors or managers often need to understand how models make predictions, in order to make their final decisions. In this thesis, we improve and propose some interpretable machine learning methods by using modern optimization. We also use two examples to illustrate how interpretable machine learning methods help to solve problems in health care. The first part of this thesis is about interpretable machine learning methods using modern optimization. In Chapter 2, we illustrate how to use robust optimization to improve the performance of SVM, Logistic Regression, and Classification Trees for imbalanced datasets. In Chapter 3, we discuss how to find optimal clusters for prediction. we use real-world datasets to illustrate this is a fast and scalable method with high accuracy. In Chapter 4, we deal with optimal regression trees with polynomial function in leaf nodes and demonstrate this method improves the out-of-sample performance. The second part of this thesis is about how interpretable machine learning methods improve the current health care system. In Chapter 5, we illustrate how we use Optimal Trees to predict the risk mortality for candidates awaiting liver transplantation. Then we develop a transplantation policy called Optimized Prediction of Mortality (OPOM), which reduces mortality significantly in simulation analysis and also improves fairness. In Chapter 6, we propose a new method based on Optimal Trees which perform better than original rules in identifying children at very low risk of clinically important traumatic brain injury (ciTBI). If this method is implemented in the electronic health record, the new rules may reduce unnecessary computed tomographies (CT)."
2020,Excitonic spin engineering for solar cells and organic light-emitting diodes,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2233,"This result highlights the general applicability of dihedral angle tuning, a molecular design strategy that can be used to improve the performance of donor-acceptor type TADF emitters without significantly changing their emission spectra. In contrast, contemporary solar cell technologies are dominated by silicon, an inorganic semiconductor. But when absorbing photons, silicon (like other semiconductors) wastes energy in excess of its bandgap. Reducing these thermalization losses is possible by sensitizing the silicon solar cell using singlet fission, a carrier multiplication phenomenon that occurs only in organic semiconductors. In this process, two triplet excitons are generated from a singlet exciton. In tetracene, those triplet excitons are energetically matched to the silicon bandgap. Transferring triplet excitons to silicon creates additional electron-hole pairs, promising to increase cell efficiencies from the single-junction limit of 29% to as high as 35%."
2020,The design and implementation of a new wide-range frequency detector,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2384,"In this thesis, I designed and implemented a wide range frequency detector for use in clock recovery and data retiming applications. The new detector works in conjunction with the existing ""mid-range"" frequency detector to accurately lock the VCO to the incoming data rate. The new detector consists of two halves: one to detect when the VCO is too fast, and one to detect when the VCO is too slow. The design and analysis of the new frequency detectors, in addition to a method for integrating it with the existing detector, is discussed. Simulation data of the new and original frequency detectors are used to support the concepts upon which the new detector is designed. Some topics for future work are suggested at the end of this thesis."
2020,Optimizing vaccine dosing kinetics for stronger antibody response,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2345,"What are the aspects of affinity maturation being altered by various temporal patterns of antigen dosing? Certain extended-duration dosing profiles increase the strength of the humoral response, with exponentially-increasing(EI) dosage providing the greatest enhancement. While this is an exciting result, it is necessary to establish a mechanistic understanding of how immune response be enhanced to further engineer and optimize the temporal patterns. From our computational model, the effect is driven by enhanced capture of antigen in lymph nodes by evolving higher-affinity antibodies early in the GC response. We validate the prediction from independent experimental data, where EI dosage result in promoted capture and retention of the antigen in lymph nodes. To our knowledge, this work is the first to demonstrate a key mechanism for vaccine kinetics in the response of B cells to immunization, and may prove to be an effective method for increasing the efficacy of subunit vaccines. 3."
2020,Topology hiding computation on all graphs,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2385,"A distributed computation in which nodes are connected by a partial communication graph is called topology-hiding if it does not reveal information about the graph beyond what is revealed by the output of the function. Previous results have shown that topology-hiding computation protocols exist for graphs of constant degree and logarithmic diameter in the number of nodes [Moran-Orlov-Richelson, TCC'15; Hirt et al., Crypto'16] as well as for other graph families, such as cycles, trees, and low circumference graphs [Akavia-Moran, Eurocrypt'17], but the feasibility question for general graphs was open. In this work we positively resolve the above open problem: we prove that topology-hiding computation is feasible for all graphs under the Decisional Diffie-Hellman assumption. Our techniques employ random or deterministic walks to generate paths covering the graph, upon which we apply the Akavia-Moran topology-hiding broadcast for chain-graphs (paths). To prevent topology information revealed by the random-walk, we design multiple graph-covering sequences that, together, are locally identical to receiving at each round a message from each neighbor and sending back a processed message from some neighbor (in a randomly permuted order)."
2020,"Relationships between functionality, security, and privacy for multiparty computation, hashing, and encryption",MIT CSAIL,MIT Student,MIT Advisor,OS,0.2294,"One of the fundamental goals of cryptography is to be able to offer security and privacy without sacrificing functionality. Cryptographers have been able to achieve the best of all three by exploiting the assumed hardness of some problems (e.g. discrete log), and have been able to build protocols for secure multiparty computation, collision-resistant hash functions, public key cryptography, and much more. This thesis explores three facets of this balance. First, we delve into Topology-Hiding Computation, which is multiparty computation where we also hide the communication network, strengthening the notion of privacy. Second, we study Property Preserving Hashing, which can be thought of as an extension of collision-resistant hashing where we add functionality. Finally, we explore Fine-Grained Cryptography, and develop a public key cryptosystem. In this model of cryptography, security takes on a much less restrictive role (e.g. adversaries must run in O(n¹⁰) time), but the protocols and security reductions must run in ""fine-grained"" time (e.g. less than O(n⁵))."
2020,Induction heating of a metallic cylinder,MIT CSAIL,MIT Student,MIT Advisor,SE,0.3192,"Thesis (B.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science; and, (B.S.)--Massachusetts Institute of Technology, Dept. of Mechanical Engineering, 1977."
2020,Efficient THz lasers and broadband amplifiers based on quantum cascade gain media,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.2134,"One of the most important applications for Terahertz (THz) quantum cascade (QC) lasers is to provide compact and powerful frequency-stabilized solid-state sources as local oscillators in heterodyne receivers for astronomical studies. The first part of the thesis is dedicated to the device cavity design, fabrication and characterization of the microstrip antenna coupled third-order distributed feedback QC lasers aimed for 2.060 THz atomic oxygen line. THz travelling-wave QC amplifiers are highly desired to achieve broadband amplification of THz radiation in free space. The second part of the thesis focuses on the development of 4.3 THz travelling-wave QC amplifier by monolithically integrating horn antennas and attaching silicon lenses at the metal-metal waveguide facets."
2020,Improved packing strategy for distribution centers to reduce freight cost,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2747,"In this thesis, we designed and implemented a data-driven packing strategy for distribution center outbound packing activities to reduce freight cost and carbon footprint. The strategy consists of two parts. First, I proposed an Carton Combination method, an algorithm that can select any predetermined number of distinct cartons from a large carton pools (over 1000 options) to be used for outbound shipment packing such that the annual total wasted air content inside the shipment is minimized. Second, I proposed an Carton Selection algorithm, which can determine the best carton, from the carton options chosen by the Carton Combination method, for an incoming or- der with known dimensions. The entire packing strategy prototype was implemented by MATLAB R2019a; the prototype was tested with the 2018 outbound shipment data from Waters Corporation Global Distribution Center (GDC) and the simulation showed that the annual averaged shipment air percentage was reduced from 60% to 40%, which projects to an annual operation cost saving of 83,000 USD and carbon dioxide emission reduction of 20 ton. The data-driven packing strategy has a potential to be scaled up and implemented via an industrial environment such as SAP ABAP."
2020,Creating a shipboard power simulation tool using electrical load behavior modeling,MIT CSAIL,MIT Student,MIT Advisor,OS,0.2803,"Trends in power system simulation that demand computationally-intensive, physics-based models may impede the acquisition of useful results for applications like condition-based maintenance [1], electrical plant load analysis (EPLA) [2], and the scheduling and tasking of finite generation and distribution resources. A tool that can quickly evaluate many scenarios, as opposed to intense, high fidelity modeling of a single operating scenario, may best serve these applications. This thesis presents a behavioral simulator that can quickly emulate the operation of a relatively large collection of electrical loads, providing ""what-if"" evaluations for more complete exploration of a design or plant operating envelope. Comparisons to field data collected from a microgrid on-board a 270 foot US Coast Guard ""Famous"" Class medium endurance cutter demonstrate the utility of this tool and approach. The usefulness of this tool is further demonstrated by showing simulated EPLA load factors within 10%of observed load factors over comparable mission sets, both inport and underway. Finally, this thesis will discuss the lessons learned during SPS development and testing, specifically, the need to expand its modeling capability so it can support direct current (DC) electrical distribution systems. The SPS, in its current form can only model alternating (AC) electrical distribution systems."
2020,StreamJIT : a commensal compiler for high-performance stream programming,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2914,"There are domain-specific libraries for many domains, enabling rapid and cost-effective development of complex applications. On the other hand, domain-specific languages are rare despite the performance advantages of compilation. We believe the reason is the multiple orders-of-magnitude higher cost of building a compiler compared to building a library. We propose commensal compilation, a new strategy for compiling embedded domain-specific languages by reusing the massive investment in modern language virtual machine platforms. Commensal compilers use the host language's front-end, use an autotuner instead of optimization heuristics, and use host platform APIs that enable back-end optimizations by the host platform JIT. The cost of implementing a commensal compiler is only the cost of implementing the domain-specific optimizations. We demonstrate the concept by implementing a commensal compiler for the stream programming language StreamJIT atop the Java platform. The StreamJIT commensal compiler takes advantage of the structure of stream programs to find the right amount of parallelism for a given machine and program. Our compiler achieves 2.4 times better performance than StreamIt's native code (via GCC) compiler with considerably less implementation effort."
2020,Data-driven pricing,MIT CSAIL,MIT Student,MIT Advisor,SE,0.2316,"In this thesis, we develop a pricing strategy that enables a firm to learn the behavior of its customers as well as optimize its profit in a monopolistic setting. The single product case as well as the multi product case are considered under different parametric forms of demand, whose parameters are unknown to the manager. For the linear demand case in the single product setting, our main contribution is an algorithm that guarantees almost sure convergence of the estimated demand parameters to the true parameters. Moreover, the pricing strategy is also asymptotically optimal. Simulations are run to study the sensitivity to different parameters.Using our results on the single product case, we extend the approach to the multi product case with linear demand. The pricing strategy we introduce is easy to implement and guarantees not only learning of the demand parameters but also maximization of the profit. Finally, other parametric forms of the demand are considered. A heuristic that can be used for many parametric forms of the demand is introduced, and is shown to have good performance in practice."
2020,Textile precision for customized assemblies,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3218,"With the potential to configure patterns and materials with stitch-level control, textiles are becoming an increasingly desirable method of producing mass customized items. However, current textile machines lack the ability to transfer three-dimensional information between digital models and production with the same level of control and accuracy as other machines. Designers are accustomed to generating three-dimensional objects in a digital model then converting these into instructions for machines such as 3D printers or laser cutters, but current design interfaces and production machines for textiles provide no comparable workflow for producing items that rely on precise control of physical size and fit. Customized assemblies-such as footwear or architectural projects with complex geometries--increasingly integrate textile components with parts produced through a variety of other industrial processes. Furthermore, there is growing interest in the use of three-dimensional data, such as 3D body scanning, to aid in the production of custom-fit products. As mass customization becomes more widespread as an alternative to mass production, general-purpose machines are increasingly capable of generating customized items with high efficiency, relying on design-to-machine workflows to control geometric changes. However, current textile machines are unable to adapt to changing geometric information with the same efficiency. The challenges to dimensional precision in textiles are wide ranging, affected by computational interfaces, production machines, and material technique. Addressing these problems, this thesis demonstrates a design-to-fabrication workflow that enables the transfer of three-dimensional information directly to a device for textile production. The proposed workflow seeks a solution to the material, mechanical, and computational bottlenecks related to spatial accuracy in textile production."
2020,Omnidirectional obstacle detection using minimal sensing,MIT CSAIL,MIT Student,MIT Advisor,Robotics,0.3432,"An integrated approach to visual obstacle detection for aerial multi-rotor vehicles (drones) is introduced. The approach achieves omnidirectional detection of obstacles via suitable synergy of hardware and software. The drone requires a specific arrangement of two cameras, opposing each other, and placed below and above the drone. A total coverage of the drone's surroundings is achieved by fitting each camera with a fisheye lens whose field of view is significantly greater than 180 degrees. The combined field of view of the cameras is omnidirectional, and may be conceptually subdivided into three regions: the monocular portions of each camera (centered at the north and south poles of the drone) and the stereo portion common to both cameras (circling the drone's equator). To use both the stereo and monocular data, a special image projection is developed, based on a model of the world as a 'capsule'."
2019,Optical frequency domain imaging of human retina and choroid,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2518,"(cont.) The 1050-nm OFDI system developed for this thesis comprised a novel wavelength-swept laser that delivered 2.7 mW of average power at a sweep rate of 18.8 kHz, representing a two-order-of-magnitude improvement in speed over previously-demonstrated lasers in the 1050-nm range and below. The system, with an optical exposure level of 550 gW, achieved resolution of 10 gm in tissue and sensitivity of >92 dB over a depth range of 2.4 mm. Two healthy volunteers were imaged with the OFDI system, with 200,000 A-lines over 10.6 seconds in each imaging session. In comparison to results from a state-of-the-art spectral-domain OCT system, the OFDI system provided deeper penetration into the choroid. This thesis demonstrates OFDI's capability for comprehensive imaging of the human retina, optic disc, and choroid in vivo. The deep penetration power of the system enabled the first simultaneous visualization of retinal and choroidal vasculature without the exogenous dyes required by angiography. The combined capability for imaging microstructure and vasculature using a single instrument may be a significant factor influencing clinical acceptance of ophthalmic OFDI technology."
2019,The reduction of acoustic noise emissions from a hard disk drive,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3201,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1994."
2019,"Probabilistic latent variable modeling for predicting future well-being and assessing behavioral influences on mood, stress and health",MIT CSAIL,MIT Student,MIT Advisor,ML,0.4076,"In recent years, there has been a shift in the psychological research literature from an emphasis on dysfunction to a focus on well-being and positive mental health. As a result, enhancing well-being in individuals has become a viable approach to improving health, in addition to treating disorders when present. Also, the availability of rich multi-modal datasets and advances in machine learning methods have spurred an increase in research studies assessing well-being objectively. However, most of these studies tend to primarily focus on using data to estimate or detect the current state of well-being as opposed to the prediction of well-being. In addition, these studies investigate how stand-alone health behaviors and not a combination of health behaviors influence well-being. Furthermore, these studies do not provide data-backed insights and recommendations to individuals seeking to improve their well-being. In this dissertation, we use a real-world dataset from a population of college students and interpretable machine learning methods to (1) predict future mood, stress and health, (2) uncover how combinations of health behaviors work together to influence well-being, and (3) understand how to make evidence-based recommendations to individuals looking to improve their well-being. The use of these methods contributes to the development of objective techniques that can help individuals monitor their wellbeing. In addition, insights from this study contribute to knowledge advancement on how combinations of daily human behaviors can affect well-being."
2019,Low variance methods for Monte Carlo simulation of phonon transport,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2412,"Computational studies in kinetic transport are of great use in micro and nanotechnologies. In this work, we focus on Monte Carlo methods for phonon transport, intended for studies in microscale heat transfer. After reviewing the theory of phonons, we use scientific literature to write a Monte Carlo code solving the Boltzmann Transport Equation for phonons. As a first improvement to the particle method presented, we choose to use the Boltzmann Equation in terms of energy as a more convenient and accurate formulation to develop such a code. Then, we use the concept of control variates in order to introduce the notion of deviational particles. Noticing that a thermalized system at equilibrium is inherently a solution of the Boltzmann Transport Equation, we take advantage of this deterministic piece of information: we only simulate the deviation from a nearby equilibrium, which removes a great part of the statistical uncertainty. Doing so, the standard deviation of the result that we obtain is proportional to the deviation from equilibrium. In other words, we are able to simulate signals of arbitrarily low amplitude with no additional computational cost. After exploring two other variants based on the idea of control variates, we validate our code on a few theoretical results derived from the Boltzmann equation. Finally, we present a few applications of the methods."
2019,Distributed cooperative control architectures for automated manufacturing systems,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3215,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1996."
2019,Analysis and transfer of photographic viewpoint and appearance,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2689,"To make a compelling photograph, photographers need to carefully choose the subject and composition of a picture, to select the right lens and viewpoint, and to make great efforts with lighting and post-processing to arrange the tones and contrast. Unfortunately, such painstaking work and advanced skill is out of reach for casual photographers. In addition, for professional photographers, it is important to improve workflow efficiency. The goal of our work is to allow users to achieve a faithful viewpoint for rephotography and a particular appearance with ease and speed. To this end, we analyze and transfer properties of a model photo to a new photo. In particular, we transfer the viewpoint of a reference photo to enable rephotography. In addition, we transfer photographic appearance from a model photo to a new input photo. In this thesis,we present two contributions that transfer photographic view and look using model photographs and one contribution that magnifies existing defocus given a single photo. First, we address the challenge of viewpoint matching for rephotography. Our interactive, computer-vision-based technique helps users match the viewpoint of a reference photograph at capture time. Next, we focus on the tonal aspects of photographic look using post-processing. Users just need to provide a pair of photos, an input and a model, and our technique automatically transfers the look from the model to the input. Finally, we magnify defocus given a single image. We analyze the existing defocus in the input image and increase the amount of defocus present in out-of focus regions."
2019,Development of multimodal spectroscopy for the detection of vulnerable atherosclerotic plaques,MIT CSAIL,MIT Student,MIT Advisor,ML,0.3018,"The combination of reflectance, fluorescence, and Raman spectroscopy - which is termed multimodal spectroscopy (MMS) - provides complementary and depth-sensitive information about tissue composition. As such, MMS can provide biochemical and morphological information useful in detecting vulnerable atherosclerotic plaques, that is, plaques most prone to rupture and causing sudden death. Early detection of these vulnerable plaques is critical to reducing patient mortality associated with cardiovascular disease. In developing MMS into a clinical diagnostic modality, several scientific and engineering directions are explored in this work: the physical motivation for MMS, the framework of quantitative extraction of spectral parameters, the spectral probes that enable the efficient collection of data, a clinical instrument able to provide real-time diagnosis, and, finally, a clinical implementation of the entire methodology. The motivation for MMS is shown through a pilot in vitro study using carotid artery specimens, which shows the promise for MMS to detect features of vulnerable plaque. Having established the motivation, the next step describes the mathematical tools used to extract quantitative spectral parameters and, moreover, to assess the uncertainty and confidence of the spectral information. In order to implement MMS, the development of an efficient, specialized MMS probe for data acquisition and a compact and practical clinical MMS instrument are described. Lastly, in vivo and ex vivo results from a relatively large clinical study of vulnerable plaque in humans show excellent agreement between MMS and histopathology. Specifically, MMS is shown to have the ability to detect a thin fibrous cap, necrotic core or superficial foam cells, and thrombus."
2019,Multiresolution statistical modeling with application to modeling groundwater flow,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3242,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1997."
2019,File system unification using LatticeFS,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2939,"LatticeFS is a namespace unification system designed to merge multiple source file systems into a single working file system. atticeFS can be used to merge multiple software package directories, work with multiple file systems as if they are one, and share a single storage medium among multiple machines. On a high level, LatticeFS takes as input an arbitrary number of file system paths, and mounts a new virtual drive that will appear to the user as a union of the input file systems. Of course, attempting to combine multiple file systems will inevitably be met with conflicts. Situations in which multiple input file systems contain files/directories with the same name will be common in large systems; which file/directory should the user be exposed to in this case? Previous work such as UnionFS solved the problem by giving each input file system a strict priority value, and when a conflict occurred, the file/directory with the highest priority was the one shown to the user. In LatticeFS, we have introduced a plug-in system in which different strategies for resolving conflicts can be easily swapped in and out; additionally, handlers for special file types can also be ""plugged"" into the system. This paper describes and evaluates all aspects of LatticeFS in detail."
2019,Theoretical models for microwave remote sensing of earth terrain,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3162,"Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1984."
2019,An implementation of a 5.25 GHz transceiver for high data rate wireless applications,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2742,"The desire for transmission of high data rate information across wireless channels has grown immensely over the past decade. Wireless devices available today including mobile phones, wireless local area networks (WLANs) and Bluetooth radios have realized a wide variety of applications at data rates ranging from 10s of kbit/s to 10s of Mbit/s. Mobile telephone design strives for large transmit distances, Bluetooth technology enables communication between two close range devices, and wireless LAN strives to achieve a high data rate wireless link within an office or home environment. This link is traditionally implemented through a central access point that communicates with one or more workstations. Due to the large number of applications demanding high speed wireless links, the aspiration for even higher data rates is prevalent."
2019,Toward a compact underwater structured light 3-D imaging system,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2692,"A compact underwater 3-D imaging system based on the principles of structured light was created for classroom demonstration and laboratory research purposes. The 3-D scanner design was based on research by the Hackengineer team at Rice University. The system is comprised of a low-power, open-source hardware single-board computer running a modified Linux distribution with OpenCV libraries, a DLP pico projector, camera board, and battery module with advanced power management. The system was designed to be low-cost, compact, and portable, while satisfying requirements for watertightness. Future development and applications may involve navigation systems for an autonomous underwater vehicle (AUV). An initial study of 3-D imaging methods is presented, and the strengths and drawbacks of each type are discussed. The structured light method was selected for further study for its ability to produce high-resolution 3-D images for a reasonable cost. The build of the 3-D imaging system was documented for reproducibility, and subsequent testing demonstrated its functions and ability to produce 3-D images. An instruction guide for operation of the device is provided for future classroom and laboratory use. The 3-D imaging system serves as a proof-of-concept for utilizing structured light methods to produce 3-D images underwater. Image resolution was limited by the output resolution of the pico projector and camera module. Further exploration in obtaining ultra high-resolution 3-D images may include use of a more powerful projector and a higher resolution camera board module with autofocus. Satisfactory 3-D scanning validated the performance of structured light scanning above water. However, contaminants in the water hindered accurate rendering by the system while submerged due to light scattering. Future development of a on-the-fly mapmaking system for AUV navigation should include algorithms for filtering light scattering, and hardware should based on an instantaneous structured light system utilizing the Kinect 2-D pattern method. Autofocus and increased projector brightness would also be worthwhile additions."
2019,Probabilistic modeling of planar pushing,MIT CSAIL,MIT Student,MIT Advisor,ML,0.3565,"This work studies the problem of data-driven modeling and stochastic filtering of complex dynamical systems. The main contributions are GP-SUM, a filtering algorithm tailored to systems expressed as Gaussian processes (GP), and the probabilistic modeling of planar pushing by combining input-dependent GPs and GP-SUM. The main advantages of GP-SUM for filtering are that it does not rely on linearizations or unimodal Gaussian approximations of the belief. Moreover, it can be seen as a combination of a sampling-based filter and a probabilistic Bayes filter as GP-SUM operates by sampling the state distribution and propagating each sample through the dynamic system and observation models. Effective sampling and accurate probabilistic propagation are possible by relying on the GP form of the system, and a Gaussian mixture form of the belief. In this thesis we show that GP-SUM outperforms several GP-Bayes and Particle Filters on a standard benchmark. To characterize the dynamics of pushing, we use input-dependent GPs to learn the motion of the pushed object after a short time step. With this approach we show that we can learn accurate data-driven models that outperform analytical models after less than 100 samples and saturate in performance with less than 1000 samples. We validate the results against a collected dataset of repeated trajectories, and use the learned models to study questions such as the nature of the variability in pushing, and the validity of the quasi-static assumption. Finally, we illustrate how our learned model for pushing can be combined with GP-SUM, and demonstrate that we can predict heteroscedasticity, i.e., different amounts of uncertainty, and multi-modality when naturally occurring in pushing."
2019,Verification of d-wave pairing symmetry by microwave intermodulation distortion measurements in yttrium barium copper oxide,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2398,"We report measurements of the temperature and power dependence of the microwave frequency intermodulation distortion (IMD) in high quality pulsed laser deposition (PLD) Yttrium Barium Copper Oxide (YBCO) on LaAlO3 substrate. A low-temperature (T < 30 K) increase in IMD is the observation of an upturn of the nonlinear coefficient of the quadratic field dependence of the penetration depth. This IMD upturn is limited by the nonlinear Meissner effect that has been predicted for d-wave high-T, superconductors. Various amounts of IMD increase are observed for different films with impurity (Ni, Zn and Ca) doping and other defects. The demonstration of the IMD upturn and the nonlinear Meissner effect were possible because the IMD measurement is an extremely sensitive method to detect the penetration depth change at even less than 0.01 nm. IMDs from various samples tend to merge at a single universal value at 0 K regardless of disorder, defects, and impurities due to the node singularity at 0 K. There is a similar converging trend in IMD towards the transition temperature T, due to the quasiparticle thermal excitation and depletion of superelectrons. It is most likely that IMD has both intrinsic and extrinsic contributions."
2019,Learning with generalized negative dependence : probabilistic models of diversity for machine learning,MIT CSAIL,MIT Student,MIT Advisor,ML,0.3902,"This thesis establishes negative dependence as a powerful and computationally efficient framework to analyze machine learning problems that require a theoretical model of diversification. Examples of such problems include experimental design and model compression: subset-selection problems that require carefully balancing the quality of each selected element with the diversity of the subset as a whole. Negative dependence, which models the behavior of ""repelling"" random variables, provides a rich mathematical framework for the analysis of such problems. Leveraging negative dependence theory for machine learning requires (a) scalable sampling and learning algorithms for negatively dependent measures, and (b) negatively dependent measures able to model the specific diversity requirements that arise in machine learning. These problems are the focus of this thesis."
2019,Optimization of electron optics in a resonator cavity using Nelder-Mead simplex search for the quantum electron microscope,MIT CSAIL,MIT Student,MIT Advisor,Security,0.273,"The Quantum Electron Microscope (QEM) is a proposed imaging modality that aims to reduce or eliminate the effects of radiation on living cells compared to traditional electron microscopy techniques. In recent years, an interaction free measurement scheme was proposed by Putnam and Yanik [1], and an implementation of this idea is being developed by an international collaboration. The current implementation foresees an electron cavity, which can be installed into a regular scanning electron microscope, to allow multiple passes of two electron wavefunctions over the specimen. In order to implement this idea, multiple different electron optical designs were proposed. Extensive simulation work is required to test and validate these designs. This work outlines the simulation work done for QEM, and proposes a general framework for optimizing electron trajectory simulations using Nelder-Mead search. It also provides a library of MATLAB wrapper functions and optimization methods to be used with the Integrated Lorentz-2E software."
2019,Biologically-plausible six-legged running : control and simulation,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2297,"This thesis presents a controller which produces a stable, dynamic 1.4 meter per second run in a simulated twelve degree of freedom six-legged robot. The algorithm is relatively simple; it consists of only a few hand-tuned feedback loops and is defined by a total of 13 parameters. The control utilizes no vestibular-type inputs to actively control orientation. Evidence from perturbation, robustness, motion analysis, and parameter sensitivity tests indicate a high degree of stability in the simulated gait. The control approach generates a run with an aerial phase, utilizes force information to signal aerial phase leg retraction, has a forward running velocity determined by a single parameter, and couples stance and swing legs using angular momentum information. Both the hypotheses behind the control and the resulting gait are argued to be plausible models of biological locomotion."
2019,Diffractive optics for maskless lithography and imaging,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2507,"Semiconductor industry has primarily been driven by the capability of lithography to pattern smaller and smaller features. However due to increasing mask costs and complexity, and increasing tool costs, the state-of-the-art technology in lithography is accessible only to a select few. Zone-plate array lithography (ZPAL) is a novel method of maskless lithography that aims to alleviate some of these issues while offering a solution that can be extended to the limits of nanolithography. In ZPAL, an array of diffractive lenses is used to form an array of spots on the substrate. Each spot is modulated independently by means of spatial-light modulators. This essentially creates a ""parallel laserwriter"". In addition, this lithography system can be converted into a parallel-confocal microscope, which enables fast, high-resolution imaging. This thesis addresses the performance of diffractive lenses, particularly high-numerical aperture zone plates for lithography and imaging using a combination of experimental and theoretical studies. A novel proximity-effect correction algorithm that was implemented effectively in a ZPAL system is also described. Variations to another diffractive lens known as the photon sieve are proposed. The first ever lithography results performed using these new elements are presented in this thesis."
2019,Constitutive equations for superelasticity in crystalline shape-memory materials,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2444,"A crystal-mechanics-based constitutive model for polycrystalline shape-memory materials has been developed. The model has been implemented in a finite-element program. Finite-element calculations of polycrystal response were performed using two methods: (1) The full-finite element method where each element represents a single crystal chosen from a set of crystal orientations which approximate the initial crystallographic texture; (2) A simplified model using the Taylor assumption (1938) where each element represents a collection of single crystals at a material point. The macroscopic stress-strain responses are calculated as volume averages over the entire aggregate. A variety of superelastic experiments were performed on initially-textured Ti-Ni rods and sheets. The predicted stress-strain curves from finite-element calculations are shown to be in good accord with the corresponding experiments. For the Ti-Ni sheet, strain-temperature response at a fixed stress was also experimentally studied. The model was also shown to accurately predict the results from these important experiments. Further, by performing superelastic experiments at moderately high strain rates, the effects of self-heating and cooling due to the phase transformations are shown to be captured well by the constitutive model. The thermo-mechanically-coupled theory is also able to capture the resulting inhomogeneous deformations associated with the nucleation and propagation of transformation fronts. Finally, an isotropic constitutive model has also been developed and implemented in a finite-element program. This simple model provides a reasonably accurate and computationally-inexpensive tool for purposes of engineering design."
2019,Real-time path-planning using mixed-integer linear programming and global cost-to-go maps,MIT CSAIL,MIT Student,MIT Advisor,ML,0.231,"With the advance in the fields of computer science, control and optimization, it is now possible to build aerial vehicles which do not need pilots. An important capability for such autonomous vehicles is to be able to generate their own path to navigate in a constrained environment and accomplish mission objectives, such as reaching waypoints in minimal time. To account for dynamic changes in the environment, perturbations, modeling errors and modifications in the mission scenario, the trajectory needs to be continuously re-optimized online based on the latest available updates. However, to allow for high update rates, the trajectory optimization problem needs to be simple enough to be solved quickly. Optimizing for a continuous trajectory of a dynamically-constrained vehicle in the presence of obstacles is an infinite-dimension nonlinear optimal control problem. Such a problem is intractable in real-time and simplifications need to be made. In this thesis, the author presents the mechanisms used to design a path-planner with real-time and long-range capabilities. The approach relies on converting the optimal control problem into a parameter optimization one whose horizon can be reduced by using a global cost-to-go function to provide an approximate cost for the tail of the trajectory."
2019,Attitude control via structural vibration : an application of compliant robotics,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2232,"We review and present techniques for effecting and controlling the reorientation of structures ""floating"" in angular-momentum-conserving environments, applicable to both space robotics and small satellite attitude control. Conventional orientation control methods require either the usage of continuously rotating structures (e.g. momentum wheels) or the jettisoning of system mass (e.g. hydrazine thrusters). However, the systems proposed herein require neither rotating structures nor mass ejection; instead, orientation is controlled by the imposition of a bounded cyclic shape change-the canonical example of such a system is a cat righting herself while falling, thereby always landing on her feet-coupled with the conservation of angular momentum, which acts analogously to a nonholonomic constraint on the system dynamics. Further, by considering the reduced system dynamics, we extend the concept to consider the class of structures where the requisite cyclic shape change is attainable via dynamical effects, such as the normal modes of structural vibration for structures with finite stiffness. This is the central novel result of this thesis and has implications for the design of space structures where the attitude control hardware is integrated directly into the preexisting structure, the development of orientation control techniques for soft robots in space and underwater, and the design of MEMS attitude control actuators for very tiny satellites. We apply mathematical tools drawn from differential geometry and geometric mechanics, which can be intimidating but which provide a comprehensive and powerful framework for understanding a wide range of locomotion problems fundamental to robotics and control theory. These tools allow us to make succinct statements regarding gait design, controllability, and optimality that would be otherwise inaccessible."
2019,Dielectric elastomer actuators for binary robotics and mechatronics,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2498,"Future robotics and mechatronics applications will require systems that are simple, robust, lightweight and inexpensive. A suggested solution for future systems is binary actuation. Binary actuation is the mechanical analogy to digital electronics, where actuators ""flip"" between two discrete states. Systems can be simple since low-level feedback control, sensors, wiring and electronics are virtually eliminated. However, conventional actuators, such as DC motors and gearbox are not appropriate for binary robotics because they are complex, heavy, and expensive. This thesis proposes a new actuation technology for binary robotics and mechatronics based on dielectric elastomer (DE) technology. DE actuators are a novel class of polymer actuators that have shown promising low-cost performance. These actuators were not well understood and, as a result, faced major reliability problems. Fundamental studies conducted in this thesis reveal that reliable, high performance DE actuation based on highly viscoelastic polymers can be obtained at high deformation rates, when used under fast, intermittent motion."
2019,A highly configurable software bug tracking system,MIT CSAIL,MIT Student,MIT Advisor,Security,0.322,"Thesis (S.B. and M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999."
2019,The second skin approach : skin strain field analysis and mechanical counter pressure prototyping for advanced spacesuit design,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2656,"The primary aim of this thesis is to advance the theory of advanced locomotion mechanical counter pressure (MCP) spacesuits by studying the changes in the human body shape during joint motion. Two experiments take advantage of three-dimensional laser scan technology to measure the shape changes of the human body. The first experiment is an analysis of the surface area and volume of the thigh, knee, calf, and entire leg during knee flexion. The second experiment is an analysis of the full-field strain on the skin surface of the leg during knee flexion. A repeatable and quantitative technique for mapping the leg skin strain field is developed. The results of the algorithm indicate the magnitude of strain over the entire surface of the leg, as well as the direction of minimum leg skin stretching during knee flexion. For 88% of the leg surface, knee flexion causes skin strain between -0.3 and 0.3 (less than 30% contraction or extension). However, just below the patella, longitudinal strain is as high as 0.7, and at the knee hollow, it is as low as -0.6. Circumferential strain values are as high as 1.0 and 0.5 just below the patella and over the calf muscle, respectively, and along the anterior surface of the lower leg, they are as low as -0.7. The leg area, volume, and skin strain results lead to quantitative design requirements for highly mobile second skin spacesuits, and they inspire two prototype MCP leg sleeves: a hybrid urethane-foam bladder garment and a skintight nylon fiber lines of non-extension garment. These two prototypes are constructed and tested for mobility and skin surface pressure. Pressurization of the hybrid foam prototype inhibits leg mobility."
2019,Development of high fidelity methods for 3D Monte Carlo transient analysis of nuclear reactors,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2767,"Monte Carlo is increasingly being used to perform high-fidelity, steady-state neutronics analysis of power reactor geometries on today's leadership class supercomputers. Extending Monte Carlo to time dependent problems has proven to be a formidable challenge due to the significant computational resource and data processing requirements. In this thesis, a transient methodology is proposed and implemented to enable accurate and computationally tractable time dependent Monte Carlo analysis. The frequency transform method has been described and implemented in Monte Carlo for the first time. The attractiveness of this method lies in its ability to accurately capture the space and time dependent distribution of the delayed neutron source throughout a transient. Nuances to the algorithmic implementation are described and validated through a series of simple analytical test problems. Comparison with the adiabatic method currently employed for Monte Carlo transient analysis shows significant improvement in the spatial distribution and magnitude of the power for a negative reactivity insertion transient in the 2D and 3D C5G7 geometry. To aid in understanding the effect of statistical uncertainty in the tallied quantities on the time dependent flux solution, a simplified point kinetics model was developed and used for insightful analysis on simple transient test problems. This revealed how the time dependent flux profiles for a series of independent trials can be approximated by a normal distribution at low uncertainties in the tallied reactivity, but deviates from a normal distribution at relatively modest uncertainties in reactivity. Given the compuational constraints of solving large problems, having a simple model that can provide insight on the expected behavior and flux distribution can be very valuable. The frequency transform methodology belongs to a class of indirect space-time factorization methods that perform high-order calculations (e.g. Monte Carlo) over long time steps and low-order, computationally-efficient calculations (e.g. Point Kinetics) over short time steps as an approach to balance performance and accuracy. The coarse mesh finite difference (CMFD) diffusion operator is employed as the low-order solver in Monte Carlo transient analysis for the first time. The CMFD diffusion operator is attractive due to its potential to increase the time step size between the computationally expensive high-order solves. Implementing this methodology is important as continuous energy Monte Carlo is reactor-agnostic and able to treat complex geometries without difficulty, opening up the possibility of solving transients on new experimental geometries for which there is little data."
2019,Algorithms for Subset Sum using linear sketching,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2595,"Given n positive integers, the Modular Subset Sum problem asks if a subset adds up to a given target t modulo a given integer m. This is a natural generalization of the Subset Sum problem (where m = + [infinity symbol]) with ties to additive combinatorics and cryptography. The non-modular case was long known to be NP-complete but to admit pseudo-polynomial time algorithms and, recently, algorithms running in near-linear pseudo-polynomial time were developed [9, 211. For the modular case, however, the best known algorithm by Koiliaris and Xu [21] runs in time 0̃ (m⁵/⁴). In this thesis we tackle this problem by devising a faster algorithm for the Modular Subset Sum problem, running in 0̃(m) randomized time, which matches a recent conditional lower bound of [1] based on the Strong Exponential Time Hypothesis. Interestingly, in contrast to most previous results on Subset Sum, our algorithm does not use the Fast Fourier Transform. Instead, it is able to simulate the ""textbook"" Dynamic Programming algorithm much faster, using ideas from linear sketching. This is one of the first applications of sketching-based techniques to obtain fast algorithms for exact combinatorial problems in an offline setting."
2019,Alterations in cardiovascular regulation and function assessed using cardiovascular system identification,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3211,"Thesis (S.B. and M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2000."
2019,Delivering real-time holographic video content with off-the-shelf PC hardware,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2748,"We present a PC based system to simultaneously compute real-time holographic video content and to serve as a framebuffer to drive a holographic video display. Our system uses only 3 PCs each equipped with an nVidia Quadro FX 3000G video card. It replaces a SGI Onyx and the custom built Cheops Image Processing System that previously served as the platform driving the MIT second-generation Holovideo display. With a prototype content generation implementation, we compute holographic stereograms and update the display at a rate of roughly 2 frames per second."
2019,Sequence-structure correlations in the MaSp1 protein of N. clavipes dragline silk,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2554,"Silk is a hierarchically structured protein fiber with exceptional tensile strength and extensibility, making it one of the toughest and most versatile biocompatible materials. While experimental studies have shown that the molecular structure of silk has a direct influence on the stiffness, toughness, and failure strength of silk, few molecular-level analyses of the nanostructure of silk assemblies, in particular under variations of genetic sequences, have been published. Here, atomistic-level structures of wildtype as well as modified MaSp1 protein from the N. clavipes spider dragline silk sequences are reported, obtained using an in silico approach based on replica exchange molecular dynamics (REMD) and explicit water molecular dynamics. In particular, the atomistic simulations discussed in this parametric study explore the effects of the poly-alanine length of the N. clavipes MaSpi peptide sequence, solvent conditions, and nanomechanical loading conditions on secondary and tertiary structure predictions as well as the nanomechanical behavior of a unit cell of 15 strands with 900-1000 total residues used to represent a cross-linking 7-sheet crystal node in the network within a fibril of the dragline silk thread. Understanding the behavior of this node at the molecular scale is critical for potentially bypassing strength limits at this length scale and vastly improving silk for medical and textile purposes as well as synthetic elastomers and polymer or aramid fiber composites with a similar molecular structure and noncovalent bonding for aerospace, armor, and medical applications. The main hypothesis tested is that there exists a critical minimum length of the poly-alanine repeat that ensures the formation of a robust cross-linking the [beta]-sheet crystal. Confirming earlier experimental and computational work, a structural analysis reveals that poly-alanine regions in silk predominantly form distinct and orderly [beta]-sheet crystal domains while disorderly regions are formed by glycine-rich repeats that consist of 310-helix type structures and 7-turns. These predictions are directly validated against experimental data based on dihedral angle pair calculations presented in Ramachandran plots combined with an analysis of the secondary structure content. The key results of this study are: e A strong dependence of the resulting silk nanostructure on the poly-alanine length. The wildtype poly-alanine repeat length of six residues defines a critical minimum length that consistently results in clearly defined [beta]-sheet nanocrystals allowing for misalignment. For poly-alanine lengths below six residues, the /-sheet nanocrystals are not well-defined or not visible at all, while for poly-alanine lengths above six the characteristic nanocomposite structure of silk emerges with no significant improvement of the quality of the sheet nanocrystal geometry. A simple biophysical model is presented that explains the minimum length scale based on the mechanistic insight gained from the molecular simulations. The efficient stacking of the [beta]-sheets of a well-defined crystal reinforces local hydrophobicity and prevents water diffusion into a crystal above a critical size. Nanomechanical testing reveals that the combination of the 12-alanine length case and central pull-out loading conditions results in delayed failure by employing a hierarchy of strong [beta]-sheets and soft, extensible semi-amorpous regions to overcome a predicted H-bond saturation. This work constitutes the most comprehensive study to-date of the molecular structure prediction and nanomechanical behavior of dragline silk. Building upon previous computational studies that used similar methods for structure prediction and mechanical analysis, e.g. REMD and force-control loading, this work presents: the first results of the near-native structures determined by REMD after equilibration in TIP3P explicit solvent, the first parametric study of the effects of modifying the wildtype poly-alanine segment length to values outside the range naturally observed for MaSp1 on structure prediction and nanomechanical behavior, and, the first comparison between previously published loading conditions, i.e. the Stretch test, and the novel Pull-out loading conditions that are hypothesized to be more appropriate for modeling of the in situ loading of the cross-linking [beta]-sheet crystal. Further parametric studies in peptide sequence to optimize bulk fiber properties must involve changes in simulated nanomechanical loading conditions to properly assess the effects of the changes in peptide sequence. These findings set the stage for understanding how variations in the spidroin sequence can be used to engineer the structure and thereby functional properties of this biological superfiber, and present a design strategy for the genetic optimization of spidroins for enhanced mechanical properties. The approach used here may also find application in the design of other self-assembled molecular structures and fibers and in particular biologically inspired or completely synthetic systems."
2019,VLSI microdisplays and optoelectronic technology,MIT CSAIL,MIT Student,MIT Advisor,Security,0.324,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2019,Tenant-level network performance isolation in Flowtune,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2782,"Performance isolation is a major concern for multi-tenant datacenters. Many service level agreements include a specification on the allotment of resources. For tenants, these resource guarantees are critical to the availability and efficiency of their services. While CPU, disk, and memory isolation are well-understood, network performance isolation is less straightforward. In this thesis, I investigate methods for enforcing bandwidth fairness guarantees for logical networks in a datacenter and implement network performance isolation in Flowtune. Flowtune is a datacenter network architecture which introduces a centralized arbiter to enforce congestion control at the flowlet level. Flowtune achieves rapid convergence to a desired allocation of network resources in addition to reducing tail latencies in various settings. However, Flowtune currently does not provide tenant-level network performance isolation."
2019,Neuromuscular modularity and behavioral correlates of motor control,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2431,"I studied organizational principles that may subserve the control and learning of forelimb movements. Among these principles, I focused on muscular coordination patterns, motor cortical excitability, and sensorimotor interactions. I found that muscle activity in grasping and reaching behaviors could be reconstructed by linear combinations of a small number of time-varying muscle synergies, each fit with coefficients unique to the behavior. However, the generalization of these synergies between behavioral conditions was limited, in part by the sensitivity of the extraction algorithm to stereotyped muscular relations within contrasted conditions. In reaching studies designed to assist or resist different movement directions, I found a gradual change in the structure, as well as recruitment, of synergies. When a perturbation was targeted to the activity within a single muscle, I found a transient, relative suppression of this muscle in response to descending motor commands. In other motor cortical microstimulation experiments, I confirmed that long-train microstimulation is able to evoke complex, convergent movements. Even during highly-trained reaching movements, I found that there was relatively little invariance of the muscular patterns in relation to kinematic variables coding for the hand's displacement and velocity."
2019,Iterative decoding of codes defined on graphs,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3198,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1998."
2019,Automated detection of frequency specific fluctuations in ECG morphology,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3194,"Thesis (B.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1991."
2019,Early word learning through communicative inference,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2943,"How do children learn their first words? Do they do it by gradually accumulating information about the co-occurrence of words and their referents over time, or are words learned via quick social inferences linking what speakers are looking at, pointing to, and talking about? Both of these conceptions of early word learning are supported by empirical data. This thesis presents a computational and theoretical framework for unifying these two different ideas by suggesting that early word learning can best be described as a process of joint inferences about speakers' referential intentions and the meanings of words. Chapter 1 describes previous empirical and computational research on ""statistical learning""--the ability of learners to use distributional patterns in their language input to learn about the elements and structure of language-and argues that capturing this abifity requires models of learning that describe inferences over structured representations, not just simple statistics. Chapter 2 argues that social signals of speakers' intentions, even eye-gaze and pointing, are at best noisy markers of reference and that in order to take advantage of these signals fully, learners must integrate information across time. Chapter 3 describes the kinds of inferences that learners can make by assuming that speakers are informative with respect to their intended meaning, introducing and testing a formalization of how Grice's pragmatic maxims can be used for word learning. Chapter 4 presents a model of cross-situational intentional word learning that both learns words and infers speakers' referential intentions from labeled corpus data."
2019,Integrating statistical and knowledge-based methods for automatic phonemic segmentation,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3104,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999."
2019,A frequency synthesizer for a 2.4 GHz spread spectrum communications application,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3176,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1997."
2019,Extracting parallelism from sequential programs,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3155,"Thesis (M.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1988."
2019,Tradeoffs of the use of SiGe buffer layers in tandem GaAsP/Si solar cells,MIT CSAIL,MIT Student,MIT Advisor,Security,0.1859,"III-V multi-junction solar cells currently have the highest reported theoretical and experimental energy conversion efficiency but their cost, mainly attributed to the use of expensive substrates, limits their widespread use for terrestrial applications. Successful integration of III--V's on a Si substrate to enable a III-V/Si tandem cell can lower the cost of energy by combining the high-efficiency of the III--V materials with the low-cost and abundance of the Si substrate. A maximum theoretical efficiency of 44.8% from a tandem cell on Si can be achieved by using a GaAsP (Eg=1.7 eV) as the top cell. Out of several possible integration routes, the use of a linearly graded SiGe buffer as interfacial layer between the two cells potentially yields the highest quality for the epitaxial GaAsP layer, an essential requirement for realization of high-efficiency solar cells. In this thesis, the impact of the SiGe buffer layer on the optical and electrical characteristics of the bottom Si cell of a GaAsP/Si tandem solar cell was assessed via experimental work. The growth of a SiGe buffer layer was shown to increase the threading dislocation density and as a result the leakage current of the bottom Si cell by about 10x. In addition, the low-bandgap SiGe absorbs more than 80% of the light that is intended for the Si sub-cell, reducing the short-circuit current of the Si cell from 33 mA/cm² to only 6 mA/cm². By using a step-cell design, in which the SiGe was partially etched to allow more light to reach the bottom cell, the current was increased to 20 mA/cm². To quantify the merits of the studied approach as well as evaluate other approaches, we have carried out a theoretical study of absorbed irradiance in a Si single-junction cell, a bonded GaAsP/Si tandem cell, a GaAsP/SiGe/Si tandem cell as well as the step-cell design. The GaAsP/Si bonded tandem cell showed 24% relative improvement in light absorption over a single-junction Si cell. The addition of a SiGe graded buffer was shown to reduce the total absorption by 25%, bringing the efficiency of GaAsP/SiGe/Si tandem cell under that of the Si single-junction cell. The step-cell design, even though successful in increasing light absorption, was not found effective in achieving a higher absorbed power density than that of the Si cell. These results suggest that any future work on integrating GaAsP cells on Si towards a high-performance tandem cell should be focused on using a higher-bandgap material as a graded buffer or using a wafer bonding technique."
2019,Making machines that make : object-oriented hardware meets object-oriented software,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2935,"Rapid prototyping has been in the limelight for the past decade. 3D printers have an evocative name that promises production of complex parts on demand. Yet current practice doesn't quite deliver on these promises of advanced manufacturing. Existing digital fabrication tools enable repeatability and precision by using codes to describe machine actions. But the infrastructure used for digital fabrication machines is difficult to extend, modify, and customize. It is very difficult for the end-user to incorporate more forms of control into the workflow. Machine design today is largely the same as it was 50 years ago, despite decades of progress in other fields such as computer science and network engineering. I argue that we need to transition from rapid prototyping to rapid prototyping of rapid prototyping. To make diverse goods, we need diverse tools. To develop diversity in digital fabrication tools, we need reconfigurable and extensible infrastructure for machine building. Using insights from object-oriented programming, end-to-end principles in network design, and the open system interconnection model, I propose a new paradigm for machine building called object-oriented hardware. In this paradigm, software objects and hardware objects are peers that have procedures, methods, ports, and presentations. Machine building modules are available as software libraries are to programmers. A machine instantiation is an assembly of objects situated in a particular context. Using this approach, a thing together with the machine that makes it becomes an application. This method transcends the additive versus subtractive manufacturing comparisons by considering both types of rapid automation. Development work is divided into infrastructural engineering, which develop modules for use in any machine, and application development, which develop specific machine instantiations. Here I present technical implementations of machine building infrastructure first. These include distributed networked controls, reconfigurable software interfaces, and modular mechanical machine components. Then I present machine instantiations that use this infrastructure to demonstrate its capability. Finally to evaluate the object-oriented hardware paradigm in the wild, I observe machine building novices using these tools in both a workshop format and in the Fab Lab network for machine building. To make the modular components for machine building accessible in this context, I developed an extensible toolkit for machine building-the Cardboard Machine Kit. Using this toolkit, novices were able to make a wide range of machines, demonstrating the power of this method."
2019,Fast interceptor of a dynamic object,MIT CSAIL,MIT Student,MIT Advisor,ML,0.248,"This thesis presents a path planning and control strategy that enables an unmanned non-holonomic vehicle to intercept a fast moving object. The path planning is performed under model uncertainty, with respect to the vehicle's maneuverability, as well as uncertainty in the estimation of the object's future trajectory and position. This problem involves the tracking of the dynamic object in a cluttered environment and the accurate estimation of its future position in the presence of noisy measurements. The ground vehicle (interceptor) is required to intercept the dynamic object at a predicted (catch) location in a finite amount of time. This time restriction presents quite a challenge given the inherent limitation in the vehicle's steering and maneuverability. The solution strategy is divided into three sub-problems: 1) prediction, 2) path planning and 3) control. The prediction of the parameters that describe the dynamic's object in space is accomplished via Kalman Filtering which, in conjunction with an impact predictor, provide the waypoints needed to construct a reference path that will place the interceptor on a collision course with the dynamic object (target.) A pure pursuit algorithm was used to steer the interceptor along a reference trajectory, which was designed to make the vehicle engage the dynamic object on a near tail-on aspect. In the endgame, the pure pursuit algorithm was modified to ensure arrival to the catch point while a position controller was added to ensure timely arrival to the predicted catch location. The problem statement was then augmented to include obstacle avoidance."
2019,Teaching computer science principles using StarLogoTNG,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3264,"This thesis outlines the development of a 3-module set of lesson plans implemented using StarLogoTNG. The purpose of these lesson plans are to serve as a vehicle for teaching and reinforcing specific learning objectives of the CollegeBoard's Advanced Placement Computer Science Principles course, which has 7 main themes. Each lesson plan has as its focus a subset of learning objectives from one of the themes of Creativity, Data, or Internet, while simultaneously incorporating additional learning goals from the themes of Abstraction, Programming, Algorithms, and Impact. These interactive lesson plans go beyond the use of StarLogoTNG to complete specific tasks by integrating meaningful class discussions and occasional peer instruction and peer review activities. Such activities become catalysts for students to develop a deeper understanding of the course materials. By connecting learning goals from different themes of the course and packaging them in cohesive lesson plans that utilize methods of teaching for understanding, this thesis aims to provide a useful and effective set of a materials for the instruction of computer science principles."
2019,Experimental and computational studies of electric thruster plasma radiation emission,MIT CSAIL,MIT Student,MIT Advisor,Security,0.243,"(cont.) Second, spectral measurements of a TAL type laboratory mini-Hall thruster, MHT-9, were presented. Third, radiation emission measurements of an experimental Helicon plasma source being studied to assess the possibility of using Helicon discharge as a propulsive system are presented and the trends are discussed. Two collisional-radiative (C-R) models are developed for Argon and Xenon plasmas to analyze the experimental spectra. In the C-R models, electron induced excitation, deexcitation and ionization collisions, and spontaneous radiative de-excitation transitions are simulated for neutral and singly charged ion species. The models are validated against measured spectra obtained using different experimental setups. The BHT-200 Hall thruster has insulator ceramic annular walls made of Boron-Nitride (BN). Erosion of ceramic walls is one of the major life limiting factors for Hall thrusters. Emission spectroscopy is used as a means to determine the trends in the thruster wall erosion rate by measuring the radiation emission of the Boron neutral 249.68nm and 249.77nm lines. Discussion about the spectral measurements and relevant analysis are presented."
2019,Design of the control logic for StarT-Voyager,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3091,"Thesis (M.Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1998."
2019,Modeling and estimation of space-time stochastic processes.,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3273,Thesis. 1977. Ph.D.--Massachusetts Institute of Technology. Dept. of Electrical Engineering and Computer Science.
2019,An autonomous forklift research platform for warehouse operations,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2294,"Autonomous vehicle technology has seen transformative change over the past decade, enabling products such as autonomous automobiles. In the field of warehouse automation, autonomous vehicles have a long history, with automatic guided vehicles (AGVs) existing since the 1950s. Early vehicles were inflexible and relied on costly infrastructure. However, advances in technology have enabled a much greater level of sophistication. Yet, currently only 16% of companies operating warehouses make use of AGVs. Additionally, modern AGVs available today, while quite sophisticated, are still relatively inflexible and costly. In this project, we develop a prototype forklift AGV research platform capable of operation in an indoor warehouse environment. The aim of the project is to provide researchers with a vehicle platform with which to experiment with advanced autonomy and push the boundaries of AGV capability. The vehicle is a fully functional 3-wheel counterbalance fork truck with 4000lb load capacity. The vehicle is equipped with cameras, laser scanners, an IMU and a powerful onboard computer. A reference software implementation is also developed and tested, which allows a base level of full autonomy, enabling the vehicle to perform autonomous pick and place tasks. In it's role as a research platform, it is anticipated that the vehicle will enable investigation into research areas such as fully autonomous operation using inexpensive sensors, manipulation of overhead loads, operation in unstructured cluttered environments, and operation in collaboration with human operators."
2019,Temporal texture modeling,MIT CSAIL,MIT Student,MIT Advisor,Security,0.318,"Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995."
2019,Learning by learning to communicate,MIT CSAIL,MIT Student,MIT Advisor,Security,0.3066,"Human intelligence is a product of cooperation among many different specialists. Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the ""high-level"" agile cooperation that permeates our daily lives. I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of communication bootstrapping, in which shared experiences form a basis for agreement on a system of signals. In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence. I then demonstrate progress along the course of investigation laid out in my roadmap: * I establish a measure of developmental cost that allows me to eliminate many possible designs * I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using failure simplification. * I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system. * I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping."
2019,Micro-optic elements for a compact opto-electronic integrated neural coprocessor,MIT CSAIL,MIT Student,MIT Advisor,Security,0.2724,"The research done for this thesis was aimed at developing the optical elements needed for the Compact Opto-electronic Integrated Neural coprocessor (COIN coprocessor) project. The COIN coprocessor is an implementation of a feed forward neural network using free-space optical interconnects to communicate between neurons. Prior work on this project had assumed these interconnects would be formed using Holographic Optical Elements (HOEs), so early work for this thesis was directed along these lines. Important limits to the use of HOEs in the COIN system were identified and evaluated. In particular, the problem of changing wavelength between the hologram recording and readout steps was examined and it was shown that there is no general solution to this problem when the hologram to be recorded is constructed with more than two plane waves interfering with each other. Two experimental techniques, the holographic bead lens and holographic liftoff, were developed as partial workarounds to the identified limitations. As an alternative to HOEs, an optical element based on the concept of the Fresnel Zone Plate was developed and experimentally tested. The zone plate based elements offer an easily scalable method for fabricating the COIN optical interconnects using standard lithographic processes and appear to be the best choice for the COIN coprocessor project at this time. In addition to the development of the optical elements for the COIN coprocessor, this thesis also looks at the impact of optical element efficiency on the power consumption of the COIN coprocessor. Finally, a model of the COIN network based on the current COIN design was used to compare the performance and cost of the COIN system with competing implementations of neural networks, with the conclusion that at this time the proposed COIN coprocessor system is still a competitive option for neural network implementations."
2019,Model-based compressive sensing with Earth Mover's Distance constraints,MIT CSAIL,MIT Student,MIT Advisor,ML,0.2841,"In compressive sensing, we want to recover ... from linear measurements of the form ... describes the measurement process. Standard results in compressive sensing show that it is possible to exactly recover the signal x from only m ... measurements for certain types of matrices. Model-based compressive sensing reduces the number of measurements even further by limiting the supports of x to a subset of the ... possible supports. Such a family of supports is called a structured sparsity model. In this thesis, we introduce a structured sparsity model for two-dimensional signals that have similar support in neighboring columns. We quantify the change in support between neighboring columns with the Earth Mover's Distance (EMD), which measures both how many elements of the support change and how far the supported elements move. We prove that for a reasonable limit on the EMD between adjacent columns, we can recover signals in our model from only ... measurements, where w is the width of the signal. This is an asymptotic improvement over the ... bound in standard compressive sensing. While developing the algorithmic tools for our proposed structured sparsity model, we also extend the model-based compressed sensing framework. In order to use a structured sparsity model in compressive sensing, we need a model projection algorithm that, given an arbitrary signal x, returns the best approximation in the model. We relax this constraint and develop a variant of IHT, an existing sparse recovery algorithm, that works with approximate model projection algorithms."
